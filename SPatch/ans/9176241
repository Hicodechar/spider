
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux VM workaround for Knights Landing A/D leak - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux VM workaround for Knights Landing A/D leak</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 14, 2016, 3:58 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1465919919-2093-1-git-send-email-lukasz.anaczkowski@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9176241/mbox/"
   >mbox</a>
|
   <a href="/patch/9176241/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9176241/">/patch/9176241/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	CE42F6075D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jun 2016 15:59:26 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BF95326A7B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jun 2016 15:59:26 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B2BCB27DF9; Tue, 14 Jun 2016 15:59:26 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0B0F726A7B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jun 2016 15:59:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752407AbcFNP7I (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 14 Jun 2016 11:59:08 -0400
Received: from mga04.intel.com ([192.55.52.120]:62585 &quot;EHLO mga04.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752166AbcFNP7E (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 14 Jun 2016 11:59:04 -0400
Received: from orsmga002.jf.intel.com ([10.7.209.21])
	by fmsmga104.fm.intel.com with ESMTP; 14 Jun 2016 08:59:03 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.26,471,1459839600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;997451927&quot;
Received: from gklab-125-033.igk.intel.com ([10.91.125.33])
	by orsmga002.jf.intel.com with ESMTP; 14 Jun 2016 08:59:00 -0700
From: Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	tglx@linutronix.de, mingo@redhat.com, dave.hansen@linux.intel.com,
	ak@linux.intel.com, kirill.shutemov@linux.intel.com,
	mhocko@suse.com, akpm@linux-foundation.org, hpa@zytor.com
Cc: lukasz.anaczkowski@intel.com, harish.srinivasappa@intel.com,
	lukasz.odzioba@intel.com
Subject: [PATCH] Linux VM workaround for Knights Landing A/D leak
Date: Tue, 14 Jun 2016 17:58:39 +0200
Message-Id: &lt;1465919919-2093-1-git-send-email-lukasz.anaczkowski@intel.com&gt;
X-Mailer: git-send-email 1.8.3.1
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a> - June 14, 2016, 3:58 p.m.</div>
<pre class="content">
<span class="from">From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>

Knights Landing has a issue that a thread setting A or D bits
may not do so atomically against checking the present bit.
A thread which is going to page fault may still set those
bits, even though the present bit was already atomically cleared.

This implies that when the kernel clears present atomically,
some time later the supposed to be zero entry could be corrupted
with stray A or D bits.

Since the PTE could be already used for storing a swap index,
or a NUMA migration index, this cannot be tolerated. Most
of the time the kernel detects the problem, but in some
rare cases it may not.

This patch enforces that the page unmap path in vmscan/direct reclaim
always flushes other CPUs after clearing each page, and also
clears the PTE again after the flush.

For reclaim this brings the performance back to before Mel&#39;s
flushing changes, but for unmap it disables batching.

This makes sure any leaked A/D bits are immediately cleared before the entry
is used for something else.

Any parallel faults that check for entry is zero may loop,
but they should eventually recover after the entry is written.

Also other users may spin in the page table lock until we
&quot;fixed&quot; the PTE. This is ensured by always taking the page table lock
even for the swap cache case. Previously this was only done
on architectures with non atomic PTE accesses (such as 32bit PTE),
but now it is also done when this bug workaround is active.

I audited apply_pte_range and other users of arch_enter_lazy...
and they seem to all not clear the present bit.

Right now the extra flush is done in the low level
architecture code, while the higher level code still
does batched TLB flush. This means there is always one extra
unnecessary TLB flush now. As a followon optimization
this could be avoided by telling the callers that
the flush already happenend.
<span class="signed-off-by">
Signed-off-by: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
---
 arch/x86/include/asm/cpufeatures.h |  1 +
 arch/x86/include/asm/hugetlb.h     |  9 ++++++++-
 arch/x86/include/asm/pgtable.h     |  5 +++++
 arch/x86/include/asm/pgtable_64.h  |  6 ++++++
 arch/x86/kernel/cpu/intel.c        | 10 ++++++++++
 arch/x86/mm/tlb.c                  | 20 ++++++++++++++++++++
 include/linux/mm.h                 |  4 ++++
 mm/memory.c                        |  3 ++-
 8 files changed, 56 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - June 14, 2016, 4:31 p.m.</div>
<pre class="content">
Hi,

[auto build test ERROR on v4.7-rc3]
[also build test ERROR on next-20160614]
[cannot apply to tip/x86/core]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Lukasz-Anaczkowski/Linux-VM-workaround-for-Knights-Landing-A-D-leak/20160615-000610
config: i386-alldefconfig (attached as .config)
compiler: gcc-6 (Debian 6.1.1-1) 6.1.1 20160430
reproduce:
        # save the attached .config to linux build tree
        make ARCH=i386 

All errors (new ones prefixed by &gt;&gt;):

   mm/built-in.o: In function `unmap_page_range&#39;:
   (.text+0x1e9e8): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o: In function `change_protection_range&#39;:
<span class="quote">&gt;&gt; mprotect.c:(.text+0x25578): undefined reference to `fix_pte_leak&#39;</span>
   mm/built-in.o: In function `move_page_tables&#39;:
   (.text+0x25d81): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o: In function `vunmap_page_range&#39;:
   vmalloc.c:(.text+0x28419): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o: In function `ptep_clear_flush&#39;:
   (.text+0x2a7b3): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o:madvise.c:(.text+0x2b4b0): more undefined references to `fix_pte_leak&#39; follow

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 14, 2016, 4:47 p.m.</div>
<pre class="content">
Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:
<span class="quote">
&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt; +{</span>
Here there should be a call to smp_mb__after_atomic() to synchronize with
switch_mm. I submitted a similar patch, which is still pending (hint).
<span class="quote">
&gt; +	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt; +		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt; +		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt; +				 addr + PAGE_SIZE);</span>
<span class="quote">&gt; +		mb();</span>
<span class="quote">&gt; +		set_pte(ptep, __pte(0));</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>

Regards,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a> - June 14, 2016, 4:54 p.m.</div>
<pre class="content">
<span class="from">From: Nadav Amit [mailto:nadav.amit@gmail.com] </span>
Sent: Tuesday, June 14, 2016 6:48 PM
<span class="quote">
&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">
&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>

Thanks, Nadav!
I&#39;ll add this and re-submit the patch.

Cheers,
Lukasz
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - June 14, 2016, 4:58 p.m.</div>
<pre class="content">
Hi,

[auto build test ERROR on v4.7-rc3]
[also build test ERROR on next-20160614]
[cannot apply to tip/x86/core]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Lukasz-Anaczkowski/Linux-VM-workaround-for-Knights-Landing-A-D-leak/20160615-000610
config: i386-randconfig-s0-201624 (attached as .config)
compiler: gcc-6 (Debian 6.1.1-1) 6.1.1 20160430
reproduce:
        # save the attached .config to linux build tree
        make ARCH=i386 

All errors (new ones prefixed by &gt;&gt;):

   arch/x86/built-in.o: In function `__ptep_modify_prot_start&#39;:
<span class="quote">&gt;&gt; paravirt.c:(.text+0x346f4): undefined reference to `fix_pte_leak&#39;</span>
   mm/built-in.o: In function `unmap_page_range&#39;:
<span class="quote">&gt;&gt; (.text+0x1dd4c): undefined reference to `fix_pte_leak&#39;</span>
   mm/built-in.o: In function `move_page_tables&#39;:
   (.text+0x274f2): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o: In function `vunmap_page_range&#39;:
<span class="quote">&gt;&gt; vmalloc.c:(.text+0x2aa78): undefined reference to `fix_pte_leak&#39;</span>
   mm/built-in.o: In function `ptep_clear_flush&#39;:
   (.text+0x2cfa7): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o:(.text+0x30dff): more undefined references to `fix_pte_leak&#39; follow

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - June 14, 2016, 5:18 p.m.</div>
<pre class="content">
On 06/14/2016 09:47 AM, Nadav Amit wrote:
<span class="quote">&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; &gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt; &gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt; &gt; +{</span>
<span class="quote">&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; &gt; +	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt; &gt; +		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt; &gt; +		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt; &gt; +				 addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt; &gt; +		mb();</span>
<span class="quote">&gt;&gt; &gt; +		set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt; &gt; +	}</span>
<span class="quote">&gt;&gt; &gt; +}</span>

Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and
not every single caller (like this code is)?

It is insane to require individual TLB flushers to be concerned with the
barriers.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - June 14, 2016, 5:19 p.m.</div>
<pre class="content">
...
<span class="quote">&gt; +extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt; +			 pte_t *ptep);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,</span>
<span class="quote">&gt;  					    unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="quote">&gt; +		fix_pte_leak(mm, addr, ptep);</span>
<span class="quote">&gt; +	return pte;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="quote">&gt; index 1a27396..9769355 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable.h</span>
<span class="quote">&gt; @@ -794,11 +794,16 @@ extern int ptep_test_and_clear_young(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  extern int ptep_clear_flush_young(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				  unsigned long address, pte_t *ptep);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt; +			 pte_t *ptep);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define __HAVE_ARCH_PTEP_GET_AND_CLEAR</span>
<span class="quote">&gt;  static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  				       pte_t *ptep)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pte_t pte = native_ptep_get_and_clear(ptep);</span>
<span class="quote">&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="quote">&gt; +		fix_pte_leak(mm, addr, ptep);</span>
<span class="quote">&gt;  	pte_update(mm, addr, ptep);</span>
<span class="quote">&gt;  	return pte;</span>
<span class="quote">&gt;  }</span>

Doesn&#39;t hugetlb.h somehow #include pgtable.h?  So why double-define
fix_pte_leak()?
<span class="quote">
&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; index 2ee7811..6fa4079 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; @@ -178,6 +178,12 @@ extern void cleanup_highmap(void);</span>
<span class="quote">&gt;  extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;  extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define ARCH_HAS_NEEDS_SWAP_PTL 1</span>
<span class="quote">&gt; +static inline bool arch_needs_swap_ptl(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return boot_cpu_has_bug(X86_BUG_PTE_LEAK);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #endif /* !__ASSEMBLY__ */</span>

I think we need a comment on this sucker.  I&#39;m not sure we should lean
solely on the commit message to record why we need this until the end of
time.

Or, refer over to fix_pte_leak() for a full description of what is going on.
<span class="quote">
&gt;  #endif /* _ASM_X86_PGTABLE_64_H */</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="quote">&gt; index 6e2ffbe..f499513 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/cpu/intel.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="quote">&gt; @@ -181,6 +181,16 @@ static void early_init_intel(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (c-&gt;x86_model == 87) {</span>
<span class="quote">&gt; +		static bool printed;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!printed) {</span>
<span class="quote">&gt; +			pr_info(&quot;Enabling PTE leaking workaround\n&quot;);</span>
<span class="quote">&gt; +			printed = true;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		set_cpu_bug(c, X86_BUG_PTE_LEAK);</span>
<span class="quote">&gt; +	}</span>

Please use the macros in here for the model id:
<span class="quote">
&gt; http://git.kernel.org/cgit/linux/kernel/git/tip/tip.git/tree/arch/x86/include/asm/intel-family.h</span>

We also probably want to prefix the pr_info() with something like
&quot;x86/intel:&quot;.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * Workaround for KNL issue:</span>

Please be specific about what this &quot;KNL issue&quot; *is*.  Refer to the
public documentation of the erratum, please.
<span class="quote">
&gt; + * A thread that is going to page fault due to P=0, may still</span>
<span class="quote">&gt; + * non atomically set A or D bits, which could corrupt swap entries.</span>
<span class="quote">&gt; + * Always flush the other CPUs and clear the PTE again to avoid</span>
<span class="quote">&gt; + * this leakage. We are excluded using the pagetable lock.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt; +		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt; +		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt; +				 addr + PAGE_SIZE);</span>
<span class="quote">&gt; +		mb();</span>
<span class="quote">&gt; +		set_pte(ptep, __pte(0));</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>

I think the comment here is a bit sparse.  Can we add some more details,
like:

	Entering here, the current CPU just cleared the PTE.  But,
	another thread may have raced and set the A or D bits, or be
	_about_ to set the bits.  Shooting their TLB entry down will
	ensure they see the cleared PTE and will not set A or D.

and by the set_pte():

	Clear the PTE one more time, in case the other thread set A/D
	before we sent the TLB flush.
<span class="quote">
&gt;  #endif /* CONFIG_SMP */</span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index 5df5feb..5c80fe09 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -2404,6 +2404,10 @@ static inline bool debug_guardpage_enabled(void) { return false; }</span>
<span class="quote">&gt;  static inline bool page_is_guard(struct page *page) { return false; }</span>
<span class="quote">&gt;  #endif /* CONFIG_DEBUG_PAGEALLOC */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifndef ARCH_HAS_NEEDS_SWAP_PTL</span>
<span class="quote">&gt; +static inline bool arch_needs_swap_ptl(void) { return false; }</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #if MAX_NUMNODES &gt; 1</span>
<span class="quote">&gt;  void __init setup_nr_node_ids(void);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index 15322b7..0d6ef39 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -1960,7 +1960,8 @@ static inline int pte_unmap_same(struct mm_struct *mm, pmd_t *pmd,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int same = 1;</span>
<span class="quote">&gt;  #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)</span>
<span class="quote">&gt; -	if (sizeof(pte_t) &gt; sizeof(unsigned long)) {</span>
<span class="quote">&gt; +	if (arch_needs_swap_ptl() ||</span>
<span class="quote">&gt; +	    sizeof(pte_t) &gt; sizeof(unsigned long)) {</span>
<span class="quote">&gt;  		spinlock_t *ptl = pte_lockptr(mm, pmd);</span>
<span class="quote">&gt;  		spin_lock(ptl);</span>
<span class="quote">&gt;  		same = pte_same(*page_table, orig_pte);</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - June 14, 2016, 5:47 p.m.</div>
<pre class="content">
Hi,

[auto build test ERROR on v4.7-rc3]
[also build test ERROR on next-20160614]
[cannot apply to tip/x86/core]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Lukasz-Anaczkowski/Linux-VM-workaround-for-Knights-Landing-A-D-leak/20160615-000610
config: i386-randconfig-r0-201624 (attached as .config)
compiler: gcc-6 (Debian 6.1.1-1) 6.1.1 20160430
reproduce:
        # save the attached .config to linux build tree
        make ARCH=i386 

All errors (new ones prefixed by &gt;&gt;):

   arch/x86/built-in.o: In function `__ptep_modify_prot_start&#39;:
   paravirt.c:(.text+0xc0e3c): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o: In function `zap_pte_range&#39;:
<span class="quote">&gt;&gt; memory.c:(.text+0xdb8b4): undefined reference to `fix_pte_leak&#39;</span>
   mm/built-in.o: In function `move_page_tables&#39;:
   (.text+0x1102ff): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o: In function `vunmap_page_range&#39;:
   vmalloc.c:(.text+0x125741): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o: In function `ptep_clear_flush&#39;:
   (.text+0x12e01f): undefined reference to `fix_pte_leak&#39;
   mm/built-in.o:madvise.c:(.text+0x131138): more undefined references to `fix_pte_leak&#39; follow

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 14, 2016, 8:16 p.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt; +				 addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		mb();</span>
<span class="quote">&gt;&gt;&gt;&gt; +		set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt; barriers.</span>

IMHO it is best to use existing flushing interfaces instead of creating
new ones. 

In theory, fix_pte_leak could have used flush_tlb_page. But the problem
is that flush_tlb_page requires the vm_area_struct as an argument, which
ptep_get_and_clear (and others) do not have.

I donâ€™t know which architecture needs the vm_area_struct, since x86 and
some others I looked at (e.g., ARM) only need the mm_struct.

Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - June 14, 2016, 9:37 p.m.</div>
<pre class="content">
On 06/14/2016 01:16 PM, Nadav Amit wrote:
<span class="quote">&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +				 addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +		mb();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +		set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt;&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt;&gt; barriers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IMHO it is best to use existing flushing interfaces instead of creating</span>
<span class="quote">&gt; new ones. </span>

Yeah, or make these things a _little_ harder to get wrong.  That little
snippet above isn&#39;t so crazy that we should be depending on open-coded
barriers to get it right.

Should we just add a barrier to mm_cpumask() itself?  That should stop
the race.  Or maybe we need a new primitive like:

/*
 * Call this if a full barrier has been executed since the last
 * pagetable modification operation.
 */
static int __other_cpus_need_tlb_flush(struct mm_struct *mm)
{
	/* cpumask_any_but() returns &gt;= nr_cpu_ids if no cpus set. */
	return cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt;
		nr_cpu_ids;
}


static int other_cpus_need_tlb_flush(struct mm_struct *mm)
{
	/*
	 * Synchronizes with switch_mm.  Makes sure that we do not
	 * observe a bit having been cleared in mm_cpumask() before
 	 * the other processor has seen our pagetable update.  See
	 * switch_mm().
	 */
	smp_mb__after_atomic();

	return __other_cpus_need_tlb_flush(mm)
}

We should be able to deploy other_cpus_need_tlb_flush() in most of the
cases where we are doing &quot;cpumask_any_but(mm_cpumask(mm),
smp_processor_id()) &lt; nr_cpu_ids&quot;.

Right?
<span class="quote">
&gt; In theory, fix_pte_leak could have used flush_tlb_page. But the problem</span>
<span class="quote">&gt; is that flush_tlb_page requires the vm_area_struct as an argument, which</span>
<span class="quote">&gt; ptep_get_and_clear (and others) do not have.</span>

That, and we do not want/need to flush the _current_ processor&#39;s TLB.
flush_tlb_page() would have done that unnecessarily.  That&#39;s not the end
of the world here, but it is a downside.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - June 15, 2016, 2:20 a.m.</div>
<pre class="content">
On Tue, Jun 14, 2016 at 2:37 PM, Dave Hansen
&lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">&gt; On 06/14/2016 01:16 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt;&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; + if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +         trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +         flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +                          addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +         mb();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +         set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; + }</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt;&gt;&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt;&gt;&gt; barriers.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; IMHO it is best to use existing flushing interfaces instead of creating</span>
<span class="quote">&gt;&gt; new ones.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, or make these things a _little_ harder to get wrong.  That little</span>
<span class="quote">&gt; snippet above isn&#39;t so crazy that we should be depending on open-coded</span>
<span class="quote">&gt; barriers to get it right.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Should we just add a barrier to mm_cpumask() itself?  That should stop</span>
<span class="quote">&gt; the race.  Or maybe we need a new primitive like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; /*</span>
<span class="quote">&gt;  * Call this if a full barrier has been executed since the last</span>
<span class="quote">&gt;  * pagetable modification operation.</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt; static int __other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         /* cpumask_any_but() returns &gt;= nr_cpu_ids if no cpus set. */</span>
<span class="quote">&gt;         return cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt;</span>
<span class="quote">&gt;                 nr_cpu_ids;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static int other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * Synchronizes with switch_mm.  Makes sure that we do not</span>
<span class="quote">&gt;          * observe a bit having been cleared in mm_cpumask() before</span>
<span class="quote">&gt;          * the other processor has seen our pagetable update.  See</span>
<span class="quote">&gt;          * switch_mm().</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;         smp_mb__after_atomic();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return __other_cpus_need_tlb_flush(mm)</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We should be able to deploy other_cpus_need_tlb_flush() in most of the</span>
<span class="quote">&gt; cases where we are doing &quot;cpumask_any_but(mm_cpumask(mm),</span>
<span class="quote">&gt; smp_processor_id()) &lt; nr_cpu_ids&quot;.</span>

IMO this is a bit nuts.  smp_mb__after_atomic() doesn&#39;t do anything on
x86.  And, even if it did, why should the flush code assume that the
previous store was atomic?

What&#39;s the issue being fixed / worked around here?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 15, 2016, 2:35 a.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; On Tue, Jun 14, 2016 at 2:37 PM, Dave Hansen</span>
<span class="quote">&gt; &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On 06/14/2016 01:16 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; + if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +                          addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         mb();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; + }</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt;&gt;&gt;&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt;&gt;&gt;&gt; barriers.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; IMHO it is best to use existing flushing interfaces instead of creating</span>
<span class="quote">&gt;&gt;&gt; new ones.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Yeah, or make these things a _little_ harder to get wrong.  That little</span>
<span class="quote">&gt;&gt; snippet above isn&#39;t so crazy that we should be depending on open-coded</span>
<span class="quote">&gt;&gt; barriers to get it right.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Should we just add a barrier to mm_cpumask() itself?  That should stop</span>
<span class="quote">&gt;&gt; the race.  Or maybe we need a new primitive like:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; /*</span>
<span class="quote">&gt;&gt; * Call this if a full barrier has been executed since the last</span>
<span class="quote">&gt;&gt; * pagetable modification operation.</span>
<span class="quote">&gt;&gt; */</span>
<span class="quote">&gt;&gt; static int __other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt;        /* cpumask_any_but() returns &gt;= nr_cpu_ids if no cpus set. */</span>
<span class="quote">&gt;&gt;        return cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt;</span>
<span class="quote">&gt;&gt;                nr_cpu_ids;</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; static int other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt;        /*</span>
<span class="quote">&gt;&gt;         * Synchronizes with switch_mm.  Makes sure that we do not</span>
<span class="quote">&gt;&gt;         * observe a bit having been cleared in mm_cpumask() before</span>
<span class="quote">&gt;&gt;         * the other processor has seen our pagetable update.  See</span>
<span class="quote">&gt;&gt;         * switch_mm().</span>
<span class="quote">&gt;&gt;         */</span>
<span class="quote">&gt;&gt;        smp_mb__after_atomic();</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;        return __other_cpus_need_tlb_flush(mm)</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; We should be able to deploy other_cpus_need_tlb_flush() in most of the</span>
<span class="quote">&gt;&gt; cases where we are doing &quot;cpumask_any_but(mm_cpumask(mm),</span>
<span class="quote">&gt;&gt; smp_processor_id()) &lt; nr_cpu_ids&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IMO this is a bit nuts.  smp_mb__after_atomic() doesn&#39;t do anything on</span>
<span class="quote">&gt; x86.  And, even if it did, why should the flush code assume that the</span>
<span class="quote">&gt; previous store was atomic?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What&#39;s the issue being fixed / worked around here?</span>

It does a compiler barrier, which prevents the decision whether a
remote TLB shootdown is required to be made before the PTE is set.

I agree that PTEs may not be written atomically in certain cases
(although I am unaware of such cases, except on full-mm flush).

Having said that, I think that all the TLB flush/shootdown logic
should not be open-coded at all and be left in the arch-specific
implementation. People keep making small mistakes when they 
reimplement the flushing logic.

This patch, for example, also has a bug in the way it traces the
flush - it marks full flush, when it flushes a single page:
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>


Regards,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - June 15, 2016, 2:36 a.m.</div>
<pre class="content">
On Tue, Jun 14, 2016 at 7:35 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Tue, Jun 14, 2016 at 2:37 PM, Dave Hansen</span>
<span class="quote">&gt;&gt; &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On 06/14/2016 01:16 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +                          addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         mb();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + }</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; barriers.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; IMHO it is best to use existing flushing interfaces instead of creating</span>
<span class="quote">&gt;&gt;&gt;&gt; new ones.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Yeah, or make these things a _little_ harder to get wrong.  That little</span>
<span class="quote">&gt;&gt;&gt; snippet above isn&#39;t so crazy that we should be depending on open-coded</span>
<span class="quote">&gt;&gt;&gt; barriers to get it right.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Should we just add a barrier to mm_cpumask() itself?  That should stop</span>
<span class="quote">&gt;&gt;&gt; the race.  Or maybe we need a new primitive like:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; /*</span>
<span class="quote">&gt;&gt;&gt; * Call this if a full barrier has been executed since the last</span>
<span class="quote">&gt;&gt;&gt; * pagetable modification operation.</span>
<span class="quote">&gt;&gt;&gt; */</span>
<span class="quote">&gt;&gt;&gt; static int __other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt;        /* cpumask_any_but() returns &gt;= nr_cpu_ids if no cpus set. */</span>
<span class="quote">&gt;&gt;&gt;        return cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt;</span>
<span class="quote">&gt;&gt;&gt;                nr_cpu_ids;</span>
<span class="quote">&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; static int other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt;        /*</span>
<span class="quote">&gt;&gt;&gt;         * Synchronizes with switch_mm.  Makes sure that we do not</span>
<span class="quote">&gt;&gt;&gt;         * observe a bit having been cleared in mm_cpumask() before</span>
<span class="quote">&gt;&gt;&gt;         * the other processor has seen our pagetable update.  See</span>
<span class="quote">&gt;&gt;&gt;         * switch_mm().</span>
<span class="quote">&gt;&gt;&gt;         */</span>
<span class="quote">&gt;&gt;&gt;        smp_mb__after_atomic();</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;        return __other_cpus_need_tlb_flush(mm)</span>
<span class="quote">&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; We should be able to deploy other_cpus_need_tlb_flush() in most of the</span>
<span class="quote">&gt;&gt;&gt; cases where we are doing &quot;cpumask_any_but(mm_cpumask(mm),</span>
<span class="quote">&gt;&gt;&gt; smp_processor_id()) &lt; nr_cpu_ids&quot;.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; IMO this is a bit nuts.  smp_mb__after_atomic() doesn&#39;t do anything on</span>
<span class="quote">&gt;&gt; x86.  And, even if it did, why should the flush code assume that the</span>
<span class="quote">&gt;&gt; previous store was atomic?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What&#39;s the issue being fixed / worked around here?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It does a compiler barrier, which prevents the decision whether a</span>
<span class="quote">&gt; remote TLB shootdown is required to be made before the PTE is set.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I agree that PTEs may not be written atomically in certain cases</span>
<span class="quote">&gt; (although I am unaware of such cases, except on full-mm flush).</span>

How about plain set_pte?  It&#39;s atomic (aligned word-sized write), but
it&#39;s not atomic in the _after_atomic sense.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 15, 2016, 2:44 a.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; On Tue, Jun 14, 2016 at 7:35 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; On Tue, Jun 14, 2016 at 2:37 PM, Dave Hansen</span>
<span class="quote">&gt;&gt;&gt; &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On 06/14/2016 01:16 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +                          addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         mb();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + }</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; barriers.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; IMHO it is best to use existing flushing interfaces instead of creating</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; new ones.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Yeah, or make these things a _little_ harder to get wrong.  That little</span>
<span class="quote">&gt;&gt;&gt;&gt; snippet above isn&#39;t so crazy that we should be depending on open-coded</span>
<span class="quote">&gt;&gt;&gt;&gt; barriers to get it right.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Should we just add a barrier to mm_cpumask() itself?  That should stop</span>
<span class="quote">&gt;&gt;&gt;&gt; the race.  Or maybe we need a new primitive like:</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; /*</span>
<span class="quote">&gt;&gt;&gt;&gt; * Call this if a full barrier has been executed since the last</span>
<span class="quote">&gt;&gt;&gt;&gt; * pagetable modification operation.</span>
<span class="quote">&gt;&gt;&gt;&gt; */</span>
<span class="quote">&gt;&gt;&gt;&gt; static int __other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt;&gt;       /* cpumask_any_but() returns &gt;= nr_cpu_ids if no cpus set. */</span>
<span class="quote">&gt;&gt;&gt;&gt;       return cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt;</span>
<span class="quote">&gt;&gt;&gt;&gt;               nr_cpu_ids;</span>
<span class="quote">&gt;&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; static int other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt;&gt;       /*</span>
<span class="quote">&gt;&gt;&gt;&gt;        * Synchronizes with switch_mm.  Makes sure that we do not</span>
<span class="quote">&gt;&gt;&gt;&gt;        * observe a bit having been cleared in mm_cpumask() before</span>
<span class="quote">&gt;&gt;&gt;&gt;        * the other processor has seen our pagetable update.  See</span>
<span class="quote">&gt;&gt;&gt;&gt;        * switch_mm().</span>
<span class="quote">&gt;&gt;&gt;&gt;        */</span>
<span class="quote">&gt;&gt;&gt;&gt;       smp_mb__after_atomic();</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;       return __other_cpus_need_tlb_flush(mm)</span>
<span class="quote">&gt;&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; We should be able to deploy other_cpus_need_tlb_flush() in most of the</span>
<span class="quote">&gt;&gt;&gt;&gt; cases where we are doing &quot;cpumask_any_but(mm_cpumask(mm),</span>
<span class="quote">&gt;&gt;&gt;&gt; smp_processor_id()) &lt; nr_cpu_ids&quot;.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; IMO this is a bit nuts.  smp_mb__after_atomic() doesn&#39;t do anything on</span>
<span class="quote">&gt;&gt;&gt; x86.  And, even if it did, why should the flush code assume that the</span>
<span class="quote">&gt;&gt;&gt; previous store was atomic?</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; What&#39;s the issue being fixed / worked around here?</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; It does a compiler barrier, which prevents the decision whether a</span>
<span class="quote">&gt;&gt; remote TLB shootdown is required to be made before the PTE is set.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I agree that PTEs may not be written atomically in certain cases</span>
<span class="quote">&gt;&gt; (although I am unaware of such cases, except on full-mm flush).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How about plain set_pte?  It&#39;s atomic (aligned word-sized write), but</span>
<span class="quote">&gt; it&#39;s not atomic in the _after_atomic sense.</span>

Can you point me to a place where set_pte is used before a TLB
invalidation/shootdown, excluding this patch and the fullmm case?

I am not claiming there is no such case, but I am unaware of such
one. PTEs are cleared on SMP using xchg, and similarly the dirty bit
is cleared with an atomic operation.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - June 15, 2016, 3:09 a.m.</div>
<pre class="content">
On Tue, Jun 14, 2016 at 7:44 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Tue, Jun 14, 2016 at 7:35 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; On Tue, Jun 14, 2016 at 2:37 PM, Dave Hansen</span>
<span class="quote">&gt;&gt;&gt;&gt; &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 06/14/2016 01:16 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +                          addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         mb();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +         set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + }</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; barriers.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; IMHO it is best to use existing flushing interfaces instead of creating</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; new ones.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Yeah, or make these things a _little_ harder to get wrong.  That little</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; snippet above isn&#39;t so crazy that we should be depending on open-coded</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; barriers to get it right.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Should we just add a barrier to mm_cpumask() itself?  That should stop</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; the race.  Or maybe we need a new primitive like:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; /*</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; * Call this if a full barrier has been executed since the last</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; * pagetable modification operation.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; static int __other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;       /* cpumask_any_but() returns &gt;= nr_cpu_ids if no cpus set. */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;       return cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;               nr_cpu_ids;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; static int other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;       /*</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;        * Synchronizes with switch_mm.  Makes sure that we do not</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;        * observe a bit having been cleared in mm_cpumask() before</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;        * the other processor has seen our pagetable update.  See</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;        * switch_mm().</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;        */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;       smp_mb__after_atomic();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;       return __other_cpus_need_tlb_flush(mm)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; We should be able to deploy other_cpus_need_tlb_flush() in most of the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; cases where we are doing &quot;cpumask_any_but(mm_cpumask(mm),</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; smp_processor_id()) &lt; nr_cpu_ids&quot;.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; IMO this is a bit nuts.  smp_mb__after_atomic() doesn&#39;t do anything on</span>
<span class="quote">&gt;&gt;&gt;&gt; x86.  And, even if it did, why should the flush code assume that the</span>
<span class="quote">&gt;&gt;&gt;&gt; previous store was atomic?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; What&#39;s the issue being fixed / worked around here?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; It does a compiler barrier, which prevents the decision whether a</span>
<span class="quote">&gt;&gt;&gt; remote TLB shootdown is required to be made before the PTE is set.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I agree that PTEs may not be written atomically in certain cases</span>
<span class="quote">&gt;&gt;&gt; (although I am unaware of such cases, except on full-mm flush).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; How about plain set_pte?  It&#39;s atomic (aligned word-sized write), but</span>
<span class="quote">&gt;&gt; it&#39;s not atomic in the _after_atomic sense.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Can you point me to a place where set_pte is used before a TLB</span>
<span class="quote">&gt; invalidation/shootdown, excluding this patch and the fullmm case?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I am not claiming there is no such case, but I am unaware of such</span>
<span class="quote">&gt; one. PTEs are cleared on SMP using xchg, and similarly the dirty bit</span>
<span class="quote">&gt; is cleared with an atomic operation.</span>
<span class="quote">&gt;</span>

Hmm, you may be right.  I still think this is all disgusting, but I
don&#39;t have any better ideas.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 15, 2016, 3:20 a.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 06/14/2016 01:16 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; On 06/14/2016 09:47 AM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; Here there should be a call to smp_mb__after_atomic() to synchronize with</span>
<span class="quote">&gt;&gt;&gt;&gt; switch_mm. I submitted a similar patch, which is still pending (hint).</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +				 addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +		mb();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +		set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Shouldn&#39;t that barrier be incorporated in the TLB flush code itself and</span>
<span class="quote">&gt;&gt;&gt; not every single caller (like this code is)?</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; It is insane to require individual TLB flushers to be concerned with the</span>
<span class="quote">&gt;&gt;&gt; barriers.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; IMHO it is best to use existing flushing interfaces instead of creating</span>
<span class="quote">&gt;&gt; new ones.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, or make these things a _little_ harder to get wrong.  That little</span>
<span class="quote">&gt; snippet above isn&#39;t so crazy that we should be depending on open-coded</span>
<span class="quote">&gt; barriers to get it right.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Should we just add a barrier to mm_cpumask() itself?  That should stop</span>
<span class="quote">&gt; the race.  Or maybe we need a new primitive like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; /*</span>
<span class="quote">&gt; * Call this if a full barrier has been executed since the last</span>
<span class="quote">&gt; * pagetable modification operation.</span>
<span class="quote">&gt; */</span>
<span class="quote">&gt; static int __other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	/* cpumask_any_but() returns &gt;= nr_cpu_ids if no cpus set. */</span>
<span class="quote">&gt; 	return cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt;</span>
<span class="quote">&gt; 		nr_cpu_ids;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static int other_cpus_need_tlb_flush(struct mm_struct *mm)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	/*</span>
<span class="quote">&gt; 	 * Synchronizes with switch_mm.  Makes sure that we do not</span>
<span class="quote">&gt; 	 * observe a bit having been cleared in mm_cpumask() before</span>
<span class="quote">&gt; 	 * the other processor has seen our pagetable update.  See</span>
<span class="quote">&gt; 	 * switch_mm().</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt; 	smp_mb__after_atomic();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	return __other_cpus_need_tlb_flush(mm)</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We should be able to deploy other_cpus_need_tlb_flush() in most of the</span>
<span class="quote">&gt; cases where we are doing &quot;cpumask_any_but(mm_cpumask(mm),</span>
<span class="quote">&gt; smp_processor_id()) &lt; nr_cpu_ids&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right?</span>
This approach may work, but I doubt other_cpus_need_tlb_flush() would
be needed by anyone, excluding this &quot;hacky&quot; workaround. There are already
five interfaces for invalidation of: a single page, a userspace range,
a whole task, a kernel range, and full flush including kernel (global)
entries.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; In theory, fix_pte_leak could have used flush_tlb_page. But the problem</span>
<span class="quote">&gt;&gt; is that flush_tlb_page requires the vm_area_struct as an argument, which</span>
<span class="quote">&gt;&gt; ptep_get_and_clear (and others) do not have.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That, and we do not want/need to flush the _current_ processor&#39;s TLB.</span>
<span class="quote">&gt; flush_tlb_page() would have done that unnecessarily.  That&#39;s not the end</span>
<span class="quote">&gt; of the world here, but it is a downside.</span>

Oops, I missed the fact a local flush is not needed in this case.

Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a> - June 15, 2016, 1:06 p.m.</div>
<pre class="content">
<span class="from">From: Dave Hansen [mailto:dave.hansen@linux.intel.com] </span>
Sent: Tuesday, June 14, 2016 7:20 PM
<span class="quote">
&gt;&gt; diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
...
<span class="quote">&gt;&gt; +extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;&gt; +			 pte_t *ptep);</span>
<span class="quote">
&gt; Doesn&#39;t hugetlb.h somehow #include pgtable.h?  So why double-define</span>
<span class="quote">&gt; fix_pte_leak()?</span>

It&#39;s other way round - pgtable.h somehow includes hugetlb.h. I&#39;ve removed
duplicated fix_pte_leak() declaration.
<span class="quote">
&gt;&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt; index 2ee7811..6fa4079 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt; @@ -178,6 +178,12 @@ extern void cleanup_highmap(void);</span>
<span class="quote">&gt;&gt;  extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;&gt;  extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#define ARCH_HAS_NEEDS_SWAP_PTL 1</span>
<span class="quote">&gt;&gt; +static inline bool arch_needs_swap_ptl(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       return boot_cpu_has_bug(X86_BUG_PTE_LEAK);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #endif /* !__ASSEMBLY__ */</span>
<span class="quote">
&gt; I think we need a comment on this sucker.  I&#39;m not sure we should lean</span>
<span class="quote">&gt; solely on the commit message to record why we need this until the end of</span>
<span class="quote">&gt; time.</span>

OK.
<span class="quote">
&gt;&gt; +	if (c-&gt;x86_model == 87) {</span>
<span class="quote">
&gt; Please use the macros in here for the model id:</span>

OK.
<span class="quote">
&gt; http://git.kernel.org/cgit/linux/kernel/git/tip/tip.git/tree/arch/x86/include/asm/intel-family.h</span>
<span class="quote">
&gt; We also probably want to prefix the pr_info() with something like</span>
<span class="quote">&gt; &quot;x86/intel:&quot;.</span>

OK
<span class="quote">
&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Workaround for KNL issue:</span>
<span class="quote">
&gt; Please be specific about what this &quot;KNL issue&quot; *is*. </span>

OK
<span class="quote">
&gt;&gt; + * A thread that is going to page fault due to P=0, may still</span>
<span class="quote">&gt;&gt; + * non atomically set A or D bits, which could corrupt swap entries.</span>
<span class="quote">&gt;&gt; + * Always flush the other CPUs and clear the PTE again to avoid</span>
<span class="quote">&gt;&gt; + * this leakage. We are excluded using the pagetable lock.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt;&gt; +		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt; +		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="quote">&gt;&gt; +				 addr + PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		mb();</span>
<span class="quote">&gt;&gt; +		set_pte(ptep, __pte(0));</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think the comment here is a bit sparse.  Can we add some more details,</span>
<span class="quote">&gt; like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;	Entering here, the current CPU just cleared the PTE.  But,</span>
<span class="quote">&gt;	another thread may have raced and set the A or D bits, or be</span>
<span class="quote">&gt;	_about_ to set the bits.  Shooting their TLB entry down will</span>
<span class="quote">&gt;	ensure they see the cleared PTE and will not set A or D.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; and by the set_pte():</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;	Clear the PTE one more time, in case the other thread set A/D</span>
<span class="quote">&gt;	before we sent the TLB flush.</span>

Thanks,
Lukasz
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 4a41348..2c48011 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -303,6 +303,7 @@</span> <span class="p_context"></span>
 #define X86_BUG_SYSRET_SS_ATTRS	X86_BUG(8) /* SYSRET doesn&#39;t fix up SS attrs */
 #define X86_BUG_NULL_SEG	X86_BUG(9) /* Nulling a selector preserves the base */
 #define X86_BUG_SWAPGS_FENCE	X86_BUG(10) /* SWAPGS without input dep on GS */
<span class="p_add">+#define X86_BUG_PTE_LEAK        X86_BUG(11) /* PTE may leak A/D bits after clear */</span>
 
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/include/asm/hugetlb.h b/arch/x86/include/asm/hugetlb.h</span>
<span class="p_header">index 3a10616..58e1ca9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hugetlb.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hugetlb.h</span>
<span class="p_chunk">@@ -41,10 +41,17 @@</span> <span class="p_context"> static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,</span>
 	set_pte_at(mm, addr, ptep, pte);
 }
 
<span class="p_add">+extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+			 pte_t *ptep);</span>
<span class="p_add">+</span>
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {
<span class="p_del">-	return ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="p_add">+		fix_pte_leak(mm, addr, ptep);</span>
<span class="p_add">+	return pte;</span>
 }
 
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 1a27396..9769355 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -794,11 +794,16 @@</span> <span class="p_context"> extern int ptep_test_and_clear_young(struct vm_area_struct *vma,</span>
 extern int ptep_clear_flush_young(struct vm_area_struct *vma,
 				  unsigned long address, pte_t *ptep);
 
<span class="p_add">+extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+			 pte_t *ptep);</span>
<span class="p_add">+</span>
 #define __HAVE_ARCH_PTEP_GET_AND_CLEAR
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pte_t *ptep)
 {
 	pte_t pte = native_ptep_get_and_clear(ptep);
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="p_add">+		fix_pte_leak(mm, addr, ptep);</span>
 	pte_update(mm, addr, ptep);
 	return pte;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 2ee7811..6fa4079 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -178,6 +178,12 @@</span> <span class="p_context"> extern void cleanup_highmap(void);</span>
 extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);
 extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);
 
<span class="p_add">+#define ARCH_HAS_NEEDS_SWAP_PTL 1</span>
<span class="p_add">+static inline bool arch_needs_swap_ptl(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+       return boot_cpu_has_bug(X86_BUG_PTE_LEAK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* !__ASSEMBLY__ */
 
 #endif /* _ASM_X86_PGTABLE_64_H */
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">index 6e2ffbe..f499513 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_chunk">@@ -181,6 +181,16 @@</span> <span class="p_context"> static void early_init_intel(struct cpuinfo_x86 *c)</span>
 		}
 	}
 
<span class="p_add">+	if (c-&gt;x86_model == 87) {</span>
<span class="p_add">+		static bool printed;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!printed) {</span>
<span class="p_add">+			pr_info(&quot;Enabling PTE leaking workaround\n&quot;);</span>
<span class="p_add">+			printed = true;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_cpu_bug(c, X86_BUG_PTE_LEAK);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Intel Quark Core DevMan_001.pdf section 6.4.11
 	 * &quot;The operating system also is required to invalidate (i.e., flush)
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 5643fd0..3d54488 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -468,4 +468,24 @@</span> <span class="p_context"> static int __init create_tlb_single_page_flush_ceiling(void)</span>
 }
 late_initcall(create_tlb_single_page_flush_ceiling);
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Workaround for KNL issue:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A thread that is going to page fault due to P=0, may still</span>
<span class="p_add">+ * non atomically set A or D bits, which could corrupt swap entries.</span>
<span class="p_add">+ * Always flush the other CPUs and clear the PTE again to avoid</span>
<span class="p_add">+ * this leakage. We are excluded using the pagetable lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="p_add">+		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="p_add">+		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="p_add">+				 addr + PAGE_SIZE);</span>
<span class="p_add">+		mb();</span>
<span class="p_add">+		set_pte(ptep, __pte(0));</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* CONFIG_SMP */
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 5df5feb..5c80fe09 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2404,6 +2404,10 @@</span> <span class="p_context"> static inline bool debug_guardpage_enabled(void) { return false; }</span>
 static inline bool page_is_guard(struct page *page) { return false; }
 #endif /* CONFIG_DEBUG_PAGEALLOC */
 
<span class="p_add">+#ifndef ARCH_HAS_NEEDS_SWAP_PTL</span>
<span class="p_add">+static inline bool arch_needs_swap_ptl(void) { return false; }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #if MAX_NUMNODES &gt; 1
 void __init setup_nr_node_ids(void);
 #else
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 15322b7..0d6ef39 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1960,7 +1960,8 @@</span> <span class="p_context"> static inline int pte_unmap_same(struct mm_struct *mm, pmd_t *pmd,</span>
 {
 	int same = 1;
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
<span class="p_del">-	if (sizeof(pte_t) &gt; sizeof(unsigned long)) {</span>
<span class="p_add">+	if (arch_needs_swap_ptl() ||</span>
<span class="p_add">+	    sizeof(pte_t) &gt; sizeof(unsigned long)) {</span>
 		spinlock_t *ptl = pte_lockptr(mm, pmd);
 		spin_lock(ptl);
 		same = pte_same(*page_table, orig_pte);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



