
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,pull] More melted spectrum mitigations for 4.16 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,pull] More melted spectrum mitigations for 4.16</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 4, 2018, 3:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.DEB.2.21.1802041609440.1035@nanos.tec.linutronix.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10199335/mbox/"
   >mbox</a>
|
   <a href="/patch/10199335/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10199335/">/patch/10199335/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	2BC9560318 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  4 Feb 2018 15:31:38 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 109AF2843C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  4 Feb 2018 15:31:38 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 049A2285A9; Sun,  4 Feb 2018 15:31:38 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1C2AF2843C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  4 Feb 2018 15:31:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752054AbeBDPb1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 4 Feb 2018 10:31:27 -0500
Received: from Galois.linutronix.de ([146.0.238.70]:51619 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750995AbeBDPbP (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 4 Feb 2018 10:31:15 -0500
Received: from p4fea5f09.dip0.t-ipconnect.de ([79.234.95.9]
	helo=nanos.glx-home) by Galois.linutronix.de with esmtpsa
	(TLS1.2:DHE_RSA_AES_256_CBC_SHA256:256) (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1eiMDF-00048C-Si; Sun, 04 Feb 2018 16:28:15 +0100
Date: Sun, 4 Feb 2018 16:31:12 +0100 (CET)
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
cc: LKML &lt;linux-kernel@vger.kernel.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	David Woodhouse &lt;dwmw@amazon.co.uk&gt;, Greg KH &lt;gregkh@linuxfoundation.org&gt;
Subject: [GIT pull] More melted spectrum mitigations for 4.16
Message-ID: &lt;alpine.DEB.2.21.1802041609440.1035@nanos.tec.linutronix.de&gt;
User-Agent: Alpine 2.21 (DEB 202 2017-01-01)
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary=&quot;8323329-284643442-1517758272=:1035&quot;
X-Linutronix-Spam-Score: -1.0
X-Linutronix-Spam-Level: -
X-Linutronix-Spam-Status: No , -1.0 points, 5.0 required, ALL_TRUSTED=-1,
	SHORTCIRCUIT=-0.0001
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Feb. 4, 2018, 3:31 p.m.</div>
<pre class="content">
Linus,

please pull the latest x86-pti-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus

The next round of updates related to melted spectrum:

   - The initial set of spectre V1 mitigations:

       - Array index speculation blocker and its usage for syscall, fdtable
       	 and the n180211 driver.

       - Speculation barrier and its usage in user access functions

   - Make indirect calls in KVM speculation safe    

   - Blacklisting of known to be broken microcodes so IPBP/IBSR are not
     touched.

   - The initial IBPB support and its usage in context switch

   - The exposure of the new speculation MSRs to KVM guests.

   - A fix for a regression in x86/32 related to the cpu entry area

   - Proper whitelisting for known to be safe CPUs from the mitigations.

   - objtool fixes to deal proper with retpolines and alternatives

   - Exclude __init functions from retpolines which speeds up the boot
     process.

   - Removal of the syscall64 fast path and related cleanups and
     simplifications

   - Removal of the unpatched paravirt mode which is yet another source of
     indirect unproteced calls.

   - A new and undisputed version of the module mismatch warning

   - A couple of cleanup and correctness fixes all over the place

Yet another step towards full mitigation. There are a few things still
missing like the RBS underflow mitigation for Skylake and other small
details, but that&#39;s being worked on.

That said, I&#39;m taking a belated christmas vacation for a week and hope that
everything is magically solved when I&#39;m back on Feb. 12th.

Thanks,

	tglx

------------------&gt;
Andi Kleen (1):
      module/retpoline: Warn about missing retpoline in module

Andy Lutomirski (3):
      x86/entry/64: Remove the SYSCALL64 fast path
      x86/entry/64: Push extra regs right away
      x86/asm: Move &#39;status&#39; from thread_struct to thread_info

Arnd Bergmann (1):
      x86/pti: Mark constant arrays as __initconst

Ashok Raj (1):
      KVM/x86: Add IBPB support

Borislav Petkov (5):
      x86/alternative: Print unadorned pointers
      x86/nospec: Fix header guards names
      x86/bugs: Drop one &quot;mitigation&quot; from dmesg
      x86/retpoline: Simplify vmexit_fill_RSB()
      x86/speculation: Simplify indirect_branch_prediction_barrier()

Colin Ian King (1):
      x86/spectre: Fix spelling mistake: &quot;vunerable&quot;-&gt; &quot;vulnerable&quot;

Dan Williams (12):
      array_index_nospec: Sanitize speculative array de-references
      x86: Implement array_index_mask_nospec
      x86: Introduce barrier_nospec
      x86: Introduce __uaccess_begin_nospec() and uaccess_try_nospec
      x86/usercopy: Replace open coded stac/clac with __uaccess_{begin, end}
      x86/uaccess: Use __uaccess_begin_nospec() and uaccess_try_nospec
      x86/get_user: Use pointer masking to limit speculation
      x86/syscall: Sanitize syscall table de-references under speculation
      vfs, fdtable: Prevent bounds-check bypass via speculative execution
      nl80211: Sanitize array index in parse_txq_params
      x86/spectre: Report get_user mitigation for spectre_v1
      x86/kvm: Update spectre-v1 mitigation

Darren Kenny (1):
      x86/speculation: Fix typo IBRS_ATT, which should be IBRS_ALL

David Woodhouse (10):
      x86/cpufeatures: Add CPUID_7_EDX CPUID leaf
      x86/cpufeatures: Add Intel feature bits for Speculation Control
      x86/cpufeatures: Add AMD feature bits for Speculation Control
      x86/msr: Add definitions for new speculation control MSRs
      x86/pti: Do not enable PTI on CPUs which are not vulnerable to Meltdown
      x86/cpufeature: Blacklist SPEC_CTRL/PRED_CMD on early Spectre v2 microcodes
      x86/speculation: Add basic IBPB (Indirect Branch Prediction Barrier) support
      x86/cpufeatures: Clean up Spectre v2 related CPUID flags
      x86/cpuid: Fix up &quot;virtual&quot; IBRS/IBPB/STIBP feature bits on Intel
      x86/retpoline: Avoid retpolines for built-in __init functions

Dou Liyang (1):
      x86/spectre: Check CONFIG_RETPOLINE in command line parser

Jim Mattson (1):
      KVM: nVMX: Eliminate vmcs02 pool

Josh Poimboeuf (4):
      objtool: Improve retpoline alternative handling
      objtool: Add support for alternatives at the end of a section
      objtool: Warn on stripped section symbol
      x86/paravirt: Remove &#39;noreplace-paravirt&#39; cmdline option

KarimAllah Ahmed (5):
      x86/spectre: Simplify spectre_v2 command line parsing
      KVM/x86: Update the reverse_cpuid list to include CPUID_7_EDX
      KVM/VMX: Emulate MSR_IA32_ARCH_CAPABILITIES
      KVM/VMX: Allow direct access to MSR_IA32_SPEC_CTRL
      KVM/SVM: Allow direct access to MSR_IA32_SPEC_CTRL

Mark Rutland (1):
      Documentation: Document array_index_nospec

Paolo Bonzini (2):
      KVM: VMX: introduce alloc_loaded_vmcs
      KVM: VMX: make MSR bitmaps per-VCPU

Peter Zijlstra (2):
      KVM: x86: Make indirect calls in emulator speculation safe
      KVM: VMX: Make indirect call speculation safe

Thomas Gleixner (1):
      x86/cpu/bugs: Make retpoline module warning conditional

Tim Chen (1):
      x86/speculation: Use Indirect Branch Prediction Barrier in context switch

William Grant (1):
      x86/mm: Fix overlap of i386 CPU_ENTRY_AREA with FIX_BTMAP


 Documentation/admin-guide/kernel-parameters.txt |   2 -
 Documentation/speculation.txt                   |  90 ++++
 arch/x86/entry/common.c                         |   9 +-
 arch/x86/entry/entry_32.S                       |   3 +-
 arch/x86/entry/entry_64.S                       | 130 +----
 arch/x86/entry/syscall_64.c                     |   7 +-
 arch/x86/include/asm/asm-prototypes.h           |   3 +
 arch/x86/include/asm/barrier.h                  |  28 +
 arch/x86/include/asm/cpufeature.h               |   7 +-
 arch/x86/include/asm/cpufeatures.h              |  22 +-
 arch/x86/include/asm/disabled-features.h        |   3 +-
 arch/x86/include/asm/fixmap.h                   |   6 +-
 arch/x86/include/asm/msr-index.h                |  12 +
 arch/x86/include/asm/msr.h                      |   3 +-
 arch/x86/include/asm/nospec-branch.h            |  86 +--
 arch/x86/include/asm/pgtable_32_types.h         |   5 +-
 arch/x86/include/asm/processor.h                |   5 +-
 arch/x86/include/asm/required-features.h        |   3 +-
 arch/x86/include/asm/syscall.h                  |   6 +-
 arch/x86/include/asm/thread_info.h              |   3 +-
 arch/x86/include/asm/tlbflush.h                 |   2 +
 arch/x86/include/asm/uaccess.h                  |  15 +-
 arch/x86/include/asm/uaccess_32.h               |   6 +-
 arch/x86/include/asm/uaccess_64.h               |  12 +-
 arch/x86/kernel/alternative.c                   |  28 +-
 arch/x86/kernel/cpu/bugs.c                      | 134 +++--
 arch/x86/kernel/cpu/common.c                    |  70 ++-
 arch/x86/kernel/cpu/intel.c                     |  66 +++
 arch/x86/kernel/cpu/scattered.c                 |   2 -
 arch/x86/kernel/process_64.c                    |   4 +-
 arch/x86/kernel/ptrace.c                        |   2 +-
 arch/x86/kernel/signal.c                        |   2 +-
 arch/x86/kvm/cpuid.c                            |  22 +-
 arch/x86/kvm/cpuid.h                            |   1 +
 arch/x86/kvm/emulate.c                          |   9 +-
 arch/x86/kvm/svm.c                              | 116 +++++
 arch/x86/kvm/vmx.c                              | 660 ++++++++++++++----------
 arch/x86/kvm/x86.c                              |   1 +
 arch/x86/lib/Makefile                           |   1 +
 arch/x86/lib/getuser.S                          |  10 +
 arch/x86/lib/retpoline.S                        |  56 ++
 arch/x86/lib/usercopy_32.c                      |   8 +-
 arch/x86/mm/tlb.c                               |  33 +-
 include/linux/fdtable.h                         |   5 +-
 include/linux/init.h                            |   9 +-
 include/linux/module.h                          |   9 +
 include/linux/nospec.h                          |  72 +++
 kernel/module.c                                 |  11 +
 net/wireless/nl80211.c                          |   9 +-
 scripts/mod/modpost.c                           |   9 +
 tools/objtool/check.c                           |  89 ++--
 tools/objtool/orc_gen.c                         |   5 +
 52 files changed, 1279 insertions(+), 632 deletions(-)
 create mode 100644 Documentation/speculation.txt
 create mode 100644 include/linux/nospec.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 4, 2018, 7:45 p.m.</div>
<pre class="content">
On Sun, Feb 4, 2018 at 7:31 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; That said, I&#39;m taking a belated christmas vacation for a week and hope that</span>
<span class="quote">&gt; everything is magically solved when I&#39;m back on Feb. 12th.</span>

Snort. Good luck with that.

               Linus
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">index 46b26bfee27b..1e762c210f1b 100644</span>
<span class="p_header">--- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2742,8 +2742,6 @@</span> <span class="p_context"></span>
 	norandmaps	Don&#39;t use address space randomization.  Equivalent to
 			echo 0 &gt; /proc/sys/kernel/randomize_va_space
 
<span class="p_del">-	noreplace-paravirt	[X86,IA-64,PV_OPS] Don&#39;t patch paravirt_ops</span>
<span class="p_del">-</span>
 	noreplace-smp	[X86-32,SMP] Don&#39;t replace SMP instructions
 			with UP alternatives
 
<span class="p_header">diff --git a/Documentation/speculation.txt b/Documentation/speculation.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..e9e6cbae2841</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/speculation.txt</span>
<span class="p_chunk">@@ -0,0 +1,90 @@</span> <span class="p_context"></span>
<span class="p_add">+This document explains potential effects of speculation, and how undesirable</span>
<span class="p_add">+effects can be mitigated portably using common APIs.</span>
<span class="p_add">+</span>
<span class="p_add">+===========</span>
<span class="p_add">+Speculation</span>
<span class="p_add">+===========</span>
<span class="p_add">+</span>
<span class="p_add">+To improve performance and minimize average latencies, many contemporary CPUs</span>
<span class="p_add">+employ speculative execution techniques such as branch prediction, performing</span>
<span class="p_add">+work which may be discarded at a later stage.</span>
<span class="p_add">+</span>
<span class="p_add">+Typically speculative execution cannot be observed from architectural state,</span>
<span class="p_add">+such as the contents of registers. However, in some cases it is possible to</span>
<span class="p_add">+observe its impact on microarchitectural state, such as the presence or</span>
<span class="p_add">+absence of data in caches. Such state may form side-channels which can be</span>
<span class="p_add">+observed to extract secret information.</span>
<span class="p_add">+</span>
<span class="p_add">+For example, in the presence of branch prediction, it is possible for bounds</span>
<span class="p_add">+checks to be ignored by code which is speculatively executed. Consider the</span>
<span class="p_add">+following code:</span>
<span class="p_add">+</span>
<span class="p_add">+	int load_array(int *array, unsigned int index)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		if (index &gt;= MAX_ARRAY_ELEMS)</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			return array[index];</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+Which, on arm64, may be compiled to an assembly sequence such as:</span>
<span class="p_add">+</span>
<span class="p_add">+	CMP	&lt;index&gt;, #MAX_ARRAY_ELEMS</span>
<span class="p_add">+	B.LT	less</span>
<span class="p_add">+	MOV	&lt;returnval&gt;, #0</span>
<span class="p_add">+	RET</span>
<span class="p_add">+  less:</span>
<span class="p_add">+	LDR	&lt;returnval&gt;, [&lt;array&gt;, &lt;index&gt;]</span>
<span class="p_add">+	RET</span>
<span class="p_add">+</span>
<span class="p_add">+It is possible that a CPU mis-predicts the conditional branch, and</span>
<span class="p_add">+speculatively loads array[index], even if index &gt;= MAX_ARRAY_ELEMS. This</span>
<span class="p_add">+value will subsequently be discarded, but the speculated load may affect</span>
<span class="p_add">+microarchitectural state which can be subsequently measured.</span>
<span class="p_add">+</span>
<span class="p_add">+More complex sequences involving multiple dependent memory accesses may</span>
<span class="p_add">+result in sensitive information being leaked. Consider the following</span>
<span class="p_add">+code, building on the prior example:</span>
<span class="p_add">+</span>
<span class="p_add">+	int load_dependent_arrays(int *arr1, int *arr2, int index)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		int val1, val2,</span>
<span class="p_add">+</span>
<span class="p_add">+		val1 = load_array(arr1, index);</span>
<span class="p_add">+		val2 = load_array(arr2, val1);</span>
<span class="p_add">+</span>
<span class="p_add">+		return val2;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+Under speculation, the first call to load_array() may return the value</span>
<span class="p_add">+of an out-of-bounds address, while the second call will influence</span>
<span class="p_add">+microarchitectural state dependent on this value. This may provide an</span>
<span class="p_add">+arbitrary read primitive.</span>
<span class="p_add">+</span>
<span class="p_add">+====================================</span>
<span class="p_add">+Mitigating speculation side-channels</span>
<span class="p_add">+====================================</span>
<span class="p_add">+</span>
<span class="p_add">+The kernel provides a generic API to ensure that bounds checks are</span>
<span class="p_add">+respected even under speculation. Architectures which are affected by</span>
<span class="p_add">+speculation-based side-channels are expected to implement these</span>
<span class="p_add">+primitives.</span>
<span class="p_add">+</span>
<span class="p_add">+The array_index_nospec() helper in &lt;linux/nospec.h&gt; can be used to</span>
<span class="p_add">+prevent information from being leaked via side-channels.</span>
<span class="p_add">+</span>
<span class="p_add">+A call to array_index_nospec(index, size) returns a sanitized index</span>
<span class="p_add">+value that is bounded to [0, size) even under cpu speculation</span>
<span class="p_add">+conditions.</span>
<span class="p_add">+</span>
<span class="p_add">+This can be used to protect the earlier load_array() example:</span>
<span class="p_add">+</span>
<span class="p_add">+	int load_array(int *array, unsigned int index)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		if (index &gt;= MAX_ARRAY_ELEMS)</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		else {</span>
<span class="p_add">+			index = array_index_nospec(index, MAX_ARRAY_ELEMS);</span>
<span class="p_add">+			return array[index];</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_header">diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c</span>
<span class="p_header">index d7d3cc24baf4..21dbdf0e476b 100644</span>
<span class="p_header">--- a/arch/x86/entry/common.c</span>
<span class="p_header">+++ b/arch/x86/entry/common.c</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/export.h&gt;
 #include &lt;linux/context_tracking.h&gt;
 #include &lt;linux/user-return-notifier.h&gt;
<span class="p_add">+#include &lt;linux/nospec.h&gt;</span>
 #include &lt;linux/uprobes.h&gt;
 #include &lt;linux/livepatch.h&gt;
 #include &lt;linux/syscalls.h&gt;
<span class="p_chunk">@@ -206,7 +207,7 @@</span> <span class="p_context"> __visible inline void prepare_exit_to_usermode(struct pt_regs *regs)</span>
 	 * special case only applies after poking regs and before the
 	 * very next return to user mode.
 	 */
<span class="p_del">-	current-&gt;thread.status &amp;= ~(TS_COMPAT|TS_I386_REGS_POKED);</span>
<span class="p_add">+	ti-&gt;status &amp;= ~(TS_COMPAT|TS_I386_REGS_POKED);</span>
 #endif
 
 	user_enter_irqoff();
<span class="p_chunk">@@ -282,7 +283,8 @@</span> <span class="p_context"> __visible void do_syscall_64(struct pt_regs *regs)</span>
 	 * regs-&gt;orig_ax, which changes the behavior of some syscalls.
 	 */
 	if (likely((nr &amp; __SYSCALL_MASK) &lt; NR_syscalls)) {
<span class="p_del">-		regs-&gt;ax = sys_call_table[nr &amp; __SYSCALL_MASK](</span>
<span class="p_add">+		nr = array_index_nospec(nr &amp; __SYSCALL_MASK, NR_syscalls);</span>
<span class="p_add">+		regs-&gt;ax = sys_call_table[nr](</span>
 			regs-&gt;di, regs-&gt;si, regs-&gt;dx,
 			regs-&gt;r10, regs-&gt;r8, regs-&gt;r9);
 	}
<span class="p_chunk">@@ -304,7 +306,7 @@</span> <span class="p_context"> static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)</span>
 	unsigned int nr = (unsigned int)regs-&gt;orig_ax;
 
 #ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	current-&gt;thread.status |= TS_COMPAT;</span>
<span class="p_add">+	ti-&gt;status |= TS_COMPAT;</span>
 #endif
 
 	if (READ_ONCE(ti-&gt;flags) &amp; _TIF_WORK_SYSCALL_ENTRY) {
<span class="p_chunk">@@ -318,6 +320,7 @@</span> <span class="p_context"> static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)</span>
 	}
 
 	if (likely(nr &lt; IA32_NR_syscalls)) {
<span class="p_add">+		nr = array_index_nospec(nr, IA32_NR_syscalls);</span>
 		/*
 		 * It&#39;s possible that a 32-bit syscall implementation
 		 * takes a 64-bit parameter but nonetheless assumes that
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index 60c4c342316c..2a35b1e0fb90 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -252,7 +252,8 @@</span> <span class="p_context"> ENTRY(__switch_to_asm)</span>
 	 * exist, overwrite the RSB with entries which capture
 	 * speculative execution to prevent attack.
 	 */
<span class="p_del">-	FILL_RETURN_BUFFER %ebx, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW</span>
<span class="p_add">+	/* Clobbers %ebx */</span>
<span class="p_add">+	FILL_RETURN_BUFFER RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW</span>
 #endif
 
 	/* restore callee-saved registers */
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index ff6f8022612c..c752abe89d80 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -236,91 +236,20 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_64_after_hwframe)</span>
 	pushq	%r9				/* pt_regs-&gt;r9 */
 	pushq	%r10				/* pt_regs-&gt;r10 */
 	pushq	%r11				/* pt_regs-&gt;r11 */
<span class="p_del">-	sub	$(6*8), %rsp			/* pt_regs-&gt;bp, bx, r12-15 not saved */</span>
<span class="p_del">-	UNWIND_HINT_REGS extra=0</span>
<span class="p_del">-</span>
<span class="p_del">-	TRACE_IRQS_OFF</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If we need to do entry work or if we guess we&#39;ll need to do</span>
<span class="p_del">-	 * exit work, go straight to the slow path.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	movq	PER_CPU_VAR(current_task), %r11</span>
<span class="p_del">-	testl	$_TIF_WORK_SYSCALL_ENTRY|_TIF_ALLWORK_MASK, TASK_TI_flags(%r11)</span>
<span class="p_del">-	jnz	entry_SYSCALL64_slow_path</span>
<span class="p_del">-</span>
<span class="p_del">-entry_SYSCALL_64_fastpath:</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Easy case: enable interrupts and issue the syscall.  If the syscall</span>
<span class="p_del">-	 * needs pt_regs, we&#39;ll call a stub that disables interrupts again</span>
<span class="p_del">-	 * and jumps to the slow path.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	TRACE_IRQS_ON</span>
<span class="p_del">-	ENABLE_INTERRUPTS(CLBR_NONE)</span>
<span class="p_del">-#if __SYSCALL_MASK == ~0</span>
<span class="p_del">-	cmpq	$__NR_syscall_max, %rax</span>
<span class="p_del">-#else</span>
<span class="p_del">-	andl	$__SYSCALL_MASK, %eax</span>
<span class="p_del">-	cmpl	$__NR_syscall_max, %eax</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	ja	1f				/* return -ENOSYS (already in pt_regs-&gt;ax) */</span>
<span class="p_del">-	movq	%r10, %rcx</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This call instruction is handled specially in stub_ptregs_64.</span>
<span class="p_del">-	 * It might end up jumping to the slow path.  If it jumps, RAX</span>
<span class="p_del">-	 * and all argument registers are clobbered.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-#ifdef CONFIG_RETPOLINE</span>
<span class="p_del">-	movq	sys_call_table(, %rax, 8), %rax</span>
<span class="p_del">-	call	__x86_indirect_thunk_rax</span>
<span class="p_del">-#else</span>
<span class="p_del">-	call	*sys_call_table(, %rax, 8)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-.Lentry_SYSCALL_64_after_fastpath_call:</span>
<span class="p_del">-</span>
<span class="p_del">-	movq	%rax, RAX(%rsp)</span>
<span class="p_del">-1:</span>
<span class="p_add">+	pushq	%rbx				/* pt_regs-&gt;rbx */</span>
<span class="p_add">+	pushq	%rbp				/* pt_regs-&gt;rbp */</span>
<span class="p_add">+	pushq	%r12				/* pt_regs-&gt;r12 */</span>
<span class="p_add">+	pushq	%r13				/* pt_regs-&gt;r13 */</span>
<span class="p_add">+	pushq	%r14				/* pt_regs-&gt;r14 */</span>
<span class="p_add">+	pushq	%r15				/* pt_regs-&gt;r15 */</span>
<span class="p_add">+	UNWIND_HINT_REGS</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If we get here, then we know that pt_regs is clean for SYSRET64.</span>
<span class="p_del">-	 * If we see that no exit work is required (which we are required</span>
<span class="p_del">-	 * to check with IRQs off), then we can go straight to SYSRET64.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	DISABLE_INTERRUPTS(CLBR_ANY)</span>
 	TRACE_IRQS_OFF
<span class="p_del">-	movq	PER_CPU_VAR(current_task), %r11</span>
<span class="p_del">-	testl	$_TIF_ALLWORK_MASK, TASK_TI_flags(%r11)</span>
<span class="p_del">-	jnz	1f</span>
<span class="p_del">-</span>
<span class="p_del">-	LOCKDEP_SYS_EXIT</span>
<span class="p_del">-	TRACE_IRQS_ON		/* user mode is traced as IRQs on */</span>
<span class="p_del">-	movq	RIP(%rsp), %rcx</span>
<span class="p_del">-	movq	EFLAGS(%rsp), %r11</span>
<span class="p_del">-	addq	$6*8, %rsp	/* skip extra regs -- they were preserved */</span>
<span class="p_del">-	UNWIND_HINT_EMPTY</span>
<span class="p_del">-	jmp	.Lpop_c_regs_except_rcx_r11_and_sysret</span>
 
<span class="p_del">-1:</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The fast path looked good when we started, but something changed</span>
<span class="p_del">-	 * along the way and we need to switch to the slow path.  Calling</span>
<span class="p_del">-	 * raise(3) will trigger this, for example.  IRQs are off.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	TRACE_IRQS_ON</span>
<span class="p_del">-	ENABLE_INTERRUPTS(CLBR_ANY)</span>
<span class="p_del">-	SAVE_EXTRA_REGS</span>
<span class="p_del">-	movq	%rsp, %rdi</span>
<span class="p_del">-	call	syscall_return_slowpath	/* returns with IRQs disabled */</span>
<span class="p_del">-	jmp	return_from_SYSCALL_64</span>
<span class="p_del">-</span>
<span class="p_del">-entry_SYSCALL64_slow_path:</span>
 	/* IRQs are off. */
<span class="p_del">-	SAVE_EXTRA_REGS</span>
 	movq	%rsp, %rdi
 	call	do_syscall_64		/* returns with IRQs disabled */
 
<span class="p_del">-return_from_SYSCALL_64:</span>
 	TRACE_IRQS_IRETQ		/* we&#39;re about to change IF */
 
 	/*
<span class="p_chunk">@@ -393,7 +322,6 @@</span> <span class="p_context"> syscall_return_via_sysret:</span>
 	/* rcx and r11 are already restored (see code above) */
 	UNWIND_HINT_EMPTY
 	POP_EXTRA_REGS
<span class="p_del">-.Lpop_c_regs_except_rcx_r11_and_sysret:</span>
 	popq	%rsi	/* skip r11 */
 	popq	%r10
 	popq	%r9
<span class="p_chunk">@@ -424,47 +352,6 @@</span> <span class="p_context"> syscall_return_via_sysret:</span>
 	USERGS_SYSRET64
 END(entry_SYSCALL_64)
 
<span class="p_del">-ENTRY(stub_ptregs_64)</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Syscalls marked as needing ptregs land here.</span>
<span class="p_del">-	 * If we are on the fast path, we need to save the extra regs,</span>
<span class="p_del">-	 * which we achieve by trying again on the slow path.  If we are on</span>
<span class="p_del">-	 * the slow path, the extra regs are already saved.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * RAX stores a pointer to the C function implementing the syscall.</span>
<span class="p_del">-	 * IRQs are on.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	cmpq	$.Lentry_SYSCALL_64_after_fastpath_call, (%rsp)</span>
<span class="p_del">-	jne	1f</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Called from fast path -- disable IRQs again, pop return address</span>
<span class="p_del">-	 * and jump to slow path</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	DISABLE_INTERRUPTS(CLBR_ANY)</span>
<span class="p_del">-	TRACE_IRQS_OFF</span>
<span class="p_del">-	popq	%rax</span>
<span class="p_del">-	UNWIND_HINT_REGS extra=0</span>
<span class="p_del">-	jmp	entry_SYSCALL64_slow_path</span>
<span class="p_del">-</span>
<span class="p_del">-1:</span>
<span class="p_del">-	JMP_NOSPEC %rax				/* Called from C */</span>
<span class="p_del">-END(stub_ptregs_64)</span>
<span class="p_del">-</span>
<span class="p_del">-.macro ptregs_stub func</span>
<span class="p_del">-ENTRY(ptregs_\func)</span>
<span class="p_del">-	UNWIND_HINT_FUNC</span>
<span class="p_del">-	leaq	\func(%rip), %rax</span>
<span class="p_del">-	jmp	stub_ptregs_64</span>
<span class="p_del">-END(ptregs_\func)</span>
<span class="p_del">-.endm</span>
<span class="p_del">-</span>
<span class="p_del">-/* Instantiate ptregs_stub for each ptregs-using syscall */</span>
<span class="p_del">-#define __SYSCALL_64_QUAL_(sym)</span>
<span class="p_del">-#define __SYSCALL_64_QUAL_ptregs(sym) ptregs_stub sym</span>
<span class="p_del">-#define __SYSCALL_64(nr, sym, qual) __SYSCALL_64_QUAL_##qual(sym)</span>
<span class="p_del">-#include &lt;asm/syscalls_64.h&gt;</span>
<span class="p_del">-</span>
 /*
  * %rdi: prev task
  * %rsi: next task
<span class="p_chunk">@@ -499,7 +386,8 @@</span> <span class="p_context"> ENTRY(__switch_to_asm)</span>
 	 * exist, overwrite the RSB with entries which capture
 	 * speculative execution to prevent attack.
 	 */
<span class="p_del">-	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW</span>
<span class="p_add">+	/* Clobbers %rbx */</span>
<span class="p_add">+	FILL_RETURN_BUFFER RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW</span>
 #endif
 
 	/* restore callee-saved registers */
<span class="p_header">diff --git a/arch/x86/entry/syscall_64.c b/arch/x86/entry/syscall_64.c</span>
<span class="p_header">index 9c09775e589d..c176d2fab1da 100644</span>
<span class="p_header">--- a/arch/x86/entry/syscall_64.c</span>
<span class="p_header">+++ b/arch/x86/entry/syscall_64.c</span>
<span class="p_chunk">@@ -7,14 +7,11 @@</span> <span class="p_context"></span>
 #include &lt;asm/asm-offsets.h&gt;
 #include &lt;asm/syscall.h&gt;
 
<span class="p_del">-#define __SYSCALL_64_QUAL_(sym) sym</span>
<span class="p_del">-#define __SYSCALL_64_QUAL_ptregs(sym) ptregs_##sym</span>
<span class="p_del">-</span>
<span class="p_del">-#define __SYSCALL_64(nr, sym, qual) extern asmlinkage long __SYSCALL_64_QUAL_##qual(sym)(unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long);</span>
<span class="p_add">+#define __SYSCALL_64(nr, sym, qual) extern asmlinkage long sym(unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long);</span>
 #include &lt;asm/syscalls_64.h&gt;
 #undef __SYSCALL_64
 
<span class="p_del">-#define __SYSCALL_64(nr, sym, qual) [nr] = __SYSCALL_64_QUAL_##qual(sym),</span>
<span class="p_add">+#define __SYSCALL_64(nr, sym, qual) [nr] = sym,</span>
 
 extern long sys_ni_syscall(unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long);
 
<span class="p_header">diff --git a/arch/x86/include/asm/asm-prototypes.h b/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_header">index 1908214b9125..4d111616524b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_chunk">@@ -38,4 +38,7 @@</span> <span class="p_context"> INDIRECT_THUNK(dx)</span>
 INDIRECT_THUNK(si)
 INDIRECT_THUNK(di)
 INDIRECT_THUNK(bp)
<span class="p_add">+asmlinkage void __fill_rsb(void);</span>
<span class="p_add">+asmlinkage void __clear_rsb(void);</span>
<span class="p_add">+</span>
 #endif /* CONFIG_RETPOLINE */
<span class="p_header">diff --git a/arch/x86/include/asm/barrier.h b/arch/x86/include/asm/barrier.h</span>
<span class="p_header">index 7fb336210e1b..30d406146016 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/barrier.h</span>
<span class="p_chunk">@@ -24,6 +24,34 @@</span> <span class="p_context"></span>
 #define wmb()	asm volatile(&quot;sfence&quot; ::: &quot;memory&quot;)
 #endif
 
<span class="p_add">+/**</span>
<span class="p_add">+ * array_index_mask_nospec() - generate a mask that is ~0UL when the</span>
<span class="p_add">+ * 	bounds check succeeds and 0 otherwise</span>
<span class="p_add">+ * @index: array element index</span>
<span class="p_add">+ * @size: number of elements in array</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns:</span>
<span class="p_add">+ *     0 - (index &lt; size)</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long array_index_mask_nospec(unsigned long index,</span>
<span class="p_add">+		unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm (&quot;cmp %1,%2; sbb %0,%0;&quot;</span>
<span class="p_add">+			:&quot;=r&quot; (mask)</span>
<span class="p_add">+			:&quot;r&quot;(size),&quot;r&quot; (index)</span>
<span class="p_add">+			:&quot;cc&quot;);</span>
<span class="p_add">+	return mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Override the default implementation from linux/nospec.h. */</span>
<span class="p_add">+#define array_index_mask_nospec array_index_mask_nospec</span>
<span class="p_add">+</span>
<span class="p_add">+/* Prevent speculative execution past this barrier. */</span>
<span class="p_add">+#define barrier_nospec() alternative_2(&quot;&quot;, &quot;mfence&quot;, X86_FEATURE_MFENCE_RDTSC, \</span>
<span class="p_add">+					   &quot;lfence&quot;, X86_FEATURE_LFENCE_RDTSC)</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_PPRO_FENCE
 #define dma_rmb()	rmb()
 #else
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index ea9a7dde62e5..70eddb3922ff 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -29,6 +29,7 @@</span> <span class="p_context"> enum cpuid_leafs</span>
 	CPUID_8000_000A_EDX,
 	CPUID_7_ECX,
 	CPUID_8000_0007_EBX,
<span class="p_add">+	CPUID_7_EDX,</span>
 };
 
 #ifdef CONFIG_X86_FEATURE_NAMES
<span class="p_chunk">@@ -79,8 +80,9 @@</span> <span class="p_context"> extern const char * const x86_bug_flags[NBUGINTS*32];</span>
 	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 15, feature_bit) ||	\
 	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 16, feature_bit) ||	\
 	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 17, feature_bit) ||	\
<span class="p_add">+	   CHECK_BIT_IN_MASK_WORD(REQUIRED_MASK, 18, feature_bit) ||	\</span>
 	   REQUIRED_MASK_CHECK					  ||	\
<span class="p_del">-	   BUILD_BUG_ON_ZERO(NCAPINTS != 18))</span>
<span class="p_add">+	   BUILD_BUG_ON_ZERO(NCAPINTS != 19))</span>
 
 #define DISABLED_MASK_BIT_SET(feature_bit)				\
 	 ( CHECK_BIT_IN_MASK_WORD(DISABLED_MASK,  0, feature_bit) ||	\
<span class="p_chunk">@@ -101,8 +103,9 @@</span> <span class="p_context"> extern const char * const x86_bug_flags[NBUGINTS*32];</span>
 	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 15, feature_bit) ||	\
 	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 16, feature_bit) ||	\
 	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 17, feature_bit) ||	\
<span class="p_add">+	   CHECK_BIT_IN_MASK_WORD(DISABLED_MASK, 18, feature_bit) ||	\</span>
 	   DISABLED_MASK_CHECK					  ||	\
<span class="p_del">-	   BUILD_BUG_ON_ZERO(NCAPINTS != 18))</span>
<span class="p_add">+	   BUILD_BUG_ON_ZERO(NCAPINTS != 19))</span>
 
 #define cpu_has(c, bit)							\
 	(__builtin_constant_p(bit) &amp;&amp; REQUIRED_MASK_BIT_SET(bit) ? 1 :	\
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 25b9375c1484..73b5fff159a4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -13,7 +13,7 @@</span> <span class="p_context"></span>
 /*
  * Defines x86 CPU feature bits
  */
<span class="p_del">-#define NCAPINTS			18	   /* N 32-bit words worth of info */</span>
<span class="p_add">+#define NCAPINTS			19	   /* N 32-bit words worth of info */</span>
 #define NBUGINTS			1	   /* N 32-bit bug flags */
 
 /*
<span class="p_chunk">@@ -203,14 +203,14 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 #define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
 #define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
<span class="p_del">-#define X86_FEATURE_RETPOLINE		( 7*32+12) /* Generic Retpoline mitigation for Spectre variant 2 */</span>
<span class="p_del">-#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */</span>
<span class="p_add">+#define X86_FEATURE_RETPOLINE		( 7*32+12) /* &quot;&quot; Generic Retpoline mitigation for Spectre variant 2 */</span>
<span class="p_add">+#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* &quot;&quot; AMD Retpoline mitigation for Spectre variant 2 */</span>
 #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
<span class="p_del">-#define X86_FEATURE_AVX512_4VNNIW	( 7*32+16) /* AVX-512 Neural Network Instructions */</span>
<span class="p_del">-#define X86_FEATURE_AVX512_4FMAPS	( 7*32+17) /* AVX-512 Multiply Accumulation Single precision */</span>
 
 #define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
<span class="p_del">-#define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* Fill RSB on context switches */</span>
<span class="p_add">+#define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* &quot;&quot; Fill RSB on context switches */</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_FEATURE_USE_IBPB		( 7*32+21) /* &quot;&quot; Indirect Branch Prediction Barrier enabled */</span>
 
 /* Virtualization flags: Linux defined, word 8 */
 #define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
<span class="p_chunk">@@ -271,6 +271,9 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 #define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 #define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
<span class="p_add">+#define X86_FEATURE_IBPB		(13*32+12) /* Indirect Branch Prediction Barrier */</span>
<span class="p_add">+#define X86_FEATURE_IBRS		(13*32+14) /* Indirect Branch Restricted Speculation */</span>
<span class="p_add">+#define X86_FEATURE_STIBP		(13*32+15) /* Single Thread Indirect Branch Predictors */</span>
 
 /* Thermal and Power Management Leaf, CPUID level 0x00000006 (EAX), word 14 */
 #define X86_FEATURE_DTHERM		(14*32+ 0) /* Digital Thermal Sensor */
<span class="p_chunk">@@ -319,6 +322,13 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_SUCCOR		(17*32+ 1) /* Uncorrectable error containment and recovery */
 #define X86_FEATURE_SMCA		(17*32+ 3) /* Scalable MCA */
 
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000007:0 (EDX), word 18 */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_4VNNIW	(18*32+ 2) /* AVX-512 Neural Network Instructions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_4FMAPS	(18*32+ 3) /* AVX-512 Multiply Accumulation Single precision */</span>
<span class="p_add">+#define X86_FEATURE_SPEC_CTRL		(18*32+26) /* &quot;&quot; Speculation Control (IBRS + IBPB) */</span>
<span class="p_add">+#define X86_FEATURE_INTEL_STIBP		(18*32+27) /* &quot;&quot; Single Thread Indirect Branch Predictors */</span>
<span class="p_add">+#define X86_FEATURE_ARCH_CAPABILITIES	(18*32+29) /* IA32_ARCH_CAPABILITIES MSR (Intel) */</span>
<span class="p_add">+</span>
 /*
  * BUG word(s)
  */
<span class="p_header">diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h</span>
<span class="p_header">index b027633e7300..33833d1909af 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/disabled-features.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/disabled-features.h</span>
<span class="p_chunk">@@ -77,6 +77,7 @@</span> <span class="p_context"></span>
 #define DISABLED_MASK15	0
 #define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP)
 #define DISABLED_MASK17	0
<span class="p_del">-#define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 18)</span>
<span class="p_add">+#define DISABLED_MASK18	0</span>
<span class="p_add">+#define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)</span>
 
 #endif /* _ASM_X86_DISABLED_FEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/fixmap.h b/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">index 64c4a30e0d39..e203169931c7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -137,8 +137,10 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 
 extern void reserve_top_address(unsigned long reserve);
 
<span class="p_del">-#define FIXADDR_SIZE	(__end_of_permanent_fixed_addresses &lt;&lt; PAGE_SHIFT)</span>
<span class="p_del">-#define FIXADDR_START	(FIXADDR_TOP - FIXADDR_SIZE)</span>
<span class="p_add">+#define FIXADDR_SIZE		(__end_of_permanent_fixed_addresses &lt;&lt; PAGE_SHIFT)</span>
<span class="p_add">+#define FIXADDR_START		(FIXADDR_TOP - FIXADDR_SIZE)</span>
<span class="p_add">+#define FIXADDR_TOT_SIZE	(__end_of_fixed_addresses &lt;&lt; PAGE_SHIFT)</span>
<span class="p_add">+#define FIXADDR_TOT_START	(FIXADDR_TOP - FIXADDR_TOT_SIZE)</span>
 
 extern int fixmaps_set;
 
<span class="p_header">diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h</span>
<span class="p_header">index e7b983a35506..e520a1e6fc11 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/msr-index.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/msr-index.h</span>
<span class="p_chunk">@@ -39,6 +39,13 @@</span> <span class="p_context"></span>
 
 /* Intel MSRs. Some also available on other CPUs */
 
<span class="p_add">+#define MSR_IA32_SPEC_CTRL		0x00000048 /* Speculation Control */</span>
<span class="p_add">+#define SPEC_CTRL_IBRS			(1 &lt;&lt; 0)   /* Indirect Branch Restricted Speculation */</span>
<span class="p_add">+#define SPEC_CTRL_STIBP			(1 &lt;&lt; 1)   /* Single Thread Indirect Branch Predictors */</span>
<span class="p_add">+</span>
<span class="p_add">+#define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */</span>
<span class="p_add">+#define PRED_CMD_IBPB			(1 &lt;&lt; 0)   /* Indirect Branch Prediction Barrier */</span>
<span class="p_add">+</span>
 #define MSR_PPIN_CTL			0x0000004e
 #define MSR_PPIN			0x0000004f
 
<span class="p_chunk">@@ -57,6 +64,11 @@</span> <span class="p_context"></span>
 #define SNB_C3_AUTO_UNDEMOTE		(1UL &lt;&lt; 28)
 
 #define MSR_MTRRcap			0x000000fe
<span class="p_add">+</span>
<span class="p_add">+#define MSR_IA32_ARCH_CAPABILITIES	0x0000010a</span>
<span class="p_add">+#define ARCH_CAP_RDCL_NO		(1 &lt;&lt; 0)   /* Not susceptible to Meltdown */</span>
<span class="p_add">+#define ARCH_CAP_IBRS_ALL		(1 &lt;&lt; 1)   /* Enhanced IBRS support */</span>
<span class="p_add">+</span>
 #define MSR_IA32_BBL_CR_CTL		0x00000119
 #define MSR_IA32_BBL_CR_CTL3		0x0000011e
 
<span class="p_header">diff --git a/arch/x86/include/asm/msr.h b/arch/x86/include/asm/msr.h</span>
<span class="p_header">index 07962f5f6fba..30df295f6d94 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/msr.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/msr.h</span>
<span class="p_chunk">@@ -214,8 +214,7 @@</span> <span class="p_context"> static __always_inline unsigned long long rdtsc_ordered(void)</span>
 	 * that some other imaginary CPU is updating continuously with a
 	 * time stamp.
 	 */
<span class="p_del">-	alternative_2(&quot;&quot;, &quot;mfence&quot;, X86_FEATURE_MFENCE_RDTSC,</span>
<span class="p_del">-			  &quot;lfence&quot;, X86_FEATURE_LFENCE_RDTSC);</span>
<span class="p_add">+	barrier_nospec();</span>
 	return rdtsc();
 }
 
<span class="p_header">diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_header">index 4ad41087ce0e..4d57894635f2 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_chunk">@@ -1,56 +1,12 @@</span> <span class="p_context"></span>
 /* SPDX-License-Identifier: GPL-2.0 */
 
<span class="p_del">-#ifndef __NOSPEC_BRANCH_H__</span>
<span class="p_del">-#define __NOSPEC_BRANCH_H__</span>
<span class="p_add">+#ifndef _ASM_X86_NOSPEC_BRANCH_H_</span>
<span class="p_add">+#define _ASM_X86_NOSPEC_BRANCH_H_</span>
 
 #include &lt;asm/alternative.h&gt;
 #include &lt;asm/alternative-asm.h&gt;
 #include &lt;asm/cpufeatures.h&gt;
 
<span class="p_del">-/*</span>
<span class="p_del">- * Fill the CPU return stack buffer.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Each entry in the RSB, if used for a speculative &#39;ret&#39;, contains an</span>
<span class="p_del">- * infinite &#39;pause; lfence; jmp&#39; loop to capture speculative execution.</span>
<span class="p_del">- *</span>
<span class="p_del">- * This is required in various cases for retpoline and IBRS-based</span>
<span class="p_del">- * mitigations for the Spectre variant 2 vulnerability. Sometimes to</span>
<span class="p_del">- * eliminate potentially bogus entries from the RSB, and sometimes</span>
<span class="p_del">- * purely to ensure that it doesn&#39;t get empty, which on some CPUs would</span>
<span class="p_del">- * allow predictions from other (unwanted!) sources to be used.</span>
<span class="p_del">- *</span>
<span class="p_del">- * We define a CPP macro such that it can be used from both .S files and</span>
<span class="p_del">- * inline assembly. It&#39;s possible to do a .macro and then include that</span>
<span class="p_del">- * from C via asm(&quot;.include &lt;asm/nospec-branch.h&gt;&quot;) but let&#39;s not go there.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-#define RSB_CLEAR_LOOPS		32	/* To forcibly overwrite all entries */</span>
<span class="p_del">-#define RSB_FILL_LOOPS		16	/* To avoid underflow */</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Google experimented with loop-unrolling and this turned out to be</span>
<span class="p_del">- * the optimal version  two calls, each with their own speculation</span>
<span class="p_del">- * trap should their return address end up getting used, in a loop.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define __FILL_RETURN_BUFFER(reg, nr, sp)	\</span>
<span class="p_del">-	mov	$(nr/2), reg;			\</span>
<span class="p_del">-771:						\</span>
<span class="p_del">-	call	772f;				\</span>
<span class="p_del">-773:	/* speculation trap */			\</span>
<span class="p_del">-	pause;					\</span>
<span class="p_del">-	lfence;					\</span>
<span class="p_del">-	jmp	773b;				\</span>
<span class="p_del">-772:						\</span>
<span class="p_del">-	call	774f;				\</span>
<span class="p_del">-775:	/* speculation trap */			\</span>
<span class="p_del">-	pause;					\</span>
<span class="p_del">-	lfence;					\</span>
<span class="p_del">-	jmp	775b;				\</span>
<span class="p_del">-774:						\</span>
<span class="p_del">-	dec	reg;				\</span>
<span class="p_del">-	jnz	771b;				\</span>
<span class="p_del">-	add	$(BITS_PER_LONG/8) * nr, sp;</span>
<span class="p_del">-</span>
 #ifdef __ASSEMBLY__
 
 /*
<span class="p_chunk">@@ -121,17 +77,10 @@</span> <span class="p_context"></span>
 #endif
 .endm
 
<span class="p_del">- /*</span>
<span class="p_del">-  * A simpler FILL_RETURN_BUFFER macro. Don&#39;t make people use the CPP</span>
<span class="p_del">-  * monstrosity above, manually.</span>
<span class="p_del">-  */</span>
<span class="p_del">-.macro FILL_RETURN_BUFFER reg:req nr:req ftr:req</span>
<span class="p_add">+/* This clobbers the BX register */</span>
<span class="p_add">+.macro FILL_RETURN_BUFFER nr:req ftr:req</span>
 #ifdef CONFIG_RETPOLINE
<span class="p_del">-	ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_del">-	ALTERNATIVE &quot;jmp .Lskip_rsb_\@&quot;,				\</span>
<span class="p_del">-		__stringify(__FILL_RETURN_BUFFER(\reg,\nr,%_ASM_SP))	\</span>
<span class="p_del">-		\ftr</span>
<span class="p_del">-.Lskip_rsb_\@:</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;call __clear_rsb&quot;, \ftr</span>
 #endif
 .endm
 
<span class="p_chunk">@@ -201,22 +150,25 @@</span> <span class="p_context"> extern char __indirect_thunk_end[];</span>
  * On VMEXIT we must ensure that no RSB predictions learned in the guest
  * can be followed in the host, by overwriting the RSB completely. Both
  * retpoline and IBRS mitigations for Spectre v2 need this; only on future
<span class="p_del">- * CPUs with IBRS_ATT *might* it be avoided.</span>
<span class="p_add">+ * CPUs with IBRS_ALL *might* it be avoided.</span>
  */
 static inline void vmexit_fill_RSB(void)
 {
 #ifdef CONFIG_RETPOLINE
<span class="p_del">-	unsigned long loops;</span>
<span class="p_del">-</span>
<span class="p_del">-	asm volatile (ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_del">-		      ALTERNATIVE(&quot;jmp 910f&quot;,</span>
<span class="p_del">-				  __stringify(__FILL_RETURN_BUFFER(%0, RSB_CLEAR_LOOPS, %1)),</span>
<span class="p_del">-				  X86_FEATURE_RETPOLINE)</span>
<span class="p_del">-		      &quot;910:&quot;</span>
<span class="p_del">-		      : &quot;=r&quot; (loops), ASM_CALL_CONSTRAINT</span>
<span class="p_del">-		      : : &quot;memory&quot; );</span>
<span class="p_add">+	alternative_input(&quot;&quot;,</span>
<span class="p_add">+			  &quot;call __fill_rsb&quot;,</span>
<span class="p_add">+			  X86_FEATURE_RETPOLINE,</span>
<span class="p_add">+			  ASM_NO_INPUT_CLOBBER(_ASM_BX, &quot;memory&quot;));</span>
 #endif
 }
 
<span class="p_add">+static inline void indirect_branch_prediction_barrier(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	alternative_input(&quot;&quot;,</span>
<span class="p_add">+			  &quot;call __ibp_barrier&quot;,</span>
<span class="p_add">+			  X86_FEATURE_USE_IBPB,</span>
<span class="p_add">+			  ASM_NO_INPUT_CLOBBER(&quot;eax&quot;, &quot;ecx&quot;, &quot;edx&quot;, &quot;memory&quot;));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* __ASSEMBLY__ */
<span class="p_del">-#endif /* __NOSPEC_BRANCH_H__ */</span>
<span class="p_add">+#endif /* _ASM_X86_NOSPEC_BRANCH_H_ */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_header">index ce245b0cdfca..0777e18a1d23 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_chunk">@@ -44,8 +44,9 @@</span> <span class="p_context"> extern bool __vmalloc_start_set; /* set once high_memory is set */</span>
  */
 #define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 40)
 
<span class="p_del">-#define CPU_ENTRY_AREA_BASE				\</span>
<span class="p_del">-	((FIXADDR_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) &amp; PMD_MASK)</span>
<span class="p_add">+#define CPU_ENTRY_AREA_BASE						\</span>
<span class="p_add">+	((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1))   \</span>
<span class="p_add">+	 &amp; PMD_MASK)</span>
 
 #define PKMAP_BASE		\
 	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) &amp; PMD_MASK)
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index d3a67fba200a..513f9604c192 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -460,8 +460,6 @@</span> <span class="p_context"> struct thread_struct {</span>
 	unsigned short		gsindex;
 #endif
 
<span class="p_del">-	u32			status;		/* thread synchronous flags */</span>
<span class="p_del">-</span>
 #ifdef CONFIG_X86_64
 	unsigned long		fsbase;
 	unsigned long		gsbase;
<span class="p_chunk">@@ -971,4 +969,7 @@</span> <span class="p_context"> bool xen_set_default_idle(void);</span>
 
 void stop_this_cpu(void *dummy);
 void df_debug(struct pt_regs *regs, long error_code);
<span class="p_add">+</span>
<span class="p_add">+void __ibp_barrier(void);</span>
<span class="p_add">+</span>
 #endif /* _ASM_X86_PROCESSOR_H */
<span class="p_header">diff --git a/arch/x86/include/asm/required-features.h b/arch/x86/include/asm/required-features.h</span>
<span class="p_header">index d91ba04dd007..fb3a6de7440b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/required-features.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/required-features.h</span>
<span class="p_chunk">@@ -106,6 +106,7 @@</span> <span class="p_context"></span>
 #define REQUIRED_MASK15	0
 #define REQUIRED_MASK16	(NEED_LA57)
 #define REQUIRED_MASK17	0
<span class="p_del">-#define REQUIRED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 18)</span>
<span class="p_add">+#define REQUIRED_MASK18	0</span>
<span class="p_add">+#define REQUIRED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)</span>
 
 #endif /* _ASM_X86_REQUIRED_FEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/syscall.h b/arch/x86/include/asm/syscall.h</span>
<span class="p_header">index e3c95e8e61c5..03eedc21246d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/syscall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/syscall.h</span>
<span class="p_chunk">@@ -60,7 +60,7 @@</span> <span class="p_context"> static inline long syscall_get_error(struct task_struct *task,</span>
 	 * TS_COMPAT is set for 32-bit syscall entries and then
 	 * remains set until we return to user mode.
 	 */
<span class="p_del">-	if (task-&gt;thread.status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
<span class="p_add">+	if (task-&gt;thread_info.status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
 		/*
 		 * Sign-extend the value so (int)-EFOO becomes (long)-EFOO
 		 * and will match correctly in comparisons.
<span class="p_chunk">@@ -116,7 +116,7 @@</span> <span class="p_context"> static inline void syscall_get_arguments(struct task_struct *task,</span>
 					 unsigned long *args)
 {
 # ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	if (task-&gt;thread.status &amp; TS_COMPAT)</span>
<span class="p_add">+	if (task-&gt;thread_info.status &amp; TS_COMPAT)</span>
 		switch (i) {
 		case 0:
 			if (!n--) break;
<span class="p_chunk">@@ -177,7 +177,7 @@</span> <span class="p_context"> static inline void syscall_set_arguments(struct task_struct *task,</span>
 					 const unsigned long *args)
 {
 # ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	if (task-&gt;thread.status &amp; TS_COMPAT)</span>
<span class="p_add">+	if (task-&gt;thread_info.status &amp; TS_COMPAT)</span>
 		switch (i) {
 		case 0:
 			if (!n--) break;
<span class="p_header">diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">index 00223333821a..eda3b6823ca4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -55,6 +55,7 @@</span> <span class="p_context"> struct task_struct;</span>
 
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
<span class="p_add">+	u32			status;		/* thread synchronous flags */</span>
 };
 
 #define INIT_THREAD_INFO(tsk)			\
<span class="p_chunk">@@ -221,7 +222,7 @@</span> <span class="p_context"> static inline int arch_within_stack_frames(const void * const stack,</span>
 #define in_ia32_syscall() true
 #else
 #define in_ia32_syscall() (IS_ENABLED(CONFIG_IA32_EMULATION) &amp;&amp; \
<span class="p_del">-			   current-&gt;thread.status &amp; TS_COMPAT)</span>
<span class="p_add">+			   current_thread_info()-&gt;status &amp; TS_COMPAT)</span>
 #endif
 
 /*
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index d33e4a26dc7e..2b8f18ca5874 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -174,6 +174,8 @@</span> <span class="p_context"> struct tlb_state {</span>
 	struct mm_struct *loaded_mm;
 	u16 loaded_mm_asid;
 	u16 next_asid;
<span class="p_add">+	/* last user mm&#39;s ctx id */</span>
<span class="p_add">+	u64 last_ctx_id;</span>
 
 	/*
 	 * We can be in one of several states:
<span class="p_header">diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">index 574dff4d2913..aae77eb8491c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -124,6 +124,11 @@</span> <span class="p_context"> extern int __get_user_bad(void);</span>
 
 #define __uaccess_begin() stac()
 #define __uaccess_end()   clac()
<span class="p_add">+#define __uaccess_begin_nospec()	\</span>
<span class="p_add">+({					\</span>
<span class="p_add">+	stac();				\</span>
<span class="p_add">+	barrier_nospec();		\</span>
<span class="p_add">+})</span>
 
 /*
  * This is a type: either unsigned long, if the argument fits into
<span class="p_chunk">@@ -445,7 +450,7 @@</span> <span class="p_context"> do {									\</span>
 ({									\
 	int __gu_err;							\
 	__inttype(*(ptr)) __gu_val;					\
<span class="p_del">-	__uaccess_begin();						\</span>
<span class="p_add">+	__uaccess_begin_nospec();					\</span>
 	__get_user_size(__gu_val, (ptr), (size), __gu_err, -EFAULT);	\
 	__uaccess_end();						\
 	(x) = (__force __typeof__(*(ptr)))__gu_val;			\
<span class="p_chunk">@@ -487,6 +492,10 @@</span> <span class="p_context"> struct __large_struct { unsigned long buf[100]; };</span>
 	__uaccess_begin();						\
 	barrier();
 
<span class="p_add">+#define uaccess_try_nospec do {						\</span>
<span class="p_add">+	current-&gt;thread.uaccess_err = 0;				\</span>
<span class="p_add">+	__uaccess_begin_nospec();					\</span>
<span class="p_add">+</span>
 #define uaccess_catch(err)						\
 	__uaccess_end();						\
 	(err) |= (current-&gt;thread.uaccess_err ? -EFAULT : 0);		\
<span class="p_chunk">@@ -548,7 +557,7 @@</span> <span class="p_context"> struct __large_struct { unsigned long buf[100]; };</span>
  *	get_user_ex(...);
  * } get_user_catch(err)
  */
<span class="p_del">-#define get_user_try		uaccess_try</span>
<span class="p_add">+#define get_user_try		uaccess_try_nospec</span>
 #define get_user_catch(err)	uaccess_catch(err)
 
 #define get_user_ex(x, ptr)	do {					\
<span class="p_chunk">@@ -582,7 +591,7 @@</span> <span class="p_context"> extern void __cmpxchg_wrong_size(void)</span>
 	__typeof__(ptr) __uval = (uval);				\
 	__typeof__(*(ptr)) __old = (old);				\
 	__typeof__(*(ptr)) __new = (new);				\
<span class="p_del">-	__uaccess_begin();						\</span>
<span class="p_add">+	__uaccess_begin_nospec();					\</span>
 	switch (size) {							\
 	case 1:								\
 	{								\
<span class="p_header">diff --git a/arch/x86/include/asm/uaccess_32.h b/arch/x86/include/asm/uaccess_32.h</span>
<span class="p_header">index 72950401b223..ba2dc1930630 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uaccess_32.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uaccess_32.h</span>
<span class="p_chunk">@@ -29,21 +29,21 @@</span> <span class="p_context"> raw_copy_from_user(void *to, const void __user *from, unsigned long n)</span>
 		switch (n) {
 		case 1:
 			ret = 0;
<span class="p_del">-			__uaccess_begin();</span>
<span class="p_add">+			__uaccess_begin_nospec();</span>
 			__get_user_asm_nozero(*(u8 *)to, from, ret,
 					      &quot;b&quot;, &quot;b&quot;, &quot;=q&quot;, 1);
 			__uaccess_end();
 			return ret;
 		case 2:
 			ret = 0;
<span class="p_del">-			__uaccess_begin();</span>
<span class="p_add">+			__uaccess_begin_nospec();</span>
 			__get_user_asm_nozero(*(u16 *)to, from, ret,
 					      &quot;w&quot;, &quot;w&quot;, &quot;=r&quot;, 2);
 			__uaccess_end();
 			return ret;
 		case 4:
 			ret = 0;
<span class="p_del">-			__uaccess_begin();</span>
<span class="p_add">+			__uaccess_begin_nospec();</span>
 			__get_user_asm_nozero(*(u32 *)to, from, ret,
 					      &quot;l&quot;, &quot;k&quot;, &quot;=r&quot;, 4);
 			__uaccess_end();
<span class="p_header">diff --git a/arch/x86/include/asm/uaccess_64.h b/arch/x86/include/asm/uaccess_64.h</span>
<span class="p_header">index f07ef3c575db..62546b3a398e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uaccess_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uaccess_64.h</span>
<span class="p_chunk">@@ -55,31 +55,31 @@</span> <span class="p_context"> raw_copy_from_user(void *dst, const void __user *src, unsigned long size)</span>
 		return copy_user_generic(dst, (__force void *)src, size);
 	switch (size) {
 	case 1:
<span class="p_del">-		__uaccess_begin();</span>
<span class="p_add">+		__uaccess_begin_nospec();</span>
 		__get_user_asm_nozero(*(u8 *)dst, (u8 __user *)src,
 			      ret, &quot;b&quot;, &quot;b&quot;, &quot;=q&quot;, 1);
 		__uaccess_end();
 		return ret;
 	case 2:
<span class="p_del">-		__uaccess_begin();</span>
<span class="p_add">+		__uaccess_begin_nospec();</span>
 		__get_user_asm_nozero(*(u16 *)dst, (u16 __user *)src,
 			      ret, &quot;w&quot;, &quot;w&quot;, &quot;=r&quot;, 2);
 		__uaccess_end();
 		return ret;
 	case 4:
<span class="p_del">-		__uaccess_begin();</span>
<span class="p_add">+		__uaccess_begin_nospec();</span>
 		__get_user_asm_nozero(*(u32 *)dst, (u32 __user *)src,
 			      ret, &quot;l&quot;, &quot;k&quot;, &quot;=r&quot;, 4);
 		__uaccess_end();
 		return ret;
 	case 8:
<span class="p_del">-		__uaccess_begin();</span>
<span class="p_add">+		__uaccess_begin_nospec();</span>
 		__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,
 			      ret, &quot;q&quot;, &quot;&quot;, &quot;=r&quot;, 8);
 		__uaccess_end();
 		return ret;
 	case 10:
<span class="p_del">-		__uaccess_begin();</span>
<span class="p_add">+		__uaccess_begin_nospec();</span>
 		__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,
 			       ret, &quot;q&quot;, &quot;&quot;, &quot;=r&quot;, 10);
 		if (likely(!ret))
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> raw_copy_from_user(void *dst, const void __user *src, unsigned long size)</span>
 		__uaccess_end();
 		return ret;
 	case 16:
<span class="p_del">-		__uaccess_begin();</span>
<span class="p_add">+		__uaccess_begin_nospec();</span>
 		__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,
 			       ret, &quot;q&quot;, &quot;&quot;, &quot;=r&quot;, 16);
 		if (likely(!ret))
<span class="p_header">diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c</span>
<span class="p_header">index 4817d743c263..a481763a3776 100644</span>
<span class="p_header">--- a/arch/x86/kernel/alternative.c</span>
<span class="p_header">+++ b/arch/x86/kernel/alternative.c</span>
<span class="p_chunk">@@ -46,17 +46,6 @@</span> <span class="p_context"> static int __init setup_noreplace_smp(char *str)</span>
 }
 __setup(&quot;noreplace-smp&quot;, setup_noreplace_smp);
 
<span class="p_del">-#ifdef CONFIG_PARAVIRT</span>
<span class="p_del">-static int __initdata_or_module noreplace_paravirt = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init setup_noreplace_paravirt(char *str)</span>
<span class="p_del">-{</span>
<span class="p_del">-	noreplace_paravirt = 1;</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-__setup(&quot;noreplace-paravirt&quot;, setup_noreplace_paravirt);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 #define DPRINTK(fmt, args...)						\
 do {									\
 	if (debug_alternative)						\
<span class="p_chunk">@@ -298,7 +287,7 @@</span> <span class="p_context"> recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insnbuf)</span>
 	tgt_rip  = next_rip + o_dspl;
 	n_dspl = tgt_rip - orig_insn;
 
<span class="p_del">-	DPRINTK(&quot;target RIP: %p, new_displ: 0x%x&quot;, tgt_rip, n_dspl);</span>
<span class="p_add">+	DPRINTK(&quot;target RIP: %px, new_displ: 0x%x&quot;, tgt_rip, n_dspl);</span>
 
 	if (tgt_rip - orig_insn &gt;= 0) {
 		if (n_dspl - 2 &lt;= 127)
<span class="p_chunk">@@ -355,7 +344,7 @@</span> <span class="p_context"> static void __init_or_module noinline optimize_nops(struct alt_instr *a, u8 *ins</span>
 	add_nops(instr + (a-&gt;instrlen - a-&gt;padlen), a-&gt;padlen);
 	local_irq_restore(flags);
 
<span class="p_del">-	DUMP_BYTES(instr, a-&gt;instrlen, &quot;%p: [%d:%d) optimized NOPs: &quot;,</span>
<span class="p_add">+	DUMP_BYTES(instr, a-&gt;instrlen, &quot;%px: [%d:%d) optimized NOPs: &quot;,</span>
 		   instr, a-&gt;instrlen - a-&gt;padlen, a-&gt;padlen);
 }
 
<span class="p_chunk">@@ -376,7 +365,7 @@</span> <span class="p_context"> void __init_or_module noinline apply_alternatives(struct alt_instr *start,</span>
 	u8 *instr, *replacement;
 	u8 insnbuf[MAX_PATCH_LEN];
 
<span class="p_del">-	DPRINTK(&quot;alt table %p -&gt; %p&quot;, start, end);</span>
<span class="p_add">+	DPRINTK(&quot;alt table %px, -&gt; %px&quot;, start, end);</span>
 	/*
 	 * The scan order should be from start to end. A later scanned
 	 * alternative code can overwrite previously scanned alternative code.
<span class="p_chunk">@@ -400,14 +389,14 @@</span> <span class="p_context"> void __init_or_module noinline apply_alternatives(struct alt_instr *start,</span>
 			continue;
 		}
 
<span class="p_del">-		DPRINTK(&quot;feat: %d*32+%d, old: (%p, len: %d), repl: (%p, len: %d), pad: %d&quot;,</span>
<span class="p_add">+		DPRINTK(&quot;feat: %d*32+%d, old: (%px len: %d), repl: (%px, len: %d), pad: %d&quot;,</span>
 			a-&gt;cpuid &gt;&gt; 5,
 			a-&gt;cpuid &amp; 0x1f,
 			instr, a-&gt;instrlen,
 			replacement, a-&gt;replacementlen, a-&gt;padlen);
 
<span class="p_del">-		DUMP_BYTES(instr, a-&gt;instrlen, &quot;%p: old_insn: &quot;, instr);</span>
<span class="p_del">-		DUMP_BYTES(replacement, a-&gt;replacementlen, &quot;%p: rpl_insn: &quot;, replacement);</span>
<span class="p_add">+		DUMP_BYTES(instr, a-&gt;instrlen, &quot;%px: old_insn: &quot;, instr);</span>
<span class="p_add">+		DUMP_BYTES(replacement, a-&gt;replacementlen, &quot;%px: rpl_insn: &quot;, replacement);</span>
 
 		memcpy(insnbuf, replacement, a-&gt;replacementlen);
 		insnbuf_sz = a-&gt;replacementlen;
<span class="p_chunk">@@ -433,7 +422,7 @@</span> <span class="p_context"> void __init_or_module noinline apply_alternatives(struct alt_instr *start,</span>
 				 a-&gt;instrlen - a-&gt;replacementlen);
 			insnbuf_sz += a-&gt;instrlen - a-&gt;replacementlen;
 		}
<span class="p_del">-		DUMP_BYTES(insnbuf, insnbuf_sz, &quot;%p: final_insn: &quot;, instr);</span>
<span class="p_add">+		DUMP_BYTES(insnbuf, insnbuf_sz, &quot;%px: final_insn: &quot;, instr);</span>
 
 		text_poke_early(instr, insnbuf, insnbuf_sz);
 	}
<span class="p_chunk">@@ -599,9 +588,6 @@</span> <span class="p_context"> void __init_or_module apply_paravirt(struct paravirt_patch_site *start,</span>
 	struct paravirt_patch_site *p;
 	char insnbuf[MAX_PATCH_LEN];
 
<span class="p_del">-	if (noreplace_paravirt)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
 	for (p = start; p &lt; end; p++) {
 		unsigned int used;
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">index 390b3dc3d438..71949bf2de5a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/init.h&gt;
 #include &lt;linux/utsname.h&gt;
 #include &lt;linux/cpu.h&gt;
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
 
 #include &lt;asm/nospec-branch.h&gt;
 #include &lt;asm/cmdline.h&gt;
<span class="p_chunk">@@ -90,20 +91,41 @@</span> <span class="p_context"> static const char *spectre_v2_strings[] = {</span>
 };
 
 #undef pr_fmt
<span class="p_del">-#define pr_fmt(fmt)     &quot;Spectre V2 mitigation: &quot; fmt</span>
<span class="p_add">+#define pr_fmt(fmt)     &quot;Spectre V2 : &quot; fmt</span>
 
 static enum spectre_v2_mitigation spectre_v2_enabled = SPECTRE_V2_NONE;
 
<span class="p_add">+#ifdef RETPOLINE</span>
<span class="p_add">+static bool spectre_v2_bad_module;</span>
<span class="p_add">+</span>
<span class="p_add">+bool retpoline_module_ok(bool has_retpoline)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_err(&quot;System may be vulnerable to spectre v2\n&quot;);</span>
<span class="p_add">+	spectre_v2_bad_module = true;</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline const char *spectre_v2_module_string(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return spectre_v2_bad_module ? &quot; - vulnerable module loaded&quot; : &quot;&quot;;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline const char *spectre_v2_module_string(void) { return &quot;&quot;; }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static void __init spec2_print_if_insecure(const char *reason)
 {
 	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
<span class="p_del">-		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+		pr_info(&quot;%s selected on command line.\n&quot;, reason);</span>
 }
 
 static void __init spec2_print_if_secure(const char *reason)
 {
 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
<span class="p_del">-		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+		pr_info(&quot;%s selected on command line.\n&quot;, reason);</span>
 }
 
 static inline bool retp_compiler(void)
<span class="p_chunk">@@ -118,42 +140,68 @@</span> <span class="p_context"> static inline bool match_option(const char *arg, int arglen, const char *opt)</span>
 	return len == arglen &amp;&amp; !strncmp(arg, opt, len);
 }
 
<span class="p_add">+static const struct {</span>
<span class="p_add">+	const char *option;</span>
<span class="p_add">+	enum spectre_v2_mitigation_cmd cmd;</span>
<span class="p_add">+	bool secure;</span>
<span class="p_add">+} mitigation_options[] = {</span>
<span class="p_add">+	{ &quot;off&quot;,               SPECTRE_V2_CMD_NONE,              false },</span>
<span class="p_add">+	{ &quot;on&quot;,                SPECTRE_V2_CMD_FORCE,             true },</span>
<span class="p_add">+	{ &quot;retpoline&quot;,         SPECTRE_V2_CMD_RETPOLINE,         false },</span>
<span class="p_add">+	{ &quot;retpoline,amd&quot;,     SPECTRE_V2_CMD_RETPOLINE_AMD,     false },</span>
<span class="p_add">+	{ &quot;retpoline,generic&quot;, SPECTRE_V2_CMD_RETPOLINE_GENERIC, false },</span>
<span class="p_add">+	{ &quot;auto&quot;,              SPECTRE_V2_CMD_AUTO,              false },</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)
 {
 	char arg[20];
<span class="p_del">-	int ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = cmdline_find_option(boot_command_line, &quot;spectre_v2&quot;, arg,</span>
<span class="p_del">-				  sizeof(arg));</span>
<span class="p_del">-	if (ret &gt; 0)  {</span>
<span class="p_del">-		if (match_option(arg, ret, &quot;off&quot;)) {</span>
<span class="p_del">-			goto disable;</span>
<span class="p_del">-		} else if (match_option(arg, ret, &quot;on&quot;)) {</span>
<span class="p_del">-			spec2_print_if_secure(&quot;force enabled on command line.&quot;);</span>
<span class="p_del">-			return SPECTRE_V2_CMD_FORCE;</span>
<span class="p_del">-		} else if (match_option(arg, ret, &quot;retpoline&quot;)) {</span>
<span class="p_del">-			spec2_print_if_insecure(&quot;retpoline selected on command line.&quot;);</span>
<span class="p_del">-			return SPECTRE_V2_CMD_RETPOLINE;</span>
<span class="p_del">-		} else if (match_option(arg, ret, &quot;retpoline,amd&quot;)) {</span>
<span class="p_del">-			if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD) {</span>
<span class="p_del">-				pr_err(&quot;retpoline,amd selected but CPU is not AMD. Switching to AUTO select\n&quot;);</span>
<span class="p_del">-				return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_del">-			}</span>
<span class="p_del">-			spec2_print_if_insecure(&quot;AMD retpoline selected on command line.&quot;);</span>
<span class="p_del">-			return SPECTRE_V2_CMD_RETPOLINE_AMD;</span>
<span class="p_del">-		} else if (match_option(arg, ret, &quot;retpoline,generic&quot;)) {</span>
<span class="p_del">-			spec2_print_if_insecure(&quot;generic retpoline selected on command line.&quot;);</span>
<span class="p_del">-			return SPECTRE_V2_CMD_RETPOLINE_GENERIC;</span>
<span class="p_del">-		} else if (match_option(arg, ret, &quot;auto&quot;)) {</span>
<span class="p_add">+	int ret, i;</span>
<span class="p_add">+	enum spectre_v2_mitigation_cmd cmd = SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;nospectre_v2&quot;))</span>
<span class="p_add">+		return SPECTRE_V2_CMD_NONE;</span>
<span class="p_add">+	else {</span>
<span class="p_add">+		ret = cmdline_find_option(boot_command_line, &quot;spectre_v2&quot;, arg,</span>
<span class="p_add">+					  sizeof(arg));</span>
<span class="p_add">+		if (ret &lt; 0)</span>
<span class="p_add">+			return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; ARRAY_SIZE(mitigation_options); i++) {</span>
<span class="p_add">+			if (!match_option(arg, ret, mitigation_options[i].option))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			cmd = mitigation_options[i].cmd;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (i &gt;= ARRAY_SIZE(mitigation_options)) {</span>
<span class="p_add">+			pr_err(&quot;unknown option (%s). Switching to AUTO select\n&quot;,</span>
<span class="p_add">+			       mitigation_options[i].option);</span>
 			return SPECTRE_V2_CMD_AUTO;
 		}
 	}
 
<span class="p_del">-	if (!cmdline_find_option_bool(boot_command_line, &quot;nospectre_v2&quot;))</span>
<span class="p_add">+	if ((cmd == SPECTRE_V2_CMD_RETPOLINE ||</span>
<span class="p_add">+	     cmd == SPECTRE_V2_CMD_RETPOLINE_AMD ||</span>
<span class="p_add">+	     cmd == SPECTRE_V2_CMD_RETPOLINE_GENERIC) &amp;&amp;</span>
<span class="p_add">+	    !IS_ENABLED(CONFIG_RETPOLINE)) {</span>
<span class="p_add">+		pr_err(&quot;%s selected but not compiled in. Switching to AUTO select\n&quot;,</span>
<span class="p_add">+		       mitigation_options[i].option);</span>
 		return SPECTRE_V2_CMD_AUTO;
<span class="p_del">-disable:</span>
<span class="p_del">-	spec2_print_if_insecure(&quot;disabled on command line.&quot;);</span>
<span class="p_del">-	return SPECTRE_V2_CMD_NONE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmd == SPECTRE_V2_CMD_RETPOLINE_AMD &amp;&amp;</span>
<span class="p_add">+	    boot_cpu_data.x86_vendor != X86_VENDOR_AMD) {</span>
<span class="p_add">+		pr_err(&quot;retpoline,amd selected but CPU is not AMD. Switching to AUTO select\n&quot;);</span>
<span class="p_add">+		return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mitigation_options[i].secure)</span>
<span class="p_add">+		spec2_print_if_secure(mitigation_options[i].option);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		spec2_print_if_insecure(mitigation_options[i].option);</span>
<span class="p_add">+</span>
<span class="p_add">+	return cmd;</span>
 }
 
 /* Check for Skylake-like CPUs (for RSB handling) */
<span class="p_chunk">@@ -191,10 +239,10 @@</span> <span class="p_context"> static void __init spectre_v2_select_mitigation(void)</span>
 		return;
 
 	case SPECTRE_V2_CMD_FORCE:
<span class="p_del">-		/* FALLTRHU */</span>
 	case SPECTRE_V2_CMD_AUTO:
<span class="p_del">-		goto retpoline_auto;</span>
<span class="p_del">-</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_RETPOLINE))</span>
<span class="p_add">+			goto retpoline_auto;</span>
<span class="p_add">+		break;</span>
 	case SPECTRE_V2_CMD_RETPOLINE_AMD:
 		if (IS_ENABLED(CONFIG_RETPOLINE))
 			goto retpoline_amd;
<span class="p_chunk">@@ -249,6 +297,12 @@</span> <span class="p_context"> static void __init spectre_v2_select_mitigation(void)</span>
 		setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 		pr_info(&quot;Filling RSB on context switch\n&quot;);
 	}
<span class="p_add">+</span>
<span class="p_add">+	/* Initialize Indirect Branch Prediction Barrier if supported */</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_IBPB)) {</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);</span>
<span class="p_add">+		pr_info(&quot;Enabling Indirect Branch Prediction Barrier\n&quot;);</span>
<span class="p_add">+	}</span>
 }
 
 #undef pr_fmt
<span class="p_chunk">@@ -269,7 +323,7 @@</span> <span class="p_context"> ssize_t cpu_show_spectre_v1(struct device *dev,</span>
 {
 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1))
 		return sprintf(buf, &quot;Not affected\n&quot;);
<span class="p_del">-	return sprintf(buf, &quot;Vulnerable\n&quot;);</span>
<span class="p_add">+	return sprintf(buf, &quot;Mitigation: __user pointer sanitization\n&quot;);</span>
 }
 
 ssize_t cpu_show_spectre_v2(struct device *dev,
<span class="p_chunk">@@ -278,6 +332,14 @@</span> <span class="p_context"> ssize_t cpu_show_spectre_v2(struct device *dev,</span>
 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
 		return sprintf(buf, &quot;Not affected\n&quot;);
 
<span class="p_del">-	return sprintf(buf, &quot;%s\n&quot;, spectre_v2_strings[spectre_v2_enabled]);</span>
<span class="p_add">+	return sprintf(buf, &quot;%s%s%s\n&quot;, spectre_v2_strings[spectre_v2_enabled],</span>
<span class="p_add">+		       boot_cpu_has(X86_FEATURE_USE_IBPB) ? &quot;, IBPB&quot; : &quot;&quot;,</span>
<span class="p_add">+		       spectre_v2_module_string());</span>
 }
 #endif
<span class="p_add">+</span>
<span class="p_add">+void __ibp_barrier(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__wrmsr(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(__ibp_barrier);</span>
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index ef29ad001991..d63f4b5706e4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -47,6 +47,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/pat.h&gt;
 #include &lt;asm/microcode.h&gt;
 #include &lt;asm/microcode_intel.h&gt;
<span class="p_add">+#include &lt;asm/intel-family.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpu_device_id.h&gt;</span>
 
 #ifdef CONFIG_X86_LOCAL_APIC
 #include &lt;asm/uv/uv.h&gt;
<span class="p_chunk">@@ -748,6 +750,26 @@</span> <span class="p_context"> static void apply_forced_caps(struct cpuinfo_x86 *c)</span>
 	}
 }
 
<span class="p_add">+static void init_speculation_control(struct cpuinfo_x86 *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The Intel SPEC_CTRL CPUID bit implies IBRS and IBPB support,</span>
<span class="p_add">+	 * and they also have a different bit for STIBP support. Also,</span>
<span class="p_add">+	 * a hypervisor might have set the individual AMD bits even on</span>
<span class="p_add">+	 * Intel CPUs, for finer-grained selection of what&#39;s available.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We use the AMD bits in 0x8000_0008 EBX as the generic hardware</span>
<span class="p_add">+	 * features, which are visible in /proc/cpuinfo and used by the</span>
<span class="p_add">+	 * kernel. So set those accordingly from the Intel bits.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {</span>
<span class="p_add">+		set_cpu_cap(c, X86_FEATURE_IBRS);</span>
<span class="p_add">+		set_cpu_cap(c, X86_FEATURE_IBPB);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))</span>
<span class="p_add">+		set_cpu_cap(c, X86_FEATURE_STIBP);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void get_cpu_cap(struct cpuinfo_x86 *c)
 {
 	u32 eax, ebx, ecx, edx;
<span class="p_chunk">@@ -769,6 +791,7 @@</span> <span class="p_context"> void get_cpu_cap(struct cpuinfo_x86 *c)</span>
 		cpuid_count(0x00000007, 0, &amp;eax, &amp;ebx, &amp;ecx, &amp;edx);
 		c-&gt;x86_capability[CPUID_7_0_EBX] = ebx;
 		c-&gt;x86_capability[CPUID_7_ECX] = ecx;
<span class="p_add">+		c-&gt;x86_capability[CPUID_7_EDX] = edx;</span>
 	}
 
 	/* Extended state features: level 0x0000000d */
<span class="p_chunk">@@ -841,6 +864,7 @@</span> <span class="p_context"> void get_cpu_cap(struct cpuinfo_x86 *c)</span>
 		c-&gt;x86_capability[CPUID_8000_000A_EDX] = cpuid_edx(0x8000000a);
 
 	init_scattered_cpuid_features(c);
<span class="p_add">+	init_speculation_control(c);</span>
 
 	/*
 	 * Clear/Set all flags overridden by options, after probe.
<span class="p_chunk">@@ -876,6 +900,41 @@</span> <span class="p_context"> static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)</span>
 #endif
 }
 
<span class="p_add">+static const __initconst struct x86_cpu_id cpu_no_speculation[] = {</span>
<span class="p_add">+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CEDARVIEW,	X86_FEATURE_ANY },</span>
<span class="p_add">+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CLOVERVIEW,	X86_FEATURE_ANY },</span>
<span class="p_add">+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_LINCROFT,	X86_FEATURE_ANY },</span>
<span class="p_add">+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PENWELL,	X86_FEATURE_ANY },</span>
<span class="p_add">+	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PINEVIEW,	X86_FEATURE_ANY },</span>
<span class="p_add">+	{ X86_VENDOR_CENTAUR,	5 },</span>
<span class="p_add">+	{ X86_VENDOR_INTEL,	5 },</span>
<span class="p_add">+	{ X86_VENDOR_NSC,	5 },</span>
<span class="p_add">+	{ X86_VENDOR_ANY,	4 },</span>
<span class="p_add">+	{}</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {</span>
<span class="p_add">+	{ X86_VENDOR_AMD },</span>
<span class="p_add">+	{}</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static bool __init cpu_vulnerable_to_meltdown(struct cpuinfo_x86 *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 ia32_cap = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (x86_match_cpu(cpu_no_meltdown))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))</span>
<span class="p_add">+		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Rogue Data Cache Load? No! */</span>
<span class="p_add">+	if (ia32_cap &amp; ARCH_CAP_RDCL_NO)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Do minimum CPU detection early.
  * Fields really needed: vendor, cpuid_level, family, model, mask,
<span class="p_chunk">@@ -923,11 +982,12 @@</span> <span class="p_context"> static void __init early_identify_cpu(struct cpuinfo_x86 *c)</span>
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 
<span class="p_del">-	if (c-&gt;x86_vendor != X86_VENDOR_AMD)</span>
<span class="p_del">-		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);</span>
<span class="p_del">-</span>
<span class="p_del">-	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);</span>
<span class="p_del">-	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);</span>
<span class="p_add">+	if (!x86_match_cpu(cpu_no_speculation)) {</span>
<span class="p_add">+		if (cpu_vulnerable_to_meltdown(c))</span>
<span class="p_add">+			setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);</span>
<span class="p_add">+		setup_force_cpu_bug(X86_BUG_SPECTRE_V1);</span>
<span class="p_add">+		setup_force_cpu_bug(X86_BUG_SPECTRE_V2);</span>
<span class="p_add">+	}</span>
 
 	fpu__init_system(c);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">index b1af22073e28..319bf989fad1 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_chunk">@@ -102,6 +102,59 @@</span> <span class="p_context"> static void probe_xeon_phi_r3mwait(struct cpuinfo_x86 *c)</span>
 		ELF_HWCAP2 |= HWCAP2_RING3MWAIT;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Early microcode releases for the Spectre v2 mitigation were broken.</span>
<span class="p_add">+ * Information taken from;</span>
<span class="p_add">+ * - https://newsroom.intel.com/wp-content/uploads/sites/11/2018/01/microcode-update-guidance.pdf</span>
<span class="p_add">+ * - https://kb.vmware.com/s/article/52345</span>
<span class="p_add">+ * - Microcode revisions observed in the wild</span>
<span class="p_add">+ * - Release note from 20180108 microcode release</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct sku_microcode {</span>
<span class="p_add">+	u8 model;</span>
<span class="p_add">+	u8 stepping;</span>
<span class="p_add">+	u32 microcode;</span>
<span class="p_add">+};</span>
<span class="p_add">+static const struct sku_microcode spectre_bad_microcodes[] = {</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0B,	0x84 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0A,	0x84 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x09,	0x84 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x84 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x84 },</span>
<span class="p_add">+	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },</span>
<span class="p_add">+	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },</span>
<span class="p_add">+	{ INTEL_FAM6_SKYLAKE_MOBILE,	0x03,	0xc2 },</span>
<span class="p_add">+	{ INTEL_FAM6_SKYLAKE_DESKTOP,	0x03,	0xc2 },</span>
<span class="p_add">+	{ INTEL_FAM6_BROADWELL_CORE,	0x04,	0x28 },</span>
<span class="p_add">+	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },</span>
<span class="p_add">+	{ INTEL_FAM6_BROADWELL_XEON_D,	0x02,	0x14 },</span>
<span class="p_add">+	{ INTEL_FAM6_BROADWELL_XEON_D,	0x03,	0x07000011 },</span>
<span class="p_add">+	{ INTEL_FAM6_BROADWELL_X,	0x01,	0x0b000025 },</span>
<span class="p_add">+	{ INTEL_FAM6_HASWELL_ULT,	0x01,	0x21 },</span>
<span class="p_add">+	{ INTEL_FAM6_HASWELL_GT3E,	0x01,	0x18 },</span>
<span class="p_add">+	{ INTEL_FAM6_HASWELL_CORE,	0x03,	0x23 },</span>
<span class="p_add">+	{ INTEL_FAM6_HASWELL_X,		0x02,	0x3b },</span>
<span class="p_add">+	{ INTEL_FAM6_HASWELL_X,		0x04,	0x10 },</span>
<span class="p_add">+	{ INTEL_FAM6_IVYBRIDGE_X,	0x04,	0x42a },</span>
<span class="p_add">+	/* Updated in the 20180108 release; blacklist until we know otherwise */</span>
<span class="p_add">+	{ INTEL_FAM6_ATOM_GEMINI_LAKE,	0x01,	0x22 },</span>
<span class="p_add">+	/* Observed in the wild */</span>
<span class="p_add">+	{ INTEL_FAM6_SANDYBRIDGE_X,	0x06,	0x61b },</span>
<span class="p_add">+	{ INTEL_FAM6_SANDYBRIDGE_X,	0x07,	0x712 },</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static bool bad_spectre_microcode(struct cpuinfo_x86 *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; ARRAY_SIZE(spectre_bad_microcodes); i++) {</span>
<span class="p_add">+		if (c-&gt;x86_model == spectre_bad_microcodes[i].model &amp;&amp;</span>
<span class="p_add">+		    c-&gt;x86_mask == spectre_bad_microcodes[i].stepping)</span>
<span class="p_add">+			return (c-&gt;microcode &lt;= spectre_bad_microcodes[i].microcode);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void early_init_intel(struct cpuinfo_x86 *c)
 {
 	u64 misc_enable;
<span class="p_chunk">@@ -122,6 +175,19 @@</span> <span class="p_context"> static void early_init_intel(struct cpuinfo_x86 *c)</span>
 	if (c-&gt;x86 &gt;= 6 &amp;&amp; !cpu_has(c, X86_FEATURE_IA64))
 		c-&gt;microcode = intel_get_microcode_revision();
 
<span class="p_add">+	/* Now if any of them are set, check the blacklist and clear the lot */</span>
<span class="p_add">+	if ((cpu_has(c, X86_FEATURE_SPEC_CTRL) ||</span>
<span class="p_add">+	     cpu_has(c, X86_FEATURE_INTEL_STIBP) ||</span>
<span class="p_add">+	     cpu_has(c, X86_FEATURE_IBRS) || cpu_has(c, X86_FEATURE_IBPB) ||</span>
<span class="p_add">+	     cpu_has(c, X86_FEATURE_STIBP)) &amp;&amp; bad_spectre_microcode(c)) {</span>
<span class="p_add">+		pr_warn(&quot;Intel Spectre v2 broken microcode detected; disabling Speculation Control\n&quot;);</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_IBRS);</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_IBPB);</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_STIBP);</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_SPEC_CTRL);</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_INTEL_STIBP);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Atom erratum AAE44/AAF40/AAG38/AAH41:
 	 *
<span class="p_header">diff --git a/arch/x86/kernel/cpu/scattered.c b/arch/x86/kernel/cpu/scattered.c</span>
<span class="p_header">index d0e69769abfd..df11f5d604be 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/scattered.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/scattered.c</span>
<span class="p_chunk">@@ -21,8 +21,6 @@</span> <span class="p_context"> struct cpuid_bit {</span>
 static const struct cpuid_bit cpuid_bits[] = {
 	{ X86_FEATURE_APERFMPERF,       CPUID_ECX,  0, 0x00000006, 0 },
 	{ X86_FEATURE_EPB,		CPUID_ECX,  3, 0x00000006, 0 },
<span class="p_del">-	{ X86_FEATURE_AVX512_4VNNIW,    CPUID_EDX,  2, 0x00000007, 0 },</span>
<span class="p_del">-	{ X86_FEATURE_AVX512_4FMAPS,    CPUID_EDX,  3, 0x00000007, 0 },</span>
 	{ X86_FEATURE_CAT_L3,		CPUID_EBX,  1, 0x00000010, 0 },
 	{ X86_FEATURE_CAT_L2,		CPUID_EBX,  2, 0x00000010, 0 },
 	{ X86_FEATURE_CDP_L3,		CPUID_ECX,  2, 0x00000010, 1 },
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index c75466232016..9eb448c7859d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -557,7 +557,7 @@</span> <span class="p_context"> static void __set_personality_x32(void)</span>
 	 * Pretend to come from a x32 execve.
 	 */
 	task_pt_regs(current)-&gt;orig_ax = __NR_x32_execve | __X32_SYSCALL_BIT;
<span class="p_del">-	current-&gt;thread.status &amp;= ~TS_COMPAT;</span>
<span class="p_add">+	current_thread_info()-&gt;status &amp;= ~TS_COMPAT;</span>
 #endif
 }
 
<span class="p_chunk">@@ -571,7 +571,7 @@</span> <span class="p_context"> static void __set_personality_ia32(void)</span>
 	current-&gt;personality |= force_personality32;
 	/* Prepare the first &quot;return&quot; to user space */
 	task_pt_regs(current)-&gt;orig_ax = __NR_ia32_execve;
<span class="p_del">-	current-&gt;thread.status |= TS_COMPAT;</span>
<span class="p_add">+	current_thread_info()-&gt;status |= TS_COMPAT;</span>
 #endif
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c</span>
<span class="p_header">index f37d18124648..ed5c4cdf0a34 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ptrace.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ptrace.c</span>
<span class="p_chunk">@@ -935,7 +935,7 @@</span> <span class="p_context"> static int putreg32(struct task_struct *child, unsigned regno, u32 value)</span>
 		 */
 		regs-&gt;orig_ax = value;
 		if (syscall_get_nr(child, regs) &gt;= 0)
<span class="p_del">-			child-&gt;thread.status |= TS_I386_REGS_POKED;</span>
<span class="p_add">+			child-&gt;thread_info.status |= TS_I386_REGS_POKED;</span>
 		break;
 
 	case offsetof(struct user32, regs.eflags):
<span class="p_header">diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c</span>
<span class="p_header">index b9e00e8f1c9b..4cdc0b27ec82 100644</span>
<span class="p_header">--- a/arch/x86/kernel/signal.c</span>
<span class="p_header">+++ b/arch/x86/kernel/signal.c</span>
<span class="p_chunk">@@ -787,7 +787,7 @@</span> <span class="p_context"> static inline unsigned long get_nr_restart_syscall(const struct pt_regs *regs)</span>
 	 * than the tracee.
 	 */
 #ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	if (current-&gt;thread.status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
<span class="p_add">+	if (current_thread_info()-&gt;status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
 		return __NR_ia32_restart_syscall;
 #endif
 #ifdef CONFIG_X86_X32_ABI
<span class="p_header">diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="p_header">index 0099e10eb045..13f5d4217e4f 100644</span>
<span class="p_header">--- a/arch/x86/kvm/cpuid.c</span>
<span class="p_header">+++ b/arch/x86/kvm/cpuid.c</span>
<span class="p_chunk">@@ -67,9 +67,7 @@</span> <span class="p_context"> u64 kvm_supported_xcr0(void)</span>
 
 #define F(x) bit(X86_FEATURE_##x)
 
<span class="p_del">-/* These are scattered features in cpufeatures.h. */</span>
<span class="p_del">-#define KVM_CPUID_BIT_AVX512_4VNNIW     2</span>
<span class="p_del">-#define KVM_CPUID_BIT_AVX512_4FMAPS     3</span>
<span class="p_add">+/* For scattered features from cpufeatures.h; we currently expose none */</span>
 #define KF(x) bit(KVM_CPUID_BIT_##x)
 
 int kvm_update_cpuid(struct kvm_vcpu *vcpu)
<span class="p_chunk">@@ -367,6 +365,10 @@</span> <span class="p_context"> static inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,</span>
 		F(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |
 		0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM);
 
<span class="p_add">+	/* cpuid 0x80000008.ebx */</span>
<span class="p_add">+	const u32 kvm_cpuid_8000_0008_ebx_x86_features =</span>
<span class="p_add">+		F(IBPB) | F(IBRS);</span>
<span class="p_add">+</span>
 	/* cpuid 0xC0000001.edx */
 	const u32 kvm_cpuid_C000_0001_edx_x86_features =
 		F(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |
<span class="p_chunk">@@ -392,7 +394,8 @@</span> <span class="p_context"> static inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,</span>
 
 	/* cpuid 7.0.edx*/
 	const u32 kvm_cpuid_7_0_edx_x86_features =
<span class="p_del">-		KF(AVX512_4VNNIW) | KF(AVX512_4FMAPS);</span>
<span class="p_add">+		F(AVX512_4VNNIW) | F(AVX512_4FMAPS) | F(SPEC_CTRL) |</span>
<span class="p_add">+		F(ARCH_CAPABILITIES);</span>
 
 	/* all calls to cpuid_count() should be made on the same cpu */
 	get_cpu();
<span class="p_chunk">@@ -477,7 +480,7 @@</span> <span class="p_context"> static inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,</span>
 			if (!tdp_enabled || !boot_cpu_has(X86_FEATURE_OSPKE))
 				entry-&gt;ecx &amp;= ~F(PKU);
 			entry-&gt;edx &amp;= kvm_cpuid_7_0_edx_x86_features;
<span class="p_del">-			entry-&gt;edx &amp;= get_scattered_cpuid_leaf(7, 0, CPUID_EDX);</span>
<span class="p_add">+			cpuid_mask(&amp;entry-&gt;edx, CPUID_7_EDX);</span>
 		} else {
 			entry-&gt;ebx = 0;
 			entry-&gt;ecx = 0;
<span class="p_chunk">@@ -627,7 +630,14 @@</span> <span class="p_context"> static inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,</span>
 		if (!g_phys_as)
 			g_phys_as = phys_as;
 		entry-&gt;eax = g_phys_as | (virt_as &lt;&lt; 8);
<span class="p_del">-		entry-&gt;ebx = entry-&gt;edx = 0;</span>
<span class="p_add">+		entry-&gt;edx = 0;</span>
<span class="p_add">+		/* IBRS and IBPB aren&#39;t necessarily present in hardware cpuid */</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_IBPB))</span>
<span class="p_add">+			entry-&gt;ebx |= F(IBPB);</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_IBRS))</span>
<span class="p_add">+			entry-&gt;ebx |= F(IBRS);</span>
<span class="p_add">+		entry-&gt;ebx &amp;= kvm_cpuid_8000_0008_ebx_x86_features;</span>
<span class="p_add">+		cpuid_mask(&amp;entry-&gt;ebx, CPUID_8000_0008_EBX);</span>
 		break;
 	}
 	case 0x80000019:
<span class="p_header">diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h</span>
<span class="p_header">index c2cea6651279..9a327d5b6d1f 100644</span>
<span class="p_header">--- a/arch/x86/kvm/cpuid.h</span>
<span class="p_header">+++ b/arch/x86/kvm/cpuid.h</span>
<span class="p_chunk">@@ -54,6 +54,7 @@</span> <span class="p_context"> static const struct cpuid_reg reverse_cpuid[] = {</span>
 	[CPUID_8000_000A_EDX] = {0x8000000a, 0, CPUID_EDX},
 	[CPUID_7_ECX]         = {         7, 0, CPUID_ECX},
 	[CPUID_8000_0007_EBX] = {0x80000007, 0, CPUID_EBX},
<span class="p_add">+	[CPUID_7_EDX]         = {         7, 0, CPUID_EDX},</span>
 };
 
 static __always_inline struct cpuid_reg x86_feature_cpuid(unsigned x86_feature)
<span class="p_header">diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c</span>
<span class="p_header">index b514b2b2845a..290ecf711aec 100644</span>
<span class="p_header">--- a/arch/x86/kvm/emulate.c</span>
<span class="p_header">+++ b/arch/x86/kvm/emulate.c</span>
<span class="p_chunk">@@ -25,6 +25,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/kvm_emulate.h&gt;
 #include &lt;linux/stringify.h&gt;
 #include &lt;asm/debugreg.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &quot;x86.h&quot;
 #include &quot;tss.h&quot;
<span class="p_chunk">@@ -1021,8 +1022,8 @@</span> <span class="p_context"> static __always_inline u8 test_cc(unsigned int condition, unsigned long flags)</span>
 	void (*fop)(void) = (void *)em_setcc + 4 * (condition &amp; 0xf);
 
 	flags = (flags &amp; EFLAGS_MASK) | X86_EFLAGS_IF;
<span class="p_del">-	asm(&quot;push %[flags]; popf; call *%[fastop]&quot;</span>
<span class="p_del">-	    : &quot;=a&quot;(rc) : [fastop]&quot;r&quot;(fop), [flags]&quot;r&quot;(flags));</span>
<span class="p_add">+	asm(&quot;push %[flags]; popf; &quot; CALL_NOSPEC</span>
<span class="p_add">+	    : &quot;=a&quot;(rc) : [thunk_target]&quot;r&quot;(fop), [flags]&quot;r&quot;(flags));</span>
 	return rc;
 }
 
<span class="p_chunk">@@ -5335,9 +5336,9 @@</span> <span class="p_context"> static int fastop(struct x86_emulate_ctxt *ctxt, void (*fop)(struct fastop *))</span>
 	if (!(ctxt-&gt;d &amp; ByteOp))
 		fop += __ffs(ctxt-&gt;dst.bytes) * FASTOP_SIZE;
 
<span class="p_del">-	asm(&quot;push %[flags]; popf; call *%[fastop]; pushf; pop %[flags]\n&quot;</span>
<span class="p_add">+	asm(&quot;push %[flags]; popf; &quot; CALL_NOSPEC &quot; ; pushf; pop %[flags]\n&quot;</span>
 	    : &quot;+a&quot;(ctxt-&gt;dst.val), &quot;+d&quot;(ctxt-&gt;src.val), [flags]&quot;+D&quot;(flags),
<span class="p_del">-	      [fastop]&quot;+S&quot;(fop), ASM_CALL_CONSTRAINT</span>
<span class="p_add">+	      [thunk_target]&quot;+S&quot;(fop), ASM_CALL_CONSTRAINT</span>
 	    : &quot;c&quot;(ctxt-&gt;src2.val));
 
 	ctxt-&gt;eflags = (ctxt-&gt;eflags &amp; ~EFLAGS_MASK) | (flags &amp; EFLAGS_MASK);
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index f40d0da1f1d3..4e3c79530526 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -184,6 +184,8 @@</span> <span class="p_context"> struct vcpu_svm {</span>
 		u64 gs_base;
 	} host;
 
<span class="p_add">+	u64 spec_ctrl;</span>
<span class="p_add">+</span>
 	u32 *msrpm;
 
 	ulong nmi_iret_rip;
<span class="p_chunk">@@ -249,6 +251,8 @@</span> <span class="p_context"> static const struct svm_direct_access_msrs {</span>
 	{ .index = MSR_CSTAR,				.always = true  },
 	{ .index = MSR_SYSCALL_MASK,			.always = true  },
 #endif
<span class="p_add">+	{ .index = MSR_IA32_SPEC_CTRL,			.always = false },</span>
<span class="p_add">+	{ .index = MSR_IA32_PRED_CMD,			.always = false },</span>
 	{ .index = MSR_IA32_LASTBRANCHFROMIP,		.always = false },
 	{ .index = MSR_IA32_LASTBRANCHTOIP,		.always = false },
 	{ .index = MSR_IA32_LASTINTFROMIP,		.always = false },
<span class="p_chunk">@@ -529,6 +533,7 @@</span> <span class="p_context"> struct svm_cpu_data {</span>
 	struct kvm_ldttss_desc *tss_desc;
 
 	struct page *save_area;
<span class="p_add">+	struct vmcb *current_vmcb;</span>
 };
 
 static DEFINE_PER_CPU(struct svm_cpu_data *, svm_data);
<span class="p_chunk">@@ -880,6 +885,25 @@</span> <span class="p_context"> static bool valid_msr_intercept(u32 index)</span>
 	return false;
 }
 
<span class="p_add">+static bool msr_write_intercepted(struct kvm_vcpu *vcpu, unsigned msr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 bit_write;</span>
<span class="p_add">+	unsigned long tmp;</span>
<span class="p_add">+	u32 offset;</span>
<span class="p_add">+	u32 *msrpm;</span>
<span class="p_add">+</span>
<span class="p_add">+	msrpm = is_guest_mode(vcpu) ? to_svm(vcpu)-&gt;nested.msrpm:</span>
<span class="p_add">+				      to_svm(vcpu)-&gt;msrpm;</span>
<span class="p_add">+</span>
<span class="p_add">+	offset    = svm_msrpm_offset(msr);</span>
<span class="p_add">+	bit_write = 2 * (msr &amp; 0x0f) + 1;</span>
<span class="p_add">+	tmp       = msrpm[offset];</span>
<span class="p_add">+</span>
<span class="p_add">+	BUG_ON(offset == MSR_INVALID);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !!test_bit(bit_write,  &amp;tmp);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void set_msr_interception(u32 *msrpm, unsigned msr,
 				 int read, int write)
 {
<span class="p_chunk">@@ -1582,6 +1606,8 @@</span> <span class="p_context"> static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)</span>
 	u32 dummy;
 	u32 eax = 1;
 
<span class="p_add">+	svm-&gt;spec_ctrl = 0;</span>
<span class="p_add">+</span>
 	if (!init_event) {
 		svm-&gt;vcpu.arch.apic_base = APIC_DEFAULT_PHYS_BASE |
 					   MSR_IA32_APICBASE_ENABLE;
<span class="p_chunk">@@ -1703,11 +1729,17 @@</span> <span class="p_context"> static void svm_free_vcpu(struct kvm_vcpu *vcpu)</span>
 	__free_pages(virt_to_page(svm-&gt;nested.msrpm), MSRPM_ALLOC_ORDER);
 	kvm_vcpu_uninit(vcpu);
 	kmem_cache_free(kvm_vcpu_cache, svm);
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The vmcb page can be recycled, causing a false negative in</span>
<span class="p_add">+	 * svm_vcpu_load(). So do a full IBPB now.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	indirect_branch_prediction_barrier();</span>
 }
 
 static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
<span class="p_add">+	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);</span>
 	int i;
 
 	if (unlikely(cpu != vcpu-&gt;cpu)) {
<span class="p_chunk">@@ -1736,6 +1768,10 @@</span> <span class="p_context"> static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 	if (static_cpu_has(X86_FEATURE_RDTSCP))
 		wrmsrl(MSR_TSC_AUX, svm-&gt;tsc_aux);
 
<span class="p_add">+	if (sd-&gt;current_vmcb != svm-&gt;vmcb) {</span>
<span class="p_add">+		sd-&gt;current_vmcb = svm-&gt;vmcb;</span>
<span class="p_add">+		indirect_branch_prediction_barrier();</span>
<span class="p_add">+	}</span>
 	avic_vcpu_load(vcpu, cpu);
 }
 
<span class="p_chunk">@@ -3593,6 +3629,13 @@</span> <span class="p_context"> static int svm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)</span>
 	case MSR_VM_CR:
 		msr_info-&gt;data = svm-&gt;nested.vm_cr_msr;
 		break;
<span class="p_add">+	case MSR_IA32_SPEC_CTRL:</span>
<span class="p_add">+		if (!msr_info-&gt;host_initiated &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		msr_info-&gt;data = svm-&gt;spec_ctrl;</span>
<span class="p_add">+		break;</span>
 	case MSR_IA32_UCODE_REV:
 		msr_info-&gt;data = 0x01000065;
 		break;
<span class="p_chunk">@@ -3684,6 +3727,49 @@</span> <span class="p_context"> static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)</span>
 	case MSR_IA32_TSC:
 		kvm_write_tsc(vcpu, msr);
 		break;
<span class="p_add">+	case MSR_IA32_SPEC_CTRL:</span>
<span class="p_add">+		if (!msr-&gt;host_initiated &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* The STIBP bit doesn&#39;t fault even if it&#39;s not advertised */</span>
<span class="p_add">+		if (data &amp; ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		svm-&gt;spec_ctrl = data;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!data)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * For non-nested:</span>
<span class="p_add">+		 * When it&#39;s written (to non-zero) for the first time, pass</span>
<span class="p_add">+		 * it through.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * For nested:</span>
<span class="p_add">+		 * The handling of the MSR bitmap for L2 guests is done in</span>
<span class="p_add">+		 * nested_svm_vmrun_msrpm.</span>
<span class="p_add">+		 * We update the L1 MSR bit as well since it will end up</span>
<span class="p_add">+		 * touching the MSR anyway now.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		set_msr_interception(svm-&gt;msrpm, MSR_IA32_SPEC_CTRL, 1, 1);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case MSR_IA32_PRED_CMD:</span>
<span class="p_add">+		if (!msr-&gt;host_initiated &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_IBPB))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (data &amp; ~PRED_CMD_IBPB)</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!data)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		wrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);</span>
<span class="p_add">+		if (is_guest_mode(vcpu))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		set_msr_interception(svm-&gt;msrpm, MSR_IA32_PRED_CMD, 0, 1);</span>
<span class="p_add">+		break;</span>
 	case MSR_STAR:
 		svm-&gt;vmcb-&gt;save.star = data;
 		break;
<span class="p_chunk">@@ -4936,6 +5022,15 @@</span> <span class="p_context"> static void svm_vcpu_run(struct kvm_vcpu *vcpu)</span>
 
 	local_irq_enable();
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If this vCPU has touched SPEC_CTRL, restore the guest&#39;s value if</span>
<span class="p_add">+	 * it&#39;s non-zero. Since vmentry is serialising on affected CPUs, there</span>
<span class="p_add">+	 * is no need to worry about the conditional branch over the wrmsr</span>
<span class="p_add">+	 * being speculatively taken.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (svm-&gt;spec_ctrl)</span>
<span class="p_add">+		wrmsrl(MSR_IA32_SPEC_CTRL, svm-&gt;spec_ctrl);</span>
<span class="p_add">+</span>
 	asm volatile (
 		&quot;push %%&quot; _ASM_BP &quot;; \n\t&quot;
 		&quot;mov %c[rbx](%[svm]), %%&quot; _ASM_BX &quot; \n\t&quot;
<span class="p_chunk">@@ -5028,6 +5123,27 @@</span> <span class="p_context"> static void svm_vcpu_run(struct kvm_vcpu *vcpu)</span>
 #endif
 		);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We do not use IBRS in the kernel. If this vCPU has used the</span>
<span class="p_add">+	 * SPEC_CTRL MSR it may have left it on; save the value and</span>
<span class="p_add">+	 * turn it off. This is much more efficient than blindly adding</span>
<span class="p_add">+	 * it to the atomic save/restore list. Especially as the former</span>
<span class="p_add">+	 * (Saving guest MSRs on vmexit) doesn&#39;t even exist in KVM.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For non-nested case:</span>
<span class="p_add">+	 * If the L01 MSR bitmap does not intercept the MSR, then we need to</span>
<span class="p_add">+	 * save it.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For nested case:</span>
<span class="p_add">+	 * If the L02 MSR bitmap does not intercept the MSR, then we need to</span>
<span class="p_add">+	 * save it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL))</span>
<span class="p_add">+		rdmsrl(MSR_IA32_SPEC_CTRL, svm-&gt;spec_ctrl);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (svm-&gt;spec_ctrl)</span>
<span class="p_add">+		wrmsrl(MSR_IA32_SPEC_CTRL, 0);</span>
<span class="p_add">+</span>
 	/* Eliminate branch target predictions from guest mode */
 	vmexit_fill_RSB();
 
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index c829d89e2e63..bee4c49f6dd0 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -34,6 +34,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/tboot.h&gt;
 #include &lt;linux/hrtimer.h&gt;
 #include &lt;linux/frame.h&gt;
<span class="p_add">+#include &lt;linux/nospec.h&gt;</span>
 #include &quot;kvm_cache_regs.h&quot;
 #include &quot;x86.h&quot;
 
<span class="p_chunk">@@ -111,6 +112,14 @@</span> <span class="p_context"> static u64 __read_mostly host_xss;</span>
 static bool __read_mostly enable_pml = 1;
 module_param_named(pml, enable_pml, bool, S_IRUGO);
 
<span class="p_add">+#define MSR_TYPE_R	1</span>
<span class="p_add">+#define MSR_TYPE_W	2</span>
<span class="p_add">+#define MSR_TYPE_RW	3</span>
<span class="p_add">+</span>
<span class="p_add">+#define MSR_BITMAP_MODE_X2APIC		1</span>
<span class="p_add">+#define MSR_BITMAP_MODE_X2APIC_APICV	2</span>
<span class="p_add">+#define MSR_BITMAP_MODE_LM		4</span>
<span class="p_add">+</span>
 #define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL
 
 /* Guest_tsc -&gt; host_tsc conversion requires 64-bit division.  */
<span class="p_chunk">@@ -185,7 +194,6 @@</span> <span class="p_context"> module_param(ple_window_max, int, S_IRUGO);</span>
 extern const ulong vmx_return;
 
 #define NR_AUTOLOAD_MSRS 8
<span class="p_del">-#define VMCS02_POOL_SIZE 1</span>
 
 struct vmcs {
 	u32 revision_id;
<span class="p_chunk">@@ -210,6 +218,7 @@</span> <span class="p_context"> struct loaded_vmcs {</span>
 	int soft_vnmi_blocked;
 	ktime_t entry_time;
 	s64 vnmi_blocked_time;
<span class="p_add">+	unsigned long *msr_bitmap;</span>
 	struct list_head loaded_vmcss_on_cpu_link;
 };
 
<span class="p_chunk">@@ -226,7 +235,7 @@</span> <span class="p_context"> struct shared_msr_entry {</span>
  * stored in guest memory specified by VMPTRLD, but is opaque to the guest,
  * which must access it using VMREAD/VMWRITE/VMCLEAR instructions.
  * More than one of these structures may exist, if L1 runs multiple L2 guests.
<span class="p_del">- * nested_vmx_run() will use the data here to build a vmcs02: a VMCS for the</span>
<span class="p_add">+ * nested_vmx_run() will use the data here to build the vmcs02: a VMCS for the</span>
  * underlying hardware which will be used to run L2.
  * This structure is packed to ensure that its layout is identical across
  * machines (necessary for live migration).
<span class="p_chunk">@@ -409,13 +418,6 @@</span> <span class="p_context"> struct __packed vmcs12 {</span>
  */
 #define VMCS12_SIZE 0x1000
 
<span class="p_del">-/* Used to remember the last vmcs02 used for some recently used vmcs12s */</span>
<span class="p_del">-struct vmcs02_list {</span>
<span class="p_del">-	struct list_head list;</span>
<span class="p_del">-	gpa_t vmptr;</span>
<span class="p_del">-	struct loaded_vmcs vmcs02;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 /*
  * The nested_vmx structure is part of vcpu_vmx, and holds information we need
  * for correct emulation of VMX (i.e., nested VMX) on this vcpu.
<span class="p_chunk">@@ -440,15 +442,15 @@</span> <span class="p_context"> struct nested_vmx {</span>
 	 */
 	bool sync_shadow_vmcs;
 
<span class="p_del">-	/* vmcs02_list cache of VMCSs recently used to run L2 guests */</span>
<span class="p_del">-	struct list_head vmcs02_pool;</span>
<span class="p_del">-	int vmcs02_num;</span>
 	bool change_vmcs01_virtual_x2apic_mode;
 	/* L2 must run next, and mustn&#39;t decide to exit to L1. */
 	bool nested_run_pending;
<span class="p_add">+</span>
<span class="p_add">+	struct loaded_vmcs vmcs02;</span>
<span class="p_add">+</span>
 	/*
<span class="p_del">-	 * Guest pages referred to in vmcs02 with host-physical pointers, so</span>
<span class="p_del">-	 * we must keep them pinned while L2 runs.</span>
<span class="p_add">+	 * Guest pages referred to in the vmcs02 with host-physical</span>
<span class="p_add">+	 * pointers, so we must keep them pinned while L2 runs.</span>
 	 */
 	struct page *apic_access_page;
 	struct page *virtual_apic_page;
<span class="p_chunk">@@ -457,8 +459,6 @@</span> <span class="p_context"> struct nested_vmx {</span>
 	bool pi_pending;
 	u16 posted_intr_nv;
 
<span class="p_del">-	unsigned long *msr_bitmap;</span>
<span class="p_del">-</span>
 	struct hrtimer preemption_timer;
 	bool preemption_timer_expired;
 
<span class="p_chunk">@@ -581,6 +581,7 @@</span> <span class="p_context"> struct vcpu_vmx {</span>
 	struct kvm_vcpu       vcpu;
 	unsigned long         host_rsp;
 	u8                    fail;
<span class="p_add">+	u8		      msr_bitmap_mode;</span>
 	u32                   exit_intr_info;
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
<span class="p_chunk">@@ -592,6 +593,10 @@</span> <span class="p_context"> struct vcpu_vmx {</span>
 	u64 		      msr_host_kernel_gs_base;
 	u64 		      msr_guest_kernel_gs_base;
 #endif
<span class="p_add">+</span>
<span class="p_add">+	u64 		      arch_capabilities;</span>
<span class="p_add">+	u64 		      spec_ctrl;</span>
<span class="p_add">+</span>
 	u32 vm_entry_controls_shadow;
 	u32 vm_exit_controls_shadow;
 	u32 secondary_exec_control;
<span class="p_chunk">@@ -898,21 +903,18 @@</span> <span class="p_context"> static const unsigned short vmcs_field_to_offset_table[] = {</span>
 
 static inline short vmcs_field_to_offset(unsigned long field)
 {
<span class="p_del">-	BUILD_BUG_ON(ARRAY_SIZE(vmcs_field_to_offset_table) &gt; SHRT_MAX);</span>
<span class="p_add">+	const size_t size = ARRAY_SIZE(vmcs_field_to_offset_table);</span>
<span class="p_add">+	unsigned short offset;</span>
 
<span class="p_del">-	if (field &gt;= ARRAY_SIZE(vmcs_field_to_offset_table))</span>
<span class="p_add">+	BUILD_BUG_ON(size &gt; SHRT_MAX);</span>
<span class="p_add">+	if (field &gt;= size)</span>
 		return -ENOENT;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * FIXME: Mitigation for CVE-2017-5753.  To be replaced with a</span>
<span class="p_del">-	 * generic mechanism.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	asm(&quot;lfence&quot;);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (vmcs_field_to_offset_table[field] == 0)</span>
<span class="p_add">+	field = array_index_nospec(field, size);</span>
<span class="p_add">+	offset = vmcs_field_to_offset_table[field];</span>
<span class="p_add">+	if (offset == 0)</span>
 		return -ENOENT;
<span class="p_del">-</span>
<span class="p_del">-	return vmcs_field_to_offset_table[field];</span>
<span class="p_add">+	return offset;</span>
 }
 
 static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
<span class="p_chunk">@@ -935,6 +937,9 @@</span> <span class="p_context"> static bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu);</span>
 static void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked);
 static bool nested_vmx_is_page_fault_vmexit(struct vmcs12 *vmcs12,
 					    u16 error_code);
<span class="p_add">+static void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu);</span>
<span class="p_add">+static void __always_inline vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,</span>
<span class="p_add">+							  u32 msr, int type);</span>
 
 static DEFINE_PER_CPU(struct vmcs *, vmxarea);
 static DEFINE_PER_CPU(struct vmcs *, current_vmcs);
<span class="p_chunk">@@ -954,12 +959,6 @@</span> <span class="p_context"> static DEFINE_PER_CPU(spinlock_t, blocked_vcpu_on_cpu_lock);</span>
 enum {
 	VMX_IO_BITMAP_A,
 	VMX_IO_BITMAP_B,
<span class="p_del">-	VMX_MSR_BITMAP_LEGACY,</span>
<span class="p_del">-	VMX_MSR_BITMAP_LONGMODE,</span>
<span class="p_del">-	VMX_MSR_BITMAP_LEGACY_X2APIC_APICV,</span>
<span class="p_del">-	VMX_MSR_BITMAP_LONGMODE_X2APIC_APICV,</span>
<span class="p_del">-	VMX_MSR_BITMAP_LEGACY_X2APIC,</span>
<span class="p_del">-	VMX_MSR_BITMAP_LONGMODE_X2APIC,</span>
 	VMX_VMREAD_BITMAP,
 	VMX_VMWRITE_BITMAP,
 	VMX_BITMAP_NR
<span class="p_chunk">@@ -969,12 +968,6 @@</span> <span class="p_context"> static unsigned long *vmx_bitmap[VMX_BITMAP_NR];</span>
 
 #define vmx_io_bitmap_a                      (vmx_bitmap[VMX_IO_BITMAP_A])
 #define vmx_io_bitmap_b                      (vmx_bitmap[VMX_IO_BITMAP_B])
<span class="p_del">-#define vmx_msr_bitmap_legacy                (vmx_bitmap[VMX_MSR_BITMAP_LEGACY])</span>
<span class="p_del">-#define vmx_msr_bitmap_longmode              (vmx_bitmap[VMX_MSR_BITMAP_LONGMODE])</span>
<span class="p_del">-#define vmx_msr_bitmap_legacy_x2apic_apicv   (vmx_bitmap[VMX_MSR_BITMAP_LEGACY_X2APIC_APICV])</span>
<span class="p_del">-#define vmx_msr_bitmap_longmode_x2apic_apicv (vmx_bitmap[VMX_MSR_BITMAP_LONGMODE_X2APIC_APICV])</span>
<span class="p_del">-#define vmx_msr_bitmap_legacy_x2apic         (vmx_bitmap[VMX_MSR_BITMAP_LEGACY_X2APIC])</span>
<span class="p_del">-#define vmx_msr_bitmap_longmode_x2apic       (vmx_bitmap[VMX_MSR_BITMAP_LONGMODE_X2APIC])</span>
 #define vmx_vmread_bitmap                    (vmx_bitmap[VMX_VMREAD_BITMAP])
 #define vmx_vmwrite_bitmap                   (vmx_bitmap[VMX_VMWRITE_BITMAP])
 
<span class="p_chunk">@@ -1918,6 +1911,52 @@</span> <span class="p_context"> static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
 	vmcs_write32(EXCEPTION_BITMAP, eb);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Check if MSR is intercepted for currently loaded MSR bitmap.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool msr_write_intercepted(struct kvm_vcpu *vcpu, u32 msr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long *msr_bitmap;</span>
<span class="p_add">+	int f = sizeof(unsigned long);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpu_has_vmx_msr_bitmap())</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	msr_bitmap = to_vmx(vcpu)-&gt;loaded_vmcs-&gt;msr_bitmap;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (msr &lt;= 0x1fff) {</span>
<span class="p_add">+		return !!test_bit(msr, msr_bitmap + 0x800 / f);</span>
<span class="p_add">+	} else if ((msr &gt;= 0xc0000000) &amp;&amp; (msr &lt;= 0xc0001fff)) {</span>
<span class="p_add">+		msr &amp;= 0x1fff;</span>
<span class="p_add">+		return !!test_bit(msr, msr_bitmap + 0xc00 / f);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Check if MSR is intercepted for L01 MSR bitmap.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool msr_write_intercepted_l01(struct kvm_vcpu *vcpu, u32 msr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long *msr_bitmap;</span>
<span class="p_add">+	int f = sizeof(unsigned long);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpu_has_vmx_msr_bitmap())</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	msr_bitmap = to_vmx(vcpu)-&gt;vmcs01.msr_bitmap;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (msr &lt;= 0x1fff) {</span>
<span class="p_add">+		return !!test_bit(msr, msr_bitmap + 0x800 / f);</span>
<span class="p_add">+	} else if ((msr &gt;= 0xc0000000) &amp;&amp; (msr &lt;= 0xc0001fff)) {</span>
<span class="p_add">+		msr &amp;= 0x1fff;</span>
<span class="p_add">+		return !!test_bit(msr, msr_bitmap + 0xc00 / f);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 		unsigned long entry, unsigned long exit)
 {
<span class="p_chunk">@@ -2296,6 +2335,7 @@</span> <span class="p_context"> static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 	if (per_cpu(current_vmcs, cpu) != vmx-&gt;loaded_vmcs-&gt;vmcs) {
 		per_cpu(current_vmcs, cpu) = vmx-&gt;loaded_vmcs-&gt;vmcs;
 		vmcs_load(vmx-&gt;loaded_vmcs-&gt;vmcs);
<span class="p_add">+		indirect_branch_prediction_barrier();</span>
 	}
 
 	if (!already_loaded) {
<span class="p_chunk">@@ -2572,36 +2612,6 @@</span> <span class="p_context"> static void move_msr_up(struct vcpu_vmx *vmx, int from, int to)</span>
 	vmx-&gt;guest_msrs[from] = tmp;
 }
 
<span class="p_del">-static void vmx_set_msr_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long *msr_bitmap;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (is_guest_mode(vcpu))</span>
<span class="p_del">-		msr_bitmap = to_vmx(vcpu)-&gt;nested.msr_bitmap;</span>
<span class="p_del">-	else if (cpu_has_secondary_exec_ctrls() &amp;&amp;</span>
<span class="p_del">-		 (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &amp;</span>
<span class="p_del">-		  SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {</span>
<span class="p_del">-		if (enable_apicv &amp;&amp; kvm_vcpu_apicv_active(vcpu)) {</span>
<span class="p_del">-			if (is_long_mode(vcpu))</span>
<span class="p_del">-				msr_bitmap = vmx_msr_bitmap_longmode_x2apic_apicv;</span>
<span class="p_del">-			else</span>
<span class="p_del">-				msr_bitmap = vmx_msr_bitmap_legacy_x2apic_apicv;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			if (is_long_mode(vcpu))</span>
<span class="p_del">-				msr_bitmap = vmx_msr_bitmap_longmode_x2apic;</span>
<span class="p_del">-			else</span>
<span class="p_del">-				msr_bitmap = vmx_msr_bitmap_legacy_x2apic;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		if (is_long_mode(vcpu))</span>
<span class="p_del">-			msr_bitmap = vmx_msr_bitmap_longmode;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			msr_bitmap = vmx_msr_bitmap_legacy;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	vmcs_write64(MSR_BITMAP, __pa(msr_bitmap));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * Set up the vmcs to automatically save and restore system
  * msrs.  Don&#39;t touch the 64-bit msrs if the guest is in legacy
<span class="p_chunk">@@ -2642,7 +2652,7 @@</span> <span class="p_context"> static void setup_msrs(struct vcpu_vmx *vmx)</span>
 	vmx-&gt;save_nmsrs = save_nmsrs;
 
 	if (cpu_has_vmx_msr_bitmap())
<span class="p_del">-		vmx_set_msr_bitmap(&amp;vmx-&gt;vcpu);</span>
<span class="p_add">+		vmx_update_msr_bitmap(&amp;vmx-&gt;vcpu);</span>
 }
 
 /*
<span class="p_chunk">@@ -3276,6 +3286,20 @@</span> <span class="p_context"> static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)</span>
 	case MSR_IA32_TSC:
 		msr_info-&gt;data = guest_read_tsc(vcpu);
 		break;
<span class="p_add">+	case MSR_IA32_SPEC_CTRL:</span>
<span class="p_add">+		if (!msr_info-&gt;host_initiated &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		msr_info-&gt;data = to_vmx(vcpu)-&gt;spec_ctrl;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case MSR_IA32_ARCH_CAPABILITIES:</span>
<span class="p_add">+		if (!msr_info-&gt;host_initiated &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+		msr_info-&gt;data = to_vmx(vcpu)-&gt;arch_capabilities;</span>
<span class="p_add">+		break;</span>
 	case MSR_IA32_SYSENTER_CS:
 		msr_info-&gt;data = vmcs_read32(GUEST_SYSENTER_CS);
 		break;
<span class="p_chunk">@@ -3383,6 +3407,70 @@</span> <span class="p_context"> static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)</span>
 	case MSR_IA32_TSC:
 		kvm_write_tsc(vcpu, msr_info);
 		break;
<span class="p_add">+	case MSR_IA32_SPEC_CTRL:</span>
<span class="p_add">+		if (!msr_info-&gt;host_initiated &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* The STIBP bit doesn&#39;t fault even if it&#39;s not advertised */</span>
<span class="p_add">+		if (data &amp; ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		vmx-&gt;spec_ctrl = data;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!data)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * For non-nested:</span>
<span class="p_add">+		 * When it&#39;s written (to non-zero) for the first time, pass</span>
<span class="p_add">+		 * it through.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * For nested:</span>
<span class="p_add">+		 * The handling of the MSR bitmap for L2 guests is done in</span>
<span class="p_add">+		 * nested_vmx_merge_msr_bitmap. We should not touch the</span>
<span class="p_add">+		 * vmcs02.msr_bitmap here since it gets completely overwritten</span>
<span class="p_add">+		 * in the merging. We update the vmcs01 here for L1 as well</span>
<span class="p_add">+		 * since it will end up touching the MSR anyway now.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		vmx_disable_intercept_for_msr(vmx-&gt;vmcs01.msr_bitmap,</span>
<span class="p_add">+					      MSR_IA32_SPEC_CTRL,</span>
<span class="p_add">+					      MSR_TYPE_RW);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case MSR_IA32_PRED_CMD:</span>
<span class="p_add">+		if (!msr_info-&gt;host_initiated &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_IBPB) &amp;&amp;</span>
<span class="p_add">+		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (data &amp; ~PRED_CMD_IBPB)</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!data)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		wrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * For non-nested:</span>
<span class="p_add">+		 * When it&#39;s written (to non-zero) for the first time, pass</span>
<span class="p_add">+		 * it through.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * For nested:</span>
<span class="p_add">+		 * The handling of the MSR bitmap for L2 guests is done in</span>
<span class="p_add">+		 * nested_vmx_merge_msr_bitmap. We should not touch the</span>
<span class="p_add">+		 * vmcs02.msr_bitmap here since it gets completely overwritten</span>
<span class="p_add">+		 * in the merging.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		vmx_disable_intercept_for_msr(vmx-&gt;vmcs01.msr_bitmap, MSR_IA32_PRED_CMD,</span>
<span class="p_add">+					      MSR_TYPE_W);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case MSR_IA32_ARCH_CAPABILITIES:</span>
<span class="p_add">+		if (!msr_info-&gt;host_initiated)</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+		vmx-&gt;arch_capabilities = data;</span>
<span class="p_add">+		break;</span>
 	case MSR_IA32_CR_PAT:
 		if (vmcs_config.vmentry_ctrl &amp; VM_ENTRY_LOAD_IA32_PAT) {
 			if (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))
<span class="p_chunk">@@ -3837,11 +3925,6 @@</span> <span class="p_context"> static struct vmcs *alloc_vmcs_cpu(int cpu)</span>
 	return vmcs;
 }
 
<span class="p_del">-static struct vmcs *alloc_vmcs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return alloc_vmcs_cpu(raw_smp_processor_id());</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void free_vmcs(struct vmcs *vmcs)
 {
 	free_pages((unsigned long)vmcs, vmcs_config.order);
<span class="p_chunk">@@ -3857,9 +3940,38 @@</span> <span class="p_context"> static void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)</span>
 	loaded_vmcs_clear(loaded_vmcs);
 	free_vmcs(loaded_vmcs-&gt;vmcs);
 	loaded_vmcs-&gt;vmcs = NULL;
<span class="p_add">+	if (loaded_vmcs-&gt;msr_bitmap)</span>
<span class="p_add">+		free_page((unsigned long)loaded_vmcs-&gt;msr_bitmap);</span>
 	WARN_ON(loaded_vmcs-&gt;shadow_vmcs != NULL);
 }
 
<span class="p_add">+static struct vmcs *alloc_vmcs(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return alloc_vmcs_cpu(raw_smp_processor_id());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	loaded_vmcs-&gt;vmcs = alloc_vmcs();</span>
<span class="p_add">+	if (!loaded_vmcs-&gt;vmcs)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	loaded_vmcs-&gt;shadow_vmcs = NULL;</span>
<span class="p_add">+	loaded_vmcs_init(loaded_vmcs);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu_has_vmx_msr_bitmap()) {</span>
<span class="p_add">+		loaded_vmcs-&gt;msr_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);</span>
<span class="p_add">+		if (!loaded_vmcs-&gt;msr_bitmap)</span>
<span class="p_add">+			goto out_vmcs;</span>
<span class="p_add">+		memset(loaded_vmcs-&gt;msr_bitmap, 0xff, PAGE_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+out_vmcs:</span>
<span class="p_add">+	free_loaded_vmcs(loaded_vmcs);</span>
<span class="p_add">+	return -ENOMEM;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void free_kvm_area(void)
 {
 	int cpu;
<span class="p_chunk">@@ -4918,10 +5030,8 @@</span> <span class="p_context"> static void free_vpid(int vpid)</span>
 	spin_unlock(&amp;vmx_vpid_lock);
 }
 
<span class="p_del">-#define MSR_TYPE_R	1</span>
<span class="p_del">-#define MSR_TYPE_W	2</span>
<span class="p_del">-static void __vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,</span>
<span class="p_del">-						u32 msr, int type)</span>
<span class="p_add">+static void __always_inline vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,</span>
<span class="p_add">+							  u32 msr, int type)</span>
 {
 	int f = sizeof(unsigned long);
 
<span class="p_chunk">@@ -4955,6 +5065,50 @@</span> <span class="p_context"> static void __vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,</span>
 	}
 }
 
<span class="p_add">+static void __always_inline vmx_enable_intercept_for_msr(unsigned long *msr_bitmap,</span>
<span class="p_add">+							 u32 msr, int type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int f = sizeof(unsigned long);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpu_has_vmx_msr_bitmap())</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals</span>
<span class="p_add">+	 * have the write-low and read-high bitmap offsets the wrong way round.</span>
<span class="p_add">+	 * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (msr &lt;= 0x1fff) {</span>
<span class="p_add">+		if (type &amp; MSR_TYPE_R)</span>
<span class="p_add">+			/* read-low */</span>
<span class="p_add">+			__set_bit(msr, msr_bitmap + 0x000 / f);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (type &amp; MSR_TYPE_W)</span>
<span class="p_add">+			/* write-low */</span>
<span class="p_add">+			__set_bit(msr, msr_bitmap + 0x800 / f);</span>
<span class="p_add">+</span>
<span class="p_add">+	} else if ((msr &gt;= 0xc0000000) &amp;&amp; (msr &lt;= 0xc0001fff)) {</span>
<span class="p_add">+		msr &amp;= 0x1fff;</span>
<span class="p_add">+		if (type &amp; MSR_TYPE_R)</span>
<span class="p_add">+			/* read-high */</span>
<span class="p_add">+			__set_bit(msr, msr_bitmap + 0x400 / f);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (type &amp; MSR_TYPE_W)</span>
<span class="p_add">+			/* write-high */</span>
<span class="p_add">+			__set_bit(msr, msr_bitmap + 0xc00 / f);</span>
<span class="p_add">+</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __always_inline vmx_set_intercept_for_msr(unsigned long *msr_bitmap,</span>
<span class="p_add">+			     			      u32 msr, int type, bool value)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (value)</span>
<span class="p_add">+		vmx_enable_intercept_for_msr(msr_bitmap, msr, type);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		vmx_disable_intercept_for_msr(msr_bitmap, msr, type);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * If a msr is allowed by L0, we should check whether it is allowed by L1.
  * The corresponding bit will be cleared unless both of L0 and L1 allow it.
<span class="p_chunk">@@ -5001,30 +5155,70 @@</span> <span class="p_context"> static void nested_vmx_disable_intercept_for_msr(unsigned long *msr_bitmap_l1,</span>
 	}
 }
 
<span class="p_del">-static void vmx_disable_intercept_for_msr(u32 msr, bool longmode_only)</span>
<span class="p_add">+static u8 vmx_msr_bitmap_mode(struct kvm_vcpu *vcpu)</span>
 {
<span class="p_del">-	if (!longmode_only)</span>
<span class="p_del">-		__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy,</span>
<span class="p_del">-						msr, MSR_TYPE_R | MSR_TYPE_W);</span>
<span class="p_del">-	__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode,</span>
<span class="p_del">-						msr, MSR_TYPE_R | MSR_TYPE_W);</span>
<span class="p_add">+	u8 mode = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu_has_secondary_exec_ctrls() &amp;&amp;</span>
<span class="p_add">+	    (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &amp;</span>
<span class="p_add">+	     SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {</span>
<span class="p_add">+		mode |= MSR_BITMAP_MODE_X2APIC;</span>
<span class="p_add">+		if (enable_apicv &amp;&amp; kvm_vcpu_apicv_active(vcpu))</span>
<span class="p_add">+			mode |= MSR_BITMAP_MODE_X2APIC_APICV;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_long_mode(vcpu))</span>
<span class="p_add">+		mode |= MSR_BITMAP_MODE_LM;</span>
<span class="p_add">+</span>
<span class="p_add">+	return mode;</span>
 }
 
<span class="p_del">-static void vmx_disable_intercept_msr_x2apic(u32 msr, int type, bool apicv_active)</span>
<span class="p_add">+#define X2APIC_MSR(r) (APIC_BASE_MSR + ((r) &gt;&gt; 4))</span>
<span class="p_add">+</span>
<span class="p_add">+static void vmx_update_msr_bitmap_x2apic(unsigned long *msr_bitmap,</span>
<span class="p_add">+					 u8 mode)</span>
 {
<span class="p_del">-	if (apicv_active) {</span>
<span class="p_del">-		__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy_x2apic_apicv,</span>
<span class="p_del">-				msr, type);</span>
<span class="p_del">-		__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode_x2apic_apicv,</span>
<span class="p_del">-				msr, type);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy_x2apic,</span>
<span class="p_del">-				msr, type);</span>
<span class="p_del">-		__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode_x2apic,</span>
<span class="p_del">-				msr, type);</span>
<span class="p_add">+	int msr;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (msr = 0x800; msr &lt;= 0x8ff; msr += BITS_PER_LONG) {</span>
<span class="p_add">+		unsigned word = msr / BITS_PER_LONG;</span>
<span class="p_add">+		msr_bitmap[word] = (mode &amp; MSR_BITMAP_MODE_X2APIC_APICV) ? 0 : ~0;</span>
<span class="p_add">+		msr_bitmap[word + (0x800 / sizeof(long))] = ~0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mode &amp; MSR_BITMAP_MODE_X2APIC) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * TPR reads and writes can be virtualized even if virtual interrupt</span>
<span class="p_add">+		 * delivery is not in use.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		vmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW);</span>
<span class="p_add">+		if (mode &amp; MSR_BITMAP_MODE_X2APIC_APICV) {</span>
<span class="p_add">+			vmx_enable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_R);</span>
<span class="p_add">+			vmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);</span>
<span class="p_add">+			vmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);</span>
<span class="p_add">+		}</span>
 	}
 }
 
<span class="p_add">+static void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vcpu_vmx *vmx = to_vmx(vcpu);</span>
<span class="p_add">+	unsigned long *msr_bitmap = vmx-&gt;vmcs01.msr_bitmap;</span>
<span class="p_add">+	u8 mode = vmx_msr_bitmap_mode(vcpu);</span>
<span class="p_add">+	u8 changed = mode ^ vmx-&gt;msr_bitmap_mode;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!changed)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	vmx_set_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW,</span>
<span class="p_add">+				  !(mode &amp; MSR_BITMAP_MODE_LM));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (changed &amp; (MSR_BITMAP_MODE_X2APIC | MSR_BITMAP_MODE_X2APIC_APICV))</span>
<span class="p_add">+		vmx_update_msr_bitmap_x2apic(msr_bitmap, mode);</span>
<span class="p_add">+</span>
<span class="p_add">+	vmx-&gt;msr_bitmap_mode = mode;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static bool vmx_get_enable_apicv(struct kvm_vcpu *vcpu)
 {
 	return enable_apicv;
<span class="p_chunk">@@ -5274,7 +5468,7 @@</span> <span class="p_context"> static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)</span>
 	}
 
 	if (cpu_has_vmx_msr_bitmap())
<span class="p_del">-		vmx_set_msr_bitmap(vcpu);</span>
<span class="p_add">+		vmx_update_msr_bitmap(vcpu);</span>
 }
 
 static u32 vmx_exec_control(struct vcpu_vmx *vmx)
<span class="p_chunk">@@ -5461,7 +5655,7 @@</span> <span class="p_context"> static void vmx_vcpu_setup(struct vcpu_vmx *vmx)</span>
 		vmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmwrite_bitmap));
 	}
 	if (cpu_has_vmx_msr_bitmap())
<span class="p_del">-		vmcs_write64(MSR_BITMAP, __pa(vmx_msr_bitmap_legacy));</span>
<span class="p_add">+		vmcs_write64(MSR_BITMAP, __pa(vmx-&gt;vmcs01.msr_bitmap));</span>
 
 	vmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */
 
<span class="p_chunk">@@ -5539,6 +5733,8 @@</span> <span class="p_context"> static void vmx_vcpu_setup(struct vcpu_vmx *vmx)</span>
 		++vmx-&gt;nmsrs;
 	}
 
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES))</span>
<span class="p_add">+		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, vmx-&gt;arch_capabilities);</span>
 
 	vm_exit_controls_init(vmx, vmcs_config.vmexit_ctrl);
 
<span class="p_chunk">@@ -5567,6 +5763,7 @@</span> <span class="p_context"> static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)</span>
 	u64 cr0;
 
 	vmx-&gt;rmode.vm86_active = 0;
<span class="p_add">+	vmx-&gt;spec_ctrl = 0;</span>
 
 	vmx-&gt;vcpu.arch.regs[VCPU_REGS_RDX] = get_rdx_init_val();
 	kvm_set_cr8(vcpu, 0);
<span class="p_chunk">@@ -6744,7 +6941,7 @@</span> <span class="p_context"> void vmx_enable_tdp(void)</span>
 
 static __init int hardware_setup(void)
 {
<span class="p_del">-	int r = -ENOMEM, i, msr;</span>
<span class="p_add">+	int r = -ENOMEM, i;</span>
 
 	rdmsrl_safe(MSR_EFER, &amp;host_efer);
 
<span class="p_chunk">@@ -6764,9 +6961,6 @@</span> <span class="p_context"> static __init int hardware_setup(void)</span>
 
 	memset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);
 
<span class="p_del">-	memset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);</span>
<span class="p_del">-	memset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);</span>
<span class="p_del">-</span>
 	if (setup_vmcs_config(&amp;vmcs_config) &lt; 0) {
 		r = -EIO;
 		goto out;
<span class="p_chunk">@@ -6835,42 +7029,8 @@</span> <span class="p_context"> static __init int hardware_setup(void)</span>
 		kvm_tsc_scaling_ratio_frac_bits = 48;
 	}
 
<span class="p_del">-	vmx_disable_intercept_for_msr(MSR_FS_BASE, false);</span>
<span class="p_del">-	vmx_disable_intercept_for_msr(MSR_GS_BASE, false);</span>
<span class="p_del">-	vmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);</span>
<span class="p_del">-	vmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);</span>
<span class="p_del">-	vmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);</span>
<span class="p_del">-	vmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);</span>
<span class="p_del">-</span>
<span class="p_del">-	memcpy(vmx_msr_bitmap_legacy_x2apic_apicv,</span>
<span class="p_del">-			vmx_msr_bitmap_legacy, PAGE_SIZE);</span>
<span class="p_del">-	memcpy(vmx_msr_bitmap_longmode_x2apic_apicv,</span>
<span class="p_del">-			vmx_msr_bitmap_longmode, PAGE_SIZE);</span>
<span class="p_del">-	memcpy(vmx_msr_bitmap_legacy_x2apic,</span>
<span class="p_del">-			vmx_msr_bitmap_legacy, PAGE_SIZE);</span>
<span class="p_del">-	memcpy(vmx_msr_bitmap_longmode_x2apic,</span>
<span class="p_del">-			vmx_msr_bitmap_longmode, PAGE_SIZE);</span>
<span class="p_del">-</span>
 	set_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */
 
<span class="p_del">-	for (msr = 0x800; msr &lt;= 0x8ff; msr++) {</span>
<span class="p_del">-		if (msr == 0x839 /* TMCCT */)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-		vmx_disable_intercept_msr_x2apic(msr, MSR_TYPE_R, true);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * TPR reads and writes can be virtualized even if virtual interrupt</span>
<span class="p_del">-	 * delivery is not in use.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	vmx_disable_intercept_msr_x2apic(0x808, MSR_TYPE_W, true);</span>
<span class="p_del">-	vmx_disable_intercept_msr_x2apic(0x808, MSR_TYPE_R | MSR_TYPE_W, false);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* EOI */</span>
<span class="p_del">-	vmx_disable_intercept_msr_x2apic(0x80b, MSR_TYPE_W, true);</span>
<span class="p_del">-	/* SELF-IPI */</span>
<span class="p_del">-	vmx_disable_intercept_msr_x2apic(0x83f, MSR_TYPE_W, true);</span>
<span class="p_del">-</span>
 	if (enable_ept)
 		vmx_enable_tdp();
 	else
<span class="p_chunk">@@ -6973,94 +7133,6 @@</span> <span class="p_context"> static int handle_monitor(struct kvm_vcpu *vcpu)</span>
 	return handle_nop(vcpu);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * To run an L2 guest, we need a vmcs02 based on the L1-specified vmcs12.</span>
<span class="p_del">- * We could reuse a single VMCS for all the L2 guests, but we also want the</span>
<span class="p_del">- * option to allocate a separate vmcs02 for each separate loaded vmcs12 - this</span>
<span class="p_del">- * allows keeping them loaded on the processor, and in the future will allow</span>
<span class="p_del">- * optimizations where prepare_vmcs02 doesn&#39;t need to set all the fields on</span>
<span class="p_del">- * every entry if they never change.</span>
<span class="p_del">- * So we keep, in vmx-&gt;nested.vmcs02_pool, a cache of size VMCS02_POOL_SIZE</span>
<span class="p_del">- * (&gt;=0) with a vmcs02 for each recently loaded vmcs12s, most recent first.</span>
<span class="p_del">- *</span>
<span class="p_del">- * The following functions allocate and free a vmcs02 in this pool.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-/* Get a VMCS from the pool to use as vmcs02 for the current vmcs12. */</span>
<span class="p_del">-static struct loaded_vmcs *nested_get_current_vmcs02(struct vcpu_vmx *vmx)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct vmcs02_list *item;</span>
<span class="p_del">-	list_for_each_entry(item, &amp;vmx-&gt;nested.vmcs02_pool, list)</span>
<span class="p_del">-		if (item-&gt;vmptr == vmx-&gt;nested.current_vmptr) {</span>
<span class="p_del">-			list_move(&amp;item-&gt;list, &amp;vmx-&gt;nested.vmcs02_pool);</span>
<span class="p_del">-			return &amp;item-&gt;vmcs02;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (vmx-&gt;nested.vmcs02_num &gt;= max(VMCS02_POOL_SIZE, 1)) {</span>
<span class="p_del">-		/* Recycle the least recently used VMCS. */</span>
<span class="p_del">-		item = list_last_entry(&amp;vmx-&gt;nested.vmcs02_pool,</span>
<span class="p_del">-				       struct vmcs02_list, list);</span>
<span class="p_del">-		item-&gt;vmptr = vmx-&gt;nested.current_vmptr;</span>
<span class="p_del">-		list_move(&amp;item-&gt;list, &amp;vmx-&gt;nested.vmcs02_pool);</span>
<span class="p_del">-		return &amp;item-&gt;vmcs02;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Create a new VMCS */</span>
<span class="p_del">-	item = kzalloc(sizeof(struct vmcs02_list), GFP_KERNEL);</span>
<span class="p_del">-	if (!item)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-	item-&gt;vmcs02.vmcs = alloc_vmcs();</span>
<span class="p_del">-	item-&gt;vmcs02.shadow_vmcs = NULL;</span>
<span class="p_del">-	if (!item-&gt;vmcs02.vmcs) {</span>
<span class="p_del">-		kfree(item);</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	loaded_vmcs_init(&amp;item-&gt;vmcs02);</span>
<span class="p_del">-	item-&gt;vmptr = vmx-&gt;nested.current_vmptr;</span>
<span class="p_del">-	list_add(&amp;(item-&gt;list), &amp;(vmx-&gt;nested.vmcs02_pool));</span>
<span class="p_del">-	vmx-&gt;nested.vmcs02_num++;</span>
<span class="p_del">-	return &amp;item-&gt;vmcs02;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Free and remove from pool a vmcs02 saved for a vmcs12 (if there is one) */</span>
<span class="p_del">-static void nested_free_vmcs02(struct vcpu_vmx *vmx, gpa_t vmptr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct vmcs02_list *item;</span>
<span class="p_del">-	list_for_each_entry(item, &amp;vmx-&gt;nested.vmcs02_pool, list)</span>
<span class="p_del">-		if (item-&gt;vmptr == vmptr) {</span>
<span class="p_del">-			free_loaded_vmcs(&amp;item-&gt;vmcs02);</span>
<span class="p_del">-			list_del(&amp;item-&gt;list);</span>
<span class="p_del">-			kfree(item);</span>
<span class="p_del">-			vmx-&gt;nested.vmcs02_num--;</span>
<span class="p_del">-			return;</span>
<span class="p_del">-		}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Free all VMCSs saved for this vcpu, except the one pointed by</span>
<span class="p_del">- * vmx-&gt;loaded_vmcs. We must be running L1, so vmx-&gt;loaded_vmcs</span>
<span class="p_del">- * must be &amp;vmx-&gt;vmcs01.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void nested_free_all_saved_vmcss(struct vcpu_vmx *vmx)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct vmcs02_list *item, *n;</span>
<span class="p_del">-</span>
<span class="p_del">-	WARN_ON(vmx-&gt;loaded_vmcs != &amp;vmx-&gt;vmcs01);</span>
<span class="p_del">-	list_for_each_entry_safe(item, n, &amp;vmx-&gt;nested.vmcs02_pool, list) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Something will leak if the above WARN triggers.  Better than</span>
<span class="p_del">-		 * a use-after-free.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (vmx-&gt;loaded_vmcs == &amp;item-&gt;vmcs02)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		free_loaded_vmcs(&amp;item-&gt;vmcs02);</span>
<span class="p_del">-		list_del(&amp;item-&gt;list);</span>
<span class="p_del">-		kfree(item);</span>
<span class="p_del">-		vmx-&gt;nested.vmcs02_num--;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * The following 3 functions, nested_vmx_succeed()/failValid()/failInvalid(),
  * set the success or error code of an emulated VMX instruction, as specified
<span class="p_chunk">@@ -7241,13 +7313,11 @@</span> <span class="p_context"> static int enter_vmx_operation(struct kvm_vcpu *vcpu)</span>
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	struct vmcs *shadow_vmcs;
<span class="p_add">+	int r;</span>
 
<span class="p_del">-	if (cpu_has_vmx_msr_bitmap()) {</span>
<span class="p_del">-		vmx-&gt;nested.msr_bitmap =</span>
<span class="p_del">-				(unsigned long *)__get_free_page(GFP_KERNEL);</span>
<span class="p_del">-		if (!vmx-&gt;nested.msr_bitmap)</span>
<span class="p_del">-			goto out_msr_bitmap;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	r = alloc_loaded_vmcs(&amp;vmx-&gt;nested.vmcs02);</span>
<span class="p_add">+	if (r &lt; 0)</span>
<span class="p_add">+		goto out_vmcs02;</span>
 
 	vmx-&gt;nested.cached_vmcs12 = kmalloc(VMCS12_SIZE, GFP_KERNEL);
 	if (!vmx-&gt;nested.cached_vmcs12)
<span class="p_chunk">@@ -7264,9 +7334,6 @@</span> <span class="p_context"> static int enter_vmx_operation(struct kvm_vcpu *vcpu)</span>
 		vmx-&gt;vmcs01.shadow_vmcs = shadow_vmcs;
 	}
 
<span class="p_del">-	INIT_LIST_HEAD(&amp;(vmx-&gt;nested.vmcs02_pool));</span>
<span class="p_del">-	vmx-&gt;nested.vmcs02_num = 0;</span>
<span class="p_del">-</span>
 	hrtimer_init(&amp;vmx-&gt;nested.preemption_timer, CLOCK_MONOTONIC,
 		     HRTIMER_MODE_REL_PINNED);
 	vmx-&gt;nested.preemption_timer.function = vmx_preemption_timer_fn;
<span class="p_chunk">@@ -7278,9 +7345,9 @@</span> <span class="p_context"> static int enter_vmx_operation(struct kvm_vcpu *vcpu)</span>
 	kfree(vmx-&gt;nested.cached_vmcs12);
 
 out_cached_vmcs12:
<span class="p_del">-	free_page((unsigned long)vmx-&gt;nested.msr_bitmap);</span>
<span class="p_add">+	free_loaded_vmcs(&amp;vmx-&gt;nested.vmcs02);</span>
 
<span class="p_del">-out_msr_bitmap:</span>
<span class="p_add">+out_vmcs02:</span>
 	return -ENOMEM;
 }
 
<span class="p_chunk">@@ -7423,10 +7490,6 @@</span> <span class="p_context"> static void free_nested(struct vcpu_vmx *vmx)</span>
 	free_vpid(vmx-&gt;nested.vpid02);
 	vmx-&gt;nested.posted_intr_nv = -1;
 	vmx-&gt;nested.current_vmptr = -1ull;
<span class="p_del">-	if (vmx-&gt;nested.msr_bitmap) {</span>
<span class="p_del">-		free_page((unsigned long)vmx-&gt;nested.msr_bitmap);</span>
<span class="p_del">-		vmx-&gt;nested.msr_bitmap = NULL;</span>
<span class="p_del">-	}</span>
 	if (enable_shadow_vmcs) {
 		vmx_disable_shadow_vmcs(vmx);
 		vmcs_clear(vmx-&gt;vmcs01.shadow_vmcs);
<span class="p_chunk">@@ -7434,7 +7497,7 @@</span> <span class="p_context"> static void free_nested(struct vcpu_vmx *vmx)</span>
 		vmx-&gt;vmcs01.shadow_vmcs = NULL;
 	}
 	kfree(vmx-&gt;nested.cached_vmcs12);
<span class="p_del">-	/* Unpin physical memory we referred to in current vmcs02 */</span>
<span class="p_add">+	/* Unpin physical memory we referred to in the vmcs02 */</span>
 	if (vmx-&gt;nested.apic_access_page) {
 		kvm_release_page_dirty(vmx-&gt;nested.apic_access_page);
 		vmx-&gt;nested.apic_access_page = NULL;
<span class="p_chunk">@@ -7450,7 +7513,7 @@</span> <span class="p_context"> static void free_nested(struct vcpu_vmx *vmx)</span>
 		vmx-&gt;nested.pi_desc = NULL;
 	}
 
<span class="p_del">-	nested_free_all_saved_vmcss(vmx);</span>
<span class="p_add">+	free_loaded_vmcs(&amp;vmx-&gt;nested.vmcs02);</span>
 }
 
 /* Emulate the VMXOFF instruction */
<span class="p_chunk">@@ -7493,8 +7556,6 @@</span> <span class="p_context"> static int handle_vmclear(struct kvm_vcpu *vcpu)</span>
 			vmptr + offsetof(struct vmcs12, launch_state),
 			&amp;zero, sizeof(zero));
 
<span class="p_del">-	nested_free_vmcs02(vmx, vmptr);</span>
<span class="p_del">-</span>
 	nested_vmx_succeed(vcpu);
 	return kvm_skip_emulated_instruction(vcpu);
 }
<span class="p_chunk">@@ -8406,10 +8467,11 @@</span> <span class="p_context"> static bool nested_vmx_exit_reflected(struct kvm_vcpu *vcpu, u32 exit_reason)</span>
 
 	/*
 	 * The host physical addresses of some pages of guest memory
<span class="p_del">-	 * are loaded into VMCS02 (e.g. L1&#39;s Virtual APIC Page). The CPU</span>
<span class="p_del">-	 * may write to these pages via their host physical address while</span>
<span class="p_del">-	 * L2 is running, bypassing any address-translation-based dirty</span>
<span class="p_del">-	 * tracking (e.g. EPT write protection).</span>
<span class="p_add">+	 * are loaded into the vmcs02 (e.g. vmcs12&#39;s Virtual APIC</span>
<span class="p_add">+	 * Page). The CPU may write to these pages via their host</span>
<span class="p_add">+	 * physical address while L2 is running, bypassing any</span>
<span class="p_add">+	 * address-translation-based dirty tracking (e.g. EPT write</span>
<span class="p_add">+	 * protection).</span>
 	 *
 	 * Mark them dirty on every exit from L2 to prevent them from
 	 * getting out of sync with dirty tracking.
<span class="p_chunk">@@ -8943,7 +9005,7 @@</span> <span class="p_context"> static void vmx_set_virtual_x2apic_mode(struct kvm_vcpu *vcpu, bool set)</span>
 	}
 	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, sec_exec_control);
 
<span class="p_del">-	vmx_set_msr_bitmap(vcpu);</span>
<span class="p_add">+	vmx_update_msr_bitmap(vcpu);</span>
 }
 
 static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
<span class="p_chunk">@@ -9129,14 +9191,14 @@</span> <span class="p_context"> static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)</span>
 #endif
 			&quot;pushf\n\t&quot;
 			__ASM_SIZE(push) &quot; $%c[cs]\n\t&quot;
<span class="p_del">-			&quot;call *%[entry]\n\t&quot;</span>
<span class="p_add">+			CALL_NOSPEC</span>
 			:
 #ifdef CONFIG_X86_64
 			[sp]&quot;=&amp;r&quot;(tmp),
 #endif
 			ASM_CALL_CONSTRAINT
 			:
<span class="p_del">-			[entry]&quot;r&quot;(entry),</span>
<span class="p_add">+			THUNK_TARGET(entry),</span>
 			[ss]&quot;i&quot;(__KERNEL_DS),
 			[cs]&quot;i&quot;(__KERNEL_CS)
 			);
<span class="p_chunk">@@ -9373,6 +9435,15 @@</span> <span class="p_context"> static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)</span>
 
 	vmx_arm_hv_timer(vcpu);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If this vCPU has touched SPEC_CTRL, restore the guest&#39;s value if</span>
<span class="p_add">+	 * it&#39;s non-zero. Since vmentry is serialising on affected CPUs, there</span>
<span class="p_add">+	 * is no need to worry about the conditional branch over the wrmsr</span>
<span class="p_add">+	 * being speculatively taken.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (vmx-&gt;spec_ctrl)</span>
<span class="p_add">+		wrmsrl(MSR_IA32_SPEC_CTRL, vmx-&gt;spec_ctrl);</span>
<span class="p_add">+</span>
 	vmx-&gt;__launched = vmx-&gt;loaded_vmcs-&gt;launched;
 	asm(
 		/* Store host registers */
<span class="p_chunk">@@ -9491,6 +9562,27 @@</span> <span class="p_context"> static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)</span>
 #endif
 	      );
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We do not use IBRS in the kernel. If this vCPU has used the</span>
<span class="p_add">+	 * SPEC_CTRL MSR it may have left it on; save the value and</span>
<span class="p_add">+	 * turn it off. This is much more efficient than blindly adding</span>
<span class="p_add">+	 * it to the atomic save/restore list. Especially as the former</span>
<span class="p_add">+	 * (Saving guest MSRs on vmexit) doesn&#39;t even exist in KVM.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For non-nested case:</span>
<span class="p_add">+	 * If the L01 MSR bitmap does not intercept the MSR, then we need to</span>
<span class="p_add">+	 * save it.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For nested case:</span>
<span class="p_add">+	 * If the L02 MSR bitmap does not intercept the MSR, then we need to</span>
<span class="p_add">+	 * save it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL))</span>
<span class="p_add">+		rdmsrl(MSR_IA32_SPEC_CTRL, vmx-&gt;spec_ctrl);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vmx-&gt;spec_ctrl)</span>
<span class="p_add">+		wrmsrl(MSR_IA32_SPEC_CTRL, 0);</span>
<span class="p_add">+</span>
 	/* Eliminate branch target predictions from guest mode */
 	vmexit_fill_RSB();
 
<span class="p_chunk">@@ -9604,6 +9696,7 @@</span> <span class="p_context"> static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)</span>
 {
 	int err;
 	struct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
<span class="p_add">+	unsigned long *msr_bitmap;</span>
 	int cpu;
 
 	if (!vmx)
<span class="p_chunk">@@ -9636,13 +9729,20 @@</span> <span class="p_context"> static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)</span>
 	if (!vmx-&gt;guest_msrs)
 		goto free_pml;
 
<span class="p_del">-	vmx-&gt;loaded_vmcs = &amp;vmx-&gt;vmcs01;</span>
<span class="p_del">-	vmx-&gt;loaded_vmcs-&gt;vmcs = alloc_vmcs();</span>
<span class="p_del">-	vmx-&gt;loaded_vmcs-&gt;shadow_vmcs = NULL;</span>
<span class="p_del">-	if (!vmx-&gt;loaded_vmcs-&gt;vmcs)</span>
<span class="p_add">+	err = alloc_loaded_vmcs(&amp;vmx-&gt;vmcs01);</span>
<span class="p_add">+	if (err &lt; 0)</span>
 		goto free_msrs;
<span class="p_del">-	loaded_vmcs_init(vmx-&gt;loaded_vmcs);</span>
 
<span class="p_add">+	msr_bitmap = vmx-&gt;vmcs01.msr_bitmap;</span>
<span class="p_add">+	vmx_disable_intercept_for_msr(msr_bitmap, MSR_FS_BASE, MSR_TYPE_RW);</span>
<span class="p_add">+	vmx_disable_intercept_for_msr(msr_bitmap, MSR_GS_BASE, MSR_TYPE_RW);</span>
<span class="p_add">+	vmx_disable_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);</span>
<span class="p_add">+	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);</span>
<span class="p_add">+	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);</span>
<span class="p_add">+	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);</span>
<span class="p_add">+	vmx-&gt;msr_bitmap_mode = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	vmx-&gt;loaded_vmcs = &amp;vmx-&gt;vmcs01;</span>
 	cpu = get_cpu();
 	vmx_vcpu_load(&amp;vmx-&gt;vcpu, cpu);
 	vmx-&gt;vcpu.cpu = cpu;
<span class="p_chunk">@@ -10105,10 +10205,25 @@</span> <span class="p_context"> static inline bool nested_vmx_merge_msr_bitmap(struct kvm_vcpu *vcpu,</span>
 	int msr;
 	struct page *page;
 	unsigned long *msr_bitmap_l1;
<span class="p_del">-	unsigned long *msr_bitmap_l0 = to_vmx(vcpu)-&gt;nested.msr_bitmap;</span>
<span class="p_add">+	unsigned long *msr_bitmap_l0 = to_vmx(vcpu)-&gt;nested.vmcs02.msr_bitmap;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * pred_cmd &amp; spec_ctrl are trying to verify two things:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 1. L0 gave a permission to L1 to actually passthrough the MSR. This</span>
<span class="p_add">+	 *    ensures that we do not accidentally generate an L02 MSR bitmap</span>
<span class="p_add">+	 *    from the L12 MSR bitmap that is too permissive.</span>
<span class="p_add">+	 * 2. That L1 or L2s have actually used the MSR. This avoids</span>
<span class="p_add">+	 *    unnecessarily merging of the bitmap if the MSR is unused. This</span>
<span class="p_add">+	 *    works properly because we only update the L01 MSR bitmap lazily.</span>
<span class="p_add">+	 *    So even if L0 should pass L1 these MSRs, the L01 bitmap is only</span>
<span class="p_add">+	 *    updated to reflect this when L1 (or its L2s) actually write to</span>
<span class="p_add">+	 *    the MSR.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bool pred_cmd = msr_write_intercepted_l01(vcpu, MSR_IA32_PRED_CMD);</span>
<span class="p_add">+	bool spec_ctrl = msr_write_intercepted_l01(vcpu, MSR_IA32_SPEC_CTRL);</span>
 
<span class="p_del">-	/* This shortcut is ok because we support only x2APIC MSRs so far. */</span>
<span class="p_del">-	if (!nested_cpu_has_virt_x2apic_mode(vmcs12))</span>
<span class="p_add">+	if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &amp;&amp;</span>
<span class="p_add">+	    !pred_cmd &amp;&amp; !spec_ctrl)</span>
 		return false;
 
 	page = kvm_vcpu_gpa_to_page(vcpu, vmcs12-&gt;msr_bitmap);
<span class="p_chunk">@@ -10141,6 +10256,19 @@</span> <span class="p_context"> static inline bool nested_vmx_merge_msr_bitmap(struct kvm_vcpu *vcpu,</span>
 				MSR_TYPE_W);
 		}
 	}
<span class="p_add">+</span>
<span class="p_add">+	if (spec_ctrl)</span>
<span class="p_add">+		nested_vmx_disable_intercept_for_msr(</span>
<span class="p_add">+					msr_bitmap_l1, msr_bitmap_l0,</span>
<span class="p_add">+					MSR_IA32_SPEC_CTRL,</span>
<span class="p_add">+					MSR_TYPE_R | MSR_TYPE_W);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pred_cmd)</span>
<span class="p_add">+		nested_vmx_disable_intercept_for_msr(</span>
<span class="p_add">+					msr_bitmap_l1, msr_bitmap_l0,</span>
<span class="p_add">+					MSR_IA32_PRED_CMD,</span>
<span class="p_add">+					MSR_TYPE_W);</span>
<span class="p_add">+</span>
 	kunmap(page);
 	kvm_release_page_clean(page);
 
<span class="p_chunk">@@ -10682,6 +10810,9 @@</span> <span class="p_context"> static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,</span>
 	if (kvm_has_tsc_control)
 		decache_tsc_multiplier(vmx);
 
<span class="p_add">+	if (cpu_has_vmx_msr_bitmap())</span>
<span class="p_add">+		vmcs_write64(MSR_BITMAP, __pa(vmx-&gt;nested.vmcs02.msr_bitmap));</span>
<span class="p_add">+</span>
 	if (enable_vpid) {
 		/*
 		 * There is no direct mapping between vpid02 and vpid12, the
<span class="p_chunk">@@ -10903,20 +11034,15 @@</span> <span class="p_context"> static int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu, bool from_vmentry)</span>
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
<span class="p_del">-	struct loaded_vmcs *vmcs02;</span>
 	u32 msr_entry_idx;
 	u32 exit_qual;
 
<span class="p_del">-	vmcs02 = nested_get_current_vmcs02(vmx);</span>
<span class="p_del">-	if (!vmcs02)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
 	enter_guest_mode(vcpu);
 
 	if (!(vmcs12-&gt;vm_entry_controls &amp; VM_ENTRY_LOAD_DEBUG_CONTROLS))
 		vmx-&gt;nested.vmcs01_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);
 
<span class="p_del">-	vmx_switch_vmcs(vcpu, vmcs02);</span>
<span class="p_add">+	vmx_switch_vmcs(vcpu, &amp;vmx-&gt;nested.vmcs02);</span>
 	vmx_segment_cache_clear(vmx);
 
 	if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &amp;exit_qual)) {
<span class="p_chunk">@@ -11485,7 +11611,7 @@</span> <span class="p_context"> static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,</span>
 	vmcs_write64(GUEST_IA32_DEBUGCTL, 0);
 
 	if (cpu_has_vmx_msr_bitmap())
<span class="p_del">-		vmx_set_msr_bitmap(vcpu);</span>
<span class="p_add">+		vmx_update_msr_bitmap(vcpu);</span>
 
 	if (nested_vmx_load_msr(vcpu, vmcs12-&gt;vm_exit_msr_load_addr,
 				vmcs12-&gt;vm_exit_msr_load_count))
<span class="p_chunk">@@ -11534,10 +11660,6 @@</span> <span class="p_context"> static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,</span>
 	vm_exit_controls_reset_shadow(vmx);
 	vmx_segment_cache_clear(vmx);
 
<span class="p_del">-	/* if no vmcs02 cache requested, remove the one we used */</span>
<span class="p_del">-	if (VMCS02_POOL_SIZE == 0)</span>
<span class="p_del">-		nested_free_vmcs02(vmx, vmx-&gt;nested.current_vmptr);</span>
<span class="p_del">-</span>
 	/* Update any VMCS fields that might have changed while L2 ran */
 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx-&gt;msr_autoload.nr);
 	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx-&gt;msr_autoload.nr);
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index c53298dfbf50..ac381437c291 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -1009,6 +1009,7 @@</span> <span class="p_context"> static u32 msrs_to_save[] = {</span>
 #endif
 	MSR_IA32_TSC, MSR_IA32_CR_PAT, MSR_VM_HSAVE_PA,
 	MSR_IA32_FEATURE_CONTROL, MSR_IA32_BNDCFGS, MSR_TSC_AUX,
<span class="p_add">+	MSR_IA32_SPEC_CTRL, MSR_IA32_ARCH_CAPABILITIES</span>
 };
 
 static unsigned num_msrs_to_save;
<span class="p_header">diff --git a/arch/x86/lib/Makefile b/arch/x86/lib/Makefile</span>
<span class="p_header">index f23934bbaf4e..69a473919260 100644</span>
<span class="p_header">--- a/arch/x86/lib/Makefile</span>
<span class="p_header">+++ b/arch/x86/lib/Makefile</span>
<span class="p_chunk">@@ -27,6 +27,7 @@</span> <span class="p_context"> lib-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem.o</span>
 lib-$(CONFIG_INSTRUCTION_DECODER) += insn.o inat.o insn-eval.o
 lib-$(CONFIG_RANDOMIZE_BASE) += kaslr.o
 lib-$(CONFIG_RETPOLINE) += retpoline.o
<span class="p_add">+OBJECT_FILES_NON_STANDARD_retpoline.o :=y</span>
 
 obj-y += msr.o msr-reg.o msr-reg-export.o hweight.o
 
<span class="p_header">diff --git a/arch/x86/lib/getuser.S b/arch/x86/lib/getuser.S</span>
<span class="p_header">index c97d935a29e8..49b167f73215 100644</span>
<span class="p_header">--- a/arch/x86/lib/getuser.S</span>
<span class="p_header">+++ b/arch/x86/lib/getuser.S</span>
<span class="p_chunk">@@ -40,6 +40,8 @@</span> <span class="p_context"> ENTRY(__get_user_1)</span>
 	mov PER_CPU_VAR(current_task), %_ASM_DX
 	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX
 	jae bad_get_user
<span class="p_add">+	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */</span>
<span class="p_add">+	and %_ASM_DX, %_ASM_AX</span>
 	ASM_STAC
 1:	movzbl (%_ASM_AX),%edx
 	xor %eax,%eax
<span class="p_chunk">@@ -54,6 +56,8 @@</span> <span class="p_context"> ENTRY(__get_user_2)</span>
 	mov PER_CPU_VAR(current_task), %_ASM_DX
 	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX
 	jae bad_get_user
<span class="p_add">+	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */</span>
<span class="p_add">+	and %_ASM_DX, %_ASM_AX</span>
 	ASM_STAC
 2:	movzwl -1(%_ASM_AX),%edx
 	xor %eax,%eax
<span class="p_chunk">@@ -68,6 +72,8 @@</span> <span class="p_context"> ENTRY(__get_user_4)</span>
 	mov PER_CPU_VAR(current_task), %_ASM_DX
 	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX
 	jae bad_get_user
<span class="p_add">+	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */</span>
<span class="p_add">+	and %_ASM_DX, %_ASM_AX</span>
 	ASM_STAC
 3:	movl -3(%_ASM_AX),%edx
 	xor %eax,%eax
<span class="p_chunk">@@ -83,6 +89,8 @@</span> <span class="p_context"> ENTRY(__get_user_8)</span>
 	mov PER_CPU_VAR(current_task), %_ASM_DX
 	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX
 	jae bad_get_user
<span class="p_add">+	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */</span>
<span class="p_add">+	and %_ASM_DX, %_ASM_AX</span>
 	ASM_STAC
 4:	movq -7(%_ASM_AX),%rdx
 	xor %eax,%eax
<span class="p_chunk">@@ -94,6 +102,8 @@</span> <span class="p_context"> ENTRY(__get_user_8)</span>
 	mov PER_CPU_VAR(current_task), %_ASM_DX
 	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX
 	jae bad_get_user_8
<span class="p_add">+	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */</span>
<span class="p_add">+	and %_ASM_DX, %_ASM_AX</span>
 	ASM_STAC
 4:	movl -7(%_ASM_AX),%edx
 5:	movl -3(%_ASM_AX),%ecx
<span class="p_header">diff --git a/arch/x86/lib/retpoline.S b/arch/x86/lib/retpoline.S</span>
<span class="p_header">index c909961e678a..480edc3a5e03 100644</span>
<span class="p_header">--- a/arch/x86/lib/retpoline.S</span>
<span class="p_header">+++ b/arch/x86/lib/retpoline.S</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/alternative-asm.h&gt;
 #include &lt;asm/export.h&gt;
 #include &lt;asm/nospec-branch.h&gt;
<span class="p_add">+#include &lt;asm/bitsperlong.h&gt;</span>
 
 .macro THUNK reg
 	.section .text.__x86.indirect_thunk
<span class="p_chunk">@@ -46,3 +47,58 @@</span> <span class="p_context"> GENERATE_THUNK(r13)</span>
 GENERATE_THUNK(r14)
 GENERATE_THUNK(r15)
 #endif
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Fill the CPU return stack buffer.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each entry in the RSB, if used for a speculative &#39;ret&#39;, contains an</span>
<span class="p_add">+ * infinite &#39;pause; lfence; jmp&#39; loop to capture speculative execution.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is required in various cases for retpoline and IBRS-based</span>
<span class="p_add">+ * mitigations for the Spectre variant 2 vulnerability. Sometimes to</span>
<span class="p_add">+ * eliminate potentially bogus entries from the RSB, and sometimes</span>
<span class="p_add">+ * purely to ensure that it doesn&#39;t get empty, which on some CPUs would</span>
<span class="p_add">+ * allow predictions from other (unwanted!) sources to be used.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Google experimented with loop-unrolling and this turned out to be</span>
<span class="p_add">+ * the optimal version - two calls, each with their own speculation</span>
<span class="p_add">+ * trap should their return address end up getting used, in a loop.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro STUFF_RSB nr:req sp:req</span>
<span class="p_add">+	mov	$(\nr / 2), %_ASM_BX</span>
<span class="p_add">+	.align 16</span>
<span class="p_add">+771:</span>
<span class="p_add">+	call	772f</span>
<span class="p_add">+773:						/* speculation trap */</span>
<span class="p_add">+	pause</span>
<span class="p_add">+	lfence</span>
<span class="p_add">+	jmp	773b</span>
<span class="p_add">+	.align 16</span>
<span class="p_add">+772:</span>
<span class="p_add">+	call	774f</span>
<span class="p_add">+775:						/* speculation trap */</span>
<span class="p_add">+	pause</span>
<span class="p_add">+	lfence</span>
<span class="p_add">+	jmp	775b</span>
<span class="p_add">+	.align 16</span>
<span class="p_add">+774:</span>
<span class="p_add">+	dec	%_ASM_BX</span>
<span class="p_add">+	jnz	771b</span>
<span class="p_add">+	add	$((BITS_PER_LONG/8) * \nr), \sp</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#define RSB_FILL_LOOPS		16	/* To avoid underflow */</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(__fill_rsb)</span>
<span class="p_add">+	STUFF_RSB RSB_FILL_LOOPS, %_ASM_SP</span>
<span class="p_add">+	ret</span>
<span class="p_add">+END(__fill_rsb)</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(__fill_rsb)</span>
<span class="p_add">+</span>
<span class="p_add">+#define RSB_CLEAR_LOOPS		32	/* To forcibly overwrite all entries */</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(__clear_rsb)</span>
<span class="p_add">+	STUFF_RSB RSB_CLEAR_LOOPS, %_ASM_SP</span>
<span class="p_add">+	ret</span>
<span class="p_add">+END(__clear_rsb)</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(__clear_rsb)</span>
<span class="p_header">diff --git a/arch/x86/lib/usercopy_32.c b/arch/x86/lib/usercopy_32.c</span>
<span class="p_header">index 1b377f734e64..7add8ba06887 100644</span>
<span class="p_header">--- a/arch/x86/lib/usercopy_32.c</span>
<span class="p_header">+++ b/arch/x86/lib/usercopy_32.c</span>
<span class="p_chunk">@@ -331,12 +331,12 @@</span> <span class="p_context"> do {									\</span>
 
 unsigned long __copy_user_ll(void *to, const void *from, unsigned long n)
 {
<span class="p_del">-	stac();</span>
<span class="p_add">+	__uaccess_begin_nospec();</span>
 	if (movsl_is_ok(to, from, n))
 		__copy_user(to, from, n);
 	else
 		n = __copy_user_intel(to, from, n);
<span class="p_del">-	clac();</span>
<span class="p_add">+	__uaccess_end();</span>
 	return n;
 }
 EXPORT_SYMBOL(__copy_user_ll);
<span class="p_chunk">@@ -344,7 +344,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(__copy_user_ll);</span>
 unsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *from,
 					unsigned long n)
 {
<span class="p_del">-	stac();</span>
<span class="p_add">+	__uaccess_begin_nospec();</span>
 #ifdef CONFIG_X86_INTEL_USERCOPY
 	if (n &gt; 64 &amp;&amp; static_cpu_has(X86_FEATURE_XMM2))
 		n = __copy_user_intel_nocache(to, from, n);
<span class="p_chunk">@@ -353,7 +353,7 @@</span> <span class="p_context"> unsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *fr</span>
 #else
 	__copy_user(to, from, n);
 #endif
<span class="p_del">-	clac();</span>
<span class="p_add">+	__uaccess_end();</span>
 	return n;
 }
 EXPORT_SYMBOL(__copy_from_user_ll_nocache_nozero);
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 5bfe61a5e8e3..012d02624848 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -6,13 +6,14 @@</span> <span class="p_context"></span>
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/export.h&gt;
 #include &lt;linux/cpu.h&gt;
<span class="p_add">+#include &lt;linux/debugfs.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/mmu_context.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 #include &lt;asm/cache.h&gt;
 #include &lt;asm/apic.h&gt;
 #include &lt;asm/uv/uv.h&gt;
<span class="p_del">-#include &lt;linux/debugfs.h&gt;</span>
 
 /*
  *	TLB flushing, formerly SMP-only
<span class="p_chunk">@@ -247,6 +248,27 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 	} else {
 		u16 new_asid;
 		bool need_flush;
<span class="p_add">+		u64 last_ctx_id = this_cpu_read(cpu_tlbstate.last_ctx_id);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Avoid user/user BTB poisoning by flushing the branch</span>
<span class="p_add">+		 * predictor when switching between processes. This stops</span>
<span class="p_add">+		 * one process from doing Spectre-v2 attacks on another.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * As an optimization, flush indirect branches only when</span>
<span class="p_add">+		 * switching into processes that disable dumping. This</span>
<span class="p_add">+		 * protects high value processes like gpg, without having</span>
<span class="p_add">+		 * too high performance overhead. IBPB is *expensive*!</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * This will not flush branches when switching into kernel</span>
<span class="p_add">+		 * threads. It will also not flush if we switch to idle</span>
<span class="p_add">+		 * thread and back to the same process. It will flush if we</span>
<span class="p_add">+		 * switch to a different non-dumpable process.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (tsk &amp;&amp; tsk-&gt;mm &amp;&amp;</span>
<span class="p_add">+		    tsk-&gt;mm-&gt;context.ctx_id != last_ctx_id &amp;&amp;</span>
<span class="p_add">+		    get_dumpable(tsk-&gt;mm) != SUID_DUMP_USER)</span>
<span class="p_add">+			indirect_branch_prediction_barrier();</span>
 
 		if (IS_ENABLED(CONFIG_VMAP_STACK)) {
 			/*
<span class="p_chunk">@@ -292,6 +314,14 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
 		}
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Record last user mm&#39;s context id, so we can avoid</span>
<span class="p_add">+		 * flushing branch buffer with IBPB if we switch back</span>
<span class="p_add">+		 * to the same user.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (next != &amp;init_mm)</span>
<span class="p_add">+			this_cpu_write(cpu_tlbstate.last_ctx_id, next-&gt;context.ctx_id);</span>
<span class="p_add">+</span>
 		this_cpu_write(cpu_tlbstate.loaded_mm, next);
 		this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
 	}
<span class="p_chunk">@@ -369,6 +399,7 @@</span> <span class="p_context"> void initialize_tlbstate_and_flush(void)</span>
 	write_cr3(build_cr3(mm-&gt;pgd, 0));
 
 	/* Reinitialize tlbstate. */
<span class="p_add">+	this_cpu_write(cpu_tlbstate.last_ctx_id, mm-&gt;context.ctx_id);</span>
 	this_cpu_write(cpu_tlbstate.loaded_mm_asid, 0);
 	this_cpu_write(cpu_tlbstate.next_asid, 1);
 	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, mm-&gt;context.ctx_id);
<span class="p_header">diff --git a/include/linux/fdtable.h b/include/linux/fdtable.h</span>
<span class="p_header">index 1c65817673db..41615f38bcff 100644</span>
<span class="p_header">--- a/include/linux/fdtable.h</span>
<span class="p_header">+++ b/include/linux/fdtable.h</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/compiler.h&gt;
 #include &lt;linux/spinlock.h&gt;
 #include &lt;linux/rcupdate.h&gt;
<span class="p_add">+#include &lt;linux/nospec.h&gt;</span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/init.h&gt;
 #include &lt;linux/fs.h&gt;
<span class="p_chunk">@@ -82,8 +83,10 @@</span> <span class="p_context"> static inline struct file *__fcheck_files(struct files_struct *files, unsigned i</span>
 {
 	struct fdtable *fdt = rcu_dereference_raw(files-&gt;fdt);
 
<span class="p_del">-	if (fd &lt; fdt-&gt;max_fds)</span>
<span class="p_add">+	if (fd &lt; fdt-&gt;max_fds) {</span>
<span class="p_add">+		fd = array_index_nospec(fd, fdt-&gt;max_fds);</span>
 		return rcu_dereference_raw(fdt-&gt;fd[fd]);
<span class="p_add">+	}</span>
 	return NULL;
 }
 
<span class="p_header">diff --git a/include/linux/init.h b/include/linux/init.h</span>
<span class="p_header">index ea1b31101d9e..506a98151131 100644</span>
<span class="p_header">--- a/include/linux/init.h</span>
<span class="p_header">+++ b/include/linux/init.h</span>
<span class="p_chunk">@@ -5,6 +5,13 @@</span> <span class="p_context"></span>
 #include &lt;linux/compiler.h&gt;
 #include &lt;linux/types.h&gt;
 
<span class="p_add">+/* Built-in __init functions needn&#39;t be compiled with retpoline */</span>
<span class="p_add">+#if defined(RETPOLINE) &amp;&amp; !defined(MODULE)</span>
<span class="p_add">+#define __noretpoline __attribute__((indirect_branch(&quot;keep&quot;)))</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __noretpoline</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* These macros are used to mark some functions or 
  * initialized data (doesn&#39;t apply to uninitialized data)
  * as `initialization&#39; functions. The kernel can take this
<span class="p_chunk">@@ -40,7 +47,7 @@</span> <span class="p_context"></span>
 
 /* These are for everybody (although not all archs will actually
    discard it in modules) */
<span class="p_del">-#define __init		__section(.init.text) __cold  __latent_entropy</span>
<span class="p_add">+#define __init		__section(.init.text) __cold  __latent_entropy __noretpoline</span>
 #define __initdata	__section(.init.data)
 #define __initconst	__section(.init.rodata)
 #define __exitdata	__section(.exit.data)
<span class="p_header">diff --git a/include/linux/module.h b/include/linux/module.h</span>
<span class="p_header">index c69b49abe877..1d8f245967be 100644</span>
<span class="p_header">--- a/include/linux/module.h</span>
<span class="p_header">+++ b/include/linux/module.h</span>
<span class="p_chunk">@@ -801,6 +801,15 @@</span> <span class="p_context"> static inline void module_bug_finalize(const Elf_Ehdr *hdr,</span>
 static inline void module_bug_cleanup(struct module *mod) {}
 #endif	/* CONFIG_GENERIC_BUG */
 
<span class="p_add">+#ifdef RETPOLINE</span>
<span class="p_add">+extern bool retpoline_module_ok(bool has_retpoline);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline bool retpoline_module_ok(bool has_retpoline)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MODULE_SIG
 static inline bool module_sig_ok(struct module *module)
 {
<span class="p_header">diff --git a/include/linux/nospec.h b/include/linux/nospec.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b99bced39ac2</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/nospec.h</span>
<span class="p_chunk">@@ -0,0 +1,72 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+// Copyright(c) 2018 Linus Torvalds. All rights reserved.</span>
<span class="p_add">+// Copyright(c) 2018 Alexei Starovoitov. All rights reserved.</span>
<span class="p_add">+// Copyright(c) 2018 Intel Corporation. All rights reserved.</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_NOSPEC_H</span>
<span class="p_add">+#define _LINUX_NOSPEC_H</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * array_index_mask_nospec() - generate a ~0 mask when index &lt; size, 0 otherwise</span>
<span class="p_add">+ * @index: array element index</span>
<span class="p_add">+ * @size: number of elements in array</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * When @index is out of bounds (@index &gt;= @size), the sign bit will be</span>
<span class="p_add">+ * set.  Extend the sign bit to all bits and invert, giving a result of</span>
<span class="p_add">+ * zero for an out of bounds index, or ~0 if within bounds [0, @size).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef array_index_mask_nospec</span>
<span class="p_add">+static inline unsigned long array_index_mask_nospec(unsigned long index,</span>
<span class="p_add">+						    unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Warn developers about inappropriate array_index_nospec() usage.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Even if the CPU speculates past the WARN_ONCE branch, the</span>
<span class="p_add">+	 * sign bit of @index is taken into account when generating the</span>
<span class="p_add">+	 * mask.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This warning is compiled out when the compiler can infer that</span>
<span class="p_add">+	 * @index and @size are less than LONG_MAX.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (WARN_ONCE(index &gt; LONG_MAX || size &gt; LONG_MAX,</span>
<span class="p_add">+			&quot;array_index_nospec() limited to range of [0, LONG_MAX]\n&quot;))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Always calculate and emit the mask even if the compiler</span>
<span class="p_add">+	 * thinks the mask is not needed. The compiler does not take</span>
<span class="p_add">+	 * into account the value of @index under speculation.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	OPTIMIZER_HIDE_VAR(index);</span>
<span class="p_add">+	return ~(long)(index | (size - 1UL - index)) &gt;&gt; (BITS_PER_LONG - 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * array_index_nospec - sanitize an array index after a bounds check</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For a code sequence like:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *     if (index &lt; size) {</span>
<span class="p_add">+ *         index = array_index_nospec(index, size);</span>
<span class="p_add">+ *         val = array[index];</span>
<span class="p_add">+ *     }</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * ...if the CPU speculates past the bounds check then</span>
<span class="p_add">+ * array_index_nospec() will clamp the index within the range of [0,</span>
<span class="p_add">+ * size).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define array_index_nospec(index, size)					\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	typeof(index) _i = (index);					\</span>
<span class="p_add">+	typeof(size) _s = (size);					\</span>
<span class="p_add">+	unsigned long _mask = array_index_mask_nospec(_i, _s);		\</span>
<span class="p_add">+									\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(_i) &gt; sizeof(long));			\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(_s) &gt; sizeof(long));			\</span>
<span class="p_add">+									\</span>
<span class="p_add">+	_i &amp;= _mask;							\</span>
<span class="p_add">+	_i;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+#endif /* _LINUX_NOSPEC_H */</span>
<span class="p_header">diff --git a/kernel/module.c b/kernel/module.c</span>
<span class="p_header">index dea01ac9cb74..09e48eee4d55 100644</span>
<span class="p_header">--- a/kernel/module.c</span>
<span class="p_header">+++ b/kernel/module.c</span>
<span class="p_chunk">@@ -2863,6 +2863,15 @@</span> <span class="p_context"> static int check_modinfo_livepatch(struct module *mod, struct load_info *info)</span>
 }
 #endif /* CONFIG_LIVEPATCH */
 
<span class="p_add">+static void check_modinfo_retpoline(struct module *mod, struct load_info *info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (retpoline_module_ok(get_modinfo(info, &quot;retpoline&quot;)))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_warn(&quot;%s: loading module not compiled with retpoline compiler.\n&quot;,</span>
<span class="p_add">+		mod-&gt;name);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Sets info-&gt;hdr and info-&gt;len. */
 static int copy_module_from_user(const void __user *umod, unsigned long len,
 				  struct load_info *info)
<span class="p_chunk">@@ -3029,6 +3038,8 @@</span> <span class="p_context"> static int check_modinfo(struct module *mod, struct load_info *info, int flags)</span>
 		add_taint_module(mod, TAINT_OOT_MODULE, LOCKDEP_STILL_OK);
 	}
 
<span class="p_add">+	check_modinfo_retpoline(mod, info);</span>
<span class="p_add">+</span>
 	if (get_modinfo(info, &quot;staging&quot;)) {
 		add_taint_module(mod, TAINT_CRAP, LOCKDEP_STILL_OK);
 		pr_warn(&quot;%s: module is from the staging directory, the quality &quot;
<span class="p_header">diff --git a/net/wireless/nl80211.c b/net/wireless/nl80211.c</span>
<span class="p_header">index 542a4fc0a8d7..4bbcfc1e2d43 100644</span>
<span class="p_header">--- a/net/wireless/nl80211.c</span>
<span class="p_header">+++ b/net/wireless/nl80211.c</span>
<span class="p_chunk">@@ -16,6 +16,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/nl80211.h&gt;
 #include &lt;linux/rtnetlink.h&gt;
 #include &lt;linux/netlink.h&gt;
<span class="p_add">+#include &lt;linux/nospec.h&gt;</span>
 #include &lt;linux/etherdevice.h&gt;
 #include &lt;net/net_namespace.h&gt;
 #include &lt;net/genetlink.h&gt;
<span class="p_chunk">@@ -2056,20 +2057,22 @@</span> <span class="p_context"> static const struct nla_policy txq_params_policy[NL80211_TXQ_ATTR_MAX + 1] = {</span>
 static int parse_txq_params(struct nlattr *tb[],
 			    struct ieee80211_txq_params *txq_params)
 {
<span class="p_add">+	u8 ac;</span>
<span class="p_add">+</span>
 	if (!tb[NL80211_TXQ_ATTR_AC] || !tb[NL80211_TXQ_ATTR_TXOP] ||
 	    !tb[NL80211_TXQ_ATTR_CWMIN] || !tb[NL80211_TXQ_ATTR_CWMAX] ||
 	    !tb[NL80211_TXQ_ATTR_AIFS])
 		return -EINVAL;
 
<span class="p_del">-	txq_params-&gt;ac = nla_get_u8(tb[NL80211_TXQ_ATTR_AC]);</span>
<span class="p_add">+	ac = nla_get_u8(tb[NL80211_TXQ_ATTR_AC]);</span>
 	txq_params-&gt;txop = nla_get_u16(tb[NL80211_TXQ_ATTR_TXOP]);
 	txq_params-&gt;cwmin = nla_get_u16(tb[NL80211_TXQ_ATTR_CWMIN]);
 	txq_params-&gt;cwmax = nla_get_u16(tb[NL80211_TXQ_ATTR_CWMAX]);
 	txq_params-&gt;aifs = nla_get_u8(tb[NL80211_TXQ_ATTR_AIFS]);
 
<span class="p_del">-	if (txq_params-&gt;ac &gt;= NL80211_NUM_ACS)</span>
<span class="p_add">+	if (ac &gt;= NL80211_NUM_ACS)</span>
 		return -EINVAL;
<span class="p_del">-</span>
<span class="p_add">+	txq_params-&gt;ac = array_index_nospec(ac, NL80211_NUM_ACS);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/scripts/mod/modpost.c b/scripts/mod/modpost.c</span>
<span class="p_header">index f51cf977c65b..6510536c06df 100644</span>
<span class="p_header">--- a/scripts/mod/modpost.c</span>
<span class="p_header">+++ b/scripts/mod/modpost.c</span>
<span class="p_chunk">@@ -2165,6 +2165,14 @@</span> <span class="p_context"> static void add_intree_flag(struct buffer *b, int is_intree)</span>
 		buf_printf(b, &quot;\nMODULE_INFO(intree, \&quot;Y\&quot;);\n&quot;);
 }
 
<span class="p_add">+/* Cannot check for assembler */</span>
<span class="p_add">+static void add_retpoline(struct buffer *b)</span>
<span class="p_add">+{</span>
<span class="p_add">+	buf_printf(b, &quot;\n#ifdef RETPOLINE\n&quot;);</span>
<span class="p_add">+	buf_printf(b, &quot;MODULE_INFO(retpoline, \&quot;Y\&quot;);\n&quot;);</span>
<span class="p_add">+	buf_printf(b, &quot;#endif\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void add_staging_flag(struct buffer *b, const char *name)
 {
 	static const char *staging_dir = &quot;drivers/staging&quot;;
<span class="p_chunk">@@ -2506,6 +2514,7 @@</span> <span class="p_context"> int main(int argc, char **argv)</span>
 		err |= check_modname_len(mod);
 		add_header(&amp;buf, mod);
 		add_intree_flag(&amp;buf, !external_module);
<span class="p_add">+		add_retpoline(&amp;buf);</span>
 		add_staging_flag(&amp;buf, mod-&gt;name);
 		err |= add_versions(&amp;buf, mod);
 		add_depends(&amp;buf, mod, modules);
<span class="p_header">diff --git a/tools/objtool/check.c b/tools/objtool/check.c</span>
<span class="p_header">index f40d46e24bcc..9cd028aa1509 100644</span>
<span class="p_header">--- a/tools/objtool/check.c</span>
<span class="p_header">+++ b/tools/objtool/check.c</span>
<span class="p_chunk">@@ -543,18 +543,14 @@</span> <span class="p_context"> static int add_call_destinations(struct objtool_file *file)</span>
 			dest_off = insn-&gt;offset + insn-&gt;len + insn-&gt;immediate;
 			insn-&gt;call_dest = find_symbol_by_offset(insn-&gt;sec,
 								dest_off);
<span class="p_del">-			/*</span>
<span class="p_del">-			 * FIXME: Thanks to retpolines, it&#39;s now considered</span>
<span class="p_del">-			 * normal for a function to call within itself.  So</span>
<span class="p_del">-			 * disable this warning for now.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-#if 0</span>
<span class="p_del">-			if (!insn-&gt;call_dest) {</span>
<span class="p_del">-				WARN_FUNC(&quot;can&#39;t find call dest symbol at offset 0x%lx&quot;,</span>
<span class="p_del">-					  insn-&gt;sec, insn-&gt;offset, dest_off);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!insn-&gt;call_dest &amp;&amp; !insn-&gt;ignore) {</span>
<span class="p_add">+				WARN_FUNC(&quot;unsupported intra-function call&quot;,</span>
<span class="p_add">+					  insn-&gt;sec, insn-&gt;offset);</span>
<span class="p_add">+				WARN(&quot;If this is a retpoline, please patch it in with alternatives and annotate it with ANNOTATE_NOSPEC_ALTERNATIVE.&quot;);</span>
 				return -1;
 			}
<span class="p_del">-#endif</span>
<span class="p_add">+</span>
 		} else if (rela-&gt;sym-&gt;type == STT_SECTION) {
 			insn-&gt;call_dest = find_symbol_by_offset(rela-&gt;sym-&gt;sec,
 								rela-&gt;addend+4);
<span class="p_chunk">@@ -598,7 +594,7 @@</span> <span class="p_context"> static int handle_group_alt(struct objtool_file *file,</span>
 			    struct instruction *orig_insn,
 			    struct instruction **new_insn)
 {
<span class="p_del">-	struct instruction *last_orig_insn, *last_new_insn, *insn, *fake_jump;</span>
<span class="p_add">+	struct instruction *last_orig_insn, *last_new_insn, *insn, *fake_jump = NULL;</span>
 	unsigned long dest_off;
 
 	last_orig_insn = NULL;
<span class="p_chunk">@@ -614,28 +610,30 @@</span> <span class="p_context"> static int handle_group_alt(struct objtool_file *file,</span>
 		last_orig_insn = insn;
 	}
 
<span class="p_del">-	if (!next_insn_same_sec(file, last_orig_insn)) {</span>
<span class="p_del">-		WARN(&quot;%s: don&#39;t know how to handle alternatives at end of section&quot;,</span>
<span class="p_del">-		     special_alt-&gt;orig_sec-&gt;name);</span>
<span class="p_del">-		return -1;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	fake_jump = malloc(sizeof(*fake_jump));</span>
<span class="p_del">-	if (!fake_jump) {</span>
<span class="p_del">-		WARN(&quot;malloc failed&quot;);</span>
<span class="p_del">-		return -1;</span>
<span class="p_add">+	if (next_insn_same_sec(file, last_orig_insn)) {</span>
<span class="p_add">+		fake_jump = malloc(sizeof(*fake_jump));</span>
<span class="p_add">+		if (!fake_jump) {</span>
<span class="p_add">+			WARN(&quot;malloc failed&quot;);</span>
<span class="p_add">+			return -1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		memset(fake_jump, 0, sizeof(*fake_jump));</span>
<span class="p_add">+		INIT_LIST_HEAD(&amp;fake_jump-&gt;alts);</span>
<span class="p_add">+		clear_insn_state(&amp;fake_jump-&gt;state);</span>
<span class="p_add">+</span>
<span class="p_add">+		fake_jump-&gt;sec = special_alt-&gt;new_sec;</span>
<span class="p_add">+		fake_jump-&gt;offset = -1;</span>
<span class="p_add">+		fake_jump-&gt;type = INSN_JUMP_UNCONDITIONAL;</span>
<span class="p_add">+		fake_jump-&gt;jump_dest = list_next_entry(last_orig_insn, list);</span>
<span class="p_add">+		fake_jump-&gt;ignore = true;</span>
 	}
<span class="p_del">-	memset(fake_jump, 0, sizeof(*fake_jump));</span>
<span class="p_del">-	INIT_LIST_HEAD(&amp;fake_jump-&gt;alts);</span>
<span class="p_del">-	clear_insn_state(&amp;fake_jump-&gt;state);</span>
<span class="p_del">-</span>
<span class="p_del">-	fake_jump-&gt;sec = special_alt-&gt;new_sec;</span>
<span class="p_del">-	fake_jump-&gt;offset = -1;</span>
<span class="p_del">-	fake_jump-&gt;type = INSN_JUMP_UNCONDITIONAL;</span>
<span class="p_del">-	fake_jump-&gt;jump_dest = list_next_entry(last_orig_insn, list);</span>
<span class="p_del">-	fake_jump-&gt;ignore = true;</span>
 
 	if (!special_alt-&gt;new_len) {
<span class="p_add">+		if (!fake_jump) {</span>
<span class="p_add">+			WARN(&quot;%s: empty alternative at end of section&quot;,</span>
<span class="p_add">+			     special_alt-&gt;orig_sec-&gt;name);</span>
<span class="p_add">+			return -1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		*new_insn = fake_jump;
 		return 0;
 	}
<span class="p_chunk">@@ -648,6 +646,8 @@</span> <span class="p_context"> static int handle_group_alt(struct objtool_file *file,</span>
 
 		last_new_insn = insn;
 
<span class="p_add">+		insn-&gt;ignore = orig_insn-&gt;ignore_alts;</span>
<span class="p_add">+</span>
 		if (insn-&gt;type != INSN_JUMP_CONDITIONAL &amp;&amp;
 		    insn-&gt;type != INSN_JUMP_UNCONDITIONAL)
 			continue;
<span class="p_chunk">@@ -656,8 +656,14 @@</span> <span class="p_context"> static int handle_group_alt(struct objtool_file *file,</span>
 			continue;
 
 		dest_off = insn-&gt;offset + insn-&gt;len + insn-&gt;immediate;
<span class="p_del">-		if (dest_off == special_alt-&gt;new_off + special_alt-&gt;new_len)</span>
<span class="p_add">+		if (dest_off == special_alt-&gt;new_off + special_alt-&gt;new_len) {</span>
<span class="p_add">+			if (!fake_jump) {</span>
<span class="p_add">+				WARN(&quot;%s: alternative jump to end of section&quot;,</span>
<span class="p_add">+				     special_alt-&gt;orig_sec-&gt;name);</span>
<span class="p_add">+				return -1;</span>
<span class="p_add">+			}</span>
 			insn-&gt;jump_dest = fake_jump;
<span class="p_add">+		}</span>
 
 		if (!insn-&gt;jump_dest) {
 			WARN_FUNC(&quot;can&#39;t find alternative jump destination&quot;,
<span class="p_chunk">@@ -672,7 +678,8 @@</span> <span class="p_context"> static int handle_group_alt(struct objtool_file *file,</span>
 		return -1;
 	}
 
<span class="p_del">-	list_add(&amp;fake_jump-&gt;list, &amp;last_new_insn-&gt;list);</span>
<span class="p_add">+	if (fake_jump)</span>
<span class="p_add">+		list_add(&amp;fake_jump-&gt;list, &amp;last_new_insn-&gt;list);</span>
 
 	return 0;
 }
<span class="p_chunk">@@ -729,10 +736,6 @@</span> <span class="p_context"> static int add_special_section_alts(struct objtool_file *file)</span>
 			goto out;
 		}
 
<span class="p_del">-		/* Ignore retpoline alternatives. */</span>
<span class="p_del">-		if (orig_insn-&gt;ignore_alts)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
 		new_insn = NULL;
 		if (!special_alt-&gt;group || special_alt-&gt;new_len) {
 			new_insn = find_insn(file, special_alt-&gt;new_sec,
<span class="p_chunk">@@ -1089,11 +1092,11 @@</span> <span class="p_context"> static int decode_sections(struct objtool_file *file)</span>
 	if (ret)
 		return ret;
 
<span class="p_del">-	ret = add_call_destinations(file);</span>
<span class="p_add">+	ret = add_special_section_alts(file);</span>
 	if (ret)
 		return ret;
 
<span class="p_del">-	ret = add_special_section_alts(file);</span>
<span class="p_add">+	ret = add_call_destinations(file);</span>
 	if (ret)
 		return ret;
 
<span class="p_chunk">@@ -1720,10 +1723,12 @@</span> <span class="p_context"> static int validate_branch(struct objtool_file *file, struct instruction *first,</span>
 
 		insn-&gt;visited = true;
 
<span class="p_del">-		list_for_each_entry(alt, &amp;insn-&gt;alts, list) {</span>
<span class="p_del">-			ret = validate_branch(file, alt-&gt;insn, state);</span>
<span class="p_del">-			if (ret)</span>
<span class="p_del">-				return 1;</span>
<span class="p_add">+		if (!insn-&gt;ignore_alts) {</span>
<span class="p_add">+			list_for_each_entry(alt, &amp;insn-&gt;alts, list) {</span>
<span class="p_add">+				ret = validate_branch(file, alt-&gt;insn, state);</span>
<span class="p_add">+				if (ret)</span>
<span class="p_add">+					return 1;</span>
<span class="p_add">+			}</span>
 		}
 
 		switch (insn-&gt;type) {
<span class="p_header">diff --git a/tools/objtool/orc_gen.c b/tools/objtool/orc_gen.c</span>
<span class="p_header">index e61fe703197b..18384d9be4e1 100644</span>
<span class="p_header">--- a/tools/objtool/orc_gen.c</span>
<span class="p_header">+++ b/tools/objtool/orc_gen.c</span>
<span class="p_chunk">@@ -98,6 +98,11 @@</span> <span class="p_context"> static int create_orc_entry(struct section *u_sec, struct section *ip_relasec,</span>
 	struct orc_entry *orc;
 	struct rela *rela;
 
<span class="p_add">+	if (!insn_sec-&gt;sym) {</span>
<span class="p_add">+		WARN(&quot;missing symbol for section %s&quot;, insn_sec-&gt;name);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* populate ORC data */
 	orc = (struct orc_entry *)u_sec-&gt;data-&gt;d_buf + idx;
 	memcpy(orc, o, sizeof(*orc));

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



