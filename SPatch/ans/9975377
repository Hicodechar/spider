
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3] mm, sysctl: make NUMA stats configurable - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3] mm, sysctl: make NUMA stats configurable</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=175911">Kemi Wang</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 28, 2017, 6:11 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1506579101-5457-1-git-send-email-kemi.wang@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9975377/mbox/"
   >mbox</a>
|
   <a href="/patch/9975377/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9975377/">/patch/9975377/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	57AEC6034B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Sep 2017 06:13:23 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 447232944C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Sep 2017 06:13:23 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 35FAF29456; Thu, 28 Sep 2017 06:13:23 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 33BBE2944C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Sep 2017 06:13:21 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751649AbdI1GNS (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 28 Sep 2017 02:13:18 -0400
Received: from mga06.intel.com ([134.134.136.31]:39797 &quot;EHLO mga06.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1750929AbdI1GNQ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 28 Sep 2017 02:13:16 -0400
Received: from fmsmga005.fm.intel.com ([10.253.24.32])
	by orsmga104.jf.intel.com with ESMTP; 27 Sep 2017 23:13:15 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.42,448,1500966000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;156433467&quot;
Received: from kemi-desktop.sh.intel.com ([10.239.13.103])
	by fmsmga005.fm.intel.com with ESMTP; 27 Sep 2017 23:13:12 -0700
From: Kemi Wang &lt;kemi.wang@intel.com&gt;
To: &quot;Luis R . Rodriguez&quot; &lt;mcgrof@kernel.org&gt;,
	Kees Cook &lt;keescook@chromium.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Jonathan Corbet &lt;corbet@lwn.net&gt;, Michal Hocko &lt;mhocko@suse.com&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Christopher Lameter &lt;cl@linux.com&gt;,
	Sebastian Andrzej Siewior &lt;bigeasy@linutronix.de&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: Dave &lt;dave.hansen@linux.intel.com&gt;, Tim Chen &lt;tim.c.chen@intel.com&gt;,
	Andi Kleen &lt;andi.kleen@intel.com&gt;,
	Jesper Dangaard Brouer &lt;brouer@redhat.com&gt;,
	Ying Huang &lt;ying.huang@intel.com&gt;,
	Aaron Lu &lt;aaron.lu@intel.com&gt;, Kemi Wang &lt;kemi.wang@intel.com&gt;,
	Proc sysctl &lt;linux-fsdevel@vger.kernel.org&gt;,
	Linux MM &lt;linux-mm@kvack.org&gt;,
	Linux Kernel &lt;linux-kernel@vger.kernel.org&gt;
Subject: [PATCH v3] mm, sysctl: make NUMA stats configurable
Date: Thu, 28 Sep 2017 14:11:41 +0800
Message-Id: &lt;1506579101-5457-1-git-send-email-kemi.wang@intel.com&gt;
X-Mailer: git-send-email 2.7.4
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175911">Kemi Wang</a> - Sept. 28, 2017, 6:11 a.m.</div>
<pre class="content">
This is the second step which introduces a tunable interface that allow
numa stats configurable for optimizing zone_statistics(), as suggested by
Dave Hansen and Ying Huang.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Sept. 29, 2017, 7:03 a.m.</div>
<pre class="content">
On 09/28/2017 08:11 AM, Kemi Wang wrote:
<span class="quote">&gt; This is the second step which introduces a tunable interface that allow</span>
<span class="quote">&gt; numa stats configurable for optimizing zone_statistics(), as suggested by</span>
<span class="quote">&gt; Dave Hansen and Ying Huang.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt; some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt; do:</span>
<span class="quote">&gt; 	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; In this case, numa counter update is ignored. We can see about</span>
<span class="quote">&gt; *4.8%*(185-&gt;176) drop of cpu cycles per single page allocation and reclaim</span>
<span class="quote">&gt; on Jesper&#39;s page_bench01 (single thread) and *8.1%*(343-&gt;315) drop of cpu</span>
<span class="quote">&gt; cycles per single page allocation and reclaim on Jesper&#39;s page_bench03 (88</span>
<span class="quote">&gt; threads) running on a 2-Socket Broadwell-based server (88 threads, 126G</span>
<span class="quote">&gt; memory).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Benchmark link provided by Jesper D Brouer(increase loop times to</span>
<span class="quote">&gt; 10000000):</span>
<span class="quote">&gt; https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/</span>
<span class="quote">&gt; bench</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt; tooling to work, you can do:</span>
<span class="quote">&gt; 	echo [S|s]trict &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; We recommend automatic detection of numa statistics by system, this is also</span>
<span class="quote">&gt; system default configuration, you can do:</span>
<span class="quote">&gt; 	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; In this case, numa counter update is skipped unless it has been read by</span>
<span class="quote">&gt; users at least once, e.g. cat /proc/zoneinfo.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Branch target selection with jump label:</span>
<span class="quote">&gt; a) When numa_stats_mode is changed to *strict*, jump to the branch for numa</span>
<span class="quote">&gt; counters update.</span>
<span class="quote">&gt; b) When numa_stats_mode is changed to *coarse*, return back directly.</span>
<span class="quote">&gt; c) When numa_stats_mode is changed to *auto*, the branch target used in</span>
<span class="quote">&gt; last time is kept, and the branch target is changed to the branch for numa</span>
<span class="quote">&gt; counters update once numa counters are *read* by users.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Therefore, with the help of jump label, the page allocation performance is</span>
<span class="quote">&gt; hardly affected when numa counters are updated with a call in</span>
<span class="quote">&gt; zone_statistics(). Meanwhile, the auto mode can give people benefit without</span>
<span class="quote">&gt; manual tuning.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Many thanks to Michal Hocko, Dave Hansen and Ying Huang for comments to</span>
<span class="quote">&gt; help improve the original patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog:</span>
<span class="quote">&gt;   V2-&gt;V3:</span>
<span class="quote">&gt;   a) Propose a better way to use jump label to eliminate the overhead of</span>
<span class="quote">&gt;   branch selection in zone_statistics(), as inspired by Ying Huang;</span>
<span class="quote">&gt;   b) Add a paragraph in commit log to describe the way for branch target</span>
<span class="quote">&gt;   selection;</span>
<span class="quote">&gt;   c) Use a more descriptive name numa_stats_mode instead of vmstat_mode,</span>
<span class="quote">&gt;   and change the description accordingly, as suggested by Michal Hocko;</span>
<span class="quote">&gt;   d) Make this functionality NUMA-specific via ifdef</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   V1-&gt;V2:</span>
<span class="quote">&gt;   a) Merge to one patch;</span>
<span class="quote">&gt;   b) Use jump label to eliminate the overhead of branch selection;</span>
<span class="quote">&gt;   c) Add a single-time log message at boot time to help tell users what</span>
<span class="quote">&gt;   happened.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reported-by: Jesper Dangaard Brouer &lt;brouer@redhat.com&gt;</span>
<span class="quote">&gt; Suggested-by: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="quote">&gt; Suggested-by: Ying Huang &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Kemi Wang &lt;kemi.wang@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  Documentation/sysctl/vm.txt |  24 +++++++++</span>
<span class="quote">&gt;  drivers/base/node.c         |   4 ++</span>
<span class="quote">&gt;  include/linux/vmstat.h      |  23 ++++++++</span>
<span class="quote">&gt;  init/main.c                 |   3 ++</span>
<span class="quote">&gt;  kernel/sysctl.c             |   7 +++</span>
<span class="quote">&gt;  mm/page_alloc.c             |  10 ++++</span>
<span class="quote">&gt;  mm/vmstat.c                 | 129 ++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  7 files changed, 200 insertions(+)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; index 9baf66a..e310e69 100644</span>
<span class="quote">&gt; --- a/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; +++ b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; @@ -61,6 +61,7 @@ Currently, these files are in /proc/sys/vm:</span>
<span class="quote">&gt;  - swappiness</span>
<span class="quote">&gt;  - user_reserve_kbytes</span>
<span class="quote">&gt;  - vfs_cache_pressure</span>
<span class="quote">&gt; +- numa_stats_mode</span>
<span class="quote">&gt;  - watermark_scale_factor</span>
<span class="quote">&gt;  - zone_reclaim_mode</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -843,6 +844,29 @@ ten times more freeable objects than there are.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  =============================================================</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +numa_stats_mode</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +This interface allows numa statistics configurable.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt; +some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt; +do:</span>
<span class="quote">&gt; +	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt; +tooling to work, you can do:</span>
<span class="quote">&gt; +	echo [S|s]trict &gt; /proc/sys/vm/numa_stat_mode</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +We recommend automatic detection of numa statistics by system, because numa</span>
<span class="quote">&gt; +statistics does not affect system&#39;s decision and it is very rarely</span>
<span class="quote">&gt; +consumed. you can do:</span>
<span class="quote">&gt; +	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; +This is also system default configuration, with this default setting, numa</span>
<span class="quote">&gt; +counters update is skipped unless the counter is *read* by users at least</span>
<span class="quote">&gt; +once.</span>

It says &quot;the counter&quot;, but it seems multiple files in /proc and /sys are
triggering this, so perhaps list them?
Also, is it possible that with contemporary userspace/distros (systemd
etc.) there will always be something that will read one of those upon boot?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +==============================================================</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  watermark_scale_factor:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  This factor controls the aggressiveness of kswapd. It defines the</span>
<span class="quote">&gt; diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="quote">&gt; index 3855902..b57b5622 100644</span>
<span class="quote">&gt; --- a/drivers/base/node.c</span>
<span class="quote">&gt; +++ b/drivers/base/node.c</span>
<span class="quote">&gt; @@ -153,6 +153,8 @@ static DEVICE_ATTR(meminfo, S_IRUGO, node_read_meminfo, NULL);</span>
<span class="quote">&gt;  static ssize_t node_read_numastat(struct device *dev,</span>
<span class="quote">&gt;  				struct device_attribute *attr, char *buf)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;  	return sprintf(buf,</span>
<span class="quote">&gt;  		       &quot;numa_hit %lu\n&quot;</span>
<span class="quote">&gt;  		       &quot;numa_miss %lu\n&quot;</span>
<span class="quote">&gt; @@ -186,6 +188,8 @@ static ssize_t node_read_vmstat(struct device *dev,</span>
<span class="quote">&gt;  		n += sprintf(buf+n, &quot;%s %lu\n&quot;,</span>
<span class="quote">&gt;  			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],</span>
<span class="quote">&gt;  			     sum_zone_numa_state(nid, i));</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="quote">&gt; diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h</span>
<span class="quote">&gt; index ade7cb5..d52e882 100644</span>
<span class="quote">&gt; --- a/include/linux/vmstat.h</span>
<span class="quote">&gt; +++ b/include/linux/vmstat.h</span>
<span class="quote">&gt; @@ -6,9 +6,28 @@</span>
<span class="quote">&gt;  #include &lt;linux/mmzone.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/vm_event_item.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/atomic.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/static_key.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern int sysctl_stat_interval;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +DECLARE_STATIC_KEY_FALSE(vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * vm_numa_stats_mode:</span>
<span class="quote">&gt; + * 0 = auto mode of NUMA stats, automatic detection of NUMA statistics.</span>
<span class="quote">&gt; + * 1 = strict mode of NUMA stats, keep NUMA statistics.</span>
<span class="quote">&gt; + * 2 = coarse mode of NUMA stats, ignore NUMA statistics.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_AUTO_MODE 0</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_STRICT_MODE  1</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_COARSE_MODE  2</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_MODE_LEN 16</span>
<span class="quote">&gt; +extern int vm_numa_stats_mode;</span>
<span class="quote">&gt; +extern char sysctl_vm_numa_stats_mode[];</span>
<span class="quote">&gt; +extern int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="quote">&gt; +		void __user *buffer, size_t *length, loff_t *ppos);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_VM_EVENT_COUNTERS</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Light weight per cpu counter implementation.</span>
<span class="quote">&gt; @@ -229,6 +248,10 @@ extern unsigned long sum_zone_node_page_state(int node,</span>
<span class="quote">&gt;  extern unsigned long sum_zone_numa_state(int node, enum numa_stat_item item);</span>
<span class="quote">&gt;  extern unsigned long node_page_state(struct pglist_data *pgdat,</span>
<span class="quote">&gt;  						enum node_stat_item item);</span>
<span class="quote">&gt; +extern void zero_zone_numa_counters(struct zone *zone);</span>
<span class="quote">&gt; +extern void zero_zones_numa_counters(void);</span>
<span class="quote">&gt; +extern void zero_global_numa_counters(void);</span>
<span class="quote">&gt; +extern void invalid_numa_statistics(void);</span>

These seem to be called only from within mm/vmstat.c where they live, so
I&#39;d suggest removing these extern declarations, and making them static
in vmstat.c.

...
<span class="quote">
&gt;  #define NUMA_STATS_THRESHOLD (U16_MAX - 2)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +int vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="quote">&gt; +char sysctl_vm_numa_stats_mode[VM_NUMA_STAT_MODE_LEN] = &quot;auto&quot;;</span>
<span class="quote">&gt; +static const char *vm_numa_stats_mode_name[3] = {&quot;auto&quot;, &quot;strict&quot;, &quot;coarse&quot;};</span>
<span class="quote">&gt; +static DEFINE_MUTEX(vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int __parse_vm_numa_stats_mode(char *s)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	const char *str = s;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (strcmp(str, &quot;auto&quot;) == 0 || strcmp(str, &quot;Auto&quot;) == 0)</span>
<span class="quote">&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="quote">&gt; +	else if (strcmp(str, &quot;strict&quot;) == 0 || strcmp(str, &quot;Strict&quot;) == 0)</span>
<span class="quote">&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_STRICT_MODE;</span>
<span class="quote">&gt; +	else if (strcmp(str, &quot;coarse&quot;) == 0 || strcmp(str, &quot;Coarse&quot;) == 0)</span>
<span class="quote">&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_COARSE_MODE;</span>
<span class="quote">&gt; +	else {</span>
<span class="quote">&gt; +		pr_warn(&quot;Ignoring invalid vm_numa_stats_mode value: %s\n&quot;, s);</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="quote">&gt; +		void __user *buffer, size_t *length, loff_t *ppos)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	char old_string[VM_NUMA_STAT_MODE_LEN];</span>
<span class="quote">&gt; +	int ret, oldval;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mutex_lock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +	if (write)</span>
<span class="quote">&gt; +		strncpy(old_string, (char *)table-&gt;data, VM_NUMA_STAT_MODE_LEN);</span>
<span class="quote">&gt; +	ret = proc_dostring(table, write, buffer, length, ppos);</span>
<span class="quote">&gt; +	if (ret || !write) {</span>
<span class="quote">&gt; +		mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	oldval = vm_numa_stats_mode;</span>
<span class="quote">&gt; +	if (__parse_vm_numa_stats_mode((char *)table-&gt;data)) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * invalid sysctl_vm_numa_stats_mode value, restore saved string</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		strncpy((char *)table-&gt;data, old_string, VM_NUMA_STAT_MODE_LEN);</span>
<span class="quote">&gt; +		vm_numa_stats_mode = oldval;</span>

Do we need to restore vm_numa_stats_mode? AFAICS it didn&#39;t change. Also,
should the EINVAL be returned also to userspace? (not sure what&#39;s the
API here, hmm man 2 sysctl doesn&#39;t list EINVAL...)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Sept. 29, 2017, 7:27 a.m.</div>
<pre class="content">
[+CC linux-api]

On 09/28/2017 08:11 AM, Kemi Wang wrote:
<span class="quote">&gt; This is the second step which introduces a tunable interface that allow</span>
<span class="quote">&gt; numa stats configurable for optimizing zone_statistics(), as suggested by</span>
<span class="quote">&gt; Dave Hansen and Ying Huang.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt; some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt; do:</span>
<span class="quote">&gt; 	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; In this case, numa counter update is ignored. We can see about</span>
<span class="quote">&gt; *4.8%*(185-&gt;176) drop of cpu cycles per single page allocation and reclaim</span>
<span class="quote">&gt; on Jesper&#39;s page_bench01 (single thread) and *8.1%*(343-&gt;315) drop of cpu</span>
<span class="quote">&gt; cycles per single page allocation and reclaim on Jesper&#39;s page_bench03 (88</span>
<span class="quote">&gt; threads) running on a 2-Socket Broadwell-based server (88 threads, 126G</span>
<span class="quote">&gt; memory).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Benchmark link provided by Jesper D Brouer(increase loop times to</span>
<span class="quote">&gt; 10000000):</span>
<span class="quote">&gt; https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/</span>
<span class="quote">&gt; bench</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt; tooling to work, you can do:</span>
<span class="quote">&gt; 	echo [S|s]trict &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; We recommend automatic detection of numa statistics by system, this is also</span>
<span class="quote">&gt; system default configuration, you can do:</span>
<span class="quote">&gt; 	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; In this case, numa counter update is skipped unless it has been read by</span>
<span class="quote">&gt; users at least once, e.g. cat /proc/zoneinfo.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Branch target selection with jump label:</span>
<span class="quote">&gt; a) When numa_stats_mode is changed to *strict*, jump to the branch for numa</span>
<span class="quote">&gt; counters update.</span>
<span class="quote">&gt; b) When numa_stats_mode is changed to *coarse*, return back directly.</span>
<span class="quote">&gt; c) When numa_stats_mode is changed to *auto*, the branch target used in</span>
<span class="quote">&gt; last time is kept, and the branch target is changed to the branch for numa</span>
<span class="quote">&gt; counters update once numa counters are *read* by users.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Therefore, with the help of jump label, the page allocation performance is</span>
<span class="quote">&gt; hardly affected when numa counters are updated with a call in</span>
<span class="quote">&gt; zone_statistics(). Meanwhile, the auto mode can give people benefit without</span>
<span class="quote">&gt; manual tuning.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Many thanks to Michal Hocko, Dave Hansen and Ying Huang for comments to</span>
<span class="quote">&gt; help improve the original patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog:</span>
<span class="quote">&gt;   V2-&gt;V3:</span>
<span class="quote">&gt;   a) Propose a better way to use jump label to eliminate the overhead of</span>
<span class="quote">&gt;   branch selection in zone_statistics(), as inspired by Ying Huang;</span>
<span class="quote">&gt;   b) Add a paragraph in commit log to describe the way for branch target</span>
<span class="quote">&gt;   selection;</span>
<span class="quote">&gt;   c) Use a more descriptive name numa_stats_mode instead of vmstat_mode,</span>
<span class="quote">&gt;   and change the description accordingly, as suggested by Michal Hocko;</span>
<span class="quote">&gt;   d) Make this functionality NUMA-specific via ifdef</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   V1-&gt;V2:</span>
<span class="quote">&gt;   a) Merge to one patch;</span>
<span class="quote">&gt;   b) Use jump label to eliminate the overhead of branch selection;</span>
<span class="quote">&gt;   c) Add a single-time log message at boot time to help tell users what</span>
<span class="quote">&gt;   happened.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reported-by: Jesper Dangaard Brouer &lt;brouer@redhat.com&gt;</span>
<span class="quote">&gt; Suggested-by: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="quote">&gt; Suggested-by: Ying Huang &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Kemi Wang &lt;kemi.wang@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  Documentation/sysctl/vm.txt |  24 +++++++++</span>
<span class="quote">&gt;  drivers/base/node.c         |   4 ++</span>
<span class="quote">&gt;  include/linux/vmstat.h      |  23 ++++++++</span>
<span class="quote">&gt;  init/main.c                 |   3 ++</span>
<span class="quote">&gt;  kernel/sysctl.c             |   7 +++</span>
<span class="quote">&gt;  mm/page_alloc.c             |  10 ++++</span>
<span class="quote">&gt;  mm/vmstat.c                 | 129 ++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  7 files changed, 200 insertions(+)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; index 9baf66a..e310e69 100644</span>
<span class="quote">&gt; --- a/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; +++ b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; @@ -61,6 +61,7 @@ Currently, these files are in /proc/sys/vm:</span>
<span class="quote">&gt;  - swappiness</span>
<span class="quote">&gt;  - user_reserve_kbytes</span>
<span class="quote">&gt;  - vfs_cache_pressure</span>
<span class="quote">&gt; +- numa_stats_mode</span>
<span class="quote">&gt;  - watermark_scale_factor</span>
<span class="quote">&gt;  - zone_reclaim_mode</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -843,6 +844,29 @@ ten times more freeable objects than there are.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  =============================================================</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +numa_stats_mode</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +This interface allows numa statistics configurable.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt; +some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt; +do:</span>
<span class="quote">&gt; +	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt; +tooling to work, you can do:</span>
<span class="quote">&gt; +	echo [S|s]trict &gt; /proc/sys/vm/numa_stat_mode</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +We recommend automatic detection of numa statistics by system, because numa</span>
<span class="quote">&gt; +statistics does not affect system&#39;s decision and it is very rarely</span>
<span class="quote">&gt; +consumed. you can do:</span>
<span class="quote">&gt; +	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; +This is also system default configuration, with this default setting, numa</span>
<span class="quote">&gt; +counters update is skipped unless the counter is *read* by users at least</span>
<span class="quote">&gt; +once.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +==============================================================</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  watermark_scale_factor:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  This factor controls the aggressiveness of kswapd. It defines the</span>
<span class="quote">&gt; diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="quote">&gt; index 3855902..b57b5622 100644</span>
<span class="quote">&gt; --- a/drivers/base/node.c</span>
<span class="quote">&gt; +++ b/drivers/base/node.c</span>
<span class="quote">&gt; @@ -153,6 +153,8 @@ static DEVICE_ATTR(meminfo, S_IRUGO, node_read_meminfo, NULL);</span>
<span class="quote">&gt;  static ssize_t node_read_numastat(struct device *dev,</span>
<span class="quote">&gt;  				struct device_attribute *attr, char *buf)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;  	return sprintf(buf,</span>
<span class="quote">&gt;  		       &quot;numa_hit %lu\n&quot;</span>
<span class="quote">&gt;  		       &quot;numa_miss %lu\n&quot;</span>
<span class="quote">&gt; @@ -186,6 +188,8 @@ static ssize_t node_read_vmstat(struct device *dev,</span>
<span class="quote">&gt;  		n += sprintf(buf+n, &quot;%s %lu\n&quot;,</span>
<span class="quote">&gt;  			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],</span>
<span class="quote">&gt;  			     sum_zone_numa_state(nid, i));</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="quote">&gt; diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h</span>
<span class="quote">&gt; index ade7cb5..d52e882 100644</span>
<span class="quote">&gt; --- a/include/linux/vmstat.h</span>
<span class="quote">&gt; +++ b/include/linux/vmstat.h</span>
<span class="quote">&gt; @@ -6,9 +6,28 @@</span>
<span class="quote">&gt;  #include &lt;linux/mmzone.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/vm_event_item.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/atomic.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/static_key.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern int sysctl_stat_interval;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +DECLARE_STATIC_KEY_FALSE(vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * vm_numa_stats_mode:</span>
<span class="quote">&gt; + * 0 = auto mode of NUMA stats, automatic detection of NUMA statistics.</span>
<span class="quote">&gt; + * 1 = strict mode of NUMA stats, keep NUMA statistics.</span>
<span class="quote">&gt; + * 2 = coarse mode of NUMA stats, ignore NUMA statistics.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_AUTO_MODE 0</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_STRICT_MODE  1</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_COARSE_MODE  2</span>
<span class="quote">&gt; +#define VM_NUMA_STAT_MODE_LEN 16</span>
<span class="quote">&gt; +extern int vm_numa_stats_mode;</span>
<span class="quote">&gt; +extern char sysctl_vm_numa_stats_mode[];</span>
<span class="quote">&gt; +extern int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="quote">&gt; +		void __user *buffer, size_t *length, loff_t *ppos);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_VM_EVENT_COUNTERS</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Light weight per cpu counter implementation.</span>
<span class="quote">&gt; @@ -229,6 +248,10 @@ extern unsigned long sum_zone_node_page_state(int node,</span>
<span class="quote">&gt;  extern unsigned long sum_zone_numa_state(int node, enum numa_stat_item item);</span>
<span class="quote">&gt;  extern unsigned long node_page_state(struct pglist_data *pgdat,</span>
<span class="quote">&gt;  						enum node_stat_item item);</span>
<span class="quote">&gt; +extern void zero_zone_numa_counters(struct zone *zone);</span>
<span class="quote">&gt; +extern void zero_zones_numa_counters(void);</span>
<span class="quote">&gt; +extern void zero_global_numa_counters(void);</span>
<span class="quote">&gt; +extern void invalid_numa_statistics(void);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  #define sum_zone_node_page_state(node, item) global_zone_page_state(item)</span>
<span class="quote">&gt;  #define node_page_state(node, item) global_node_page_state(item)</span>
<span class="quote">&gt; diff --git a/init/main.c b/init/main.c</span>
<span class="quote">&gt; index 0ee9c686..1e300a8 100644</span>
<span class="quote">&gt; --- a/init/main.c</span>
<span class="quote">&gt; +++ b/init/main.c</span>
<span class="quote">&gt; @@ -567,6 +567,9 @@ asmlinkage __visible void __init start_kernel(void)</span>
<span class="quote">&gt;  	sort_main_extable();</span>
<span class="quote">&gt;  	trap_init();</span>
<span class="quote">&gt;  	mm_init();</span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +	pr_info(&quot;vmstat: NUMA stats is skipped unless it has been consumed\n&quot;);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ftrace_init();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/kernel/sysctl.c b/kernel/sysctl.c</span>
<span class="quote">&gt; index 6648fbb..0678668 100644</span>
<span class="quote">&gt; --- a/kernel/sysctl.c</span>
<span class="quote">&gt; +++ b/kernel/sysctl.c</span>
<span class="quote">&gt; @@ -1374,6 +1374,13 @@ static struct ctl_table vm_table[] = {</span>
<span class="quote">&gt;  		.mode           = 0644,</span>
<span class="quote">&gt;  		.proc_handler   = &amp;hugetlb_mempolicy_sysctl_handler,</span>
<span class="quote">&gt;  	},</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		.procname	= &quot;numa_stats_mode&quot;,</span>
<span class="quote">&gt; +		.data		= sysctl_vm_numa_stats_mode,</span>
<span class="quote">&gt; +		.maxlen		= VM_NUMA_STAT_MODE_LEN,</span>
<span class="quote">&gt; +		.mode		= 0644,</span>
<span class="quote">&gt; +		.proc_handler	= sysctl_vm_numa_stats_mode_handler,</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	 {</span>
<span class="quote">&gt;  		.procname	= &quot;hugetlb_shm_group&quot;,</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index c841af8..6d7ea18 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -83,6 +83,8 @@ DEFINE_PER_CPU(int, numa_node);</span>
<span class="quote">&gt;  EXPORT_PER_CPU_SYMBOL(numa_node);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +DEFINE_STATIC_KEY_FALSE(vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_HAVE_MEMORYLESS_NODES</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * N.B., Do NOT reference the &#39;_numa_mem_&#39; per cpu variable directly.</span>
<span class="quote">&gt; @@ -2743,6 +2745,14 @@ static inline void zone_statistics(struct zone *preferred_zone, struct zone *z)</span>
<span class="quote">&gt;  #ifdef CONFIG_NUMA</span>
<span class="quote">&gt;  	enum numa_stat_item local_stat = NUMA_LOCAL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * skip zone_statistics() if NUMA stats is set to coarse mode or</span>
<span class="quote">&gt; +	 * NUMA stats is never consumed in auto mode.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;vm_numa_stats_mode_key))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (z-&gt;node != numa_node_id())</span>
<span class="quote">&gt;  		local_stat = NUMA_OTHER;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="quote">&gt; index 4bb13e7..469599c 100644</span>
<span class="quote">&gt; --- a/mm/vmstat.c</span>
<span class="quote">&gt; +++ b/mm/vmstat.c</span>
<span class="quote">&gt; @@ -32,6 +32,91 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define NUMA_STATS_THRESHOLD (U16_MAX - 2)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +int vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="quote">&gt; +char sysctl_vm_numa_stats_mode[VM_NUMA_STAT_MODE_LEN] = &quot;auto&quot;;</span>
<span class="quote">&gt; +static const char *vm_numa_stats_mode_name[3] = {&quot;auto&quot;, &quot;strict&quot;, &quot;coarse&quot;};</span>
<span class="quote">&gt; +static DEFINE_MUTEX(vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int __parse_vm_numa_stats_mode(char *s)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	const char *str = s;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (strcmp(str, &quot;auto&quot;) == 0 || strcmp(str, &quot;Auto&quot;) == 0)</span>
<span class="quote">&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="quote">&gt; +	else if (strcmp(str, &quot;strict&quot;) == 0 || strcmp(str, &quot;Strict&quot;) == 0)</span>
<span class="quote">&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_STRICT_MODE;</span>
<span class="quote">&gt; +	else if (strcmp(str, &quot;coarse&quot;) == 0 || strcmp(str, &quot;Coarse&quot;) == 0)</span>
<span class="quote">&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_COARSE_MODE;</span>
<span class="quote">&gt; +	else {</span>
<span class="quote">&gt; +		pr_warn(&quot;Ignoring invalid vm_numa_stats_mode value: %s\n&quot;, s);</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="quote">&gt; +		void __user *buffer, size_t *length, loff_t *ppos)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	char old_string[VM_NUMA_STAT_MODE_LEN];</span>
<span class="quote">&gt; +	int ret, oldval;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mutex_lock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +	if (write)</span>
<span class="quote">&gt; +		strncpy(old_string, (char *)table-&gt;data, VM_NUMA_STAT_MODE_LEN);</span>
<span class="quote">&gt; +	ret = proc_dostring(table, write, buffer, length, ppos);</span>
<span class="quote">&gt; +	if (ret || !write) {</span>
<span class="quote">&gt; +		mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	oldval = vm_numa_stats_mode;</span>
<span class="quote">&gt; +	if (__parse_vm_numa_stats_mode((char *)table-&gt;data)) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * invalid sysctl_vm_numa_stats_mode value, restore saved string</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		strncpy((char *)table-&gt;data, old_string, VM_NUMA_STAT_MODE_LEN);</span>
<span class="quote">&gt; +		vm_numa_stats_mode = oldval;</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * check whether numa stats mode changes or not</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (vm_numa_stats_mode == oldval) {</span>
<span class="quote">&gt; +			/* no change */</span>
<span class="quote">&gt; +			mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +			return 0;</span>
<span class="quote">&gt; +		} else if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Keep the branch selection in last time when numa stats</span>
<span class="quote">&gt; +			 * is changed to auto mode.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			pr_info(&quot;numa stats changes from %s mode to auto mode\n&quot;,</span>
<span class="quote">&gt; +					vm_numa_stats_mode_name[oldval]);</span>
<span class="quote">&gt; +		else if (vm_numa_stats_mode == VM_NUMA_STAT_STRICT_MODE) {</span>
<span class="quote">&gt; +			static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +			pr_info(&quot;numa stats changes from %s mode to strict mode\n&quot;,</span>
<span class="quote">&gt; +					vm_numa_stats_mode_name[oldval]);</span>
<span class="quote">&gt; +		} else if (vm_numa_stats_mode == VM_NUMA_STAT_COARSE_MODE) {</span>
<span class="quote">&gt; +			static_branch_disable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Invalidate numa counters when vmstat mode is set to coarse</span>
<span class="quote">&gt; +			 * mode, because users can&#39;t tell the difference between the</span>
<span class="quote">&gt; +			 * dead state and when allocator activity is quiet once</span>
<span class="quote">&gt; +			 * zone_statistics() is turned off.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			invalid_numa_statistics();</span>
<span class="quote">&gt; +			pr_info(&quot;numa stats changes from %s mode to coarse mode\n&quot;,</span>
<span class="quote">&gt; +					vm_numa_stats_mode_name[oldval]);</span>
<span class="quote">&gt; +		} else</span>
<span class="quote">&gt; +			pr_warn(&quot;invalid vm_numa_stats_mode:%d\n&quot;, vm_numa_stats_mode);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_VM_EVENT_COUNTERS</span>
<span class="quote">&gt;  DEFINE_PER_CPU(struct vm_event_state, vm_event_states) = {{0}};</span>
<span class="quote">&gt;  EXPORT_PER_CPU_SYMBOL(vm_event_states);</span>
<span class="quote">&gt; @@ -914,6 +999,42 @@ unsigned long sum_zone_numa_state(int node,</span>
<span class="quote">&gt;  	return count;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* zero numa counters within a zone */</span>
<span class="quote">&gt; +void zero_zone_numa_counters(struct zone *zone)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int item, cpu;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (item = 0; item &lt; NR_VM_NUMA_STAT_ITEMS; item++) {</span>
<span class="quote">&gt; +		atomic_long_set(&amp;zone-&gt;vm_numa_stat[item], 0);</span>
<span class="quote">&gt; +		for_each_online_cpu(cpu)</span>
<span class="quote">&gt; +			per_cpu_ptr(zone-&gt;pageset, cpu)-&gt;vm_numa_stat_diff[item] = 0;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* zero numa counters of all the populated zones */</span>
<span class="quote">&gt; +void zero_zones_numa_counters(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct zone *zone;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for_each_populated_zone(zone)</span>
<span class="quote">&gt; +		zero_zone_numa_counters(zone);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* zero global numa counters */</span>
<span class="quote">&gt; +void zero_global_numa_counters(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int item;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (item = 0; item &lt; NR_VM_NUMA_STAT_ITEMS; item++)</span>
<span class="quote">&gt; +		atomic_long_set(&amp;vm_numa_stat[item], 0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void invalid_numa_statistics(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	zero_zones_numa_counters();</span>
<span class="quote">&gt; +	zero_global_numa_counters();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Determine the per node value of a stat item.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; @@ -1582,6 +1703,10 @@ static int zoneinfo_show(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pg_data_t *pgdat = (pg_data_t *)arg;</span>
<span class="quote">&gt;  	walk_zones_in_node(m, pgdat, false, false, zoneinfo_show_print);</span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1678,6 +1803,10 @@ static int vmstat_show(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void vmstat_stop(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  	kfree(m-&gt;private);</span>
<span class="quote">&gt;  	m-&gt;private = NULL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 3, 2017, 9:23 a.m.</div>
<pre class="content">
On Thu 28-09-17 14:11:41, Kemi Wang wrote:
<span class="quote">&gt; This is the second step which introduces a tunable interface that allow</span>
<span class="quote">&gt; numa stats configurable for optimizing zone_statistics(), as suggested by</span>
<span class="quote">&gt; Dave Hansen and Ying Huang.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt; some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt; do:</span>
<span class="quote">&gt; 	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; In this case, numa counter update is ignored. We can see about</span>
<span class="quote">&gt; *4.8%*(185-&gt;176) drop of cpu cycles per single page allocation and reclaim</span>
<span class="quote">&gt; on Jesper&#39;s page_bench01 (single thread) and *8.1%*(343-&gt;315) drop of cpu</span>
<span class="quote">&gt; cycles per single page allocation and reclaim on Jesper&#39;s page_bench03 (88</span>
<span class="quote">&gt; threads) running on a 2-Socket Broadwell-based server (88 threads, 126G</span>
<span class="quote">&gt; memory).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Benchmark link provided by Jesper D Brouer(increase loop times to</span>
<span class="quote">&gt; 10000000):</span>
<span class="quote">&gt; https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/</span>
<span class="quote">&gt; bench</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt; tooling to work, you can do:</span>
<span class="quote">&gt; 	echo [S|s]trict &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; =========================================================================</span>
<span class="quote">&gt; We recommend automatic detection of numa statistics by system, this is also</span>
<span class="quote">&gt; system default configuration, you can do:</span>
<span class="quote">&gt; 	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; In this case, numa counter update is skipped unless it has been read by</span>
<span class="quote">&gt; users at least once, e.g. cat /proc/zoneinfo.</span>

I am still not convinced the auto mode is worth all the additional code
and a safe default to use. The whole thing could have been 0/1 with a
simpler parsing and less code to catch readers.

E.g. why do we have to do static_branch_enable on any read or even
vmstat_stop? Wouldn&#39;t open be sufficient?
<span class="quote">
&gt; @@ -153,6 +153,8 @@ static DEVICE_ATTR(meminfo, S_IRUGO, node_read_meminfo, NULL);</span>
<span class="quote">&gt;  static ssize_t node_read_numastat(struct device *dev,</span>
<span class="quote">&gt;  				struct device_attribute *attr, char *buf)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;  	return sprintf(buf,</span>
<span class="quote">&gt;  		       &quot;numa_hit %lu\n&quot;</span>
<span class="quote">&gt;  		       &quot;numa_miss %lu\n&quot;</span>
<span class="quote">&gt; @@ -186,6 +188,8 @@ static ssize_t node_read_vmstat(struct device *dev,</span>
<span class="quote">&gt;  		n += sprintf(buf+n, &quot;%s %lu\n&quot;,</span>
<span class="quote">&gt;  			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],</span>
<span class="quote">&gt;  			     sum_zone_numa_state(nid, i));</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
[...]
<span class="quote">&gt; @@ -1582,6 +1703,10 @@ static int zoneinfo_show(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pg_data_t *pgdat = (pg_data_t *)arg;</span>
<span class="quote">&gt;  	walk_zones_in_node(m, pgdat, false, false, zoneinfo_show_print);</span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1678,6 +1803,10 @@ static int vmstat_show(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void vmstat_stop(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  	kfree(m-&gt;private);</span>
<span class="quote">&gt;  	m-&gt;private = NULL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.7.4</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175911">Kemi Wang</a> - Oct. 9, 2017, 2:20 a.m.</div>
<pre class="content">
On 20170929 15:03, Vlastimil Babka wrote:
<span class="quote">&gt; On 09/28/2017 08:11 AM, Kemi Wang wrote:</span>
<span class="quote">&gt;&gt; This is the second step which introduces a tunable interface that allow</span>
<span class="quote">&gt;&gt; numa stats configurable for optimizing zone_statistics(), as suggested by</span>
<span class="quote">&gt;&gt; Dave Hansen and Ying Huang.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; =========================================================================</span>
<span class="quote">&gt;&gt; When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt;&gt; some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt;&gt; do:</span>
<span class="quote">&gt;&gt; 	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt; In this case, numa counter update is ignored. We can see about</span>
<span class="quote">&gt;&gt; *4.8%*(185-&gt;176) drop of cpu cycles per single page allocation and reclaim</span>
<span class="quote">&gt;&gt; on Jesper&#39;s page_bench01 (single thread) and *8.1%*(343-&gt;315) drop of cpu</span>
<span class="quote">&gt;&gt; cycles per single page allocation and reclaim on Jesper&#39;s page_bench03 (88</span>
<span class="quote">&gt;&gt; threads) running on a 2-Socket Broadwell-based server (88 threads, 126G</span>
<span class="quote">&gt;&gt; memory).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Benchmark link provided by Jesper D Brouer(increase loop times to</span>
<span class="quote">&gt;&gt; 10000000):</span>
<span class="quote">&gt;&gt; https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/</span>
<span class="quote">&gt;&gt; bench</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; =========================================================================</span>
<span class="quote">&gt;&gt; When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt;&gt; tooling to work, you can do:</span>
<span class="quote">&gt;&gt; 	echo [S|s]trict &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; =========================================================================</span>
<span class="quote">&gt;&gt; We recommend automatic detection of numa statistics by system, this is also</span>
<span class="quote">&gt;&gt; system default configuration, you can do:</span>
<span class="quote">&gt;&gt; 	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt; In this case, numa counter update is skipped unless it has been read by</span>
<span class="quote">&gt;&gt; users at least once, e.g. cat /proc/zoneinfo.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Branch target selection with jump label:</span>
<span class="quote">&gt;&gt; a) When numa_stats_mode is changed to *strict*, jump to the branch for numa</span>
<span class="quote">&gt;&gt; counters update.</span>
<span class="quote">&gt;&gt; b) When numa_stats_mode is changed to *coarse*, return back directly.</span>
<span class="quote">&gt;&gt; c) When numa_stats_mode is changed to *auto*, the branch target used in</span>
<span class="quote">&gt;&gt; last time is kept, and the branch target is changed to the branch for numa</span>
<span class="quote">&gt;&gt; counters update once numa counters are *read* by users.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Therefore, with the help of jump label, the page allocation performance is</span>
<span class="quote">&gt;&gt; hardly affected when numa counters are updated with a call in</span>
<span class="quote">&gt;&gt; zone_statistics(). Meanwhile, the auto mode can give people benefit without</span>
<span class="quote">&gt;&gt; manual tuning.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Many thanks to Michal Hocko, Dave Hansen and Ying Huang for comments to</span>
<span class="quote">&gt;&gt; help improve the original patch.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog:</span>
<span class="quote">&gt;&gt;   V2-&gt;V3:</span>
<span class="quote">&gt;&gt;   a) Propose a better way to use jump label to eliminate the overhead of</span>
<span class="quote">&gt;&gt;   branch selection in zone_statistics(), as inspired by Ying Huang;</span>
<span class="quote">&gt;&gt;   b) Add a paragraph in commit log to describe the way for branch target</span>
<span class="quote">&gt;&gt;   selection;</span>
<span class="quote">&gt;&gt;   c) Use a more descriptive name numa_stats_mode instead of vmstat_mode,</span>
<span class="quote">&gt;&gt;   and change the description accordingly, as suggested by Michal Hocko;</span>
<span class="quote">&gt;&gt;   d) Make this functionality NUMA-specific via ifdef</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   V1-&gt;V2:</span>
<span class="quote">&gt;&gt;   a) Merge to one patch;</span>
<span class="quote">&gt;&gt;   b) Use jump label to eliminate the overhead of branch selection;</span>
<span class="quote">&gt;&gt;   c) Add a single-time log message at boot time to help tell users what</span>
<span class="quote">&gt;&gt;   happened.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reported-by: Jesper Dangaard Brouer &lt;brouer@redhat.com&gt;</span>
<span class="quote">&gt;&gt; Suggested-by: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="quote">&gt;&gt; Suggested-by: Ying Huang &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Kemi Wang &lt;kemi.wang@intel.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  Documentation/sysctl/vm.txt |  24 +++++++++</span>
<span class="quote">&gt;&gt;  drivers/base/node.c         |   4 ++</span>
<span class="quote">&gt;&gt;  include/linux/vmstat.h      |  23 ++++++++</span>
<span class="quote">&gt;&gt;  init/main.c                 |   3 ++</span>
<span class="quote">&gt;&gt;  kernel/sysctl.c             |   7 +++</span>
<span class="quote">&gt;&gt;  mm/page_alloc.c             |  10 ++++</span>
<span class="quote">&gt;&gt;  mm/vmstat.c                 | 129 ++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;  7 files changed, 200 insertions(+)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt;&gt; index 9baf66a..e310e69 100644</span>
<span class="quote">&gt;&gt; --- a/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt;&gt; +++ b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt;&gt; @@ -61,6 +61,7 @@ Currently, these files are in /proc/sys/vm:</span>
<span class="quote">&gt;&gt;  - swappiness</span>
<span class="quote">&gt;&gt;  - user_reserve_kbytes</span>
<span class="quote">&gt;&gt;  - vfs_cache_pressure</span>
<span class="quote">&gt;&gt; +- numa_stats_mode</span>
<span class="quote">&gt;&gt;  - watermark_scale_factor</span>
<span class="quote">&gt;&gt;  - zone_reclaim_mode</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; @@ -843,6 +844,29 @@ ten times more freeable objects than there are.</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  =============================================================</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +numa_stats_mode</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +This interface allows numa statistics configurable.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt;&gt; +some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt;&gt; +do:</span>
<span class="quote">&gt;&gt; +	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt;&gt; +tooling to work, you can do:</span>
<span class="quote">&gt;&gt; +	echo [S|s]trict &gt; /proc/sys/vm/numa_stat_mode</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +We recommend automatic detection of numa statistics by system, because numa</span>
<span class="quote">&gt;&gt; +statistics does not affect system&#39;s decision and it is very rarely</span>
<span class="quote">&gt;&gt; +consumed. you can do:</span>
<span class="quote">&gt;&gt; +	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt; +This is also system default configuration, with this default setting, numa</span>
<span class="quote">&gt;&gt; +counters update is skipped unless the counter is *read* by users at least</span>
<span class="quote">&gt;&gt; +once.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It says &quot;the counter&quot;, but it seems multiple files in /proc and /sys are</span>
<span class="quote">&gt; triggering this, so perhaps list them?</span>

Exactly, four files use it.
/proc/zoneinfo
/proc/vmstat
/sys/devices/system/node/node*/vmstat
/sys/devices/system/node/node*/numastat
Well, I am not sure that it is worthy to list here.
<span class="quote">
&gt; Also, is it possible that with contemporary userspace/distros (systemd</span>
<span class="quote">&gt; etc.) there will always be something that will read one of those upon boot?</span>
<span class="quote">&gt; </span>
It depends on the tools used in userspace. If some tool really read it,
the active state in auto mode will be triggered.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +==============================================================</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  watermark_scale_factor:</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  This factor controls the aggressiveness of kswapd. It defines the</span>
<span class="quote">&gt;&gt; diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="quote">&gt;&gt; index 3855902..b57b5622 100644</span>
<span class="quote">&gt;&gt; --- a/drivers/base/node.c</span>
<span class="quote">&gt;&gt; +++ b/drivers/base/node.c</span>
<span class="quote">&gt;&gt; @@ -153,6 +153,8 @@ static DEVICE_ATTR(meminfo, S_IRUGO, node_read_meminfo, NULL);</span>
<span class="quote">&gt;&gt;  static ssize_t node_read_numastat(struct device *dev,</span>
<span class="quote">&gt;&gt;  				struct device_attribute *attr, char *buf)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt;&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;&gt;  	return sprintf(buf,</span>
<span class="quote">&gt;&gt;  		       &quot;numa_hit %lu\n&quot;</span>
<span class="quote">&gt;&gt;  		       &quot;numa_miss %lu\n&quot;</span>
<span class="quote">&gt;&gt; @@ -186,6 +188,8 @@ static ssize_t node_read_vmstat(struct device *dev,</span>
<span class="quote">&gt;&gt;  		n += sprintf(buf+n, &quot;%s %lu\n&quot;,</span>
<span class="quote">&gt;&gt;  			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],</span>
<span class="quote">&gt;&gt;  			     sum_zone_numa_state(nid, i));</span>
<span class="quote">&gt;&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt;&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h</span>
<span class="quote">&gt;&gt; index ade7cb5..d52e882 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/vmstat.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/vmstat.h</span>
<span class="quote">&gt;&gt; @@ -6,9 +6,28 @@</span>
<span class="quote">&gt;&gt;  #include &lt;linux/mmzone.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/vm_event_item.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/atomic.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/static_key.h&gt;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  extern int sysctl_stat_interval;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt;&gt; +DECLARE_STATIC_KEY_FALSE(vm_numa_stats_mode_key);</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * vm_numa_stats_mode:</span>
<span class="quote">&gt;&gt; + * 0 = auto mode of NUMA stats, automatic detection of NUMA statistics.</span>
<span class="quote">&gt;&gt; + * 1 = strict mode of NUMA stats, keep NUMA statistics.</span>
<span class="quote">&gt;&gt; + * 2 = coarse mode of NUMA stats, ignore NUMA statistics.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define VM_NUMA_STAT_AUTO_MODE 0</span>
<span class="quote">&gt;&gt; +#define VM_NUMA_STAT_STRICT_MODE  1</span>
<span class="quote">&gt;&gt; +#define VM_NUMA_STAT_COARSE_MODE  2</span>
<span class="quote">&gt;&gt; +#define VM_NUMA_STAT_MODE_LEN 16</span>
<span class="quote">&gt;&gt; +extern int vm_numa_stats_mode;</span>
<span class="quote">&gt;&gt; +extern char sysctl_vm_numa_stats_mode[];</span>
<span class="quote">&gt;&gt; +extern int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="quote">&gt;&gt; +		void __user *buffer, size_t *length, loff_t *ppos);</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #ifdef CONFIG_VM_EVENT_COUNTERS</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * Light weight per cpu counter implementation.</span>
<span class="quote">&gt;&gt; @@ -229,6 +248,10 @@ extern unsigned long sum_zone_node_page_state(int node,</span>
<span class="quote">&gt;&gt;  extern unsigned long sum_zone_numa_state(int node, enum numa_stat_item item);</span>
<span class="quote">&gt;&gt;  extern unsigned long node_page_state(struct pglist_data *pgdat,</span>
<span class="quote">&gt;&gt;  						enum node_stat_item item);</span>
<span class="quote">&gt;&gt; +extern void zero_zone_numa_counters(struct zone *zone);</span>
<span class="quote">&gt;&gt; +extern void zero_zones_numa_counters(void);</span>
<span class="quote">&gt;&gt; +extern void zero_global_numa_counters(void);</span>
<span class="quote">&gt;&gt; +extern void invalid_numa_statistics(void);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; These seem to be called only from within mm/vmstat.c where they live, so</span>
<span class="quote">&gt; I&#39;d suggest removing these extern declarations, and making them static</span>
<span class="quote">&gt; in vmstat.c.</span>
<span class="quote">&gt; </span>

Agree. Thanks for catching it.
<span class="quote">
&gt; ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;  #define NUMA_STATS_THRESHOLD (U16_MAX - 2)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt;&gt; +int vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="quote">&gt;&gt; +char sysctl_vm_numa_stats_mode[VM_NUMA_STAT_MODE_LEN] = &quot;auto&quot;;</span>
<span class="quote">&gt;&gt; +static const char *vm_numa_stats_mode_name[3] = {&quot;auto&quot;, &quot;strict&quot;, &quot;coarse&quot;};</span>
<span class="quote">&gt;&gt; +static DEFINE_MUTEX(vm_numa_stats_mode_lock);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int __parse_vm_numa_stats_mode(char *s)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	const char *str = s;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (strcmp(str, &quot;auto&quot;) == 0 || strcmp(str, &quot;Auto&quot;) == 0)</span>
<span class="quote">&gt;&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="quote">&gt;&gt; +	else if (strcmp(str, &quot;strict&quot;) == 0 || strcmp(str, &quot;Strict&quot;) == 0)</span>
<span class="quote">&gt;&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_STRICT_MODE;</span>
<span class="quote">&gt;&gt; +	else if (strcmp(str, &quot;coarse&quot;) == 0 || strcmp(str, &quot;Coarse&quot;) == 0)</span>
<span class="quote">&gt;&gt; +		vm_numa_stats_mode = VM_NUMA_STAT_COARSE_MODE;</span>
<span class="quote">&gt;&gt; +	else {</span>
<span class="quote">&gt;&gt; +		pr_warn(&quot;Ignoring invalid vm_numa_stats_mode value: %s\n&quot;, s);</span>
<span class="quote">&gt;&gt; +		return -EINVAL;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="quote">&gt;&gt; +		void __user *buffer, size_t *length, loff_t *ppos)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	char old_string[VM_NUMA_STAT_MODE_LEN];</span>
<span class="quote">&gt;&gt; +	int ret, oldval;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	mutex_lock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt;&gt; +	if (write)</span>
<span class="quote">&gt;&gt; +		strncpy(old_string, (char *)table-&gt;data, VM_NUMA_STAT_MODE_LEN);</span>
<span class="quote">&gt;&gt; +	ret = proc_dostring(table, write, buffer, length, ppos);</span>
<span class="quote">&gt;&gt; +	if (ret || !write) {</span>
<span class="quote">&gt;&gt; +		mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="quote">&gt;&gt; +		return ret;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	oldval = vm_numa_stats_mode;</span>
<span class="quote">&gt;&gt; +	if (__parse_vm_numa_stats_mode((char *)table-&gt;data)) {</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * invalid sysctl_vm_numa_stats_mode value, restore saved string</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		strncpy((char *)table-&gt;data, old_string, VM_NUMA_STAT_MODE_LEN);</span>
<span class="quote">&gt;&gt; +		vm_numa_stats_mode = oldval;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we need to restore vm_numa_stats_mode? </span>

Not necessary. 

AFAICS it didn&#39;t change. Also,
<span class="quote">&gt; should the EINVAL be returned also to userspace? (not sure what&#39;s the</span>
<span class="quote">&gt; API here, hmm man 2 sysctl doesn&#39;t list EINVAL...)</span>
<span class="quote">&gt; </span>

I don&#39;t think so. __parse is only be called in sysctl handler and returns
an invalid value to help restore back.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175911">Kemi Wang</a> - Oct. 9, 2017, 6:34 a.m.</div>
<pre class="content">
On 20171003 17:23, Michal Hocko wrote:
<span class="quote">&gt; On Thu 28-09-17 14:11:41, Kemi Wang wrote:</span>
<span class="quote">&gt;&gt; This is the second step which introduces a tunable interface that allow</span>
<span class="quote">&gt;&gt; numa stats configurable for optimizing zone_statistics(), as suggested by</span>
<span class="quote">&gt;&gt; Dave Hansen and Ying Huang.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; =========================================================================</span>
<span class="quote">&gt;&gt; When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt;&gt; some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt;&gt; do:</span>
<span class="quote">&gt;&gt; 	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt; In this case, numa counter update is ignored. We can see about</span>
<span class="quote">&gt;&gt; *4.8%*(185-&gt;176) drop of cpu cycles per single page allocation and reclaim</span>
<span class="quote">&gt;&gt; on Jesper&#39;s page_bench01 (single thread) and *8.1%*(343-&gt;315) drop of cpu</span>
<span class="quote">&gt;&gt; cycles per single page allocation and reclaim on Jesper&#39;s page_bench03 (88</span>
<span class="quote">&gt;&gt; threads) running on a 2-Socket Broadwell-based server (88 threads, 126G</span>
<span class="quote">&gt;&gt; memory).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Benchmark link provided by Jesper D Brouer(increase loop times to</span>
<span class="quote">&gt;&gt; 10000000):</span>
<span class="quote">&gt;&gt; https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/</span>
<span class="quote">&gt;&gt; bench</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; =========================================================================</span>
<span class="quote">&gt;&gt; When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt;&gt; tooling to work, you can do:</span>
<span class="quote">&gt;&gt; 	echo [S|s]trict &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; =========================================================================</span>
<span class="quote">&gt;&gt; We recommend automatic detection of numa statistics by system, this is also</span>
<span class="quote">&gt;&gt; system default configuration, you can do:</span>
<span class="quote">&gt;&gt; 	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt;&gt; In this case, numa counter update is skipped unless it has been read by</span>
<span class="quote">&gt;&gt; users at least once, e.g. cat /proc/zoneinfo.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am still not convinced the auto mode is worth all the additional code</span>
<span class="quote">&gt; and a safe default to use. The whole thing could have been 0/1 with a</span>
<span class="quote">&gt; simpler parsing and less code to catch readers.</span>
<span class="quote">&gt; </span>

I understood your concern. 
Well, we may get rid of auto mode if there is some obvious disadvantage
here. Now, I tend to keep it because most people may not touch this interface,
and auto mode is helpful in such case.
<span class="quote">
&gt; E.g. why do we have to do static_branch_enable on any read or even</span>
<span class="quote">&gt; vmstat_stop? Wouldn&#39;t open be sufficient?</span>
<span class="quote">&gt; </span>

NUMA stats is used in four files:
/proc/zoneinfo
/proc/vmstat
/sys/devices/system/node/node*/numastat
/sys/devices/system/node/node*/vmstat
In auto mode, each *read* will trigger the update of NUMA counter. 
So, we should make sure the target branch is jumped to the branch 
for NUMA counter update once the file is read from user space.
the intension of static_branch_enable in vmstat_stop(in the call site 
of file-&gt;file_ops.read) is for reading /proc/vmstat in case.  

I guess the *open* means file-&gt;file_op.open here, right?
Do you suggest to move static_branch_enable to file-&gt;file_op.open? Thanks.
<span class="quote">
&gt;&gt; @@ -153,6 +153,8 @@ static DEVICE_ATTR(meminfo, S_IRUGO, node_read_meminfo, NULL);</span>
<span class="quote">&gt;&gt;  static ssize_t node_read_numastat(struct device *dev,</span>
<span class="quote">&gt;&gt;  				struct device_attribute *attr, char *buf)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt;&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;&gt;  	return sprintf(buf,</span>
<span class="quote">&gt;&gt;  		       &quot;numa_hit %lu\n&quot;</span>
<span class="quote">&gt;&gt;  		       &quot;numa_miss %lu\n&quot;</span>
<span class="quote">&gt;&gt; @@ -186,6 +188,8 @@ static ssize_t node_read_vmstat(struct device *dev,</span>
<span class="quote">&gt;&gt;  		n += sprintf(buf+n, &quot;%s %lu\n&quot;,</span>
<span class="quote">&gt;&gt;  			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],</span>
<span class="quote">&gt;&gt;  			     sum_zone_numa_state(nid, i));</span>
<span class="quote">&gt;&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt;&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;&gt; @@ -1582,6 +1703,10 @@ static int zoneinfo_show(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	pg_data_t *pgdat = (pg_data_t *)arg;</span>
<span class="quote">&gt;&gt;  	walk_zones_in_node(m, pgdat, false, false, zoneinfo_show_print);</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt;&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt;&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;  	return 0;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; @@ -1678,6 +1803,10 @@ static int vmstat_show(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static void vmstat_stop(struct seq_file *m, void *arg)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_NUMA</span>
<span class="quote">&gt;&gt; +	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="quote">&gt;&gt; +		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;  	kfree(m-&gt;private);</span>
<span class="quote">&gt;&gt;  	m-&gt;private = NULL;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; -- </span>
<span class="quote">&gt;&gt; 2.7.4</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 9, 2017, 7:55 a.m.</div>
<pre class="content">
On Mon 09-10-17 14:34:11, kemi wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 20171003 17:23, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Thu 28-09-17 14:11:41, Kemi Wang wrote:</span>
<span class="quote">&gt; &gt;&gt; This is the second step which introduces a tunable interface that allow</span>
<span class="quote">&gt; &gt;&gt; numa stats configurable for optimizing zone_statistics(), as suggested by</span>
<span class="quote">&gt; &gt;&gt; Dave Hansen and Ying Huang.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; =========================================================================</span>
<span class="quote">&gt; &gt;&gt; When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="quote">&gt; &gt;&gt; some possible tool breakage and decreased numa counter precision, you can</span>
<span class="quote">&gt; &gt;&gt; do:</span>
<span class="quote">&gt; &gt;&gt; 	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; &gt;&gt; In this case, numa counter update is ignored. We can see about</span>
<span class="quote">&gt; &gt;&gt; *4.8%*(185-&gt;176) drop of cpu cycles per single page allocation and reclaim</span>
<span class="quote">&gt; &gt;&gt; on Jesper&#39;s page_bench01 (single thread) and *8.1%*(343-&gt;315) drop of cpu</span>
<span class="quote">&gt; &gt;&gt; cycles per single page allocation and reclaim on Jesper&#39;s page_bench03 (88</span>
<span class="quote">&gt; &gt;&gt; threads) running on a 2-Socket Broadwell-based server (88 threads, 126G</span>
<span class="quote">&gt; &gt;&gt; memory).</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Benchmark link provided by Jesper D Brouer(increase loop times to</span>
<span class="quote">&gt; &gt;&gt; 10000000):</span>
<span class="quote">&gt; &gt;&gt; https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/</span>
<span class="quote">&gt; &gt;&gt; bench</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; =========================================================================</span>
<span class="quote">&gt; &gt;&gt; When page allocation performance is not a bottleneck and you want all</span>
<span class="quote">&gt; &gt;&gt; tooling to work, you can do:</span>
<span class="quote">&gt; &gt;&gt; 	echo [S|s]trict &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; =========================================================================</span>
<span class="quote">&gt; &gt;&gt; We recommend automatic detection of numa statistics by system, this is also</span>
<span class="quote">&gt; &gt;&gt; system default configuration, you can do:</span>
<span class="quote">&gt; &gt;&gt; 	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="quote">&gt; &gt;&gt; In this case, numa counter update is skipped unless it has been read by</span>
<span class="quote">&gt; &gt;&gt; users at least once, e.g. cat /proc/zoneinfo.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I am still not convinced the auto mode is worth all the additional code</span>
<span class="quote">&gt; &gt; and a safe default to use. The whole thing could have been 0/1 with a</span>
<span class="quote">&gt; &gt; simpler parsing and less code to catch readers.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I understood your concern. </span>
<span class="quote">&gt; Well, we may get rid of auto mode if there is some obvious disadvantage</span>
<span class="quote">&gt; here. Now, I tend to keep it because most people may not touch this interface,</span>
<span class="quote">&gt; and auto mode is helpful in such case.</span>

But you cannot guarantee it won&#39;t break any existing users, can you?
Besides I do not remember anybody complaining about the performance
impact of these counters other than very specialized workloads which are
going to disable the accounting altogether. So I simply fail to see a
reason to add more code with a questionable semantic (see below on
partial reads).
<span class="quote">
&gt; &gt; E.g. why do we have to do static_branch_enable on any read or even</span>
<span class="quote">&gt; &gt; vmstat_stop? Wouldn&#39;t open be sufficient?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NUMA stats is used in four files:</span>
<span class="quote">&gt; /proc/zoneinfo</span>
<span class="quote">&gt; /proc/vmstat</span>
<span class="quote">&gt; /sys/devices/system/node/node*/numastat</span>
<span class="quote">&gt; /sys/devices/system/node/node*/vmstat</span>
<span class="quote">&gt; In auto mode, each *read* will trigger the update of NUMA counter. </span>
<span class="quote">&gt; So, we should make sure the target branch is jumped to the branch </span>
<span class="quote">&gt; for NUMA counter update once the file is read from user space.</span>
<span class="quote">&gt; the intension of static_branch_enable in vmstat_stop(in the call site </span>
<span class="quote">&gt; of file-&gt;file_ops.read) is for reading /proc/vmstat in case.  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess the *open* means file-&gt;file_op.open here, right?</span>
<span class="quote">&gt; Do you suggest to move static_branch_enable to file-&gt;file_op.open? Thanks.</span>

I haven&#39;t checked closely but what happens (or should happen) when you
do a partial read? Should you get an inconsistent results? Or is this
impossible?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 10, 2017, 5:49 a.m.</div>
<pre class="content">
On Mon 09-10-17 09:55:49, Michal Hocko wrote:
<span class="quote">&gt; I haven&#39;t checked closely but what happens (or should happen) when you</span>
<span class="quote">&gt; do a partial read? Should you get an inconsistent results? Or is this</span>
<span class="quote">&gt; impossible?</span>

Well, after thinking about it little bit more, partial reads are always
inconsistent so this wouldn&#39;t add a new problem.

Anyway I still stand by my position that this sounds over-engineered and
a simple 0/1 resp. on/off interface would be both simpler and safer. If
anybody wants an auto mode it can be added later (as a value 2 resp.
auto).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175911">Kemi Wang</a> - Oct. 10, 2017, 5:54 a.m.</div>
<pre class="content">
On 20171010 13:49, Michal Hocko wrote:
<span class="quote">&gt; On Mon 09-10-17 09:55:49, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt; I haven&#39;t checked closely but what happens (or should happen) when you</span>
<span class="quote">&gt;&gt; do a partial read? Should you get an inconsistent results? Or is this</span>
<span class="quote">&gt;&gt; impossible?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well, after thinking about it little bit more, partial reads are always</span>
<span class="quote">&gt; inconsistent so this wouldn&#39;t add a new problem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anyway I still stand by my position that this sounds over-engineered and</span>
<span class="quote">&gt; a simple 0/1 resp. on/off interface would be both simpler and safer. If</span>
<span class="quote">&gt; anybody wants an auto mode it can be added later (as a value 2 resp.</span>
<span class="quote">&gt; auto).</span>
<span class="quote">&gt; </span>

It sounds good to me. If Andrew also tends to be a simple 0/1, I will submit
V4 patch for it. Thanks
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Oct. 10, 2017, 2:29 p.m.</div>
<pre class="content">
On 10/09/2017 10:49 PM, Michal Hocko wrote:
<span class="quote">&gt; On Mon 09-10-17 09:55:49, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt; I haven&#39;t checked closely but what happens (or should happen) when you</span>
<span class="quote">&gt;&gt; do a partial read? Should you get an inconsistent results? Or is this</span>
<span class="quote">&gt;&gt; impossible?</span>
<span class="quote">&gt; Well, after thinking about it little bit more, partial reads are always</span>
<span class="quote">&gt; inconsistent so this wouldn&#39;t add a new problem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anyway I still stand by my position that this sounds over-engineered and</span>
<span class="quote">&gt; a simple 0/1 resp. on/off interface would be both simpler and safer. If</span>
<span class="quote">&gt; anybody wants an auto mode it can be added later (as a value 2 resp.</span>
<span class="quote">&gt; auto).</span>

0/1 with the default set to the strict, slower mode?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 10, 2017, 2:31 p.m.</div>
<pre class="content">
On Tue 10-10-17 07:29:31, Dave Hansen wrote:
<span class="quote">&gt; On 10/09/2017 10:49 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Mon 09-10-17 09:55:49, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt;&gt; I haven&#39;t checked closely but what happens (or should happen) when you</span>
<span class="quote">&gt; &gt;&gt; do a partial read? Should you get an inconsistent results? Or is this</span>
<span class="quote">&gt; &gt;&gt; impossible?</span>
<span class="quote">&gt; &gt; Well, after thinking about it little bit more, partial reads are always</span>
<span class="quote">&gt; &gt; inconsistent so this wouldn&#39;t add a new problem.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Anyway I still stand by my position that this sounds over-engineered and</span>
<span class="quote">&gt; &gt; a simple 0/1 resp. on/off interface would be both simpler and safer. If</span>
<span class="quote">&gt; &gt; anybody wants an auto mode it can be added later (as a value 2 resp.</span>
<span class="quote">&gt; &gt; auto).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 0/1 with the default set to the strict, slower mode?</span>

yes, keep the current semantic and allow users who care to disable
something that stands in the way.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Oct. 10, 2017, 2:53 p.m.</div>
<pre class="content">
On 10/10/2017 07:31 AM, Michal Hocko wrote:
<span class="quote">&gt; On Tue 10-10-17 07:29:31, Dave Hansen wrote:</span>
<span class="quote">&gt;&gt; On 10/09/2017 10:49 PM, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt;&gt; Anyway I still stand by my position that this sounds over-engineered and</span>
<span class="quote">&gt;&gt;&gt; a simple 0/1 resp. on/off interface would be both simpler and safer. If</span>
<span class="quote">&gt;&gt;&gt; anybody wants an auto mode it can be added later (as a value 2 resp.</span>
<span class="quote">&gt;&gt;&gt; auto).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 0/1 with the default set to the strict, slower mode?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; yes, keep the current semantic and allow users who care to disable</span>
<span class="quote">&gt; something that stands in the way.</span>

But, let&#39;s be honest, this leaves us with an option that nobody is ever
going to turn on.  IOW, nobody except a very small portion of our users
will ever see any benefit from this.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 10, 2017, 2:57 p.m.</div>
<pre class="content">
On Tue 10-10-17 07:53:50, Dave Hansen wrote:
<span class="quote">&gt; On 10/10/2017 07:31 AM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Tue 10-10-17 07:29:31, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt;&gt; On 10/09/2017 10:49 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; Anyway I still stand by my position that this sounds over-engineered and</span>
<span class="quote">&gt; &gt;&gt;&gt; a simple 0/1 resp. on/off interface would be both simpler and safer. If</span>
<span class="quote">&gt; &gt;&gt;&gt; anybody wants an auto mode it can be added later (as a value 2 resp.</span>
<span class="quote">&gt; &gt;&gt;&gt; auto).</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; 0/1 with the default set to the strict, slower mode?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; yes, keep the current semantic and allow users who care to disable</span>
<span class="quote">&gt; &gt; something that stands in the way.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But, let&#39;s be honest, this leaves us with an option that nobody is ever</span>
<span class="quote">&gt; going to turn on.  IOW, nobody except a very small portion of our users</span>
<span class="quote">&gt; will ever see any benefit from this.</span>

But aren&#39;t those small groups who would like to squeeze every single
cycle out from the page allocator path the targeted audience?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1731">Christoph Lameter</a> - Oct. 10, 2017, 3:14 p.m.</div>
<pre class="content">
On Tue, 10 Oct 2017, Michal Hocko wrote:
<span class="quote">
&gt; &gt; But, let&#39;s be honest, this leaves us with an option that nobody is ever</span>
<span class="quote">&gt; &gt; going to turn on.  IOW, nobody except a very small portion of our users</span>
<span class="quote">&gt; &gt; will ever see any benefit from this.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But aren&#39;t those small groups who would like to squeeze every single</span>
<span class="quote">&gt; cycle out from the page allocator path the targeted audience?</span>

Those have long sine raised the white flag and succumbed to the
featuritis. Resigned to try to keep the bloat restricted to a couple of
cores so that the rest of the cores stay usable.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Oct. 10, 2017, 3:39 p.m.</div>
<pre class="content">
On 10/10/2017 07:57 AM, Michal Hocko wrote:
<span class="quote">&gt;&gt; But, let&#39;s be honest, this leaves us with an option that nobody is ever</span>
<span class="quote">&gt;&gt; going to turn on.  IOW, nobody except a very small portion of our users</span>
<span class="quote">&gt;&gt; will ever see any benefit from this.</span>
<span class="quote">&gt; But aren&#39;t those small groups who would like to squeeze every single</span>
<span class="quote">&gt; cycle out from the page allocator path the targeted audience?</span>

They&#39;re the reason we started looking at this.  They also care the most.

But, the cost of these stats, especially we get more and more cores in a
NUMA node is really making them show up in profiles.  It would be nice
to get rid of them there, too.

Aaron, do you remember offhand how much of the allocator overhead was
coming from NUMA stats?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 10, 2017, 5:51 p.m.</div>
<pre class="content">
On Tue 10-10-17 08:39:47, Dave Hansen wrote:
<span class="quote">&gt; On 10/10/2017 07:57 AM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt;&gt; But, let&#39;s be honest, this leaves us with an option that nobody is ever</span>
<span class="quote">&gt; &gt;&gt; going to turn on.  IOW, nobody except a very small portion of our users</span>
<span class="quote">&gt; &gt;&gt; will ever see any benefit from this.</span>
<span class="quote">&gt; &gt; But aren&#39;t those small groups who would like to squeeze every single</span>
<span class="quote">&gt; &gt; cycle out from the page allocator path the targeted audience?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; They&#39;re the reason we started looking at this.  They also care the most.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But, the cost of these stats, especially we get more and more cores in a</span>
<span class="quote">&gt; NUMA node is really making them show up in profiles.  It would be nice</span>
<span class="quote">&gt; to get rid of them there, too.</span>

I am not opposing to the auto mode. I am just not sure it is a safe
default and I also think that we should add this on top if it is really
needed.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - Oct. 10, 2017, 9:34 p.m.</div>
<pre class="content">
On Tue, 10 Oct 2017 19:51:43 +0200 Michal Hocko &lt;mhocko@kernel.org&gt; wrote:
<span class="quote">
&gt; On Tue 10-10-17 08:39:47, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt; On 10/10/2017 07:57 AM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt;&gt; But, let&#39;s be honest, this leaves us with an option that nobody is ever</span>
<span class="quote">&gt; &gt; &gt;&gt; going to turn on.  IOW, nobody except a very small portion of our users</span>
<span class="quote">&gt; &gt; &gt;&gt; will ever see any benefit from this.</span>
<span class="quote">&gt; &gt; &gt; But aren&#39;t those small groups who would like to squeeze every single</span>
<span class="quote">&gt; &gt; &gt; cycle out from the page allocator path the targeted audience?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; They&#39;re the reason we started looking at this.  They also care the most.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But, the cost of these stats, especially we get more and more cores in a</span>
<span class="quote">&gt; &gt; NUMA node is really making them show up in profiles.  It would be nice</span>
<span class="quote">&gt; &gt; to get rid of them there, too.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not opposing to the auto mode. I am just not sure it is a safe</span>
<span class="quote">&gt; default and I also think that we should add this on top if it is really</span>
<span class="quote">&gt; needed.</span>

Yup.  Let&#39;s keep things simple unless a real need is demonstrated, please.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Oct. 11, 2017, 6:16 a.m.</div>
<pre class="content">
On 10/10/2017 05:39 PM, Dave Hansen wrote:
<span class="quote">&gt; On 10/10/2017 07:57 AM, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt;&gt; But, let&#39;s be honest, this leaves us with an option that nobody is ever</span>
<span class="quote">&gt;&gt;&gt; going to turn on.  IOW, nobody except a very small portion of our users</span>
<span class="quote">&gt;&gt;&gt; will ever see any benefit from this.</span>
<span class="quote">&gt;&gt; But aren&#39;t those small groups who would like to squeeze every single</span>
<span class="quote">&gt;&gt; cycle out from the page allocator path the targeted audience?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; They&#39;re the reason we started looking at this.  They also care the most.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But, the cost of these stats, especially we get more and more cores in a</span>
<span class="quote">&gt; NUMA node is really making them show up in profiles.  It would be nice</span>
<span class="quote">&gt; to get rid of them there, too.</span>

Furthermore, the group that actually looks at those stats, could be also
expected to be quite small. The group that cares neither about the
stats, nor relies on top allocator performance, might still arguably
benefit from improved allocator performance, but won&#39;t for sure benefit
from the stats.
<span class="quote">
&gt; Aaron, do you remember offhand how much of the allocator overhead was</span>
<span class="quote">&gt; coming from NUMA stats?</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
=========================================================================
When page allocation performance becomes a bottleneck and you can tolerate
some possible tool breakage and decreased numa counter precision, you can
do:
	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode
In this case, numa counter update is ignored. We can see about
*4.8%*(185-&gt;176) drop of cpu cycles per single page allocation and reclaim
on Jesper&#39;s page_bench01 (single thread) and *8.1%*(343-&gt;315) drop of cpu
cycles per single page allocation and reclaim on Jesper&#39;s page_bench03 (88
threads) running on a 2-Socket Broadwell-based server (88 threads, 126G
memory).

Benchmark link provided by Jesper D Brouer(increase loop times to
10000000):
https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/
bench

=========================================================================
When page allocation performance is not a bottleneck and you want all
tooling to work, you can do:
	echo [S|s]trict &gt; /proc/sys/vm/numa_stats_mode

=========================================================================
We recommend automatic detection of numa statistics by system, this is also
system default configuration, you can do:
	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode
In this case, numa counter update is skipped unless it has been read by
users at least once, e.g. cat /proc/zoneinfo.

Branch target selection with jump label:
a) When numa_stats_mode is changed to *strict*, jump to the branch for numa
counters update.
b) When numa_stats_mode is changed to *coarse*, return back directly.
c) When numa_stats_mode is changed to *auto*, the branch target used in
last time is kept, and the branch target is changed to the branch for numa
counters update once numa counters are *read* by users.

Therefore, with the help of jump label, the page allocation performance is
hardly affected when numa counters are updated with a call in
zone_statistics(). Meanwhile, the auto mode can give people benefit without
manual tuning.

Many thanks to Michal Hocko, Dave Hansen and Ying Huang for comments to
help improve the original patch.

ChangeLog:
  V2-&gt;V3:
  a) Propose a better way to use jump label to eliminate the overhead of
  branch selection in zone_statistics(), as inspired by Ying Huang;
  b) Add a paragraph in commit log to describe the way for branch target
  selection;
  c) Use a more descriptive name numa_stats_mode instead of vmstat_mode,
  and change the description accordingly, as suggested by Michal Hocko;
  d) Make this functionality NUMA-specific via ifdef

  V1-&gt;V2:
  a) Merge to one patch;
  b) Use jump label to eliminate the overhead of branch selection;
  c) Add a single-time log message at boot time to help tell users what
  happened.

Reported-by: Jesper Dangaard Brouer &lt;brouer@redhat.com&gt;
Suggested-by: Dave Hansen &lt;dave.hansen@intel.com&gt;
Suggested-by: Ying Huang &lt;ying.huang@intel.com&gt;
Signed-off-by: Kemi Wang &lt;kemi.wang@intel.com&gt;
<span class="p_del">---</span>
 Documentation/sysctl/vm.txt |  24 +++++++++
 drivers/base/node.c         |   4 ++
 include/linux/vmstat.h      |  23 ++++++++
 init/main.c                 |   3 ++
 kernel/sysctl.c             |   7 +++
 mm/page_alloc.c             |  10 ++++
 mm/vmstat.c                 | 129 ++++++++++++++++++++++++++++++++++++++++++++
 7 files changed, 200 insertions(+)

<span class="p_header">diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="p_header">index 9baf66a..e310e69 100644</span>
<span class="p_header">--- a/Documentation/sysctl/vm.txt</span>
<span class="p_header">+++ b/Documentation/sysctl/vm.txt</span>
<span class="p_chunk">@@ -61,6 +61,7 @@</span> <span class="p_context"> Currently, these files are in /proc/sys/vm:</span>
 - swappiness
 - user_reserve_kbytes
 - vfs_cache_pressure
<span class="p_add">+- numa_stats_mode</span>
 - watermark_scale_factor
 - zone_reclaim_mode
 
<span class="p_chunk">@@ -843,6 +844,29 @@</span> <span class="p_context"> ten times more freeable objects than there are.</span>
 
 =============================================================
 
<span class="p_add">+numa_stats_mode</span>
<span class="p_add">+</span>
<span class="p_add">+This interface allows numa statistics configurable.</span>
<span class="p_add">+</span>
<span class="p_add">+When page allocation performance becomes a bottleneck and you can tolerate</span>
<span class="p_add">+some possible tool breakage and decreased numa counter precision, you can</span>
<span class="p_add">+do:</span>
<span class="p_add">+	echo [C|c]oarse &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="p_add">+</span>
<span class="p_add">+When page allocation performance is not a bottleneck and you want all</span>
<span class="p_add">+tooling to work, you can do:</span>
<span class="p_add">+	echo [S|s]trict &gt; /proc/sys/vm/numa_stat_mode</span>
<span class="p_add">+</span>
<span class="p_add">+We recommend automatic detection of numa statistics by system, because numa</span>
<span class="p_add">+statistics does not affect system&#39;s decision and it is very rarely</span>
<span class="p_add">+consumed. you can do:</span>
<span class="p_add">+	echo [A|a]uto &gt; /proc/sys/vm/numa_stats_mode</span>
<span class="p_add">+This is also system default configuration, with this default setting, numa</span>
<span class="p_add">+counters update is skipped unless the counter is *read* by users at least</span>
<span class="p_add">+once.</span>
<span class="p_add">+</span>
<span class="p_add">+==============================================================</span>
<span class="p_add">+</span>
 watermark_scale_factor:
 
 This factor controls the aggressiveness of kswapd. It defines the
<span class="p_header">diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="p_header">index 3855902..b57b5622 100644</span>
<span class="p_header">--- a/drivers/base/node.c</span>
<span class="p_header">+++ b/drivers/base/node.c</span>
<span class="p_chunk">@@ -153,6 +153,8 @@</span> <span class="p_context"> static DEVICE_ATTR(meminfo, S_IRUGO, node_read_meminfo, NULL);</span>
 static ssize_t node_read_numastat(struct device *dev,
 				struct device_attribute *attr, char *buf)
 {
<span class="p_add">+	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="p_add">+		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
 	return sprintf(buf,
 		       &quot;numa_hit %lu\n&quot;
 		       &quot;numa_miss %lu\n&quot;
<span class="p_chunk">@@ -186,6 +188,8 @@</span> <span class="p_context"> static ssize_t node_read_vmstat(struct device *dev,</span>
 		n += sprintf(buf+n, &quot;%s %lu\n&quot;,
 			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],
 			     sum_zone_numa_state(nid, i));
<span class="p_add">+	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="p_add">+		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
 #endif
 
 	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)
<span class="p_header">diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h</span>
<span class="p_header">index ade7cb5..d52e882 100644</span>
<span class="p_header">--- a/include/linux/vmstat.h</span>
<span class="p_header">+++ b/include/linux/vmstat.h</span>
<span class="p_chunk">@@ -6,9 +6,28 @@</span> <span class="p_context"></span>
 #include &lt;linux/mmzone.h&gt;
 #include &lt;linux/vm_event_item.h&gt;
 #include &lt;linux/atomic.h&gt;
<span class="p_add">+#include &lt;linux/static_key.h&gt;</span>
 
 extern int sysctl_stat_interval;
 
<span class="p_add">+#ifdef CONFIG_NUMA</span>
<span class="p_add">+DECLARE_STATIC_KEY_FALSE(vm_numa_stats_mode_key);</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * vm_numa_stats_mode:</span>
<span class="p_add">+ * 0 = auto mode of NUMA stats, automatic detection of NUMA statistics.</span>
<span class="p_add">+ * 1 = strict mode of NUMA stats, keep NUMA statistics.</span>
<span class="p_add">+ * 2 = coarse mode of NUMA stats, ignore NUMA statistics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define VM_NUMA_STAT_AUTO_MODE 0</span>
<span class="p_add">+#define VM_NUMA_STAT_STRICT_MODE  1</span>
<span class="p_add">+#define VM_NUMA_STAT_COARSE_MODE  2</span>
<span class="p_add">+#define VM_NUMA_STAT_MODE_LEN 16</span>
<span class="p_add">+extern int vm_numa_stats_mode;</span>
<span class="p_add">+extern char sysctl_vm_numa_stats_mode[];</span>
<span class="p_add">+extern int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="p_add">+		void __user *buffer, size_t *length, loff_t *ppos);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_VM_EVENT_COUNTERS
 /*
  * Light weight per cpu counter implementation.
<span class="p_chunk">@@ -229,6 +248,10 @@</span> <span class="p_context"> extern unsigned long sum_zone_node_page_state(int node,</span>
 extern unsigned long sum_zone_numa_state(int node, enum numa_stat_item item);
 extern unsigned long node_page_state(struct pglist_data *pgdat,
 						enum node_stat_item item);
<span class="p_add">+extern void zero_zone_numa_counters(struct zone *zone);</span>
<span class="p_add">+extern void zero_zones_numa_counters(void);</span>
<span class="p_add">+extern void zero_global_numa_counters(void);</span>
<span class="p_add">+extern void invalid_numa_statistics(void);</span>
 #else
 #define sum_zone_node_page_state(node, item) global_zone_page_state(item)
 #define node_page_state(node, item) global_node_page_state(item)
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 0ee9c686..1e300a8 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -567,6 +567,9 @@</span> <span class="p_context"> asmlinkage __visible void __init start_kernel(void)</span>
 	sort_main_extable();
 	trap_init();
 	mm_init();
<span class="p_add">+#ifdef CONFIG_NUMA</span>
<span class="p_add">+	pr_info(&quot;vmstat: NUMA stats is skipped unless it has been consumed\n&quot;);</span>
<span class="p_add">+#endif</span>
 
 	ftrace_init();
 
<span class="p_header">diff --git a/kernel/sysctl.c b/kernel/sysctl.c</span>
<span class="p_header">index 6648fbb..0678668 100644</span>
<span class="p_header">--- a/kernel/sysctl.c</span>
<span class="p_header">+++ b/kernel/sysctl.c</span>
<span class="p_chunk">@@ -1374,6 +1374,13 @@</span> <span class="p_context"> static struct ctl_table vm_table[] = {</span>
 		.mode           = 0644,
 		.proc_handler   = &amp;hugetlb_mempolicy_sysctl_handler,
 	},
<span class="p_add">+	{</span>
<span class="p_add">+		.procname	= &quot;numa_stats_mode&quot;,</span>
<span class="p_add">+		.data		= sysctl_vm_numa_stats_mode,</span>
<span class="p_add">+		.maxlen		= VM_NUMA_STAT_MODE_LEN,</span>
<span class="p_add">+		.mode		= 0644,</span>
<span class="p_add">+		.proc_handler	= sysctl_vm_numa_stats_mode_handler,</span>
<span class="p_add">+	},</span>
 #endif
 	 {
 		.procname	= &quot;hugetlb_shm_group&quot;,
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index c841af8..6d7ea18 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -83,6 +83,8 @@</span> <span class="p_context"> DEFINE_PER_CPU(int, numa_node);</span>
 EXPORT_PER_CPU_SYMBOL(numa_node);
 #endif
 
<span class="p_add">+DEFINE_STATIC_KEY_FALSE(vm_numa_stats_mode_key);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HAVE_MEMORYLESS_NODES
 /*
  * N.B., Do NOT reference the &#39;_numa_mem_&#39; per cpu variable directly.
<span class="p_chunk">@@ -2743,6 +2745,14 @@</span> <span class="p_context"> static inline void zone_statistics(struct zone *preferred_zone, struct zone *z)</span>
 #ifdef CONFIG_NUMA
 	enum numa_stat_item local_stat = NUMA_LOCAL;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * skip zone_statistics() if NUMA stats is set to coarse mode or</span>
<span class="p_add">+	 * NUMA stats is never consumed in auto mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;vm_numa_stats_mode_key))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	if (z-&gt;node != numa_node_id())
 		local_stat = NUMA_OTHER;
 
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 4bb13e7..469599c 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -32,6 +32,91 @@</span> <span class="p_context"></span>
 
 #define NUMA_STATS_THRESHOLD (U16_MAX - 2)
 
<span class="p_add">+#ifdef CONFIG_NUMA</span>
<span class="p_add">+int vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="p_add">+char sysctl_vm_numa_stats_mode[VM_NUMA_STAT_MODE_LEN] = &quot;auto&quot;;</span>
<span class="p_add">+static const char *vm_numa_stats_mode_name[3] = {&quot;auto&quot;, &quot;strict&quot;, &quot;coarse&quot;};</span>
<span class="p_add">+static DEFINE_MUTEX(vm_numa_stats_mode_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+static int __parse_vm_numa_stats_mode(char *s)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const char *str = s;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (strcmp(str, &quot;auto&quot;) == 0 || strcmp(str, &quot;Auto&quot;) == 0)</span>
<span class="p_add">+		vm_numa_stats_mode = VM_NUMA_STAT_AUTO_MODE;</span>
<span class="p_add">+	else if (strcmp(str, &quot;strict&quot;) == 0 || strcmp(str, &quot;Strict&quot;) == 0)</span>
<span class="p_add">+		vm_numa_stats_mode = VM_NUMA_STAT_STRICT_MODE;</span>
<span class="p_add">+	else if (strcmp(str, &quot;coarse&quot;) == 0 || strcmp(str, &quot;Coarse&quot;) == 0)</span>
<span class="p_add">+		vm_numa_stats_mode = VM_NUMA_STAT_COARSE_MODE;</span>
<span class="p_add">+	else {</span>
<span class="p_add">+		pr_warn(&quot;Ignoring invalid vm_numa_stats_mode value: %s\n&quot;, s);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int sysctl_vm_numa_stats_mode_handler(struct ctl_table *table, int write,</span>
<span class="p_add">+		void __user *buffer, size_t *length, loff_t *ppos)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char old_string[VM_NUMA_STAT_MODE_LEN];</span>
<span class="p_add">+	int ret, oldval;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;vm_numa_stats_mode_lock);</span>
<span class="p_add">+	if (write)</span>
<span class="p_add">+		strncpy(old_string, (char *)table-&gt;data, VM_NUMA_STAT_MODE_LEN);</span>
<span class="p_add">+	ret = proc_dostring(table, write, buffer, length, ppos);</span>
<span class="p_add">+	if (ret || !write) {</span>
<span class="p_add">+		mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	oldval = vm_numa_stats_mode;</span>
<span class="p_add">+	if (__parse_vm_numa_stats_mode((char *)table-&gt;data)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * invalid sysctl_vm_numa_stats_mode value, restore saved string</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		strncpy((char *)table-&gt;data, old_string, VM_NUMA_STAT_MODE_LEN);</span>
<span class="p_add">+		vm_numa_stats_mode = oldval;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * check whether numa stats mode changes or not</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (vm_numa_stats_mode == oldval) {</span>
<span class="p_add">+			/* no change */</span>
<span class="p_add">+			mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		} else if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Keep the branch selection in last time when numa stats</span>
<span class="p_add">+			 * is changed to auto mode.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			pr_info(&quot;numa stats changes from %s mode to auto mode\n&quot;,</span>
<span class="p_add">+					vm_numa_stats_mode_name[oldval]);</span>
<span class="p_add">+		else if (vm_numa_stats_mode == VM_NUMA_STAT_STRICT_MODE) {</span>
<span class="p_add">+			static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="p_add">+			pr_info(&quot;numa stats changes from %s mode to strict mode\n&quot;,</span>
<span class="p_add">+					vm_numa_stats_mode_name[oldval]);</span>
<span class="p_add">+		} else if (vm_numa_stats_mode == VM_NUMA_STAT_COARSE_MODE) {</span>
<span class="p_add">+			static_branch_disable(&amp;vm_numa_stats_mode_key);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Invalidate numa counters when vmstat mode is set to coarse</span>
<span class="p_add">+			 * mode, because users can&#39;t tell the difference between the</span>
<span class="p_add">+			 * dead state and when allocator activity is quiet once</span>
<span class="p_add">+			 * zone_statistics() is turned off.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			invalid_numa_statistics();</span>
<span class="p_add">+			pr_info(&quot;numa stats changes from %s mode to coarse mode\n&quot;,</span>
<span class="p_add">+					vm_numa_stats_mode_name[oldval]);</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			pr_warn(&quot;invalid vm_numa_stats_mode:%d\n&quot;, vm_numa_stats_mode);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_unlock(&amp;vm_numa_stats_mode_lock);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_VM_EVENT_COUNTERS
 DEFINE_PER_CPU(struct vm_event_state, vm_event_states) = {{0}};
 EXPORT_PER_CPU_SYMBOL(vm_event_states);
<span class="p_chunk">@@ -914,6 +999,42 @@</span> <span class="p_context"> unsigned long sum_zone_numa_state(int node,</span>
 	return count;
 }
 
<span class="p_add">+/* zero numa counters within a zone */</span>
<span class="p_add">+void zero_zone_numa_counters(struct zone *zone)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int item, cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (item = 0; item &lt; NR_VM_NUMA_STAT_ITEMS; item++) {</span>
<span class="p_add">+		atomic_long_set(&amp;zone-&gt;vm_numa_stat[item], 0);</span>
<span class="p_add">+		for_each_online_cpu(cpu)</span>
<span class="p_add">+			per_cpu_ptr(zone-&gt;pageset, cpu)-&gt;vm_numa_stat_diff[item] = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* zero numa counters of all the populated zones */</span>
<span class="p_add">+void zero_zones_numa_counters(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct zone *zone;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_populated_zone(zone)</span>
<span class="p_add">+		zero_zone_numa_counters(zone);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* zero global numa counters */</span>
<span class="p_add">+void zero_global_numa_counters(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int item;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (item = 0; item &lt; NR_VM_NUMA_STAT_ITEMS; item++)</span>
<span class="p_add">+		atomic_long_set(&amp;vm_numa_stat[item], 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void invalid_numa_statistics(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	zero_zones_numa_counters();</span>
<span class="p_add">+	zero_global_numa_counters();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Determine the per node value of a stat item.
  */
<span class="p_chunk">@@ -1582,6 +1703,10 @@</span> <span class="p_context"> static int zoneinfo_show(struct seq_file *m, void *arg)</span>
 {
 	pg_data_t *pgdat = (pg_data_t *)arg;
 	walk_zones_in_node(m, pgdat, false, false, zoneinfo_show_print);
<span class="p_add">+#ifdef CONFIG_NUMA</span>
<span class="p_add">+	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="p_add">+		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="p_add">+#endif</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1678,6 +1803,10 @@</span> <span class="p_context"> static int vmstat_show(struct seq_file *m, void *arg)</span>
 
 static void vmstat_stop(struct seq_file *m, void *arg)
 {
<span class="p_add">+#ifdef CONFIG_NUMA</span>
<span class="p_add">+	if (vm_numa_stats_mode == VM_NUMA_STAT_AUTO_MODE)</span>
<span class="p_add">+		static_branch_enable(&amp;vm_numa_stats_mode_key);</span>
<span class="p_add">+#endif</span>
 	kfree(m-&gt;private);
 	m-&gt;private = NULL;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



