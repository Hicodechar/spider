
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V2,2/7] futex: Hash private futexes per process - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V2,2/7] futex: Hash private futexes per process</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 5, 2016, 8:44 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160505204353.973009518@linutronix.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9027391/mbox/"
   >mbox</a>
|
   <a href="/patch/9027391/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9027391/">/patch/9027391/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 4C185BF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  5 May 2016 20:46:23 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 2286B203DF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  5 May 2016 20:46:22 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id BBE86203E6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  5 May 2016 20:46:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757086AbcEEUqB (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 5 May 2016 16:46:01 -0400
Received: from www.linutronix.de ([62.245.132.108]:56766 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756390AbcEEUpw (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 5 May 2016 16:45:52 -0400
Received: from localhost ([127.0.0.1] helo=[127.0.1.1])
	by Galois.linutronix.de with esmtp (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1ayQ9U-0005Zv-OJ; Thu, 05 May 2016 22:45:40 +0200
Message-Id: &lt;20160505204353.973009518@linutronix.de&gt;
User-Agent: quilt/0.63-1
Date: Thu, 05 May 2016 20:44:04 -0000
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: LKML &lt;linux-kernel@vger.kernel.org&gt;
Cc: Sebastian Andrzej Siewior &lt;bigeasy@linutronix.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Darren Hart &lt;darren@dvhart.com&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;,
	Michael Kerrisk &lt;mtk.manpages@googlemail.com&gt;,
	Davidlohr Bueso &lt;dave@stgolabs.net&gt;, Chris Mason &lt;clm@fb.com&gt;,
	Carlos O&#39;Donell &lt;carlos@redhat.com&gt;, Torvald Riegel &lt;triegel@redhat.com&gt;,
	Eric Dumazet &lt;edumazet@google.com&gt;
Subject: [patch V2 2/7] futex: Hash private futexes per process
References: &lt;20160505204230.932454245@linutronix.de&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=ISO-8859-15
Content-Disposition: inline;
	filename=futex-Hash-private-futexes-per-process.patch
X-Linutronix-Spam-Score: -1.0
X-Linutronix-Spam-Level: -
X-Linutronix-Spam-Status: No , -1.0 points, 5.0 required, ALL_TRUSTED=-1,
	SHORTCIRCUIT=-0.0001, URIBL_BLOCKED=0.001
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-9.0 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - May 5, 2016, 8:44 p.m.</div>
<pre class="content">
<span class="from">From: Sebastian Siewior &lt;bigeasy@linutronix.de&gt;</span>

The standard futex mechanism in the Linux kernel uses a global hash to store
transient state. Collisions on that hash can lead to performance degradation
especially on NUMA systems and on real-time enabled kernels even to priority
inversions.

To mitigate that problem we provide per process private hashing. On the first
futex operation in a process the kernel allocates a hash table. The hash table
is accessible via the process mm_struct. On Numa systems the hash is allocated
node local.

If the allocation fails then the global hash table is used as fallback, so
there is no user space visible impact of this feature.

The hash size is a default value which can be tweaked by the sys admin. The
sysctl interface is implemented in a follow up patch to make the review
simpler. For applications which have special requirements for the private hash
and to allow preallocation of the hash for RT applications, we&#39;ll provide a
futex OP in a follow up patch.

Performance data acquired on a 4 socket (node) Intel machine with perf bench
futex-hash:

Threads  G 65536  P 4	  P 8      P 16       P 32     P 64     P 128    P 256

1        8175006  8645465  8617469  8628686   8625223  8664491  8590934  8631582
2	 8149869  8618385  8578185  8622267   8603253  8618787  8595073  8590591
4	 7479482  5867840  7882991  7604838   7894380  7882850  7884911  7886278
8	 7308822  2378057  5731051  5550479   7691198  7672814  7711939  7681549
16	 7295893   677414  2670682  3453552   7158906  7688978  7677603  7690290

So with the proper hash size of the private hash is ~5% faster than the global
hash.

With a full perf bench futex-hash run with one process (36 threads) per node
and 1024 futexes per thread the following results are achieved:

G 65536	 P 4     P 8     P 16     P 32     P 64     P 128    P 256    P 512    P 1024  P 2048     
2673390  368952  682626  1223908  1845922  3003524  3538313  4118533  4286925  4289589 4274020

Ratio:   0,14    0,26    0,46     0,69	   1,12     1,32     1,54     1,60     1,60    1,60

So with a private hash size of 256 buckets and above the performance is almost
steady in this pathological test case and factor 1.6 better than the global
hash. Even a 64 buckets hash is already 10% faster,
<span class="signed-off-by">
Signed-off-by: Sebastian Siewior &lt;bigeasy@linutronix.de&gt;</span>
<span class="signed-off-by">Signed-off-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
---
 include/linux/futex.h       |   38 ++++++++--
 include/linux/futex_types.h |   12 +++
 include/linux/mm_types.h    |    4 +
 init/Kconfig                |    4 +
 kernel/fork.c               |    3 
 kernel/futex.c              |  162 +++++++++++++++++++++++++++++++++++++++++++-
 6 files changed, 212 insertions(+), 11 deletions(-)
 create mode 100644 include/linux/futex_types.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=106771">Darren Hart</a> - May 6, 2016, 6:09 p.m.</div>
<pre class="content">
On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:
<span class="quote">&gt; From: Sebastian Siewior &lt;bigeasy@linutronix.de&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The standard futex mechanism in the Linux kernel uses a global hash to store</span>
<span class="quote">&gt; transient state. Collisions on that hash can lead to performance degradation</span>
<span class="quote">&gt; especially on NUMA systems and on real-time enabled kernels even to priority</span>
<span class="quote">&gt; inversions.</span>

I think it is worth noting the how this causes an unbounded priority inversion
as it wasn&#39;t obvious to me. At least mention that &quot;CPU pinning&quot; can result in an
unbounded priority inversion.
<span class="quote">
&gt; </span>
<span class="quote">&gt; To mitigate that problem we provide per process private hashing. On the first</span>
<span class="quote">&gt; futex operation in a process the kernel allocates a hash table. The hash table</span>
<span class="quote">&gt; is accessible via the process mm_struct. On Numa systems the hash is allocated</span>
<span class="quote">&gt; node local.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the allocation fails then the global hash table is used as fallback, so</span>
<span class="quote">&gt; there is no user space visible impact of this feature.</span>
<span class="quote">&gt; </span>

It would be good to have a way to detect that the process private hash table was
successfully created. Perhaps a /proc/pid/ feature? This would allow us to write
a functional futex test for tools/testing/selftests/futex
<span class="quote">
&gt; The hash size is a default value which can be tweaked by the sys admin. The</span>
<span class="quote">&gt; sysctl interface is implemented in a follow up patch to make the review</span>
<span class="quote">&gt; simpler. For applications which have special requirements for the private hash</span>
<span class="quote">&gt; and to allow preallocation of the hash for RT applications, we&#39;ll provide a</span>
<span class="quote">&gt; futex OP in a follow up patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Performance data acquired on a 4 socket (node) Intel machine with perf bench</span>
<span class="quote">&gt; futex-hash:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Threads  G 65536  P 4	  P 8      P 16       P 32     P 64     P 128    P 256</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1        8175006  8645465  8617469  8628686   8625223  8664491  8590934  8631582</span>
<span class="quote">&gt; 2	 8149869  8618385  8578185  8622267   8603253  8618787  8595073  8590591</span>
<span class="quote">&gt; 4	 7479482  5867840  7882991  7604838   7894380  7882850  7884911  7886278</span>
<span class="quote">&gt; 8	 7308822  2378057  5731051  5550479   7691198  7672814  7711939  7681549</span>
<span class="quote">&gt; 16	 7295893   677414  2670682  3453552   7158906  7688978  7677603  7690290</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So with the proper hash size of the private hash is ~5% faster than the global</span>
<span class="quote">&gt; hash.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With a full perf bench futex-hash run with one process (36 threads) per node</span>
<span class="quote">&gt; and 1024 futexes per thread the following results are achieved:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; G 65536	 P 4     P 8     P 16     P 32     P 64     P 128    P 256    P 512    P 1024  P 2048     </span>
<span class="quote">&gt; 2673390  368952  682626  1223908  1845922  3003524  3538313  4118533  4286925  4289589 4274020</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ratio:   0,14    0,26    0,46     0,69	   1,12     1,32     1,54     1,60     1,60    1,60</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So with a private hash size of 256 buckets and above the performance is almost</span>
<span class="quote">&gt; steady in this pathological test case and factor 1.6 better than the global</span>
<span class="quote">&gt; hash. Even a 64 buckets hash is already 10% faster,</span>
<span class="quote">&gt; </span>

Nice!
<span class="quote">
&gt; Signed-off-by: Sebastian Siewior &lt;bigeasy@linutronix.de&gt;</span>
<span class="quote">&gt; Signed-off-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/futex.h       |   38 ++++++++--</span>
<span class="quote">&gt;  include/linux/futex_types.h |   12 +++</span>
<span class="quote">&gt;  include/linux/mm_types.h    |    4 +</span>
<span class="quote">&gt;  init/Kconfig                |    4 +</span>
<span class="quote">&gt;  kernel/fork.c               |    3 </span>
<span class="quote">&gt;  kernel/futex.c              |  162 +++++++++++++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  6 files changed, 212 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 include/linux/futex_types.h</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --- a/include/linux/futex.h</span>
<span class="quote">&gt; +++ b/include/linux/futex.h</span>
<span class="quote">&gt; @@ -1,6 +1,7 @@</span>
<span class="quote">&gt;  #ifndef _LINUX_FUTEX_H</span>
<span class="quote">&gt;  #define _LINUX_FUTEX_H</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &lt;linux/futex_types.h&gt;</span>
<span class="quote">&gt;  #include &lt;uapi/linux/futex.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct inode;</span>
<span class="quote">&gt; @@ -21,16 +22,19 @@ handle_futex_death(u32 __user *uaddr, st</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * offset is aligned to a multiple of sizeof(u32) (== 4) by definition.</span>
<span class="quote">&gt;   * We use the two low order bits of offset to tell what is the kind of key :</span>
<span class="quote">&gt; - *  00 : Private process futex (PTHREAD_PROCESS_PRIVATE)</span>
<span class="quote">&gt; - *       (no reference on an inode or mm)</span>
<span class="quote">&gt; + *  00 : Private process futex (PTHREAD_PROCESS_PRIVATE) using process private</span>
<span class="quote">&gt; + *	 hash (no reference on an inode or mm)</span>
<span class="quote">&gt;   *  01 : Shared futex (PTHREAD_PROCESS_SHARED)</span>
<span class="quote">&gt;   *	mapped on a file (reference on the underlying inode)</span>
<span class="quote">&gt;   *  10 : Shared futex (PTHREAD_PROCESS_SHARED)</span>
<span class="quote">&gt;   *       (but private mapping on an mm, and reference taken on it)</span>
<span class="quote">&gt; + *  11 : Private process futex (PTHREAD_PROCESS_PRIVATE) using global hash</span>
<span class="quote">&gt; + *	 (no reference on an inode or mm)</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define FUT_OFF_INODE    1 /* We set bit 0 if key has a reference on inode */</span>
<span class="quote">&gt; -#define FUT_OFF_MMSHARED 2 /* We set bit 1 if key has a reference on mm */</span>
<span class="quote">&gt; +#define FUT_OFF_INODE		0x01 /* Key has a reference on inode */</span>
<span class="quote">&gt; +#define FUT_OFF_MMSHARED	0x02 /* Key has a reference on mm */</span>
<span class="quote">&gt; +#define FUT_OFF_PRIVATE		0x03 /* Key has no ref on inode/mm */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  union futex_key {</span>
<span class="quote">&gt;  	struct {</span>
<span class="quote">&gt; @@ -60,12 +64,30 @@ extern void exit_pi_state_list(struct ta</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  extern int futex_cmpxchg_enabled;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; -static inline void exit_robust_list(struct task_struct *curr)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -static inline void exit_pi_state_list(struct task_struct *curr)</span>
<span class="quote">&gt; +static inline void exit_robust_list(struct task_struct *curr) { }</span>
<span class="quote">&gt; +static inline void exit_pi_state_list(struct task_struct *curr) { }</span>
<span class="quote">&gt; +#endif</span>

These appear to be unrelated changes, can they preceed this patch?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; +/* Process private hash data for futexes */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern unsigned int futex_default_hash_bits;</span>
<span class="quote">&gt; +extern unsigned int futex_max_hash_bits;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void futex_mm_hash_exit(struct mm_struct *mm);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void futex_mm_hash_init(struct mm_struct *mm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	raw_spin_lock_init(&amp;mm-&gt;futex_hash.lock);</span>
<span class="quote">&gt; +	mm-&gt;futex_hash.hash = NULL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void futex_mm_hash_init(struct mm_struct *mm) { }</span>
<span class="quote">&gt; +static inline void futex_mm_hash_exit(struct mm_struct *mm) { }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +</span>

Ah, the above was to make it consistent with this... mmmm... kay. Nevermind.
<span class="quote">
&gt;  #endif</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/futex_types.h</span>
<span class="quote">&gt; @@ -0,0 +1,12 @@</span>
<span class="quote">&gt; +#ifndef _LINUX_FUTEX_TYPES_H</span>
<span class="quote">&gt; +#define _LINUX_FUTEX_TYPES_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct futex_hash_bucket;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct futex_hash {</span>
<span class="quote">&gt; +	struct raw_spinlock		lock;</span>

As it isn&#39;t always obvious to everone, it would be good to add a single line
comment stating why a *raw* spinlock is necessary.

In this case... I suppose this could lead to some nasty scenarios setting up IPC
mechanisms between threads if they weren&#39;t strictly serialized? Something else?
<span class="quote">
&gt; +	unsigned int			hash_bits;</span>
<span class="quote">&gt; +	struct futex_hash_bucket	*hash;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -11,6 +11,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/completion.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/cpumask.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/uprobes.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/futex_types.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page-flags-layout.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/page.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/mmu.h&gt;</span>
<span class="quote">&gt; @@ -442,6 +443,9 @@ struct mm_struct {</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	struct linux_binfmt *binfmt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; +	struct futex_hash futex_hash;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  	cpumask_var_t cpu_vm_mask_var;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Architecture-specific MM context */</span>
<span class="quote">&gt; --- a/init/Kconfig</span>
<span class="quote">&gt; +++ b/init/Kconfig</span>
<span class="quote">&gt; @@ -1498,6 +1498,10 @@ config FUTEX</span>
<span class="quote">&gt;  	  support for &quot;fast userspace mutexes&quot;.  The resulting kernel may not</span>
<span class="quote">&gt;  	  run glibc-based applications correctly.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; +	bool</span>
<span class="quote">&gt; +	default FUTEX &amp;&amp; SMP</span>
<span class="quote">&gt; +</span>

So no prompt, not user selectable. If you have SMP, you get this? I think
automatic is a good call... but is SMP the right criteria, or would NUMA be more
appropriate since I thought it was keeping the hash local to the NUMA node that
was the big win?
<span class="quote">
&gt;  config HAVE_FUTEX_CMPXCHG</span>
<span class="quote">&gt;  	bool</span>
<span class="quote">&gt;  	depends on FUTEX</span>
<span class="quote">&gt; --- a/kernel/fork.c</span>
<span class="quote">&gt; +++ b/kernel/fork.c</span>
<span class="quote">&gt; @@ -617,6 +617,8 @@ static struct mm_struct *mm_init(struct</span>
<span class="quote">&gt;  	mm_init_owner(mm, p);</span>
<span class="quote">&gt;  	mmu_notifier_mm_init(mm);</span>
<span class="quote">&gt;  	clear_tlb_flush_pending(mm);</span>
<span class="quote">&gt; +	futex_mm_hash_init(mm);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS</span>
<span class="quote">&gt;  	mm-&gt;pmd_huge_pte = NULL;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -713,6 +715,7 @@ void mmput(struct mm_struct *mm)</span>
<span class="quote">&gt;  		khugepaged_exit(mm); /* must run before exit_mmap */</span>
<span class="quote">&gt;  		exit_mmap(mm);</span>
<span class="quote">&gt;  		set_mm_exe_file(mm, NULL);</span>
<span class="quote">&gt; +		futex_mm_hash_exit(mm);</span>
<span class="quote">&gt;  		if (!list_empty(&amp;mm-&gt;mmlist)) {</span>
<span class="quote">&gt;  			spin_lock(&amp;mmlist_lock);</span>
<span class="quote">&gt;  			list_del(&amp;mm-&gt;mmlist);</span>
<span class="quote">&gt; --- a/kernel/futex.c</span>
<span class="quote">&gt; +++ b/kernel/futex.c</span>
<span class="quote">&gt; @@ -23,6 +23,9 @@</span>
<span class="quote">&gt;   *  Copyright (C) IBM Corporation, 2009</span>
<span class="quote">&gt;   *  Thanks to Thomas Gleixner for conceptual design and careful reviews.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; + *  Private hashed futex support by Sebastian Siewior and Thomas Gleixner</span>
<span class="quote">&gt; + *  Copyright (C) Linutronix GmbH, 2016</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt;   *  Thanks to Ben LaHaise for yelling &quot;hashed waitqueues&quot; loudly</span>
<span class="quote">&gt;   *  enough at me, Linus for the original (flawed) idea, Matthew</span>
<span class="quote">&gt;   *  Kirkwood for proof-of-concept implementation.</span>
<span class="quote">&gt; @@ -49,6 +52,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/fs.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/file.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/jhash.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hash.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/init.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/futex.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mount.h&gt;</span>
<span class="quote">&gt; @@ -169,6 +173,34 @@</span>
<span class="quote">&gt;   * the code that actually moves the futex(es) between hash buckets (requeue_futex)</span>
<span class="quote">&gt;   * will do the additional required waiter count housekeeping. This is done for</span>
<span class="quote">&gt;   * double_lock_hb() and double_unlock_hb(), respectively.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * For private futexes we (pre)allocate a per process hash. We check lockless</span>
<span class="quote">&gt; + * whether the hash is already allocated. To access the hash later we need</span>
<span class="quote">&gt; + * information about the hash properties as well. This requires barriers as</span>
<span class="quote">&gt; + * follows:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * CPU 0					CPU 1</span>
<span class="quote">&gt; + * check_hash_allocation()</span>
<span class="quote">&gt; + *	if (mm-&gt;futex_hash.hash)</span>
<span class="quote">&gt; + *		return;</span>
<span class="quote">&gt; + *	hash = alloc_hash()</span>
<span class="quote">&gt; + *	lock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="quote">&gt; + *	if (!mm-&gt;futex_hash.hash) {</span>
<span class="quote">&gt; + *	  mm-&gt;futex_hash.par = params;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *	  smp_wmb(); (A0) &lt;-paired with-|</span>
<span class="quote">&gt; + *					|</span>
<span class="quote">&gt; + *	  mm-&gt;futex_hash.hash = hash;	|</span>
<span class="quote">&gt; + *					|	check_hash_allocation()</span>
<span class="quote">&gt; + *					|	   if (mm-&gt;futex_hash.hash)</span>
<span class="quote">&gt; + *					|		return;</span>
<span class="quote">&gt; + *	  unlock(&amp;mm-&gt;futex_hash.lock);	|	get_futex_key_refs()</span>
<span class="quote">&gt; + *					|</span>
<span class="quote">&gt; + *					|--------- smp_mb() (B)</span>
<span class="quote">&gt; + *						s = hash(f, mm-&gt;futex_hash.par);</span>
<span class="quote">&gt; + *						hb = &amp;mm-&gt;futex_hash.hash[s];</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * So we utilize the existing smp_mb() in get_futex_key_refs().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifndef CONFIG_HAVE_FUTEX_CMPXCHG</span>
<span class="quote">&gt; @@ -255,6 +287,22 @@ struct futex_hash_bucket {</span>
<span class="quote">&gt;  	struct plist_head chain;</span>
<span class="quote">&gt;  } ____cacheline_aligned_in_smp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Process private hash for non-shared futexes</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define FUTEX_USE_GLOBAL_HASH		((void *) 0x03)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define FUTEX_MIN_HASH_BITS		order_base_2(4UL)</span>
<span class="quote">&gt; +#define FUTEX_DEF_HASH_BITS		order_base_2(8UL)</span>
<span class="quote">&gt; +#define FUTEX_MAX_HASH_BITS		order_base_2(256UL)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +unsigned int futex_default_hash_bits	= FUTEX_DEF_HASH_BITS;</span>
<span class="quote">&gt; +unsigned int futex_max_hash_bits	= FUTEX_MAX_HASH_BITS;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static const unsigned int futex_default_hash_bits = 0;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * The base of the bucket array and its size are always used together</span>
<span class="quote">&gt;   * (after initialization only in hash_futex()), so ensure that they</span>
<span class="quote">&gt; @@ -374,13 +422,13 @@ static inline int hb_waiters_pending(str</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; - * hash_futex - Return the hash bucket in the global hash</span>
<span class="quote">&gt; + * hash_global_futex - Return the hash bucket in the global hash</span>
<span class="quote">&gt;   * @key:	Pointer to the futex key for which the hash is calculated</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * We hash on the keys returned from get_futex_key (see below) and return the</span>
<span class="quote">&gt;   * corresponding hash bucket in the global hash.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="quote">&gt; +static struct futex_hash_bucket *hash_global_futex(union futex_key *key)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	u32 hash = jhash2((u32*)&amp;key-&gt;both.word,</span>
<span class="quote">&gt;  			  (sizeof(key-&gt;both.word)+sizeof(key-&gt;both.ptr))/4,</span>
<span class="quote">&gt; @@ -388,9 +436,33 @@ static struct futex_hash_bucket *hash_fu</span>
<span class="quote">&gt;  	return &amp;futex_queues[hash &amp; (futex_hashsize - 1)];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * hash_futex - Get the hash bucket for a futex</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Returns either the process private or the global hash bucket which fits the</span>
<span class="quote">&gt; + * key.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; +	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; +	unsigned int slot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="quote">&gt; +		return hash_global_futex(key);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="quote">&gt; +	return &amp;mm-&gt;futex_hash.hash[slot];</span>

Don&#39;t we also need to check if the private hash exists? Per the commit
description, if we fail to allocate the private hash, we fall back to using the
global hash...

...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=106771">Darren Hart</a> - May 6, 2016, 9:56 p.m.</div>
<pre class="content">
On Fri, May 06, 2016 at 11:09:33AM -0700, Darren Hart wrote:
<span class="quote">&gt; On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:</span>
<span class="quote">&gt; &gt; From: Sebastian Siewior &lt;bigeasy@linutronix.de&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The standard futex mechanism in the Linux kernel uses a global hash to store</span>
<span class="quote">&gt; &gt; transient state. Collisions on that hash can lead to performance degradation</span>
<span class="quote">&gt; &gt; especially on NUMA systems and on real-time enabled kernels even to priority</span>
<span class="quote">&gt; &gt; inversions.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think it is worth noting the how this causes an unbounded priority inversion</span>
<span class="quote">&gt; as it wasn&#39;t obvious to me. At least mention that &quot;CPU pinning&quot; can result in an</span>
<span class="quote">&gt; unbounded priority inversion.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; To mitigate that problem we provide per process private hashing. On the first</span>
<span class="quote">&gt; &gt; futex operation in a process the kernel allocates a hash table. The hash table</span>
<span class="quote">&gt; &gt; is accessible via the process mm_struct. On Numa systems the hash is allocated</span>
<span class="quote">&gt; &gt; node local.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If the allocation fails then the global hash table is used as fallback, so</span>
<span class="quote">&gt; &gt; there is no user space visible impact of this feature.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would be good to have a way to detect that the process private hash table was</span>
<span class="quote">&gt; successfully created. Perhaps a /proc/pid/ feature? This would allow us to write</span>
<span class="quote">&gt; a functional futex test for tools/testing/selftests/futex</span>

I suppose we could just use FUTEX_PREALLOC_HASH for this purpose, passing in the
default hash size. This will either return the default, the previously set
value, or 0, indicating the global hash is being used. That should be sufficient
for programatically determining the state of the system.

The /proc/pid/futex_hash_size option may still be convenient for other purposes.
Perhaps with a -1 indicating it hasn&#39;t been set yet.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - May 7, 2016, 8:44 a.m.</div>
<pre class="content">
On Fri, 6 May 2016, Darren Hart wrote:
<span class="quote">&gt; On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:</span>
<span class="quote">&gt; &gt; --- /dev/null</span>
<span class="quote">&gt; &gt; +++ b/include/linux/futex_types.h</span>
<span class="quote">&gt; &gt; @@ -0,0 +1,12 @@</span>
<span class="quote">&gt; &gt; +#ifndef _LINUX_FUTEX_TYPES_H</span>
<span class="quote">&gt; &gt; +#define _LINUX_FUTEX_TYPES_H</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +struct futex_hash_bucket;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +struct futex_hash {</span>
<span class="quote">&gt; &gt; +	struct raw_spinlock		lock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As it isn&#39;t always obvious to everone, it would be good to add a single line</span>
<span class="quote">&gt; comment stating why a *raw* spinlock is necessary.</span>

Well. Necessary. It protects the hash pointer and the hash bits. So the scope
is very limited and really does not need the heavy weight version of a
sleeping spinlock in RT.
<span class="quote"> 
&gt; In this case... I suppose this could lead to some nasty scenarios setting up IPC</span>
<span class="quote">&gt; mechanisms between threads if they weren&#39;t strictly serialized? Something else?</span>

Sure, we need to serialize attempts to populate the hash. Especially in the
non preallocated case. The thing with raw vs. non raw spinlocks is that the
latter are expensive on RT and if there are just 5 instructions to protect it
does not make any sense to chose the heavy version.
<span class="quote"> 
&gt; &gt; +config FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; &gt; +	bool</span>
<span class="quote">&gt; &gt; +	default FUTEX &amp;&amp; SMP</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So no prompt, not user selectable. If you have SMP, you get this? I think</span>
<span class="quote">&gt; automatic is a good call... but is SMP the right criteria, or would NUMA be more</span>
<span class="quote">&gt; appropriate since I thought it was keeping the hash local to the NUMA node that</span>
<span class="quote">&gt; was the big win?</span>

Yes, we can make it depend on NUMA. I even thought about making a run time
decision for non preallocated ones when the machine is not numa. But for test
coverage I wanted to have it as widely used as possible.
<span class="quote"> 
&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="quote">&gt; &gt; +		return hash_global_futex(key);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="quote">&gt; &gt; +	return &amp;mm-&gt;futex_hash.hash[slot];</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Don&#39;t we also need to check if the private hash exists? Per the commit</span>
<span class="quote">&gt; description, if we fail to allocate the private hash, we fall back to using the</span>
<span class="quote">&gt; global hash...</span>

If we fall back to the global hash, then the lower bits in offset are not
0. So the hash is guaranteed to be available.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - May 7, 2016, 8:45 a.m.</div>
<pre class="content">
On Fri, 6 May 2016, Darren Hart wrote:
<span class="quote"> &gt; It would be good to have a way to detect that the process private hash table was</span>
<span class="quote">&gt; &gt; successfully created. Perhaps a /proc/pid/ feature? This would allow us to write</span>
<span class="quote">&gt; &gt; a functional futex test for tools/testing/selftests/futex</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I suppose we could just use FUTEX_PREALLOC_HASH for this purpose, passing in the</span>
<span class="quote">&gt; default hash size. This will either return the default, the previously set</span>
<span class="quote">&gt; value, or 0, indicating the global hash is being used. That should be sufficient</span>
<span class="quote">&gt; for programatically determining the state of the system.</span>

Right.
<span class="quote"> 
&gt; The /proc/pid/futex_hash_size option may still be convenient for other purposes.</span>
<span class="quote">&gt; Perhaps with a -1 indicating it hasn&#39;t been set yet.</span>

Dunno, whether that&#39;s valuable, but it can be done on top.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=106771">Darren Hart</a> - May 11, 2016, 9:07 p.m.</div>
<pre class="content">
On Sat, May 07, 2016 at 10:44:39AM +0200, Thomas Gleixner wrote:
<span class="quote">&gt; On Fri, 6 May 2016, Darren Hart wrote:</span>
<span class="quote">&gt; &gt; On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:</span>
<span class="quote">&gt; &gt; &gt; --- /dev/null</span>
<span class="quote">&gt; &gt; &gt; +++ b/include/linux/futex_types.h</span>
<span class="quote">&gt; &gt; &gt; @@ -0,0 +1,12 @@</span>
<span class="quote">&gt; &gt; &gt; +#ifndef _LINUX_FUTEX_TYPES_H</span>
<span class="quote">&gt; &gt; &gt; +#define _LINUX_FUTEX_TYPES_H</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +struct futex_hash_bucket;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +struct futex_hash {</span>
<span class="quote">&gt; &gt; &gt; +	struct raw_spinlock		lock;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; As it isn&#39;t always obvious to everone, it would be good to add a single line</span>
<span class="quote">&gt; &gt; comment stating why a *raw* spinlock is necessary.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well. Necessary. It protects the hash pointer and the hash bits. So the scope</span>
<span class="quote">&gt; is very limited and really does not need the heavy weight version of a</span>
<span class="quote">&gt; sleeping spinlock in RT.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; &gt; In this case... I suppose this could lead to some nasty scenarios setting up IPC</span>
<span class="quote">&gt; &gt; mechanisms between threads if they weren&#39;t strictly serialized? Something else?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, we need to serialize attempts to populate the hash. Especially in the</span>
<span class="quote">&gt; non preallocated case. The thing with raw vs. non raw spinlocks is that the</span>
<span class="quote">&gt; latter are expensive on RT and if there are just 5 instructions to protect it</span>
<span class="quote">&gt; does not make any sense to chose the heavy version.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; &gt; &gt; +config FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; &gt; &gt; +	bool</span>
<span class="quote">&gt; &gt; &gt; +	default FUTEX &amp;&amp; SMP</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So no prompt, not user selectable. If you have SMP, you get this? I think</span>
<span class="quote">&gt; &gt; automatic is a good call... but is SMP the right criteria, or would NUMA be more</span>
<span class="quote">&gt; &gt; appropriate since I thought it was keeping the hash local to the NUMA node that</span>
<span class="quote">&gt; &gt; was the big win?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, we can make it depend on NUMA. I even thought about making a run time</span>
<span class="quote">&gt; decision for non preallocated ones when the machine is not numa. But for test</span>
<span class="quote">&gt; coverage I wanted to have it as widely used as possible.</span>

OK, understood to here.
<span class="quote">
&gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; +	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="quote">&gt; &gt; &gt; +		return hash_global_futex(key);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="quote">&gt; &gt; &gt; +	return &amp;mm-&gt;futex_hash.hash[slot];</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Don&#39;t we also need to check if the private hash exists? Per the commit</span>
<span class="quote">&gt; &gt; description, if we fail to allocate the private hash, we fall back to using the</span>
<span class="quote">&gt; &gt; global hash...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we fall back to the global hash, then the lower bits in offset are not</span>
<span class="quote">&gt; 0. So the hash is guaranteed to be available.</span>
<span class="quote">&gt; </span>

Ah right. Since the position of the bits in the two flags isn&#39;t obvious when
reading the test, the comment about the lower bits being cleared didn&#39;t
translate to that case being implicitly covered by the test.

Maybe make this explicit?

/*
 * Only private futexes use the per process hash and they will not have
 * FUT_OFF_INODE nor FUT_OFF_MMSHARED set.
 */
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=106771">Darren Hart</a> - May 11, 2016, 9:08 p.m.</div>
<pre class="content">
On Sat, May 07, 2016 at 10:45:57AM +0200, Thomas Gleixner wrote:
<span class="quote">&gt; On Fri, 6 May 2016, Darren Hart wrote:</span>
<span class="quote">&gt;  &gt; It would be good to have a way to detect that the process private hash table was</span>
<span class="quote">&gt; &gt; &gt; successfully created. Perhaps a /proc/pid/ feature? This would allow us to write</span>
<span class="quote">&gt; &gt; &gt; a functional futex test for tools/testing/selftests/futex</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I suppose we could just use FUTEX_PREALLOC_HASH for this purpose, passing in the</span>
<span class="quote">&gt; &gt; default hash size. This will either return the default, the previously set</span>
<span class="quote">&gt; &gt; value, or 0, indicating the global hash is being used. That should be sufficient</span>
<span class="quote">&gt; &gt; for programatically determining the state of the system.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; &gt; The /proc/pid/futex_hash_size option may still be convenient for other purposes.</span>
<span class="quote">&gt; &gt; Perhaps with a -1 indicating it hasn&#39;t been set yet.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Dunno, whether that&#39;s valuable, but it can be done on top.</span>

Agreed. We can leave that to the kselftest patch, and if it&#39;s easily done
without this, then we&#39;re done. If not, we can look at it then.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 19, 2016, 12:21 p.m.</div>
<pre class="content">
On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:
<span class="quote">&gt; +struct futex_hash {</span>
<span class="quote">&gt; +	struct raw_spinlock		lock;</span>

	raw_spinlock_t ?
<span class="quote">
&gt; +	unsigned int			hash_bits;</span>
<span class="quote">&gt; +	struct futex_hash_bucket	*hash;</span>
<span class="quote">&gt; +};</span>
<span class="quote">
&gt; +static void futex_populate_hash(unsigned int hash_bits)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; +	struct futex_hash_bucket *hb = NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We don&#39;t need an explicit smp_mb() when the hash is populated</span>
<span class="quote">&gt; +	 * because before we dereference mm-&gt;futex_hash.hash_bits in the hash</span>
<span class="quote">&gt; +	 * function we have an smp_mb() in futex_get_key_refs() already.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (mm-&gt;futex_hash.hash)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If we failed to allocate a hash on the fly, fall back to the global</span>
<span class="quote">&gt; +	 * hash.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	hb = futex_alloc_hash(hash_bits);</span>
<span class="quote">&gt; +	if (!hb)</span>
<span class="quote">&gt; +		hb = FUTEX_USE_GLOBAL_HASH;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	raw_spin_lock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="quote">&gt; +	/* We might have raced with another task allocating the hash. */</span>
<span class="quote">&gt; +	if (!mm-&gt;futex_hash.hash) {</span>
<span class="quote">&gt; +		mm-&gt;futex_hash.hash_bits = hash_bits;</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Ensure that the above is visible before we store</span>
<span class="quote">&gt; +		 * the pointer.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		smp_wmb(); /* (A0) Pairs with (B) */</span>
<span class="quote">&gt; +		mm-&gt;futex_hash.hash = hb;</span>

		smp_store_release(&amp;mm-&gt;futex_hash.hash, hb); ?
<span class="quote">
&gt; +		hb = NULL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	raw_spin_unlock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="quote">&gt; +	kfree(hb);</span>
<span class="quote">&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 19, 2016, 12:24 p.m.</div>
<pre class="content">
On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:
<span class="quote">&gt; +static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; +	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; +	unsigned int slot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="quote">&gt; +		return hash_global_futex(key);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="quote">&gt; +	return &amp;mm-&gt;futex_hash.hash[slot];</span>

Do we want the option to WARN if we get collisions in this per-process
hash?

Because afaiu there is no guarantee what so ever this doesn&#39;t happen,
and collisions here can create the very same priority inversions as are
possible in the global hash.

Less likely etc.. more contained since its only the threads of the one
process that get tangled up, but still possible.
<span class="quote">
&gt; +#else</span>
<span class="quote">&gt; +	return hash_global_futex(key);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=481">Sebastian Andrzej Siewior</a> - May 27, 2016, 4:36 p.m.</div>
<pre class="content">
On 2016-05-07 10:44:39 [+0200], Thomas Gleixner wrote:
<span class="quote">&gt; On Fri, 6 May 2016, Darren Hart wrote:</span>
<span class="quote">&gt; &gt; On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:</span>
<span class="quote">&gt; Sure, we need to serialize attempts to populate the hash. Especially in the</span>
<span class="quote">&gt; non preallocated case. The thing with raw vs. non raw spinlocks is that the</span>
<span class="quote">&gt; latter are expensive on RT and if there are just 5 instructions to protect it</span>
<span class="quote">&gt; does not make any sense to chose the heavy version.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; &gt; &gt; +config FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; &gt; &gt; +	bool</span>
<span class="quote">&gt; &gt; &gt; +	default FUTEX &amp;&amp; SMP</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So no prompt, not user selectable. If you have SMP, you get this? I think</span>
<span class="quote">&gt; &gt; automatic is a good call... but is SMP the right criteria, or would NUMA be more</span>
<span class="quote">&gt; &gt; appropriate since I thought it was keeping the hash local to the NUMA node that</span>
<span class="quote">&gt; &gt; was the big win?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, we can make it depend on NUMA. I even thought about making a run time</span>
<span class="quote">&gt; decision for non preallocated ones when the machine is not numa. But for test</span>
<span class="quote">&gt; coverage I wanted to have it as widely used as possible.</span>

Do we want to change it to autodetect NUMA at runtime or do we want to
keep it as is for now?

Sebastian
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1543">Sebastian Andrzej Siewior</a> - May 27, 2016, 4:52 p.m.</div>
<pre class="content">
On 2016-05-19 14:21:48 [+0200], Peter Zijlstra wrote:
<span class="quote">&gt; &gt; +static void futex_populate_hash(unsigned int hash_bits)</span>
<span class="quote">&gt; &gt; +{</span>

<span class="quote">&gt; &gt; +	raw_spin_lock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="quote">&gt; &gt; +	/* We might have raced with another task allocating the hash. */</span>
<span class="quote">&gt; &gt; +	if (!mm-&gt;futex_hash.hash) {</span>
<span class="quote">&gt; &gt; +		mm-&gt;futex_hash.hash_bits = hash_bits;</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * Ensure that the above is visible before we store</span>
<span class="quote">&gt; &gt; +		 * the pointer.</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +		smp_wmb(); /* (A0) Pairs with (B) */</span>
<span class="quote">&gt; &gt; +		mm-&gt;futex_hash.hash = hb;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		smp_store_release(&amp;mm-&gt;futex_hash.hash, hb); ?</span>

just to be clear: You suggest to use &quot;smp_store_release()&quot; instead
smp_wmb() followed by the assignment?

Sebastian
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1543">Sebastian Andrzej Siewior</a> - May 27, 2016, 5:10 p.m.</div>
<pre class="content">
On 2016-05-19 14:24:06 [+0200], Peter Zijlstra wrote:
<span class="quote">&gt; On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:</span>
<span class="quote">&gt; &gt; +static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; &gt; +	unsigned int slot;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="quote">&gt; &gt; +		return hash_global_futex(key);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="quote">&gt; &gt; +	return &amp;mm-&gt;futex_hash.hash[slot];</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we want the option to WARN if we get collisions in this per-process</span>
<span class="quote">&gt; hash?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because afaiu there is no guarantee what so ever this doesn&#39;t happen,</span>
<span class="quote">&gt; and collisions here can create the very same priority inversions as are</span>
<span class="quote">&gt; possible in the global hash.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Less likely etc.. more contained since its only the threads of the one</span>
<span class="quote">&gt; process that get tangled up, but still possible.</span>

Since the collision is contained the same process it is less dramatic.
But how do you want to warn the user? A trace-event would be handy to
dump the uaddr and slot. The user would have to check the trace and
figure out which slot was assigend to different uaddr. But due to ASLR
the same application might result in a different behaviour on each run.
However, it might be good for a indication about the size of the private
hash

Sebastian
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 30, 2016, 8:43 a.m.</div>
<pre class="content">
On Fri, May 27, 2016 at 06:52:11PM +0200, Sebastian Andrzej Siewior wrote:
<span class="quote">&gt; On 2016-05-19 14:21:48 [+0200], Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; &gt; +static void futex_populate_hash(unsigned int hash_bits)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; +	raw_spin_lock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="quote">&gt; &gt; &gt; +	/* We might have raced with another task allocating the hash. */</span>
<span class="quote">&gt; &gt; &gt; +	if (!mm-&gt;futex_hash.hash) {</span>
<span class="quote">&gt; &gt; &gt; +		mm-&gt;futex_hash.hash_bits = hash_bits;</span>
<span class="quote">&gt; &gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; &gt; +		 * Ensure that the above is visible before we store</span>
<span class="quote">&gt; &gt; &gt; +		 * the pointer.</span>
<span class="quote">&gt; &gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; &gt; +		smp_wmb(); /* (A0) Pairs with (B) */</span>
<span class="quote">&gt; &gt; &gt; +		mm-&gt;futex_hash.hash = hb;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 		smp_store_release(&amp;mm-&gt;futex_hash.hash, hb); ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; just to be clear: You suggest to use &quot;smp_store_release()&quot; instead</span>
<span class="quote">&gt; smp_wmb() followed by the assignment?</span>

Yes, smp_store_release() is the most natural way to publish things like
this. Note that rcu_assign_pointer() also switched to using that. See
commit: 88c1863066cc (&quot;rcu: Define rcu_assign_pointer() in terms of
smp_store_release()&quot;) for detail on the difference.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 30, 2016, 8:58 a.m.</div>
<pre class="content">
On Fri, May 27, 2016 at 07:10:01PM +0200, Sebastian Andrzej Siewior wrote:
<span class="quote">&gt; On 2016-05-19 14:24:06 [+0200], Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:</span>
<span class="quote">&gt; &gt; &gt; +static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt; &gt; &gt; +	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; &gt; &gt; +	unsigned int slot;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; +	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="quote">&gt; &gt; &gt; +		return hash_global_futex(key);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="quote">&gt; &gt; &gt; +	return &amp;mm-&gt;futex_hash.hash[slot];</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Do we want the option to WARN if we get collisions in this per-process</span>
<span class="quote">&gt; &gt; hash?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Because afaiu there is no guarantee what so ever this doesn&#39;t happen,</span>
<span class="quote">&gt; &gt; and collisions here can create the very same priority inversions as are</span>
<span class="quote">&gt; &gt; possible in the global hash.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Less likely etc.. more contained since its only the threads of the one</span>
<span class="quote">&gt; &gt; process that get tangled up, but still possible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since the collision is contained the same process it is less dramatic.</span>

Right, but can still cause significant malfunction inside the process.
So its not something to completely ignore. If your room sized CNC
machine gets the priorities of the logging thread and the motor control
thread confused bad things could happen.
<span class="quote">
&gt; But how do you want to warn the user? A trace-event would be handy to</span>
<span class="quote">&gt; dump the uaddr and slot.</span>

So I think there&#39;s a number of cases:

 - PREALLOC_HASH finds a taken bucket; in this case we can simply return
   an error.
 - PREALLOC_HASH succeeds, but an on demand hash later hits the same
   bucket. This is harder; we could maybe mark all buckets taken by
   PREALLOC_HASH and allow for a signal when this collision hits. Dunno.
<span class="quote">
&gt; The user would have to check the trace and</span>
<span class="quote">&gt; figure out which slot was assigend to different uaddr. </span>

Yeah, that&#39;s not really workable, might work for debugging, but blergh.
<span class="quote">
&gt; But due to ASLR</span>
<span class="quote">&gt; the same application might result in a different behaviour on each run.</span>

Yeah, ASLR makes this all somewhat non deterministic, which is why you
really don&#39;t want a silent collision for your PREALLOC_HASH buckets.
Because once every 100 runs it does weird,..
<span class="quote">
&gt; However, it might be good for a indication about the size of the private</span>
<span class="quote">&gt; hash</span>

Yeah, now if online resize wasn&#39;t such a pain ;-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=481">Sebastian Andrzej Siewior</a> - May 30, 2016, 11:08 a.m.</div>
<pre class="content">
On 05/30/2016 10:58 AM, Peter Zijlstra wrote:
<span class="quote">&gt; On Fri, May 27, 2016 at 07:10:01PM +0200, Sebastian Andrzej Siewior wrote:</span>
<span class="quote">&gt;&gt; On 2016-05-19 14:24:06 [+0200], Peter Zijlstra wrote:</span>
<span class="quote">&gt;&gt;&gt; On Thu, May 05, 2016 at 08:44:04PM -0000, Thomas Gleixner wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; +static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="quote">&gt;&gt;&gt;&gt; +	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned int slot;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt;&gt;&gt; +	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="quote">&gt;&gt;&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="quote">&gt;&gt;&gt;&gt; +		return hash_global_futex(key);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	return &amp;mm-&gt;futex_hash.hash[slot];</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Do we want the option to WARN if we get collisions in this per-process</span>
<span class="quote">&gt;&gt;&gt; hash?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Because afaiu there is no guarantee what so ever this doesn&#39;t happen,</span>
<span class="quote">&gt;&gt;&gt; and collisions here can create the very same priority inversions as are</span>
<span class="quote">&gt;&gt;&gt; possible in the global hash.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Less likely etc.. more contained since its only the threads of the one</span>
<span class="quote">&gt;&gt;&gt; process that get tangled up, but still possible.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Since the collision is contained the same process it is less dramatic.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right, but can still cause significant malfunction inside the process.</span>
<span class="quote">&gt; So its not something to completely ignore. If your room sized CNC</span>
<span class="quote">&gt; machine gets the priorities of the logging thread and the motor control</span>
<span class="quote">&gt; thread confused bad things could happen.</span>
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; But how do you want to warn the user? A trace-event would be handy to</span>
<span class="quote">&gt;&gt; dump the uaddr and slot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I think there&#39;s a number of cases:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - PREALLOC_HASH finds a taken bucket; in this case we can simply return</span>
<span class="quote">&gt;    an error.</span>
<span class="quote">&gt;  - PREALLOC_HASH succeeds, but an on demand hash later hits the same</span>
<span class="quote">&gt;    bucket. This is harder; we could maybe mark all buckets taken by</span>
<span class="quote">&gt;    PREALLOC_HASH and allow for a signal when this collision hits. Dunno.</span>

PREALLOC_HASH happens once before any (contended) lock operation. We
never rehash the hash. To rehash the hash on runtime we would need an
empty futex hash and some locking in the fast path. And rehash seems
not to be required since we tried to come up with a sane default value
and the user/RT task can set it to the current max value.

So back to when does the collision happen. Since glibc visits the
kernel only on contention we might learn about the collision when it is
too late. We could have a lock operation by thread1 followed by lock
operation by thread2 on different uaddr resulting in the same bucket.
In this case we learn about this once the spin_lock() operation blocks.

Also marking a bucket as taken (on contention) might give false
positive results since we know nothing about lock&#39;s lifetime (i.e. the
lock might have been free()ed).

But if I may bring some ideas from v1. In v1 we had &quot;tickets / IDs&quot; for
the futex per thread. In v2 we don&#39;t have them anymore. We still have
the &quot;private&quot; futex hash buckets but per process this time.
We could introduce the &quot;tickets / IDs&quot; back and make them process wide.
We could hide them in pthread_mutex_init() and pthread_mutex_destroy()
since their IDs are no longer thread unique. I think I had something in
glibc&#39;s pthread variable where we could store 16bit if I split another
32bit variable.

That would be guaranteed collision free and hidden in glibc. But it
would take some time to get it used since it does no longer work out of
the box by updating the kernel. We could also add it later if people
scream for it since we can&#39;t change the behavior of &quot;PRIVATE&quot; futex
(uaddr vs ticket number).
<span class="quote">
&gt;&gt; But due to ASLR</span>
<span class="quote">&gt;&gt; the same application might result in a different behaviour on each run.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, ASLR makes this all somewhat non deterministic, which is why you</span>
<span class="quote">&gt; really don&#39;t want a silent collision for your PREALLOC_HASH buckets.</span>
<span class="quote">&gt; Because once every 100 runs it does weird,..</span>

I think that you might learn about your collision too late and there
is nothing you can do about it. Logging in a logfile or restart the
application after 5 minutes of runtime? Not to mention the locks which
are contended in an error situation.

A futex op returning the kernel&#39;s hash value might be sane. You would
expose some implementation detail but the application could check its
mutex for collision before starting (doing the real work). On collision
it would have to restart and hope for the best if the collision from
ASLR.
But since you don&#39;t know about all mutexes, like those which are part
of library, you wouldn&#39;t never be 100% collision free here. So it is
probably a bad idea.
<span class="quote">
&gt;&gt; However, it might be good for a indication about the size of the private</span>
<span class="quote">&gt;&gt; hash</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, now if online resize wasn&#39;t such a pain ;-)</span>

My point was during development / testing to figure out which initial
value is sane / reasonable. Start your APP with 32 slot. Collisions.
Again with 128 slots. Oh better.

Sebastian
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 30, 2016, 12:06 p.m.</div>
<pre class="content">
On Mon, May 30, 2016 at 01:08:53PM +0200, Sebastian Andrzej Siewior wrote:
<span class="quote">&gt; &gt; So I think there&#39;s a number of cases:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  - PREALLOC_HASH finds a taken bucket; in this case we can simply return</span>
<span class="quote">&gt; &gt;    an error.</span>
<span class="quote">&gt; &gt;  - PREALLOC_HASH succeeds, but an on demand hash later hits the same</span>
<span class="quote">&gt; &gt;    bucket. This is harder; we could maybe mark all buckets taken by</span>
<span class="quote">&gt; &gt;    PREALLOC_HASH and allow for a signal when this collision hits. Dunno.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; PREALLOC_HASH happens once before any (contended) lock operation. </span>

I know ;-)
<span class="quote">
&gt; We</span>
<span class="quote">&gt; never rehash the hash. To rehash the hash on runtime we would need an</span>
<span class="quote">&gt; empty futex hash and some locking in the fast path.</span>

Nah; you can do lockless rehashing. Little tricky, but entirely doable.
At worst we&#39;d need an rcu_read_lock(), but with a little extra work we
can extend an existing preempt_disable section to cover the hash lookup
if it isn&#39;t already covered.
<span class="quote">
&gt; And rehash seems</span>
<span class="quote">&gt; not to be required since we tried to come up with a sane default value</span>
<span class="quote">&gt; and the user/RT task can set it to the current max value.</span>

I&#39;m not sure this statement is true; given that the hash function
reduces the entire address space down to a few bits you&#39;re guaranteed to
have collisions at some point. A good hash will not have more than given
by the birthday paradox, but you cannot have less.

So while the sizing can certainly reduce the chance on collisions,
nothing guarantees you will not have them, quite the opposite.
<span class="quote">
&gt; So back to when does the collision happen. Since glibc visits the</span>
<span class="quote">&gt; kernel only on contention we might learn about the collision when it is</span>
<span class="quote">&gt; too late. We could have a lock operation by thread1 followed by lock</span>
<span class="quote">&gt; operation by thread2 on different uaddr resulting in the same bucket.</span>
<span class="quote">&gt; In this case we learn about this once the spin_lock() operation blocks.</span>

Yep, so if this is two ondemand futexes hitting the same bucket, we
don&#39;t care. But if this is an ondemand futex hitting a prealloc one, we
do care.

And yes, this is late, but its the only possible time. And notification
is better than silent weird behaviour.
<span class="quote">
&gt; Also marking a bucket as taken (on contention) might give false</span>
<span class="quote">&gt; positive results since we know nothing about lock&#39;s lifetime (i.e. the</span>
<span class="quote">&gt; lock might have been free()ed).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But if I may bring some ideas from v1. In v1 we had &quot;tickets / IDs&quot; for</span>
<span class="quote">&gt; the futex per thread. In v2 we don&#39;t have them anymore. We still have</span>
<span class="quote">&gt; the &quot;private&quot; futex hash buckets but per process this time.</span>
<span class="quote">&gt; We could introduce the &quot;tickets / IDs&quot; back and make them process wide.</span>
<span class="quote">&gt; We could hide them in pthread_mutex_init() and pthread_mutex_destroy()</span>
<span class="quote">&gt; since their IDs are no longer thread unique. I think I had something in</span>
<span class="quote">&gt; glibc&#39;s pthread variable where we could store 16bit if I split another</span>
<span class="quote">&gt; 32bit variable.</span>

Not sure you need a ticket (and I never got around to reading v1), a
simple twin to PREALLOCATE_HASH to call on mutex_destroy would be
sufficient I think. It could free the hash bucket.
<span class="quote">
&gt; That would be guaranteed collision free and hidden in glibc.</span>

I&#39;m still not seeing how you can guarantee anything with it; ondemand
local hashing can still collide, right?

Or are you thinking two local hashtables, one for ondemand things and
one for prealloc/free type thingies?
<span class="quote">
&gt; My point was during development / testing to figure out which initial</span>
<span class="quote">&gt; value is sane / reasonable. Start your APP with 32 slot. Collisions.</span>
<span class="quote">&gt; Again with 128 slots. Oh better.</span>

Ah, right, but given the above and ASLR, you cannot exhaustively test
this.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=481">Sebastian Andrzej Siewior</a> - May 30, 2016, 1:37 p.m.</div>
<pre class="content">
On 05/30/2016 02:06 PM, Peter Zijlstra wrote:
<span class="quote">&gt;&gt; We</span>
<span class="quote">&gt;&gt; never rehash the hash. To rehash the hash on runtime we would need an</span>
<span class="quote">&gt;&gt; empty futex hash and some locking in the fast path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Nah; you can do lockless rehashing. Little tricky, but entirely doable.</span>
<span class="quote">&gt; At worst we&#39;d need an rcu_read_lock(), but with a little extra work we</span>
<span class="quote">&gt; can extend an existing preempt_disable section to cover the hash lookup</span>
<span class="quote">&gt; if it isn&#39;t already covered.</span>

you also need ensure that all new users use the new hash but all old
users go for the old one until you can switch them over. And it is
likely we miss a corner case because it is futex after all.

And still, we all that? You need an upper limit how often / until which
size you do resize. And if you want to avoid collisions at all costs
(because) go for the max value if you think you need it so there is no
need to resize. And if the task does not pre-allocate why care at all?
<span class="quote">
&gt;&gt; And rehash seems</span>
<span class="quote">&gt;&gt; not to be required since we tried to come up with a sane default value</span>
<span class="quote">&gt;&gt; and the user/RT task can set it to the current max value.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure this statement is true; given that the hash function</span>
<span class="quote">&gt; reduces the entire address space down to a few bits you&#39;re guaranteed to</span>
<span class="quote">&gt; have collisions at some point. A good hash will not have more than given</span>
<span class="quote">&gt; by the birthday paradox, but you cannot have less.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So while the sizing can certainly reduce the chance on collisions,</span>
<span class="quote">&gt; nothing guarantees you will not have them, quite the opposite.</span>

Yes, I did not try to discuss the hash collision away. From RT&#39;s point
of view the problems I am aware are of the following scenario:
Task #1 pinned to CPU1 task #2 pinned to CPU2 but share the same
bucket. Task #2 got a wakeup and should run but is blocked on the
bucket lock - otherwise it could run. Usually it would PI-boost task#1
but task#1 got preempted itself by task and since task#1 prio is lower
it can&#39;t boost its way free towards the lock and so so CPU #2 may
remain idle for some time.

The same thing can happen within a Task if you take my story from above
and replace task with thread. Completely understood.
<span class="quote">
&gt;&gt; So back to when does the collision happen. Since glibc visits the</span>
<span class="quote">&gt;&gt; kernel only on contention we might learn about the collision when it is</span>
<span class="quote">&gt;&gt; too late. We could have a lock operation by thread1 followed by lock</span>
<span class="quote">&gt;&gt; operation by thread2 on different uaddr resulting in the same bucket.</span>
<span class="quote">&gt;&gt; In this case we learn about this once the spin_lock() operation blocks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yep, so if this is two ondemand futexes hitting the same bucket, we</span>
<span class="quote">&gt; don&#39;t care. But if this is an ondemand futex hitting a prealloc one, we</span>
<span class="quote">&gt; do care.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And yes, this is late, but its the only possible time. And notification</span>
<span class="quote">&gt; is better than silent weird behaviour.</span>

We don&#39;t distinguish right now between &quot;user asked to preallocate the
hash&quot; and &quot;we created this hash because we had a locking job in
kernel&quot;. The latter is the behaviour we used to have and the former is
something like &quot;this new preallocate thingy does not always work&quot;.
<span class="quote">
&gt;&gt; Also marking a bucket as taken (on contention) might give false</span>
<span class="quote">&gt;&gt; positive results since we know nothing about lock&#39;s lifetime (i.e. the</span>
<span class="quote">&gt;&gt; lock might have been free()ed).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But if I may bring some ideas from v1. In v1 we had &quot;tickets / IDs&quot; for</span>
<span class="quote">&gt;&gt; the futex per thread. In v2 we don&#39;t have them anymore. We still have</span>
<span class="quote">&gt;&gt; the &quot;private&quot; futex hash buckets but per process this time.</span>
<span class="quote">&gt;&gt; We could introduce the &quot;tickets / IDs&quot; back and make them process wide.</span>
<span class="quote">&gt;&gt; We could hide them in pthread_mutex_init() and pthread_mutex_destroy()</span>
<span class="quote">&gt;&gt; since their IDs are no longer thread unique. I think I had something in</span>
<span class="quote">&gt;&gt; glibc&#39;s pthread variable where we could store 16bit if I split another</span>
<span class="quote">&gt;&gt; 32bit variable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Not sure you need a ticket (and I never got around to reading v1), a</span>

so v1 in short. To lock you do:
  futex(uaddr, FUTEX_LOCK_PI | PRIVATE, )

instead uaddr, we used tickets and called the mode attached:
  futex(ticket, FUTEX_LOCK_PI | PRIVATE | ATTACHED, )

and the ticket was the index in our array where we kept the hash
buckets. That means each user space futex has its own hash bucket
without contention. But since we had the tickets unique in every thread
it was hard to handle. Keeping them the same process wide should make
things simple again.
<span class="quote">
&gt; simple twin to PREALLOCATE_HASH to call on mutex_destroy would be</span>
<span class="quote">&gt; sufficient I think. It could free the hash bucket.</span>

You do PREALLOCATE_HASH _once_ on startup not for every mutex. But yes,
you would mark each mutex in mutex_destroy() as gone.

So this would &quot;work&quot;, you would find a collision during a slot lookup.
You could increase the coverage if you add another marker in
pthread_mutex_init() (well, except for static initializes). That means
we need glibc changes for this to work.
And what do you suggest to report such a collision? A trace point, a
file in proc? WARN_ON_ONCE() would not result in CVE but is not that
pretty and informative. Not sure if glibc is all for a return code
which is not part of POSIX.

Also I&#39;m not sure what to do with this information unless this happens
during development. That is why I suggest the above which is guaranteed
hash clash free :)
<span class="quote">
&gt;&gt; That would be guaranteed collision free and hidden in glibc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m still not seeing how you can guarantee anything with it; ondemand</span>
<span class="quote">&gt; local hashing can still collide, right?</span>

Since we have a process local hash table for private futex, yes
collisions can happen.
<span class="quote">
&gt; Or are you thinking two local hashtables, one for ondemand things and</span>
<span class="quote">&gt; one for prealloc/free type thingies?</span>

I think one hash table for private futexes as we have now and if people
still complain about collisions get the dedicated &quot;struct
futex_hash_bucket&quot; for each pthread_mutex_t which we had in v1 and then
use the returned ticket for every lock operation. But this time keep it
ticket number the same within a process.
This would guarantee that no collision happens within a task and
requires changes in glibc. So the entry barrier is a little higher but
we could merge this (more or less) as is and the the collision-free
extension on top if there is demand for it.
<span class="quote">
&gt;&gt; My point was during development / testing to figure out which initial</span>
<span class="quote">&gt;&gt; value is sane / reasonable. Start your APP with 32 slot. Collisions.</span>
<span class="quote">&gt;&gt; Again with 128 slots. Oh better.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ah, right, but given the above and ASLR, you cannot exhaustively test</span>
<span class="quote">&gt; this.</span>

correct. You could disable ASLR but security wise you might not want do
that (but then if everything runs as root anyway, disabling ASLR is the
next logical step :) ).

Sebastian
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 30, 2016, 1:49 p.m.</div>
<pre class="content">
On Mon, May 30, 2016 at 03:37:48PM +0200, Sebastian Andrzej Siewior wrote:
<span class="quote">&gt; Yes, I did not try to discuss the hash collision away. From RT&#39;s point</span>
<span class="quote">&gt; of view the problems I am aware are of the following scenario:</span>
<span class="quote">&gt; Task #1 pinned to CPU1 task #2 pinned to CPU2 but share the same</span>
<span class="quote">&gt; bucket. Task #2 got a wakeup and should run but is blocked on the</span>
<span class="quote">&gt; bucket lock - otherwise it could run. Usually it would PI-boost task#1</span>
<span class="quote">&gt; but task#1 got preempted itself by task and since task#1 prio is lower</span>
<span class="quote">&gt; it can&#39;t boost its way free towards the lock and so so CPU #2 may</span>
<span class="quote">&gt; remain idle for some time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The same thing can happen within a Task if you take my story from above</span>
<span class="quote">&gt; and replace task with thread. Completely understood.</span>

Right; so I don&#39;t see the point of PREALLOCATE_HASH to cater for RT
workloads if it still doesn&#39;t guarantee anything, esp. if the failure
case is silent and obscure.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=481">Sebastian Andrzej Siewior</a> - May 30, 2016, 1:59 p.m.</div>
<pre class="content">
On 05/30/2016 03:49 PM, Peter Zijlstra wrote:
<span class="quote">&gt;&gt; The same thing can happen within a Task if you take my story from above</span>
<span class="quote">&gt;&gt; and replace task with thread. Completely understood.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right; so I don&#39;t see the point of PREALLOCATE_HASH to cater for RT</span>
<span class="quote">&gt; workloads if it still doesn&#39;t guarantee anything, esp. if the failure</span>
<span class="quote">&gt; case is silent and obscure.</span>

So what do you suggest? Adding trace points in order to learn about
possible collisions or using tickets (on top of this) to guarantee
being collision free?
Note: this as it, is already a win on NUMA boxes since the memory is
not referenced cross node.

Sebastian
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 30, 2016, 2:02 p.m.</div>
<pre class="content">
On Mon, May 30, 2016 at 03:59:51PM +0200, Sebastian Andrzej Siewior wrote:
<span class="quote">&gt; On 05/30/2016 03:49 PM, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt;&gt; The same thing can happen within a Task if you take my story from above</span>
<span class="quote">&gt; &gt;&gt; and replace task with thread. Completely understood.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Right; so I don&#39;t see the point of PREALLOCATE_HASH to cater for RT</span>
<span class="quote">&gt; &gt; workloads if it still doesn&#39;t guarantee anything, esp. if the failure</span>
<span class="quote">&gt; &gt; case is silent and obscure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So what do you suggest? Adding trace points in order to learn about</span>
<span class="quote">&gt; possible collisions or using tickets (on top of this) to guarantee</span>
<span class="quote">&gt; being collision free?</span>

I have no idea about the ticket stuff, i&#39;ve not seen it. But yes, you
need to somehow guarantee no collisions.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/include/linux/futex.h</span>
<span class="p_header">+++ b/include/linux/futex.h</span>
<span class="p_chunk">@@ -1,6 +1,7 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_FUTEX_H
 #define _LINUX_FUTEX_H
 
<span class="p_add">+#include &lt;linux/futex_types.h&gt;</span>
 #include &lt;uapi/linux/futex.h&gt;
 
 struct inode;
<span class="p_chunk">@@ -21,16 +22,19 @@</span> <span class="p_context"> handle_futex_death(u32 __user *uaddr, st</span>
  *
  * offset is aligned to a multiple of sizeof(u32) (== 4) by definition.
  * We use the two low order bits of offset to tell what is the kind of key :
<span class="p_del">- *  00 : Private process futex (PTHREAD_PROCESS_PRIVATE)</span>
<span class="p_del">- *       (no reference on an inode or mm)</span>
<span class="p_add">+ *  00 : Private process futex (PTHREAD_PROCESS_PRIVATE) using process private</span>
<span class="p_add">+ *	 hash (no reference on an inode or mm)</span>
  *  01 : Shared futex (PTHREAD_PROCESS_SHARED)
  *	mapped on a file (reference on the underlying inode)
  *  10 : Shared futex (PTHREAD_PROCESS_SHARED)
  *       (but private mapping on an mm, and reference taken on it)
<span class="p_add">+ *  11 : Private process futex (PTHREAD_PROCESS_PRIVATE) using global hash</span>
<span class="p_add">+ *	 (no reference on an inode or mm)</span>
 */
 
<span class="p_del">-#define FUT_OFF_INODE    1 /* We set bit 0 if key has a reference on inode */</span>
<span class="p_del">-#define FUT_OFF_MMSHARED 2 /* We set bit 1 if key has a reference on mm */</span>
<span class="p_add">+#define FUT_OFF_INODE		0x01 /* Key has a reference on inode */</span>
<span class="p_add">+#define FUT_OFF_MMSHARED	0x02 /* Key has a reference on mm */</span>
<span class="p_add">+#define FUT_OFF_PRIVATE		0x03 /* Key has no ref on inode/mm */</span>
 
 union futex_key {
 	struct {
<span class="p_chunk">@@ -60,12 +64,30 @@</span> <span class="p_context"> extern void exit_pi_state_list(struct ta</span>
 #else
 extern int futex_cmpxchg_enabled;
 #endif
<span class="p_add">+</span>
 #else
<span class="p_del">-static inline void exit_robust_list(struct task_struct *curr)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-static inline void exit_pi_state_list(struct task_struct *curr)</span>
<span class="p_add">+static inline void exit_robust_list(struct task_struct *curr) { }</span>
<span class="p_add">+static inline void exit_pi_state_list(struct task_struct *curr) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="p_add">+/* Process private hash data for futexes */</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned int futex_default_hash_bits;</span>
<span class="p_add">+extern unsigned int futex_max_hash_bits;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void futex_mm_hash_exit(struct mm_struct *mm);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void futex_mm_hash_init(struct mm_struct *mm)</span>
 {
<span class="p_add">+	raw_spin_lock_init(&amp;mm-&gt;futex_hash.lock);</span>
<span class="p_add">+	mm-&gt;futex_hash.hash = NULL;</span>
 }
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void futex_mm_hash_init(struct mm_struct *mm) { }</span>
<span class="p_add">+static inline void futex_mm_hash_exit(struct mm_struct *mm) { }</span>
 #endif
<span class="p_add">+</span>
 #endif
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/futex_types.h</span>
<span class="p_chunk">@@ -0,0 +1,12 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _LINUX_FUTEX_TYPES_H</span>
<span class="p_add">+#define _LINUX_FUTEX_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+struct futex_hash_bucket;</span>
<span class="p_add">+</span>
<span class="p_add">+struct futex_hash {</span>
<span class="p_add">+	struct raw_spinlock		lock;</span>
<span class="p_add">+	unsigned int			hash_bits;</span>
<span class="p_add">+	struct futex_hash_bucket	*hash;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/completion.h&gt;
 #include &lt;linux/cpumask.h&gt;
 #include &lt;linux/uprobes.h&gt;
<span class="p_add">+#include &lt;linux/futex_types.h&gt;</span>
 #include &lt;linux/page-flags-layout.h&gt;
 #include &lt;asm/page.h&gt;
 #include &lt;asm/mmu.h&gt;
<span class="p_chunk">@@ -442,6 +443,9 @@</span> <span class="p_context"> struct mm_struct {</span>
 
 	struct linux_binfmt *binfmt;
 
<span class="p_add">+#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="p_add">+	struct futex_hash futex_hash;</span>
<span class="p_add">+#endif</span>
 	cpumask_var_t cpu_vm_mask_var;
 
 	/* Architecture-specific MM context */
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -1498,6 +1498,10 @@</span> <span class="p_context"> config FUTEX</span>
 	  support for &quot;fast userspace mutexes&quot;.  The resulting kernel may not
 	  run glibc-based applications correctly.
 
<span class="p_add">+config FUTEX_PRIVATE_HASH</span>
<span class="p_add">+	bool</span>
<span class="p_add">+	default FUTEX &amp;&amp; SMP</span>
<span class="p_add">+</span>
 config HAVE_FUTEX_CMPXCHG
 	bool
 	depends on FUTEX
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -617,6 +617,8 @@</span> <span class="p_context"> static struct mm_struct *mm_init(struct</span>
 	mm_init_owner(mm, p);
 	mmu_notifier_mm_init(mm);
 	clear_tlb_flush_pending(mm);
<span class="p_add">+	futex_mm_hash_init(mm);</span>
<span class="p_add">+</span>
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS
 	mm-&gt;pmd_huge_pte = NULL;
 #endif
<span class="p_chunk">@@ -713,6 +715,7 @@</span> <span class="p_context"> void mmput(struct mm_struct *mm)</span>
 		khugepaged_exit(mm); /* must run before exit_mmap */
 		exit_mmap(mm);
 		set_mm_exe_file(mm, NULL);
<span class="p_add">+		futex_mm_hash_exit(mm);</span>
 		if (!list_empty(&amp;mm-&gt;mmlist)) {
 			spin_lock(&amp;mmlist_lock);
 			list_del(&amp;mm-&gt;mmlist);
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -23,6 +23,9 @@</span> <span class="p_context"></span>
  *  Copyright (C) IBM Corporation, 2009
  *  Thanks to Thomas Gleixner for conceptual design and careful reviews.
  *
<span class="p_add">+ *  Private hashed futex support by Sebastian Siewior and Thomas Gleixner</span>
<span class="p_add">+ *  Copyright (C) Linutronix GmbH, 2016</span>
<span class="p_add">+ *</span>
  *  Thanks to Ben LaHaise for yelling &quot;hashed waitqueues&quot; loudly
  *  enough at me, Linus for the original (flawed) idea, Matthew
  *  Kirkwood for proof-of-concept implementation.
<span class="p_chunk">@@ -49,6 +52,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/file.h&gt;
 #include &lt;linux/jhash.h&gt;
<span class="p_add">+#include &lt;linux/hash.h&gt;</span>
 #include &lt;linux/init.h&gt;
 #include &lt;linux/futex.h&gt;
 #include &lt;linux/mount.h&gt;
<span class="p_chunk">@@ -169,6 +173,34 @@</span> <span class="p_context"></span>
  * the code that actually moves the futex(es) between hash buckets (requeue_futex)
  * will do the additional required waiter count housekeeping. This is done for
  * double_lock_hb() and double_unlock_hb(), respectively.
<span class="p_add">+ *</span>
<span class="p_add">+ * For private futexes we (pre)allocate a per process hash. We check lockless</span>
<span class="p_add">+ * whether the hash is already allocated. To access the hash later we need</span>
<span class="p_add">+ * information about the hash properties as well. This requires barriers as</span>
<span class="p_add">+ * follows:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * CPU 0					CPU 1</span>
<span class="p_add">+ * check_hash_allocation()</span>
<span class="p_add">+ *	if (mm-&gt;futex_hash.hash)</span>
<span class="p_add">+ *		return;</span>
<span class="p_add">+ *	hash = alloc_hash()</span>
<span class="p_add">+ *	lock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="p_add">+ *	if (!mm-&gt;futex_hash.hash) {</span>
<span class="p_add">+ *	  mm-&gt;futex_hash.par = params;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	  smp_wmb(); (A0) &lt;-paired with-|</span>
<span class="p_add">+ *					|</span>
<span class="p_add">+ *	  mm-&gt;futex_hash.hash = hash;	|</span>
<span class="p_add">+ *					|	check_hash_allocation()</span>
<span class="p_add">+ *					|	   if (mm-&gt;futex_hash.hash)</span>
<span class="p_add">+ *					|		return;</span>
<span class="p_add">+ *	  unlock(&amp;mm-&gt;futex_hash.lock);	|	get_futex_key_refs()</span>
<span class="p_add">+ *					|</span>
<span class="p_add">+ *					|--------- smp_mb() (B)</span>
<span class="p_add">+ *						s = hash(f, mm-&gt;futex_hash.par);</span>
<span class="p_add">+ *						hb = &amp;mm-&gt;futex_hash.hash[s];</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * So we utilize the existing smp_mb() in get_futex_key_refs().</span>
  */
 
 #ifndef CONFIG_HAVE_FUTEX_CMPXCHG
<span class="p_chunk">@@ -255,6 +287,22 @@</span> <span class="p_context"> struct futex_hash_bucket {</span>
 	struct plist_head chain;
 } ____cacheline_aligned_in_smp;
 
<span class="p_add">+#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Process private hash for non-shared futexes</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define FUTEX_USE_GLOBAL_HASH		((void *) 0x03)</span>
<span class="p_add">+</span>
<span class="p_add">+#define FUTEX_MIN_HASH_BITS		order_base_2(4UL)</span>
<span class="p_add">+#define FUTEX_DEF_HASH_BITS		order_base_2(8UL)</span>
<span class="p_add">+#define FUTEX_MAX_HASH_BITS		order_base_2(256UL)</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned int futex_default_hash_bits	= FUTEX_DEF_HASH_BITS;</span>
<span class="p_add">+unsigned int futex_max_hash_bits	= FUTEX_MAX_HASH_BITS;</span>
<span class="p_add">+#else</span>
<span class="p_add">+static const unsigned int futex_default_hash_bits = 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * The base of the bucket array and its size are always used together
  * (after initialization only in hash_futex()), so ensure that they
<span class="p_chunk">@@ -374,13 +422,13 @@</span> <span class="p_context"> static inline int hb_waiters_pending(str</span>
 }
 
 /**
<span class="p_del">- * hash_futex - Return the hash bucket in the global hash</span>
<span class="p_add">+ * hash_global_futex - Return the hash bucket in the global hash</span>
  * @key:	Pointer to the futex key for which the hash is calculated
  *
  * We hash on the keys returned from get_futex_key (see below) and return the
  * corresponding hash bucket in the global hash.
  */
<span class="p_del">-static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="p_add">+static struct futex_hash_bucket *hash_global_futex(union futex_key *key)</span>
 {
 	u32 hash = jhash2((u32*)&amp;key-&gt;both.word,
 			  (sizeof(key-&gt;both.word)+sizeof(key-&gt;both.ptr))/4,
<span class="p_chunk">@@ -388,9 +436,33 @@</span> <span class="p_context"> static struct futex_hash_bucket *hash_fu</span>
 	return &amp;futex_queues[hash &amp; (futex_hashsize - 1)];
 }
 
<span class="p_add">+/**</span>
<span class="p_add">+ * hash_futex - Get the hash bucket for a futex</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns either the process private or the global hash bucket which fits the</span>
<span class="p_add">+ * key.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	unsigned int slot;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Futexes which use the per process hash have the lower bits cleared</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (key-&gt;both.offset &amp; (FUT_OFF_INODE | FUT_OFF_MMSHARED))</span>
<span class="p_add">+		return hash_global_futex(key);</span>
<span class="p_add">+</span>
<span class="p_add">+	slot = hash_long(key-&gt;private.address, mm-&gt;futex_hash.hash_bits);</span>
<span class="p_add">+	return &amp;mm-&gt;futex_hash.hash[slot];</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return hash_global_futex(key);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
 
 /**
<span class="p_del">- * match_futex - Check whether to futex keys are equal</span>
<span class="p_add">+ * match_futex - Check whether two futex keys are equal</span>
  * @key1:	Pointer to key1
  * @key2:	Pointer to key2
  *
<span class="p_chunk">@@ -505,7 +577,20 @@</span> <span class="p_context"> get_futex_key(u32 __user *uaddr, int fsh</span>
 	 */
 	if (!fshared) {
 		key-&gt;private.mm = mm;
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If we have a process private hash, then we store uaddr</span>
<span class="p_add">+		 * instead of the page base address.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="p_add">+		if (mm-&gt;futex_hash.hash != FUTEX_USE_GLOBAL_HASH) {</span>
<span class="p_add">+			key-&gt;private.address = (unsigned long) uaddr;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			key-&gt;private.address = address;</span>
<span class="p_add">+			key-&gt;both.offset |= FUT_OFF_PRIVATE;</span>
<span class="p_add">+		}</span>
<span class="p_add">+#else</span>
 		key-&gt;private.address = address;
<span class="p_add">+#endif</span>
 		get_futex_key_refs(key);  /* implies smp_mb(); (B) */
 		return 0;
 	}
<span class="p_chunk">@@ -3153,6 +3238,75 @@</span> <span class="p_context"> void exit_robust_list(struct task_struct</span>
 				   curr, pip);
 }
 
<span class="p_add">+#ifdef CONFIG_FUTEX_PRIVATE_HASH</span>
<span class="p_add">+</span>
<span class="p_add">+void futex_mm_hash_exit(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (mm-&gt;futex_hash.hash &amp;&amp; mm-&gt;futex_hash.hash != FUTEX_USE_GLOBAL_HASH)</span>
<span class="p_add">+		kfree(mm-&gt;futex_hash.hash);</span>
<span class="p_add">+	mm-&gt;futex_hash.hash = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct futex_hash_bucket *futex_alloc_hash(unsigned int hash_bits)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct futex_hash_bucket *hb;</span>
<span class="p_add">+	size_t hash_size, size;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	hash_size = 1 &lt;&lt; hash_bits;</span>
<span class="p_add">+	size = hash_size * sizeof(struct futex_hash_bucket);</span>
<span class="p_add">+	hb = kzalloc_node(size, GFP_KERNEL, numa_node_id());</span>
<span class="p_add">+	if (!hb)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; hash_size; i++) {</span>
<span class="p_add">+		atomic_set(&amp;hb[i].waiters, 0);</span>
<span class="p_add">+		plist_head_init(&amp;hb[i].chain);</span>
<span class="p_add">+		spin_lock_init(&amp;hb[i].lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return hb;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void futex_populate_hash(unsigned int hash_bits)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	struct futex_hash_bucket *hb = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We don&#39;t need an explicit smp_mb() when the hash is populated</span>
<span class="p_add">+	 * because before we dereference mm-&gt;futex_hash.hash_bits in the hash</span>
<span class="p_add">+	 * function we have an smp_mb() in futex_get_key_refs() already.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mm-&gt;futex_hash.hash)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we failed to allocate a hash on the fly, fall back to the global</span>
<span class="p_add">+	 * hash.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	hb = futex_alloc_hash(hash_bits);</span>
<span class="p_add">+	if (!hb)</span>
<span class="p_add">+		hb = FUTEX_USE_GLOBAL_HASH;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="p_add">+	/* We might have raced with another task allocating the hash. */</span>
<span class="p_add">+	if (!mm-&gt;futex_hash.hash) {</span>
<span class="p_add">+		mm-&gt;futex_hash.hash_bits = hash_bits;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Ensure that the above is visible before we store</span>
<span class="p_add">+		 * the pointer.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		smp_wmb(); /* (A0) Pairs with (B) */</span>
<span class="p_add">+		mm-&gt;futex_hash.hash = hb;</span>
<span class="p_add">+		hb = NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	raw_spin_unlock(&amp;mm-&gt;futex_hash.lock);</span>
<span class="p_add">+	kfree(hb);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else /* CONFIG_FUTEX_PRIVATE_HASH */</span>
<span class="p_add">+static inline void futex_populate_hash(unsigned int hash_bits) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 long do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,
 		u32 __user *uaddr2, u32 val2, u32 val3)
 {
<span class="p_chunk">@@ -3161,6 +3315,8 @@</span> <span class="p_context"> long do_futex(u32 __user *uaddr, int op,</span>
 
 	if (!(op &amp; FUTEX_PRIVATE_FLAG))
 		flags |= FLAGS_SHARED;
<span class="p_add">+	else</span>
<span class="p_add">+		futex_populate_hash(futex_default_hash_bits);</span>
 
 	if (op &amp; FUTEX_CLOCK_REALTIME) {
 		flags |= FLAGS_CLOCKRT;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



