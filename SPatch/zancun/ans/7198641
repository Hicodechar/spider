
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[7/7] arm64: Mark kernel page ranges contiguous - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [7/7] arm64: Mark kernel page ranges contiguous</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=139841">Jeremy Linton</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 16, 2015, 7:03 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1442430186-9083-8-git-send-email-jeremy.linton@arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7198641/mbox/"
   >mbox</a>
|
   <a href="/patch/7198641/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7198641/">/patch/7198641/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 9D5A69F336
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 16 Sep 2015 19:05:41 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id A5F64207C2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 16 Sep 2015 19:05:40 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A28DF20865
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 16 Sep 2015 19:05:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753474AbbIPTFf (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 16 Sep 2015 15:05:35 -0400
Received: from eu-smtp-delivery-143.mimecast.com ([146.101.78.143]:52472
	&quot;EHLO eu-smtp-delivery-143.mimecast.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752827AbbIPTDu (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 16 Sep 2015 15:03:50 -0400
Received: from cam-owa1.Emea.Arm.com (fw-tnat.cambridge.arm.com
	[217.140.96.140]) by eu-smtp-1.mimecast.com with ESMTP id
	uk-mta-5-6bdPlWWRT0aMmRW2vgUHjQ-1; Wed, 16 Sep 2015 20:03:48 +0100
Received: from mammon-v1.localdomain.com ([10.1.2.79]) by
	cam-owa1.Emea.Arm.com with Microsoft SMTPSVC(6.0.3790.3959); 
	Wed, 16 Sep 2015 20:03:47 +0100
From: Jeremy Linton &lt;jeremy.linton@arm.com&gt;
To: linux-arm-kernel@lists.infradead.org
Cc: catalin.marinas@arm.com, will.deacon@arm.com,
	linux-kernel@vger.kernel.org, dwoods@ezcip.com,
	steve.capper@arm.com, shijie.huang@arm.com,
	Jeremy Linton &lt;jeremy.linton@arm.com&gt;
Subject: [PATCH 7/7] arm64: Mark kernel page ranges contiguous
Date: Wed, 16 Sep 2015 14:03:06 -0500
Message-Id: &lt;1442430186-9083-8-git-send-email-jeremy.linton@arm.com&gt;
X-Mailer: git-send-email 2.4.3
In-Reply-To: &lt;1442430186-9083-1-git-send-email-jeremy.linton@arm.com&gt;
References: &lt;1442430186-9083-1-git-send-email-jeremy.linton@arm.com&gt;
X-OriginalArrivalTime: 16 Sep 2015 19:03:48.0072 (UTC)
	FILETIME=[6AD1BE80:01D0F0B2]
X-MC-Unique: 6bdPlWWRT0aMmRW2vgUHjQ-1
Content-Type: text/plain; charset=WINDOWS-1252
Content-Transfer-Encoding: quoted-printable
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=139841">Jeremy Linton</a> - Sept. 16, 2015, 7:03 p.m.</div>
<pre class="content">
With 64k pages, the next larger segment size is 512M. The linux
kernel also uses different protection flags to cover its code and data.
Because of this requirement, the vast majority of the kernel code and
data structures end up being mapped with 64k pages instead of the larger
pages common with a 4k page kernel.

Recent ARM processors support a contiguous bit in the
page tables which allows the a TLB to cover a range larger than a
single PTE if that range is mapped into physically contiguous
ram.

So, for the kernel its a good idea to set this flag. Some basic
micro benchmarks show it can significantly reduce the number of
L1 dTLB refills.
<span class="signed-off-by">
Signed-off-by: Jeremy Linton &lt;jeremy.linton@arm.com&gt;</span>
---
 arch/arm64/mm/mmu.c | 70 +++++++++++++++++++++++++++++++++++++++++++++++------
 1 file changed, 62 insertions(+), 8 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64911">Steve Capper</a> - Sept. 17, 2015, 5:23 p.m.</div>
<pre class="content">
Hi Jeremy,
One quick comment for now below.
I ran into a problem testing this on my Seattle board, and needed the fix below.

Cheers,
--
Steve

On 16 September 2015 at 20:03, Jeremy Linton &lt;jeremy.linton@arm.com&gt; wrote:
<span class="quote">&gt; With 64k pages, the next larger segment size is 512M. The linux</span>
<span class="quote">&gt; kernel also uses different protection flags to cover its code and data.</span>
<span class="quote">&gt; Because of this requirement, the vast majority of the kernel code and</span>
<span class="quote">&gt; data structures end up being mapped with 64k pages instead of the larger</span>
<span class="quote">&gt; pages common with a 4k page kernel.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Recent ARM processors support a contiguous bit in the</span>
<span class="quote">&gt; page tables which allows the a TLB to cover a range larger than a</span>
<span class="quote">&gt; single PTE if that range is mapped into physically contiguous</span>
<span class="quote">&gt; ram.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So, for the kernel its a good idea to set this flag. Some basic</span>
<span class="quote">&gt; micro benchmarks show it can significantly reduce the number of</span>
<span class="quote">&gt; L1 dTLB refills.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Jeremy Linton &lt;jeremy.linton@arm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/mm/mmu.c | 70 +++++++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  1 file changed, 62 insertions(+), 8 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; index 9211b85..c7abbcc 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; @@ -80,19 +80,55 @@ static void split_pmd(pmd_t *pmd, pte_t *pte)</span>
<span class="quote">&gt;         do {</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * Need to have the least restrictive permissions available</span>
<span class="quote">&gt; -                * permissions will be fixed up later</span>
<span class="quote">&gt; +                * permissions will be fixed up later. Default the new page</span>
<span class="quote">&gt; +                * range as contiguous ptes.</span>
<span class="quote">&gt;                  */</span>
<span class="quote">&gt; -               set_pte(pte, pfn_pte(pfn, PAGE_KERNEL_EXEC));</span>
<span class="quote">&gt; +               set_pte(pte, pfn_pte(pfn, PAGE_KERNEL_EXEC_CONT));</span>
<span class="quote">&gt;                 pfn++;</span>
<span class="quote">&gt;         } while (pte++, i++, i &lt; PTRS_PER_PTE);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Given a PTE with the CONT bit set, determine where the CONT range</span>
<span class="quote">&gt; + * starts, and clear the entire range of PTE CONT bits.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static void clear_cont_pte_range(pte_t *pte, unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pte -= CONT_RANGE_OFFSET(addr);</span>
<span class="quote">&gt; +       for (i = 0; i &lt; CONT_RANGE; i++) {</span>
<span class="quote">&gt; +               set_pte(pte, pte_mknoncont(*pte));</span>
<span class="quote">&gt; +               pte++;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       flush_tlb_all();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Given a range of PTEs set the pfn and provided page protection flags</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static void __populate_init_pte(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt; +                               unsigned long end, phys_addr_t phys,</span>
<span class="quote">&gt; +                               pgprot_t prot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       unsigned long pfn = __phys_to_pfn(phys);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       do {</span>
<span class="quote">&gt; +               /* clear all the bits except the pfn, then apply the prot */</span>
<span class="quote">&gt; +               set_pte(pte, pfn_pte(pfn, prot));</span>
<span class="quote">&gt; +               pte++;</span>
<span class="quote">&gt; +               pfn++;</span>
<span class="quote">&gt; +               addr += PAGE_SIZE;</span>
<span class="quote">&gt; +       } while (addr != end);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static void alloc_init_pte(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; -                                 unsigned long end, unsigned long pfn,</span>
<span class="quote">&gt; +                                 unsigned long end, phys_addr_t phys,</span>
<span class="quote">&gt;                                   pgprot_t prot,</span>
<span class="quote">&gt;                                   void *(*alloc)(unsigned long size))</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         pte_t *pte;</span>
<span class="quote">&gt; +       unsigned long next;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (pmd_none(*pmd) || pmd_sect(*pmd)) {</span>
<span class="quote">&gt;                 pte = alloc(PTRS_PER_PTE * sizeof(pte_t));</span>
<span class="quote">&gt; @@ -105,9 +141,28 @@ static void alloc_init_pte(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         pte = pte_offset_kernel(pmd, addr);</span>
<span class="quote">&gt;         do {</span>
<span class="quote">&gt; -               set_pte(pte, pfn_pte(pfn, prot));</span>
<span class="quote">&gt; -               pfn++;</span>
<span class="quote">&gt; -       } while (pte++, addr += PAGE_SIZE, addr != end);</span>
<span class="quote">&gt; +               next = min(end, (addr + CONT_SIZE) &amp; CONT_MASK);</span>
<span class="quote">&gt; +               if (((addr | next | phys) &amp; CONT_RANGE_MASK) == 0) {</span>
<span class="quote">&gt; +                       /* a block of CONT_RANGE_SIZE PTEs */</span>
<span class="quote">&gt; +                       __populate_init_pte(pte, addr, next, phys,</span>
<span class="quote">&gt; +                                           prot | __pgprot(PTE_CONT));</span>
<span class="quote">&gt; +                       pte += CONT_RANGE;</span>
<span class="quote">&gt; +               } else {</span>
<span class="quote">&gt; +                       /*</span>
<span class="quote">&gt; +                        * If the range being split is already inside of a</span>
<span class="quote">&gt; +                        * contiguous range but this PTE isn&#39;t going to be</span>
<span class="quote">&gt; +                        * contiguous, then we want to unmark the adjacent</span>
<span class="quote">&gt; +                        * ranges, then update the portion of the range we</span>
<span class="quote">&gt; +                        * are interrested in.</span>
<span class="quote">&gt; +                        */</span>
<span class="quote">&gt; +                        clear_cont_pte_range(pte, addr);</span>
<span class="quote">&gt; +                        __populate_init_pte(pte, addr, next, phys, prot);</span>
<span class="quote">&gt; +                        pte += CONT_RANGE_OFFSET(next - addr);</span>

I think this should instead be:
pte += (next - addr) &gt;&gt; PAGE_SHIFT;

Without the above change, I get panics on boot with my Seattle board
when efi_rtc is initialised.
(I think the EFI runtime stuff exacerbates the non-contiguous code
path hence I notice it on my system).
<span class="quote">
&gt; +               }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               phys += next - addr;</span>
<span class="quote">&gt; +               addr = next;</span>
<span class="quote">&gt; +       } while (addr != end);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  void split_pud(pud_t *old_pud, pmd_t *pmd)</span>
<span class="quote">&gt; @@ -168,8 +223,7 @@ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,</span>
<span class="quote">&gt;                                 }</span>
<span class="quote">&gt;                         }</span>
<span class="quote">&gt;                 } else {</span>
<span class="quote">&gt; -                       alloc_init_pte(pmd, addr, next, __phys_to_pfn(phys),</span>
<span class="quote">&gt; -                                      prot, alloc);</span>
<span class="quote">&gt; +                       alloc_init_pte(pmd, addr, next, phys, prot, alloc);</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;                 phys += next - addr;</span>
<span class="quote">&gt;         } while (pmd++, addr = next, addr != end);</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 2.4.3</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; linux-arm-kernel mailing list</span>
<span class="quote">&gt; linux-arm-kernel@lists.infradead.org</span>
<span class="quote">&gt; http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=139841">Jeremy Linton</a> - Sept. 17, 2015, 5:33 p.m.</div>
<pre class="content">
|Hi Jeremy,
|One quick comment for now below.
|I ran into a problem testing this on my Seattle board, and needed the fix
|below.
&lt;snip&gt;
|&gt; -       } while (pte++, addr += PAGE_SIZE, addr != end);
|&gt; +               next = min(end, (addr + CONT_SIZE) &amp; CONT_MASK);
|&gt; +               if (((addr | next | phys) &amp; CONT_RANGE_MASK) == 0) {
|&gt; +                       /* a block of CONT_RANGE_SIZE PTEs */
|&gt; +                       __populate_init_pte(pte, addr, next, phys,
|&gt; +                                           prot | __pgprot(PTE_CONT));
|&gt; +                       pte += CONT_RANGE;
|&gt; +               } else {
|&gt; +                       /*
|&gt; +                        * If the range being split is already inside of a
|&gt; +                        * contiguous range but this PTE isn&#39;t going to be
|&gt; +                        * contiguous, then we want to unmark the adjacent
|&gt; +                        * ranges, then update the portion of the range we
|&gt; +                        * are interrested in.
|&gt; +                        */
|&gt; +                        clear_cont_pte_range(pte, addr);
|&gt; +                        __populate_init_pte(pte, addr, next, phys, prot);
|&gt; +                        pte += CONT_RANGE_OFFSET(next - addr);
|
|I think this should instead be:
|pte += (next - addr) &gt;&gt; PAGE_SHIFT;
|
|Without the above change, I get panics on boot with my Seattle board when
|efi_rtc is initialised.
|(I think the EFI runtime stuff exacerbates the non-contiguous code path
|hence I notice it on my system).

I think that implies you have linear mappings &gt;= 2M that arenâ€™t aligned. Ok, but that almost sounds like something we want to complain about if we detect it.






-- IMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium.  Thank you.

ARM Limited, Registered office 110 Fulbourn Road, Cambridge CB1 9NJ, Registered in England &amp; Wales, Company No:  2557590
ARM Holdings plc, Registered office 110 Fulbourn Road, Cambridge CB1 9NJ, Registered in England &amp; Wales, Company No:  2548782
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="p_header">index 9211b85..c7abbcc 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mmu.c</span>
<span class="p_header">+++ b/arch/arm64/mm/mmu.c</span>
<span class="p_chunk">@@ -80,19 +80,55 @@</span> <span class="p_context"> static void split_pmd(pmd_t *pmd, pte_t *pte)</span>
 	do {
 		/*
 		 * Need to have the least restrictive permissions available
<span class="p_del">-		 * permissions will be fixed up later</span>
<span class="p_add">+		 * permissions will be fixed up later. Default the new page</span>
<span class="p_add">+		 * range as contiguous ptes.</span>
 		 */
<span class="p_del">-		set_pte(pte, pfn_pte(pfn, PAGE_KERNEL_EXEC));</span>
<span class="p_add">+		set_pte(pte, pfn_pte(pfn, PAGE_KERNEL_EXEC_CONT));</span>
 		pfn++;
 	} while (pte++, i++, i &lt; PTRS_PER_PTE);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Given a PTE with the CONT bit set, determine where the CONT range</span>
<span class="p_add">+ * starts, and clear the entire range of PTE CONT bits.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void clear_cont_pte_range(pte_t *pte, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte -= CONT_RANGE_OFFSET(addr);</span>
<span class="p_add">+	for (i = 0; i &lt; CONT_RANGE; i++) {</span>
<span class="p_add">+		set_pte(pte, pte_mknoncont(*pte));</span>
<span class="p_add">+		pte++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Given a range of PTEs set the pfn and provided page protection flags</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __populate_init_pte(pte_t *pte, unsigned long addr,</span>
<span class="p_add">+				unsigned long end, phys_addr_t phys,</span>
<span class="p_add">+				pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = __phys_to_pfn(phys);</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		/* clear all the bits except the pfn, then apply the prot */</span>
<span class="p_add">+		set_pte(pte, pfn_pte(pfn, prot));</span>
<span class="p_add">+		pte++;</span>
<span class="p_add">+		pfn++;</span>
<span class="p_add">+		addr += PAGE_SIZE;</span>
<span class="p_add">+	} while (addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void alloc_init_pte(pmd_t *pmd, unsigned long addr,
<span class="p_del">-				  unsigned long end, unsigned long pfn,</span>
<span class="p_add">+				  unsigned long end, phys_addr_t phys,</span>
 				  pgprot_t prot,
 				  void *(*alloc)(unsigned long size))
 {
 	pte_t *pte;
<span class="p_add">+	unsigned long next;</span>
 
 	if (pmd_none(*pmd) || pmd_sect(*pmd)) {
 		pte = alloc(PTRS_PER_PTE * sizeof(pte_t));
<span class="p_chunk">@@ -105,9 +141,28 @@</span> <span class="p_context"> static void alloc_init_pte(pmd_t *pmd, unsigned long addr,</span>
 
 	pte = pte_offset_kernel(pmd, addr);
 	do {
<span class="p_del">-		set_pte(pte, pfn_pte(pfn, prot));</span>
<span class="p_del">-		pfn++;</span>
<span class="p_del">-	} while (pte++, addr += PAGE_SIZE, addr != end);</span>
<span class="p_add">+		next = min(end, (addr + CONT_SIZE) &amp; CONT_MASK);</span>
<span class="p_add">+		if (((addr | next | phys) &amp; CONT_RANGE_MASK) == 0) {</span>
<span class="p_add">+			/* a block of CONT_RANGE_SIZE PTEs */</span>
<span class="p_add">+			__populate_init_pte(pte, addr, next, phys,</span>
<span class="p_add">+					    prot | __pgprot(PTE_CONT));</span>
<span class="p_add">+			pte += CONT_RANGE;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If the range being split is already inside of a</span>
<span class="p_add">+			 * contiguous range but this PTE isn&#39;t going to be</span>
<span class="p_add">+			 * contiguous, then we want to unmark the adjacent</span>
<span class="p_add">+			 * ranges, then update the portion of the range we</span>
<span class="p_add">+			 * are interrested in.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			 clear_cont_pte_range(pte, addr);</span>
<span class="p_add">+			 __populate_init_pte(pte, addr, next, phys, prot);</span>
<span class="p_add">+			 pte += CONT_RANGE_OFFSET(next - addr);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		phys += next - addr;</span>
<span class="p_add">+		addr = next;</span>
<span class="p_add">+	} while (addr != end);</span>
 }
 
 void split_pud(pud_t *old_pud, pmd_t *pmd)
<span class="p_chunk">@@ -168,8 +223,7 @@</span> <span class="p_context"> static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,</span>
 				}
 			}
 		} else {
<span class="p_del">-			alloc_init_pte(pmd, addr, next, __phys_to_pfn(phys),</span>
<span class="p_del">-				       prot, alloc);</span>
<span class="p_add">+			alloc_init_pte(pmd, addr, next, phys, prot, alloc);</span>
 		}
 		phys += next - addr;
 	} while (pmd++, addr = next, addr != end);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



