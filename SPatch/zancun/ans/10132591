
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 4.14.9 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 4.14.9</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 25, 2017, 2:12 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171225141241.GB24547@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10132591/mbox/"
   >mbox</a>
|
   <a href="/patch/10132591/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10132591/">/patch/10132591/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6CF2C602BC for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Dec 2017 14:13:43 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F1DF72EFBC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Dec 2017 14:13:42 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E31B02F0CF; Mon, 25 Dec 2017 14:13:42 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B461B2EFBC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Dec 2017 14:13:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753231AbdLYONZ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 25 Dec 2017 09:13:25 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:58918 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752806AbdLYOMm (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 25 Dec 2017 09:12:42 -0500
Received: from localhost (LFbn-1-12262-44.w90-92.abo.wanadoo.fr
	[90.92.75.44])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 6C52D486;
	Mon, 25 Dec 2017 14:12:38 +0000 (UTC)
Date: Mon, 25 Dec 2017 15:12:41 +0100
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 4.14.9
Message-ID: &lt;20171225141241.GB24547@kroah.com&gt;
References: &lt;20171225141235.GA24547@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: &lt;20171225141235.GA24547@kroah.com&gt;
User-Agent: Mutt/1.9.2 (2017-12-15)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Dec. 25, 2017, 2:12 p.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/orc-unwinder.txt b/Documentation/x86/orc-unwinder.txt</span>
<span class="p_header">index af0c9a4c65a6..cd4b29be29af 100644</span>
<span class="p_header">--- a/Documentation/x86/orc-unwinder.txt</span>
<span class="p_header">+++ b/Documentation/x86/orc-unwinder.txt</span>
<span class="p_chunk">@@ -4,7 +4,7 @@</span> <span class="p_context"> ORC unwinder</span>
 Overview
 --------
 
<span class="p_del">-The kernel CONFIG_ORC_UNWINDER option enables the ORC unwinder, which is</span>
<span class="p_add">+The kernel CONFIG_UNWINDER_ORC option enables the ORC unwinder, which is</span>
 similar in concept to a DWARF unwinder.  The difference is that the
 format of the ORC data is much simpler than DWARF, which in turn allows
 the ORC unwinder to be much simpler and faster.
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index b0798e281aa6..3448e675b462 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -34,7 +34,7 @@</span> <span class="p_context"> ff92000000000000 - ffd1ffffffffffff (=54 bits) vmalloc/ioremap space</span>
 ffd2000000000000 - ffd3ffffffffffff (=49 bits) hole
 ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)
 ... unused hole ...
<span class="p_del">-ffd8000000000000 - fff7ffffffffffff (=53 bits) kasan shadow memory (8PB)</span>
<span class="p_add">+ffdf000000000000 - fffffc0000000000 (=53 bits) kasan shadow memory (8PB)</span>
 ... unused hole ...
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 97b5ae76ac8c..ed2132c6d286 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
 VERSION = 4
 PATCHLEVEL = 14
<span class="p_del">-SUBLEVEL = 8</span>
<span class="p_add">+SUBLEVEL = 9</span>
 EXTRAVERSION =
 NAME = Petit Gorille
 
<span class="p_chunk">@@ -935,8 +935,8 @@</span> <span class="p_context"> ifdef CONFIG_STACK_VALIDATION</span>
   ifeq ($(has_libelf),1)
     objtool_target := tools/objtool FORCE
   else
<span class="p_del">-    ifdef CONFIG_ORC_UNWINDER</span>
<span class="p_del">-      $(error &quot;Cannot generate ORC metadata for CONFIG_ORC_UNWINDER=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel&quot;)</span>
<span class="p_add">+    ifdef CONFIG_UNWINDER_ORC</span>
<span class="p_add">+      $(error &quot;Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel&quot;)</span>
     else
       $(warning &quot;Cannot use CONFIG_STACK_VALIDATION=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel&quot;)
     endif
<span class="p_header">diff --git a/arch/arm/configs/exynos_defconfig b/arch/arm/configs/exynos_defconfig</span>
<span class="p_header">index 8c2a2619971b..f1d7834990ec 100644</span>
<span class="p_header">--- a/arch/arm/configs/exynos_defconfig</span>
<span class="p_header">+++ b/arch/arm/configs/exynos_defconfig</span>
<span class="p_chunk">@@ -244,7 +244,7 @@</span> <span class="p_context"> CONFIG_USB_STORAGE_ONETOUCH=m</span>
 CONFIG_USB_STORAGE_KARMA=m
 CONFIG_USB_STORAGE_CYPRESS_ATACB=m
 CONFIG_USB_STORAGE_ENE_UB6250=m
<span class="p_del">-CONFIG_USB_UAS=m</span>
<span class="p_add">+CONFIG_USB_UAS=y</span>
 CONFIG_USB_DWC3=y
 CONFIG_USB_DWC2=y
 CONFIG_USB_HSIC_USB3503=y
<span class="p_header">diff --git a/arch/arm/include/asm/ptrace.h b/arch/arm/include/asm/ptrace.h</span>
<span class="p_header">index e9c9a117bd25..c7cdbb43ae7c 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/ptrace.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/ptrace.h</span>
<span class="p_chunk">@@ -126,8 +126,7 @@</span> <span class="p_context"> extern unsigned long profile_pc(struct pt_regs *regs);</span>
 /*
  * kprobe-based event tracer support
  */
<span class="p_del">-#include &lt;linux/stddef.h&gt;</span>
<span class="p_del">-#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
 #define MAX_REG_OFFSET (offsetof(struct pt_regs, ARM_ORIG_r0))
 
 extern int regs_query_register_offset(const char *name);
<span class="p_header">diff --git a/arch/arm64/include/asm/fixmap.h b/arch/arm64/include/asm/fixmap.h</span>
<span class="p_header">index caf86be815ba..4052ec39e8db 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -51,6 +51,13 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 
 	FIX_EARLYCON_MEM_BASE,
 	FIX_TEXT_POKE0,
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ACPI_APEI_GHES</span>
<span class="p_add">+	/* Used for GHES mapping from assorted contexts */</span>
<span class="p_add">+	FIX_APEI_GHES_IRQ,</span>
<span class="p_add">+	FIX_APEI_GHES_NMI,</span>
<span class="p_add">+#endif /* CONFIG_ACPI_APEI_GHES */</span>
<span class="p_add">+</span>
 	__end_of_permanent_fixed_addresses,
 
 	/*
<span class="p_header">diff --git a/arch/powerpc/kernel/watchdog.c b/arch/powerpc/kernel/watchdog.c</span>
<span class="p_header">index 57190f384f63..ce848ff84edd 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/watchdog.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/watchdog.c</span>
<span class="p_chunk">@@ -276,9 +276,12 @@</span> <span class="p_context"> void arch_touch_nmi_watchdog(void)</span>
 {
 	unsigned long ticks = tb_ticks_per_usec * wd_timer_period_ms * 1000;
 	int cpu = smp_processor_id();
<span class="p_add">+	u64 tb = get_tb();</span>
 
<span class="p_del">-	if (get_tb() - per_cpu(wd_timer_tb, cpu) &gt;= ticks)</span>
<span class="p_del">-		watchdog_timer_interrupt(cpu);</span>
<span class="p_add">+	if (tb - per_cpu(wd_timer_tb, cpu) &gt;= ticks) {</span>
<span class="p_add">+		per_cpu(wd_timer_tb, cpu) = tb;</span>
<span class="p_add">+		wd_smp_clear_cpu_pending(cpu, tb);</span>
<span class="p_add">+	}</span>
 }
 EXPORT_SYMBOL(arch_touch_nmi_watchdog);
 
<span class="p_header">diff --git a/arch/powerpc/net/bpf_jit_comp64.c b/arch/powerpc/net/bpf_jit_comp64.c</span>
<span class="p_header">index a66e64b0b251..5d115bd32539 100644</span>
<span class="p_header">--- a/arch/powerpc/net/bpf_jit_comp64.c</span>
<span class="p_header">+++ b/arch/powerpc/net/bpf_jit_comp64.c</span>
<span class="p_chunk">@@ -762,7 +762,8 @@</span> <span class="p_context"> static int bpf_jit_build_body(struct bpf_prog *fp, u32 *image,</span>
 			func = (u8 *) __bpf_call_base + imm;
 
 			/* Save skb pointer if we need to re-cache skb data */
<span class="p_del">-			if (bpf_helper_changes_pkt_data(func))</span>
<span class="p_add">+			if ((ctx-&gt;seen &amp; SEEN_SKB) &amp;&amp;</span>
<span class="p_add">+			    bpf_helper_changes_pkt_data(func))</span>
 				PPC_BPF_STL(3, 1, bpf_jit_stack_local(ctx));
 
 			bpf_jit_emit_func_call(image, ctx, (u64)func);
<span class="p_chunk">@@ -771,7 +772,8 @@</span> <span class="p_context"> static int bpf_jit_build_body(struct bpf_prog *fp, u32 *image,</span>
 			PPC_MR(b2p[BPF_REG_0], 3);
 
 			/* refresh skb cache */
<span class="p_del">-			if (bpf_helper_changes_pkt_data(func)) {</span>
<span class="p_add">+			if ((ctx-&gt;seen &amp; SEEN_SKB) &amp;&amp;</span>
<span class="p_add">+			    bpf_helper_changes_pkt_data(func)) {</span>
 				/* reload skb pointer to r3 */
 				PPC_BPF_LL(3, 1, bpf_jit_stack_local(ctx));
 				bpf_jit_emit_skb_loads(image, ctx);
<span class="p_header">diff --git a/arch/powerpc/xmon/xmon.c b/arch/powerpc/xmon/xmon.c</span>
<span class="p_header">index c008083fbc4f..2c8b325591cc 100644</span>
<span class="p_header">--- a/arch/powerpc/xmon/xmon.c</span>
<span class="p_header">+++ b/arch/powerpc/xmon/xmon.c</span>
<span class="p_chunk">@@ -530,14 +530,19 @@</span> <span class="p_context"> static int xmon_core(struct pt_regs *regs, int fromipi)</span>
 
  waiting:
 	secondary = 1;
<span class="p_add">+	spin_begin();</span>
 	while (secondary &amp;&amp; !xmon_gate) {
 		if (in_xmon == 0) {
<span class="p_del">-			if (fromipi)</span>
<span class="p_add">+			if (fromipi) {</span>
<span class="p_add">+				spin_end();</span>
 				goto leave;
<span class="p_add">+			}</span>
 			secondary = test_and_set_bit(0, &amp;in_xmon);
 		}
<span class="p_del">-		barrier();</span>
<span class="p_add">+		spin_cpu_relax();</span>
<span class="p_add">+		touch_nmi_watchdog();</span>
 	}
<span class="p_add">+	spin_end();</span>
 
 	if (!secondary &amp;&amp; !xmon_gate) {
 		/* we are the first cpu to come in */
<span class="p_chunk">@@ -568,21 +573,25 @@</span> <span class="p_context"> static int xmon_core(struct pt_regs *regs, int fromipi)</span>
 		mb();
 		xmon_gate = 1;
 		barrier();
<span class="p_add">+		touch_nmi_watchdog();</span>
 	}
 
  cmdloop:
 	while (in_xmon) {
 		if (secondary) {
<span class="p_add">+			spin_begin();</span>
 			if (cpu == xmon_owner) {
 				if (!test_and_set_bit(0, &amp;xmon_taken)) {
 					secondary = 0;
<span class="p_add">+					spin_end();</span>
 					continue;
 				}
 				/* missed it */
 				while (cpu == xmon_owner)
<span class="p_del">-					barrier();</span>
<span class="p_add">+					spin_cpu_relax();</span>
 			}
<span class="p_del">-			barrier();</span>
<span class="p_add">+			spin_cpu_relax();</span>
<span class="p_add">+			touch_nmi_watchdog();</span>
 		} else {
 			cmd = cmds(regs);
 			if (cmd != 0) {
<span class="p_header">diff --git a/arch/s390/net/bpf_jit_comp.c b/arch/s390/net/bpf_jit_comp.c</span>
<span class="p_header">index b15cd2f0320f..33e2785f6842 100644</span>
<span class="p_header">--- a/arch/s390/net/bpf_jit_comp.c</span>
<span class="p_header">+++ b/arch/s390/net/bpf_jit_comp.c</span>
<span class="p_chunk">@@ -55,8 +55,7 @@</span> <span class="p_context"> struct bpf_jit {</span>
 #define SEEN_LITERAL	8	/* code uses literals */
 #define SEEN_FUNC	16	/* calls C functions */
 #define SEEN_TAIL_CALL	32	/* code uses tail calls */
<span class="p_del">-#define SEEN_SKB_CHANGE	64	/* code changes skb data */</span>
<span class="p_del">-#define SEEN_REG_AX	128	/* code uses constant blinding */</span>
<span class="p_add">+#define SEEN_REG_AX	64	/* code uses constant blinding */</span>
 #define SEEN_STACK	(SEEN_FUNC | SEEN_MEM | SEEN_SKB)
 
 /*
<span class="p_chunk">@@ -448,12 +447,12 @@</span> <span class="p_context"> static void bpf_jit_prologue(struct bpf_jit *jit)</span>
 			EMIT6_DISP_LH(0xe3000000, 0x0024, REG_W1, REG_0,
 				      REG_15, 152);
 	}
<span class="p_del">-	if (jit-&gt;seen &amp; SEEN_SKB)</span>
<span class="p_add">+	if (jit-&gt;seen &amp; SEEN_SKB) {</span>
 		emit_load_skb_data_hlen(jit);
<span class="p_del">-	if (jit-&gt;seen &amp; SEEN_SKB_CHANGE)</span>
 		/* stg %b1,ST_OFF_SKBP(%r0,%r15) */
 		EMIT6_DISP_LH(0xe3000000, 0x0024, BPF_REG_1, REG_0, REG_15,
 			      STK_OFF_SKBP);
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_chunk">@@ -983,8 +982,8 @@</span> <span class="p_context"> static noinline int bpf_jit_insn(struct bpf_jit *jit, struct bpf_prog *fp, int i</span>
 		EMIT2(0x0d00, REG_14, REG_W1);
 		/* lgr %b0,%r2: load return value into %b0 */
 		EMIT4(0xb9040000, BPF_REG_0, REG_2);
<span class="p_del">-		if (bpf_helper_changes_pkt_data((void *)func)) {</span>
<span class="p_del">-			jit-&gt;seen |= SEEN_SKB_CHANGE;</span>
<span class="p_add">+		if ((jit-&gt;seen &amp; SEEN_SKB) &amp;&amp;</span>
<span class="p_add">+		    bpf_helper_changes_pkt_data((void *)func)) {</span>
 			/* lg %b1,ST_OFF_SKBP(%r15) */
 			EMIT6_DISP_LH(0xe3000000, 0x0004, BPF_REG_1, REG_0,
 				      REG_15, STK_OFF_SKBP);
<span class="p_header">diff --git a/arch/sparc/include/asm/ptrace.h b/arch/sparc/include/asm/ptrace.h</span>
<span class="p_header">index 6a339a78f4f4..71dd82b43cc5 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/ptrace.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/ptrace.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #if defined(__sparc__) &amp;&amp; defined(__arch64__)
 #ifndef __ASSEMBLY__
 
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
 #include &lt;linux/threads.h&gt;
 #include &lt;asm/switch_to.h&gt;
 
<span class="p_header">diff --git a/arch/sparc/net/bpf_jit_comp_64.c b/arch/sparc/net/bpf_jit_comp_64.c</span>
<span class="p_header">index 5765e7e711f7..ff5f9cb3039a 100644</span>
<span class="p_header">--- a/arch/sparc/net/bpf_jit_comp_64.c</span>
<span class="p_header">+++ b/arch/sparc/net/bpf_jit_comp_64.c</span>
<span class="p_chunk">@@ -1245,14 +1245,16 @@</span> <span class="p_context"> static int build_insn(const struct bpf_insn *insn, struct jit_ctx *ctx)</span>
 		u8 *func = ((u8 *)__bpf_call_base) + imm;
 
 		ctx-&gt;saw_call = true;
<span class="p_add">+		if (ctx-&gt;saw_ld_abs_ind &amp;&amp; bpf_helper_changes_pkt_data(func))</span>
<span class="p_add">+			emit_reg_move(bpf2sparc[BPF_REG_1], L7, ctx);</span>
 
 		emit_call((u32 *)func, ctx);
 		emit_nop(ctx);
 
 		emit_reg_move(O0, bpf2sparc[BPF_REG_0], ctx);
 
<span class="p_del">-		if (bpf_helper_changes_pkt_data(func) &amp;&amp; ctx-&gt;saw_ld_abs_ind)</span>
<span class="p_del">-			load_skb_regs(ctx, bpf2sparc[BPF_REG_6]);</span>
<span class="p_add">+		if (ctx-&gt;saw_ld_abs_ind &amp;&amp; bpf_helper_changes_pkt_data(func))</span>
<span class="p_add">+			load_skb_regs(ctx, L7);</span>
 		break;
 	}
 
<span class="p_header">diff --git a/arch/um/include/asm/Kbuild b/arch/um/include/asm/Kbuild</span>
<span class="p_header">index 50a32c33d729..73c57f614c9e 100644</span>
<span class="p_header">--- a/arch/um/include/asm/Kbuild</span>
<span class="p_header">+++ b/arch/um/include/asm/Kbuild</span>
<span class="p_chunk">@@ -1,4 +1,5 @@</span> <span class="p_context"></span>
 generic-y += barrier.h
<span class="p_add">+generic-y += bpf_perf_event.h</span>
 generic-y += bug.h
 generic-y += clkdev.h
 generic-y += current.h
<span class="p_header">diff --git a/arch/um/include/shared/init.h b/arch/um/include/shared/init.h</span>
<span class="p_header">index 390572daa40d..b3f5865a92c9 100644</span>
<span class="p_header">--- a/arch/um/include/shared/init.h</span>
<span class="p_header">+++ b/arch/um/include/shared/init.h</span>
<span class="p_chunk">@@ -41,7 +41,7 @@</span> <span class="p_context"></span>
 typedef int (*initcall_t)(void);
 typedef void (*exitcall_t)(void);
 
<span class="p_del">-#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/compiler_types.h&gt;</span>
 
 /* These are for everybody (although not all archs will actually
    discard it in modules) */
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 9bceea6a5852..48646160eb83 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -108,7 +108,7 @@</span> <span class="p_context"> config X86</span>
 	select HAVE_ARCH_AUDITSYSCALL
 	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
 	select HAVE_ARCH_JUMP_LABEL
<span class="p_del">-	select HAVE_ARCH_KASAN			if X86_64 &amp;&amp; SPARSEMEM_VMEMMAP</span>
<span class="p_add">+	select HAVE_ARCH_KASAN			if X86_64</span>
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_KMEMCHECK
 	select HAVE_ARCH_MMAP_RND_BITS		if MMU
<span class="p_chunk">@@ -171,7 +171,7 @@</span> <span class="p_context"> config X86</span>
 	select HAVE_PERF_USER_STACK_DUMP
 	select HAVE_RCU_TABLE_FREE
 	select HAVE_REGS_AND_STACK_ACCESS_API
<span class="p_del">-	select HAVE_RELIABLE_STACKTRACE		if X86_64 &amp;&amp; FRAME_POINTER_UNWINDER &amp;&amp; STACK_VALIDATION</span>
<span class="p_add">+	select HAVE_RELIABLE_STACKTRACE		if X86_64 &amp;&amp; UNWINDER_FRAME_POINTER &amp;&amp; STACK_VALIDATION</span>
 	select HAVE_STACK_VALIDATION		if X86_64
 	select HAVE_SYSCALL_TRACEPOINTS
 	select HAVE_UNSTABLE_SCHED_CLOCK
<span class="p_chunk">@@ -303,7 +303,6 @@</span> <span class="p_context"> config ARCH_SUPPORTS_DEBUG_PAGEALLOC</span>
 config KASAN_SHADOW_OFFSET
 	hex
 	depends on KASAN
<span class="p_del">-	default 0xdff8000000000000 if X86_5LEVEL</span>
 	default 0xdffffc0000000000
 
 config HAVE_INTEL_TXT
<span class="p_header">diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="p_header">index 90b123056f4b..6293a8768a91 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig.debug</span>
<span class="p_header">+++ b/arch/x86/Kconfig.debug</span>
<span class="p_chunk">@@ -359,28 +359,14 @@</span> <span class="p_context"> config PUNIT_ATOM_DEBUG</span>
 
 choice
 	prompt &quot;Choose kernel unwinder&quot;
<span class="p_del">-	default FRAME_POINTER_UNWINDER</span>
<span class="p_add">+	default UNWINDER_ORC if X86_64</span>
<span class="p_add">+	default UNWINDER_FRAME_POINTER if X86_32</span>
 	---help---
 	  This determines which method will be used for unwinding kernel stack
 	  traces for panics, oopses, bugs, warnings, perf, /proc/&lt;pid&gt;/stack,
 	  livepatch, lockdep, and more.
 
<span class="p_del">-config FRAME_POINTER_UNWINDER</span>
<span class="p_del">-	bool &quot;Frame pointer unwinder&quot;</span>
<span class="p_del">-	select FRAME_POINTER</span>
<span class="p_del">-	---help---</span>
<span class="p_del">-	  This option enables the frame pointer unwinder for unwinding kernel</span>
<span class="p_del">-	  stack traces.</span>
<span class="p_del">-</span>
<span class="p_del">-	  The unwinder itself is fast and it uses less RAM than the ORC</span>
<span class="p_del">-	  unwinder, but the kernel text size will grow by ~3% and the kernel&#39;s</span>
<span class="p_del">-	  overall performance will degrade by roughly 5-10%.</span>
<span class="p_del">-</span>
<span class="p_del">-	  This option is recommended if you want to use the livepatch</span>
<span class="p_del">-	  consistency model, as this is currently the only way to get a</span>
<span class="p_del">-	  reliable stack trace (CONFIG_HAVE_RELIABLE_STACKTRACE).</span>
<span class="p_del">-</span>
<span class="p_del">-config ORC_UNWINDER</span>
<span class="p_add">+config UNWINDER_ORC</span>
 	bool &quot;ORC unwinder&quot;
 	depends on X86_64
 	select STACK_VALIDATION
<span class="p_chunk">@@ -396,7 +382,22 @@</span> <span class="p_context"> config ORC_UNWINDER</span>
 	  Enabling this option will increase the kernel&#39;s runtime memory usage
 	  by roughly 2-4MB, depending on your kernel config.
 
<span class="p_del">-config GUESS_UNWINDER</span>
<span class="p_add">+config UNWINDER_FRAME_POINTER</span>
<span class="p_add">+	bool &quot;Frame pointer unwinder&quot;</span>
<span class="p_add">+	select FRAME_POINTER</span>
<span class="p_add">+	---help---</span>
<span class="p_add">+	  This option enables the frame pointer unwinder for unwinding kernel</span>
<span class="p_add">+	  stack traces.</span>
<span class="p_add">+</span>
<span class="p_add">+	  The unwinder itself is fast and it uses less RAM than the ORC</span>
<span class="p_add">+	  unwinder, but the kernel text size will grow by ~3% and the kernel&#39;s</span>
<span class="p_add">+	  overall performance will degrade by roughly 5-10%.</span>
<span class="p_add">+</span>
<span class="p_add">+	  This option is recommended if you want to use the livepatch</span>
<span class="p_add">+	  consistency model, as this is currently the only way to get a</span>
<span class="p_add">+	  reliable stack trace (CONFIG_HAVE_RELIABLE_STACKTRACE).</span>
<span class="p_add">+</span>
<span class="p_add">+config UNWINDER_GUESS</span>
 	bool &quot;Guess unwinder&quot;
 	depends on EXPERT
 	---help---
<span class="p_chunk">@@ -411,7 +412,7 @@</span> <span class="p_context"> config GUESS_UNWINDER</span>
 endchoice
 
 config FRAME_POINTER
<span class="p_del">-	depends on !ORC_UNWINDER &amp;&amp; !GUESS_UNWINDER</span>
<span class="p_add">+	depends on !UNWINDER_ORC &amp;&amp; !UNWINDER_GUESS</span>
 	bool
 
 endmenu
<span class="p_header">diff --git a/arch/x86/configs/tiny.config b/arch/x86/configs/tiny.config</span>
<span class="p_header">index 550cd5012b73..66c9e2aab16c 100644</span>
<span class="p_header">--- a/arch/x86/configs/tiny.config</span>
<span class="p_header">+++ b/arch/x86/configs/tiny.config</span>
<span class="p_chunk">@@ -1,5 +1,5 @@</span> <span class="p_context"></span>
 CONFIG_NOHIGHMEM=y
 # CONFIG_HIGHMEM4G is not set
 # CONFIG_HIGHMEM64G is not set
<span class="p_del">-CONFIG_GUESS_UNWINDER=y</span>
<span class="p_del">-# CONFIG_FRAME_POINTER_UNWINDER is not set</span>
<span class="p_add">+CONFIG_UNWINDER_GUESS=y</span>
<span class="p_add">+# CONFIG_UNWINDER_FRAME_POINTER is not set</span>
<span class="p_header">diff --git a/arch/x86/configs/x86_64_defconfig b/arch/x86/configs/x86_64_defconfig</span>
<span class="p_header">index 4a4b16e56d35..e32fc1f274d8 100644</span>
<span class="p_header">--- a/arch/x86/configs/x86_64_defconfig</span>
<span class="p_header">+++ b/arch/x86/configs/x86_64_defconfig</span>
<span class="p_chunk">@@ -299,6 +299,7 @@</span> <span class="p_context"> CONFIG_DEBUG_STACKOVERFLOW=y</span>
 # CONFIG_DEBUG_RODATA_TEST is not set
 CONFIG_DEBUG_BOOT_PARAMS=y
 CONFIG_OPTIMIZE_INLINING=y
<span class="p_add">+CONFIG_UNWINDER_ORC=y</span>
 CONFIG_SECURITY=y
 CONFIG_SECURITY_NETWORK=y
 CONFIG_SECURITY_SELINUX=y
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 6e160031cfea..3fd8bc560fae 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -142,56 +142,25 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	UNWIND_HINT_REGS offset=\offset
 	.endm
 
<span class="p_del">-	.macro RESTORE_EXTRA_REGS offset=0</span>
<span class="p_del">-	movq 0*8+\offset(%rsp), %r15</span>
<span class="p_del">-	movq 1*8+\offset(%rsp), %r14</span>
<span class="p_del">-	movq 2*8+\offset(%rsp), %r13</span>
<span class="p_del">-	movq 3*8+\offset(%rsp), %r12</span>
<span class="p_del">-	movq 4*8+\offset(%rsp), %rbp</span>
<span class="p_del">-	movq 5*8+\offset(%rsp), %rbx</span>
<span class="p_del">-	UNWIND_HINT_REGS offset=\offset extra=0</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-</span>
<span class="p_del">-	.macro RESTORE_C_REGS_HELPER rstor_rax=1, rstor_rcx=1, rstor_r11=1, rstor_r8910=1, rstor_rdx=1</span>
<span class="p_del">-	.if \rstor_r11</span>
<span class="p_del">-	movq 6*8(%rsp), %r11</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	.if \rstor_r8910</span>
<span class="p_del">-	movq 7*8(%rsp), %r10</span>
<span class="p_del">-	movq 8*8(%rsp), %r9</span>
<span class="p_del">-	movq 9*8(%rsp), %r8</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	.if \rstor_rax</span>
<span class="p_del">-	movq 10*8(%rsp), %rax</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	.if \rstor_rcx</span>
<span class="p_del">-	movq 11*8(%rsp), %rcx</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	.if \rstor_rdx</span>
<span class="p_del">-	movq 12*8(%rsp), %rdx</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	movq 13*8(%rsp), %rsi</span>
<span class="p_del">-	movq 14*8(%rsp), %rdi</span>
<span class="p_del">-	UNWIND_HINT_IRET_REGS offset=16*8</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro RESTORE_C_REGS</span>
<span class="p_del">-	RESTORE_C_REGS_HELPER 1,1,1,1,1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro RESTORE_C_REGS_EXCEPT_RAX</span>
<span class="p_del">-	RESTORE_C_REGS_HELPER 0,1,1,1,1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro RESTORE_C_REGS_EXCEPT_RCX</span>
<span class="p_del">-	RESTORE_C_REGS_HELPER 1,0,1,1,1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro RESTORE_C_REGS_EXCEPT_R11</span>
<span class="p_del">-	RESTORE_C_REGS_HELPER 1,1,0,1,1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro RESTORE_C_REGS_EXCEPT_RCX_R11</span>
<span class="p_del">-	RESTORE_C_REGS_HELPER 1,0,0,1,1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-</span>
<span class="p_del">-	.macro REMOVE_PT_GPREGS_FROM_STACK addskip=0</span>
<span class="p_del">-	subq $-(15*8+\addskip), %rsp</span>
<span class="p_add">+	.macro POP_EXTRA_REGS</span>
<span class="p_add">+	popq %r15</span>
<span class="p_add">+	popq %r14</span>
<span class="p_add">+	popq %r13</span>
<span class="p_add">+	popq %r12</span>
<span class="p_add">+	popq %rbp</span>
<span class="p_add">+	popq %rbx</span>
<span class="p_add">+	.endm</span>
<span class="p_add">+</span>
<span class="p_add">+	.macro POP_C_REGS</span>
<span class="p_add">+	popq %r11</span>
<span class="p_add">+	popq %r10</span>
<span class="p_add">+	popq %r9</span>
<span class="p_add">+	popq %r8</span>
<span class="p_add">+	popq %rax</span>
<span class="p_add">+	popq %rcx</span>
<span class="p_add">+	popq %rdx</span>
<span class="p_add">+	popq %rsi</span>
<span class="p_add">+	popq %rdi</span>
 	.endm
 
 	.macro icebp
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index 4838037f97f6..bd8b57a5c874 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -941,7 +941,8 @@</span> <span class="p_context"> ENTRY(debug)</span>
 	movl	%esp, %eax			# pt_regs pointer
 
 	/* Are we currently on the SYSENTER stack? */
<span class="p_del">-	PER_CPU(cpu_tss + CPU_TSS_SYSENTER_stack + SIZEOF_SYSENTER_stack, %ecx)</span>
<span class="p_add">+	movl	PER_CPU_VAR(cpu_entry_area), %ecx</span>
<span class="p_add">+	addl	$CPU_ENTRY_AREA_SYSENTER_stack + SIZEOF_SYSENTER_stack, %ecx</span>
 	subl	%eax, %ecx	/* ecx = (end of SYSENTER_stack) - esp */
 	cmpl	$SIZEOF_SYSENTER_stack, %ecx
 	jb	.Ldebug_from_sysenter_stack
<span class="p_chunk">@@ -984,7 +985,8 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	movl	%esp, %eax			# pt_regs pointer
 
 	/* Are we currently on the SYSENTER stack? */
<span class="p_del">-	PER_CPU(cpu_tss + CPU_TSS_SYSENTER_stack + SIZEOF_SYSENTER_stack, %ecx)</span>
<span class="p_add">+	movl	PER_CPU_VAR(cpu_entry_area), %ecx</span>
<span class="p_add">+	addl	$CPU_ENTRY_AREA_SYSENTER_stack + SIZEOF_SYSENTER_stack, %ecx</span>
 	subl	%eax, %ecx	/* ecx = (end of SYSENTER_stack) - esp */
 	cmpl	$SIZEOF_SYSENTER_stack, %ecx
 	jb	.Lnmi_from_sysenter_stack
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index 2e956afe272c..6abe3fcaece9 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -136,6 +136,64 @@</span> <span class="p_context"> END(native_usergs_sysret64)</span>
  * with them due to bugs in both AMD and Intel CPUs.
  */
 
<span class="p_add">+	.pushsection .entry_trampoline, &quot;ax&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The code in here gets remapped into cpu_entry_area&#39;s trampoline.  This means</span>
<span class="p_add">+ * that the assembler and linker have the wrong idea as to where this code</span>
<span class="p_add">+ * lives (and, in fact, it&#39;s mapped more than once, so it&#39;s not even at a</span>
<span class="p_add">+ * fixed address).  So we can&#39;t reference any symbols outside the entry</span>
<span class="p_add">+ * trampoline and expect it to work.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Instead, we carefully abuse %rip-relative addressing.</span>
<span class="p_add">+ * _entry_trampoline(%rip) refers to the start of the remapped) entry</span>
<span class="p_add">+ * trampoline.  We can thus find cpu_entry_area with this macro:</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA \</span>
<span class="p_add">+	_entry_trampoline - CPU_ENTRY_AREA_entry_trampoline(%rip)</span>
<span class="p_add">+</span>
<span class="p_add">+/* The top word of the SYSENTER stack is hot and is usable as scratch space. */</span>
<span class="p_add">+#define RSP_SCRATCH	CPU_ENTRY_AREA_SYSENTER_stack + \</span>
<span class="p_add">+			SIZEOF_SYSENTER_stack - 8 + CPU_ENTRY_AREA</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(entry_SYSCALL_64_trampoline)</span>
<span class="p_add">+	UNWIND_HINT_EMPTY</span>
<span class="p_add">+	swapgs</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Stash the user RSP. */</span>
<span class="p_add">+	movq	%rsp, RSP_SCRATCH</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Load the top of the task stack into RSP */</span>
<span class="p_add">+	movq	CPU_ENTRY_AREA_tss + TSS_sp1 + CPU_ENTRY_AREA, %rsp</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Start building the simulated IRET frame. */</span>
<span class="p_add">+	pushq	$__USER_DS			/* pt_regs-&gt;ss */</span>
<span class="p_add">+	pushq	RSP_SCRATCH			/* pt_regs-&gt;sp */</span>
<span class="p_add">+	pushq	%r11				/* pt_regs-&gt;flags */</span>
<span class="p_add">+	pushq	$__USER_CS			/* pt_regs-&gt;cs */</span>
<span class="p_add">+	pushq	%rcx				/* pt_regs-&gt;ip */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * x86 lacks a near absolute jump, and we can&#39;t jump to the real</span>
<span class="p_add">+	 * entry text with a relative jump.  We could push the target</span>
<span class="p_add">+	 * address and then use retq, but this destroys the pipeline on</span>
<span class="p_add">+	 * many CPUs (wasting over 20 cycles on Sandy Bridge).  Instead,</span>
<span class="p_add">+	 * spill RDI and restore it in a second-stage trampoline.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pushq	%rdi</span>
<span class="p_add">+	movq	$entry_SYSCALL_64_stage2, %rdi</span>
<span class="p_add">+	jmp	*%rdi</span>
<span class="p_add">+END(entry_SYSCALL_64_trampoline)</span>
<span class="p_add">+</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(entry_SYSCALL_64_stage2)</span>
<span class="p_add">+	UNWIND_HINT_EMPTY</span>
<span class="p_add">+	popq	%rdi</span>
<span class="p_add">+	jmp	entry_SYSCALL_64_after_hwframe</span>
<span class="p_add">+END(entry_SYSCALL_64_stage2)</span>
<span class="p_add">+</span>
 ENTRY(entry_SYSCALL_64)
 	UNWIND_HINT_EMPTY
 	/*
<span class="p_chunk">@@ -221,10 +279,9 @@</span> <span class="p_context"> entry_SYSCALL_64_fastpath:</span>
 	TRACE_IRQS_ON		/* user mode is traced as IRQs on */
 	movq	RIP(%rsp), %rcx
 	movq	EFLAGS(%rsp), %r11
<span class="p_del">-	RESTORE_C_REGS_EXCEPT_RCX_R11</span>
<span class="p_del">-	movq	RSP(%rsp), %rsp</span>
<span class="p_add">+	addq	$6*8, %rsp	/* skip extra regs -- they were preserved */</span>
 	UNWIND_HINT_EMPTY
<span class="p_del">-	USERGS_SYSRET64</span>
<span class="p_add">+	jmp	.Lpop_c_regs_except_rcx_r11_and_sysret</span>
 
 1:
 	/*
<span class="p_chunk">@@ -246,17 +303,18 @@</span> <span class="p_context"> entry_SYSCALL64_slow_path:</span>
 	call	do_syscall_64		/* returns with IRQs disabled */
 
 return_from_SYSCALL_64:
<span class="p_del">-	RESTORE_EXTRA_REGS</span>
 	TRACE_IRQS_IRETQ		/* we&#39;re about to change IF */
 
 	/*
 	 * Try to use SYSRET instead of IRET if we&#39;re returning to
<span class="p_del">-	 * a completely clean 64-bit userspace context.</span>
<span class="p_add">+	 * a completely clean 64-bit userspace context.  If we&#39;re not,</span>
<span class="p_add">+	 * go to the slow exit path.</span>
 	 */
 	movq	RCX(%rsp), %rcx
 	movq	RIP(%rsp), %r11
<span class="p_del">-	cmpq	%rcx, %r11			/* RCX == RIP */</span>
<span class="p_del">-	jne	opportunistic_sysret_failed</span>
<span class="p_add">+</span>
<span class="p_add">+	cmpq	%rcx, %r11	/* SYSRET requires RCX == RIP */</span>
<span class="p_add">+	jne	swapgs_restore_regs_and_return_to_usermode</span>
 
 	/*
 	 * On Intel CPUs, SYSRET with non-canonical RCX/RIP will #GP
<span class="p_chunk">@@ -274,14 +332,14 @@</span> <span class="p_context"> return_from_SYSCALL_64:</span>
 
 	/* If this changed %rcx, it was not canonical */
 	cmpq	%rcx, %r11
<span class="p_del">-	jne	opportunistic_sysret_failed</span>
<span class="p_add">+	jne	swapgs_restore_regs_and_return_to_usermode</span>
 
 	cmpq	$__USER_CS, CS(%rsp)		/* CS must match SYSRET */
<span class="p_del">-	jne	opportunistic_sysret_failed</span>
<span class="p_add">+	jne	swapgs_restore_regs_and_return_to_usermode</span>
 
 	movq	R11(%rsp), %r11
 	cmpq	%r11, EFLAGS(%rsp)		/* R11 == RFLAGS */
<span class="p_del">-	jne	opportunistic_sysret_failed</span>
<span class="p_add">+	jne	swapgs_restore_regs_and_return_to_usermode</span>
 
 	/*
 	 * SYSCALL clears RF when it saves RFLAGS in R11 and SYSRET cannot
<span class="p_chunk">@@ -302,12 +360,12 @@</span> <span class="p_context"> return_from_SYSCALL_64:</span>
 	 * would never get past &#39;stuck_here&#39;.
 	 */
 	testq	$(X86_EFLAGS_RF|X86_EFLAGS_TF), %r11
<span class="p_del">-	jnz	opportunistic_sysret_failed</span>
<span class="p_add">+	jnz	swapgs_restore_regs_and_return_to_usermode</span>
 
 	/* nothing to check for RSP */
 
 	cmpq	$__USER_DS, SS(%rsp)		/* SS must match SYSRET */
<span class="p_del">-	jne	opportunistic_sysret_failed</span>
<span class="p_add">+	jne	swapgs_restore_regs_and_return_to_usermode</span>
 
 	/*
 	 * We win! This label is here just for ease of understanding
<span class="p_chunk">@@ -315,14 +373,36 @@</span> <span class="p_context"> return_from_SYSCALL_64:</span>
 	 */
 syscall_return_via_sysret:
 	/* rcx and r11 are already restored (see code above) */
<span class="p_del">-	RESTORE_C_REGS_EXCEPT_RCX_R11</span>
<span class="p_del">-	movq	RSP(%rsp), %rsp</span>
 	UNWIND_HINT_EMPTY
<span class="p_del">-	USERGS_SYSRET64</span>
<span class="p_add">+	POP_EXTRA_REGS</span>
<span class="p_add">+.Lpop_c_regs_except_rcx_r11_and_sysret:</span>
<span class="p_add">+	popq	%rsi	/* skip r11 */</span>
<span class="p_add">+	popq	%r10</span>
<span class="p_add">+	popq	%r9</span>
<span class="p_add">+	popq	%r8</span>
<span class="p_add">+	popq	%rax</span>
<span class="p_add">+	popq	%rsi	/* skip rcx */</span>
<span class="p_add">+	popq	%rdx</span>
<span class="p_add">+	popq	%rsi</span>
 
<span class="p_del">-opportunistic_sysret_failed:</span>
<span class="p_del">-	SWAPGS</span>
<span class="p_del">-	jmp	restore_c_regs_and_iret</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Now all regs are restored except RSP and RDI.</span>
<span class="p_add">+	 * Save old stack pointer and switch to trampoline stack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	%rsp, %rdi</span>
<span class="p_add">+	movq	PER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp</span>
<span class="p_add">+</span>
<span class="p_add">+	pushq	RSP-RDI(%rdi)	/* RSP */</span>
<span class="p_add">+	pushq	(%rdi)		/* RDI */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We are on the trampoline stack.  All regs except RDI are live.</span>
<span class="p_add">+	 * We can do future final exit work right here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	popq	%rdi</span>
<span class="p_add">+	popq	%rsp</span>
<span class="p_add">+	USERGS_SYSRET64</span>
 END(entry_SYSCALL_64)
 
 ENTRY(stub_ptregs_64)
<span class="p_chunk">@@ -423,8 +503,7 @@</span> <span class="p_context"> ENTRY(ret_from_fork)</span>
 	movq	%rsp, %rdi
 	call	syscall_return_slowpath	/* returns with IRQs disabled */
 	TRACE_IRQS_ON			/* user mode is traced as IRQS on */
<span class="p_del">-	SWAPGS</span>
<span class="p_del">-	jmp	restore_regs_and_iret</span>
<span class="p_add">+	jmp	swapgs_restore_regs_and_return_to_usermode</span>
 
 1:
 	/* kernel thread */
<span class="p_chunk">@@ -457,12 +536,13 @@</span> <span class="p_context"> END(irq_entries_start)</span>
 
 .macro DEBUG_ENTRY_ASSERT_IRQS_OFF
 #ifdef CONFIG_DEBUG_ENTRY
<span class="p_del">-	pushfq</span>
<span class="p_del">-	testl $X86_EFLAGS_IF, (%rsp)</span>
<span class="p_add">+	pushq %rax</span>
<span class="p_add">+	SAVE_FLAGS(CLBR_RAX)</span>
<span class="p_add">+	testl $X86_EFLAGS_IF, %eax</span>
 	jz .Lokay_\@
 	ud2
 .Lokay_\@:
<span class="p_del">-	addq $8, %rsp</span>
<span class="p_add">+	popq %rax</span>
 #endif
 .endm
 
<span class="p_chunk">@@ -554,6 +634,13 @@</span> <span class="p_context"> END(irq_entries_start)</span>
 /* 0(%rsp): ~(interrupt number) */
 	.macro interrupt func
 	cld
<span class="p_add">+</span>
<span class="p_add">+	testb	$3, CS-ORIG_RAX(%rsp)</span>
<span class="p_add">+	jz	1f</span>
<span class="p_add">+	SWAPGS</span>
<span class="p_add">+	call	switch_to_thread_stack</span>
<span class="p_add">+1:</span>
<span class="p_add">+</span>
 	ALLOC_PT_GPREGS_ON_STACK
 	SAVE_C_REGS
 	SAVE_EXTRA_REGS
<span class="p_chunk">@@ -563,12 +650,8 @@</span> <span class="p_context"> END(irq_entries_start)</span>
 	jz	1f
 
 	/*
<span class="p_del">-	 * IRQ from user mode.  Switch to kernel gsbase and inform context</span>
<span class="p_del">-	 * tracking that we&#39;re in kernel mode.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	SWAPGS</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_add">+	 * IRQ from user mode.</span>
<span class="p_add">+	 *</span>
 	 * We need to tell lockdep that IRQs are off.  We can&#39;t do this until
 	 * we fix gsbase, and we should do it before enter_from_user_mode
 	 * (which can take locks).  Since TRACE_IRQS_OFF idempotent,
<span class="p_chunk">@@ -612,8 +695,52 @@</span> <span class="p_context"> GLOBAL(retint_user)</span>
 	mov	%rsp,%rdi
 	call	prepare_exit_to_usermode
 	TRACE_IRQS_IRETQ
<span class="p_add">+</span>
<span class="p_add">+GLOBAL(swapgs_restore_regs_and_return_to_usermode)</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_ENTRY</span>
<span class="p_add">+	/* Assert that pt_regs indicates user mode. */</span>
<span class="p_add">+	testb	$3, CS(%rsp)</span>
<span class="p_add">+	jnz	1f</span>
<span class="p_add">+	ud2</span>
<span class="p_add">+1:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	POP_EXTRA_REGS</span>
<span class="p_add">+	popq	%r11</span>
<span class="p_add">+	popq	%r10</span>
<span class="p_add">+	popq	%r9</span>
<span class="p_add">+	popq	%r8</span>
<span class="p_add">+	popq	%rax</span>
<span class="p_add">+	popq	%rcx</span>
<span class="p_add">+	popq	%rdx</span>
<span class="p_add">+	popq	%rsi</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The stack is now user RDI, orig_ax, RIP, CS, EFLAGS, RSP, SS.</span>
<span class="p_add">+	 * Save old stack pointer and switch to trampoline stack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	%rsp, %rdi</span>
<span class="p_add">+	movq	PER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Copy the IRET frame to the trampoline stack. */</span>
<span class="p_add">+	pushq	6*8(%rdi)	/* SS */</span>
<span class="p_add">+	pushq	5*8(%rdi)	/* RSP */</span>
<span class="p_add">+	pushq	4*8(%rdi)	/* EFLAGS */</span>
<span class="p_add">+	pushq	3*8(%rdi)	/* CS */</span>
<span class="p_add">+	pushq	2*8(%rdi)	/* RIP */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Push user RDI on the trampoline stack. */</span>
<span class="p_add">+	pushq	(%rdi)</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We are on the trampoline stack.  All regs except RDI are live.</span>
<span class="p_add">+	 * We can do future final exit work right here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Restore RDI. */</span>
<span class="p_add">+	popq	%rdi</span>
 	SWAPGS
<span class="p_del">-	jmp	restore_regs_and_iret</span>
<span class="p_add">+	INTERRUPT_RETURN</span>
<span class="p_add">+</span>
 
 /* Returning to kernel space */
 retint_kernel:
<span class="p_chunk">@@ -633,15 +760,17 @@</span> <span class="p_context"> retint_kernel:</span>
 	 */
 	TRACE_IRQS_IRETQ
 
<span class="p_del">-/*</span>
<span class="p_del">- * At this label, code paths which return to kernel and to user,</span>
<span class="p_del">- * which come from interrupts/exception and from syscalls, merge.</span>
<span class="p_del">- */</span>
<span class="p_del">-GLOBAL(restore_regs_and_iret)</span>
<span class="p_del">-	RESTORE_EXTRA_REGS</span>
<span class="p_del">-restore_c_regs_and_iret:</span>
<span class="p_del">-	RESTORE_C_REGS</span>
<span class="p_del">-	REMOVE_PT_GPREGS_FROM_STACK 8</span>
<span class="p_add">+GLOBAL(restore_regs_and_return_to_kernel)</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_ENTRY</span>
<span class="p_add">+	/* Assert that pt_regs indicates kernel mode. */</span>
<span class="p_add">+	testb	$3, CS(%rsp)</span>
<span class="p_add">+	jz	1f</span>
<span class="p_add">+	ud2</span>
<span class="p_add">+1:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	POP_EXTRA_REGS</span>
<span class="p_add">+	POP_C_REGS</span>
<span class="p_add">+	addq	$8, %rsp	/* skip regs-&gt;orig_ax */</span>
 	INTERRUPT_RETURN
 
 ENTRY(native_iret)
<span class="p_chunk">@@ -805,7 +934,33 @@</span> <span class="p_context"> apicinterrupt IRQ_WORK_VECTOR			irq_work_interrupt		smp_irq_work_interrupt</span>
 /*
  * Exception entry points.
  */
<span class="p_del">-#define CPU_TSS_IST(x) PER_CPU_VAR(cpu_tss) + (TSS_ist + ((x) - 1) * 8)</span>
<span class="p_add">+#define CPU_TSS_IST(x) PER_CPU_VAR(cpu_tss_rw) + (TSS_ist + ((x) - 1) * 8)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Switch to the thread stack.  This is called with the IRET frame and</span>
<span class="p_add">+ * orig_ax on the stack.  (That is, RDI..R12 are not on the stack and</span>
<span class="p_add">+ * space has not been allocated for them.)</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(switch_to_thread_stack)</span>
<span class="p_add">+	UNWIND_HINT_FUNC</span>
<span class="p_add">+</span>
<span class="p_add">+	pushq	%rdi</span>
<span class="p_add">+	movq	%rsp, %rdi</span>
<span class="p_add">+	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp</span>
<span class="p_add">+	UNWIND_HINT sp_offset=16 sp_reg=ORC_REG_DI</span>
<span class="p_add">+</span>
<span class="p_add">+	pushq	7*8(%rdi)		/* regs-&gt;ss */</span>
<span class="p_add">+	pushq	6*8(%rdi)		/* regs-&gt;rsp */</span>
<span class="p_add">+	pushq	5*8(%rdi)		/* regs-&gt;eflags */</span>
<span class="p_add">+	pushq	4*8(%rdi)		/* regs-&gt;cs */</span>
<span class="p_add">+	pushq	3*8(%rdi)		/* regs-&gt;ip */</span>
<span class="p_add">+	pushq	2*8(%rdi)		/* regs-&gt;orig_ax */</span>
<span class="p_add">+	pushq	8(%rdi)			/* return address */</span>
<span class="p_add">+	UNWIND_HINT_FUNC</span>
<span class="p_add">+</span>
<span class="p_add">+	movq	(%rdi), %rdi</span>
<span class="p_add">+	ret</span>
<span class="p_add">+END(switch_to_thread_stack)</span>
 
 .macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1
 ENTRY(\sym)
<span class="p_chunk">@@ -818,17 +973,18 @@</span> <span class="p_context"> ENTRY(\sym)</span>
 
 	ASM_CLAC
 
<span class="p_del">-	.ifeq \has_error_code</span>
<span class="p_add">+	.if \has_error_code == 0</span>
 	pushq	$-1				/* ORIG_RAX: no syscall to restart */
 	.endif
 
 	ALLOC_PT_GPREGS_ON_STACK
 
<span class="p_del">-	.if \paranoid</span>
<span class="p_del">-	.if \paranoid == 1</span>
<span class="p_add">+	.if \paranoid &lt; 2</span>
 	testb	$3, CS(%rsp)			/* If coming from userspace, switch stacks */
<span class="p_del">-	jnz	1f</span>
<span class="p_add">+	jnz	.Lfrom_usermode_switch_stack_\@</span>
 	.endif
<span class="p_add">+</span>
<span class="p_add">+	.if \paranoid</span>
 	call	paranoid_entry
 	.else
 	call	error_entry
<span class="p_chunk">@@ -870,20 +1026,15 @@</span> <span class="p_context"> ENTRY(\sym)</span>
 	jmp	error_exit
 	.endif
 
<span class="p_del">-	.if \paranoid == 1</span>
<span class="p_add">+	.if \paranoid &lt; 2</span>
 	/*
<span class="p_del">-	 * Paranoid entry from userspace.  Switch stacks and treat it</span>
<span class="p_add">+	 * Entry from userspace.  Switch stacks and treat it</span>
 	 * as a normal entry.  This means that paranoid handlers
 	 * run in real process context if user_mode(regs).
 	 */
<span class="p_del">-1:</span>
<span class="p_add">+.Lfrom_usermode_switch_stack_\@:</span>
 	call	error_entry
 
<span class="p_del">-</span>
<span class="p_del">-	movq	%rsp, %rdi			/* pt_regs pointer */</span>
<span class="p_del">-	call	sync_regs</span>
<span class="p_del">-	movq	%rax, %rsp			/* switch stack */</span>
<span class="p_del">-</span>
 	movq	%rsp, %rdi			/* pt_regs pointer */
 
 	.if \has_error_code
<span class="p_chunk">@@ -1059,6 +1210,7 @@</span> <span class="p_context"> idtentry int3			do_int3			has_error_code=0	paranoid=1 shift_ist=DEBUG_STACK</span>
 idtentry stack_segment		do_stack_segment	has_error_code=1
 
 #ifdef CONFIG_XEN
<span class="p_add">+idtentry xennmi			do_nmi			has_error_code=0</span>
 idtentry xendebug		do_debug		has_error_code=0
 idtentry xenint3		do_int3			has_error_code=0
 #endif
<span class="p_chunk">@@ -1112,17 +1264,14 @@</span> <span class="p_context"> ENTRY(paranoid_exit)</span>
 	DISABLE_INTERRUPTS(CLBR_ANY)
 	TRACE_IRQS_OFF_DEBUG
 	testl	%ebx, %ebx			/* swapgs needed? */
<span class="p_del">-	jnz	paranoid_exit_no_swapgs</span>
<span class="p_add">+	jnz	.Lparanoid_exit_no_swapgs</span>
 	TRACE_IRQS_IRETQ
 	SWAPGS_UNSAFE_STACK
<span class="p_del">-	jmp	paranoid_exit_restore</span>
<span class="p_del">-paranoid_exit_no_swapgs:</span>
<span class="p_add">+	jmp	.Lparanoid_exit_restore</span>
<span class="p_add">+.Lparanoid_exit_no_swapgs:</span>
 	TRACE_IRQS_IRETQ_DEBUG
<span class="p_del">-paranoid_exit_restore:</span>
<span class="p_del">-	RESTORE_EXTRA_REGS</span>
<span class="p_del">-	RESTORE_C_REGS</span>
<span class="p_del">-	REMOVE_PT_GPREGS_FROM_STACK 8</span>
<span class="p_del">-	INTERRUPT_RETURN</span>
<span class="p_add">+.Lparanoid_exit_restore:</span>
<span class="p_add">+	jmp restore_regs_and_return_to_kernel</span>
 END(paranoid_exit)
 
 /*
<span class="p_chunk">@@ -1146,6 +1295,14 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	SWAPGS
 
 .Lerror_entry_from_usermode_after_swapgs:
<span class="p_add">+	/* Put us onto the real thread stack. */</span>
<span class="p_add">+	popq	%r12				/* save return addr in %12 */</span>
<span class="p_add">+	movq	%rsp, %rdi			/* arg0 = pt_regs pointer */</span>
<span class="p_add">+	call	sync_regs</span>
<span class="p_add">+	movq	%rax, %rsp			/* switch stack */</span>
<span class="p_add">+	ENCODE_FRAME_POINTER</span>
<span class="p_add">+	pushq	%r12</span>
<span class="p_add">+</span>
 	/*
 	 * We need to tell lockdep that IRQs are off.  We can&#39;t do this until
 	 * we fix gsbase, and we should do it before enter_from_user_mode
<span class="p_chunk">@@ -1223,10 +1380,13 @@</span> <span class="p_context"> ENTRY(error_exit)</span>
 	jmp	retint_user
 END(error_exit)
 
<span class="p_del">-/* Runs on exception stack */</span>
<span class="p_del">-/* XXX: broken on Xen PV */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Runs on exception stack.  Xen PV does not go through this path at all,</span>
<span class="p_add">+ * so we can use real assembly here.</span>
<span class="p_add">+ */</span>
 ENTRY(nmi)
 	UNWIND_HINT_IRET_REGS
<span class="p_add">+</span>
 	/*
 	 * We allow breakpoints in NMIs. If a breakpoint occurs, then
 	 * the iretq it performs will take us out of NMI context.
<span class="p_chunk">@@ -1284,7 +1444,7 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	 * stacks lest we corrupt the &quot;NMI executing&quot; variable.
 	 */
 
<span class="p_del">-	SWAPGS_UNSAFE_STACK</span>
<span class="p_add">+	swapgs</span>
 	cld
 	movq	%rsp, %rdx
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
<span class="p_chunk">@@ -1328,8 +1488,7 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	 * Return back to user mode.  We must *not* do the normal exit
 	 * work, because we don&#39;t want to enable interrupts.
 	 */
<span class="p_del">-	SWAPGS</span>
<span class="p_del">-	jmp	restore_regs_and_iret</span>
<span class="p_add">+	jmp	swapgs_restore_regs_and_return_to_usermode</span>
 
 .Lnmi_from_kernel:
 	/*
<span class="p_chunk">@@ -1450,7 +1609,7 @@</span> <span class="p_context"> nested_nmi_out:</span>
 	popq	%rdx
 
 	/* We are returning to kernel mode, so this cannot result in a fault. */
<span class="p_del">-	INTERRUPT_RETURN</span>
<span class="p_add">+	iretq</span>
 
 first_nmi:
 	/* Restore rdx. */
<span class="p_chunk">@@ -1481,7 +1640,7 @@</span> <span class="p_context"> first_nmi:</span>
 	pushfq			/* RFLAGS */
 	pushq	$__KERNEL_CS	/* CS */
 	pushq	$1f		/* RIP */
<span class="p_del">-	INTERRUPT_RETURN	/* continues at repeat_nmi below */</span>
<span class="p_add">+	iretq			/* continues at repeat_nmi below */</span>
 	UNWIND_HINT_IRET_REGS
 1:
 #endif
<span class="p_chunk">@@ -1544,29 +1703,34 @@</span> <span class="p_context"> end_repeat_nmi:</span>
 nmi_swapgs:
 	SWAPGS_UNSAFE_STACK
 nmi_restore:
<span class="p_del">-	RESTORE_EXTRA_REGS</span>
<span class="p_del">-	RESTORE_C_REGS</span>
<span class="p_add">+	POP_EXTRA_REGS</span>
<span class="p_add">+	POP_C_REGS</span>
 
<span class="p_del">-	/* Point RSP at the &quot;iret&quot; frame. */</span>
<span class="p_del">-	REMOVE_PT_GPREGS_FROM_STACK 6*8</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Skip orig_ax and the &quot;outermost&quot; frame to point RSP at the &quot;iret&quot;</span>
<span class="p_add">+	 * at the &quot;iret&quot; frame.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	addq	$6*8, %rsp</span>
 
 	/*
 	 * Clear &quot;NMI executing&quot;.  Set DF first so that we can easily
 	 * distinguish the remaining code between here and IRET from
<span class="p_del">-	 * the SYSCALL entry and exit paths.  On a native kernel, we</span>
<span class="p_del">-	 * could just inspect RIP, but, on paravirt kernels,</span>
<span class="p_del">-	 * INTERRUPT_RETURN can translate into a jump into a</span>
<span class="p_del">-	 * hypercall page.</span>
<span class="p_add">+	 * the SYSCALL entry and exit paths.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We arguably should just inspect RIP instead, but I (Andy) wrote</span>
<span class="p_add">+	 * this code when I had the misapprehension that Xen PV supported</span>
<span class="p_add">+	 * NMIs, and Xen PV would break that approach.</span>
 	 */
 	std
 	movq	$0, 5*8(%rsp)		/* clear &quot;NMI executing&quot; */
 
 	/*
<span class="p_del">-	 * INTERRUPT_RETURN reads the &quot;iret&quot; frame and exits the NMI</span>
<span class="p_del">-	 * stack in a single instruction.  We are returning to kernel</span>
<span class="p_del">-	 * mode, so this cannot result in a fault.</span>
<span class="p_add">+	 * iretq reads the &quot;iret&quot; frame and exits the NMI stack in a</span>
<span class="p_add">+	 * single instruction.  We are returning to kernel mode, so this</span>
<span class="p_add">+	 * cannot result in a fault.  Similarly, we don&#39;t need to worry</span>
<span class="p_add">+	 * about espfix64 on the way back to kernel mode.</span>
 	 */
<span class="p_del">-	INTERRUPT_RETURN</span>
<span class="p_add">+	iretq</span>
 END(nmi)
 
 ENTRY(ignore_sysret)
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index b5c7a56ed256..95ad40eb7eff 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -48,7 +48,7 @@</span> <span class="p_context"></span>
  */
 ENTRY(entry_SYSENTER_compat)
 	/* Interrupts are off on entry. */
<span class="p_del">-	SWAPGS_UNSAFE_STACK</span>
<span class="p_add">+	SWAPGS</span>
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 
 	/*
<span class="p_chunk">@@ -306,8 +306,11 @@</span> <span class="p_context"> ENTRY(entry_INT80_compat)</span>
 	 */
 	movl	%eax, %eax
 
<span class="p_del">-	/* Construct struct pt_regs on stack (iret frame is already on stack) */</span>
 	pushq	%rax			/* pt_regs-&gt;orig_ax */
<span class="p_add">+</span>
<span class="p_add">+	/* switch to thread stack expects orig_ax to be pushed */</span>
<span class="p_add">+	call	switch_to_thread_stack</span>
<span class="p_add">+</span>
 	pushq	%rdi			/* pt_regs-&gt;di */
 	pushq	%rsi			/* pt_regs-&gt;si */
 	pushq	%rdx			/* pt_regs-&gt;dx */
<span class="p_chunk">@@ -337,8 +340,7 @@</span> <span class="p_context"> ENTRY(entry_INT80_compat)</span>
 
 	/* Go back to user mode. */
 	TRACE_IRQS_ON
<span class="p_del">-	SWAPGS</span>
<span class="p_del">-	jmp	restore_regs_and_iret</span>
<span class="p_add">+	jmp	swapgs_restore_regs_and_return_to_usermode</span>
 END(entry_INT80_compat)
 
 ENTRY(stub32_clone)
<span class="p_header">diff --git a/arch/x86/entry/syscalls/Makefile b/arch/x86/entry/syscalls/Makefile</span>
<span class="p_header">index 331f1dca5085..6fb9b57ed5ba 100644</span>
<span class="p_header">--- a/arch/x86/entry/syscalls/Makefile</span>
<span class="p_header">+++ b/arch/x86/entry/syscalls/Makefile</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
<span class="p_del">-out := $(obj)/../../include/generated/asm</span>
<span class="p_del">-uapi := $(obj)/../../include/generated/uapi/asm</span>
<span class="p_add">+out := arch/$(SRCARCH)/include/generated/asm</span>
<span class="p_add">+uapi := arch/$(SRCARCH)/include/generated/uapi/asm</span>
 
 # Create output directory if not already present
 _dummy := $(shell [ -d &#39;$(out)&#39; ] || mkdir -p &#39;$(out)&#39;) \
<span class="p_header">diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c</span>
<span class="p_header">index 80534d3c2480..589af1eec7c1 100644</span>
<span class="p_header">--- a/arch/x86/events/core.c</span>
<span class="p_header">+++ b/arch/x86/events/core.c</span>
<span class="p_chunk">@@ -2371,7 +2371,7 @@</span> <span class="p_context"> static unsigned long get_segment_base(unsigned int segment)</span>
 		struct ldt_struct *ldt;
 
 		/* IRQs are off, so this synchronizes with smp_store_release */
<span class="p_del">-		ldt = lockless_dereference(current-&gt;active_mm-&gt;context.ldt);</span>
<span class="p_add">+		ldt = READ_ONCE(current-&gt;active_mm-&gt;context.ldt);</span>
 		if (!ldt || idx &gt;= ldt-&gt;nr_entries)
 			return 0;
 
<span class="p_header">diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c</span>
<span class="p_header">index f94855000d4e..09c26a4f139c 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/core.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/core.c</span>
<span class="p_chunk">@@ -2958,6 +2958,10 @@</span> <span class="p_context"> static unsigned long intel_pmu_free_running_flags(struct perf_event *event)</span>
 
 	if (event-&gt;attr.use_clockid)
 		flags &amp;= ~PERF_SAMPLE_TIME;
<span class="p_add">+	if (!event-&gt;attr.exclude_kernel)</span>
<span class="p_add">+		flags &amp;= ~PERF_SAMPLE_REGS_USER;</span>
<span class="p_add">+	if (event-&gt;attr.sample_regs_user &amp; ~PEBS_REGS)</span>
<span class="p_add">+		flags &amp;= ~(PERF_SAMPLE_REGS_USER | PERF_SAMPLE_REGS_INTR);</span>
 	return flags;
 }
 
<span class="p_header">diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h</span>
<span class="p_header">index 4196f81ec0e1..f7aaadf9331f 100644</span>
<span class="p_header">--- a/arch/x86/events/perf_event.h</span>
<span class="p_header">+++ b/arch/x86/events/perf_event.h</span>
<span class="p_chunk">@@ -85,13 +85,15 @@</span> <span class="p_context"> struct amd_nb {</span>
  * Flags PEBS can handle without an PMI.
  *
  * TID can only be handled by flushing at context switch.
<span class="p_add">+ * REGS_USER can be handled for events limited to ring 3.</span>
  *
  */
 #define PEBS_FREERUNNING_FLAGS \
 	(PERF_SAMPLE_IP | PERF_SAMPLE_TID | PERF_SAMPLE_ADDR | \
 	PERF_SAMPLE_ID | PERF_SAMPLE_CPU | PERF_SAMPLE_STREAM_ID | \
 	PERF_SAMPLE_DATA_SRC | PERF_SAMPLE_IDENTIFIER | \
<span class="p_del">-	PERF_SAMPLE_TRANSACTION | PERF_SAMPLE_PHYS_ADDR)</span>
<span class="p_add">+	PERF_SAMPLE_TRANSACTION | PERF_SAMPLE_PHYS_ADDR | \</span>
<span class="p_add">+	PERF_SAMPLE_REGS_INTR | PERF_SAMPLE_REGS_USER)</span>
 
 /*
  * A debug store configuration.
<span class="p_chunk">@@ -110,6 +112,26 @@</span> <span class="p_context"> struct debug_store {</span>
 	u64	pebs_event_reset[MAX_PEBS_EVENTS];
 };
 
<span class="p_add">+#define PEBS_REGS \</span>
<span class="p_add">+	(PERF_REG_X86_AX | \</span>
<span class="p_add">+	 PERF_REG_X86_BX | \</span>
<span class="p_add">+	 PERF_REG_X86_CX | \</span>
<span class="p_add">+	 PERF_REG_X86_DX | \</span>
<span class="p_add">+	 PERF_REG_X86_DI | \</span>
<span class="p_add">+	 PERF_REG_X86_SI | \</span>
<span class="p_add">+	 PERF_REG_X86_SP | \</span>
<span class="p_add">+	 PERF_REG_X86_BP | \</span>
<span class="p_add">+	 PERF_REG_X86_IP | \</span>
<span class="p_add">+	 PERF_REG_X86_FLAGS | \</span>
<span class="p_add">+	 PERF_REG_X86_R8 | \</span>
<span class="p_add">+	 PERF_REG_X86_R9 | \</span>
<span class="p_add">+	 PERF_REG_X86_R10 | \</span>
<span class="p_add">+	 PERF_REG_X86_R11 | \</span>
<span class="p_add">+	 PERF_REG_X86_R12 | \</span>
<span class="p_add">+	 PERF_REG_X86_R13 | \</span>
<span class="p_add">+	 PERF_REG_X86_R14 | \</span>
<span class="p_add">+	 PERF_REG_X86_R15)</span>
<span class="p_add">+</span>
 /*
  * Per register state.
  */
<span class="p_header">diff --git a/arch/x86/hyperv/hv_init.c b/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">index a5db63f728a2..a0b86cf486e0 100644</span>
<span class="p_header">--- a/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">+++ b/arch/x86/hyperv/hv_init.c</span>
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> void hyperv_init(void)</span>
 	u64 guest_id;
 	union hv_x64_msr_hypercall_contents hypercall_msr;
 
<span class="p_del">-	if (x86_hyper != &amp;x86_hyper_ms_hyperv)</span>
<span class="p_add">+	if (x86_hyper_type != X86_HYPER_MS_HYPERV)</span>
 		return;
 
 	/* Allocate percpu VP index */
<span class="p_header">diff --git a/arch/x86/include/asm/archrandom.h b/arch/x86/include/asm/archrandom.h</span>
<span class="p_header">index 5b0579abb398..3ac991d81e74 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/archrandom.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/archrandom.h</span>
<span class="p_chunk">@@ -45,7 +45,7 @@</span> <span class="p_context"> static inline bool rdrand_long(unsigned long *v)</span>
 	bool ok;
 	unsigned int retry = RDRAND_RETRY_LOOPS;
 	do {
<span class="p_del">-		asm volatile(RDRAND_LONG &quot;\n\t&quot;</span>
<span class="p_add">+		asm volatile(RDRAND_LONG</span>
 			     CC_SET(c)
 			     : CC_OUT(c) (ok), &quot;=a&quot; (*v));
 		if (ok)
<span class="p_chunk">@@ -59,7 +59,7 @@</span> <span class="p_context"> static inline bool rdrand_int(unsigned int *v)</span>
 	bool ok;
 	unsigned int retry = RDRAND_RETRY_LOOPS;
 	do {
<span class="p_del">-		asm volatile(RDRAND_INT &quot;\n\t&quot;</span>
<span class="p_add">+		asm volatile(RDRAND_INT</span>
 			     CC_SET(c)
 			     : CC_OUT(c) (ok), &quot;=a&quot; (*v));
 		if (ok)
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> static inline bool rdrand_int(unsigned int *v)</span>
 static inline bool rdseed_long(unsigned long *v)
 {
 	bool ok;
<span class="p_del">-	asm volatile(RDSEED_LONG &quot;\n\t&quot;</span>
<span class="p_add">+	asm volatile(RDSEED_LONG</span>
 		     CC_SET(c)
 		     : CC_OUT(c) (ok), &quot;=a&quot; (*v));
 	return ok;
<span class="p_chunk">@@ -80,7 +80,7 @@</span> <span class="p_context"> static inline bool rdseed_long(unsigned long *v)</span>
 static inline bool rdseed_int(unsigned int *v)
 {
 	bool ok;
<span class="p_del">-	asm volatile(RDSEED_INT &quot;\n\t&quot;</span>
<span class="p_add">+	asm volatile(RDSEED_INT</span>
 		     CC_SET(c)
 		     : CC_OUT(c) (ok), &quot;=a&quot; (*v));
 	return ok;
<span class="p_header">diff --git a/arch/x86/include/asm/bitops.h b/arch/x86/include/asm/bitops.h</span>
<span class="p_header">index 2bcf47314959..3fa039855b8f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/bitops.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/bitops.h</span>
<span class="p_chunk">@@ -143,7 +143,7 @@</span> <span class="p_context"> static __always_inline void __clear_bit(long nr, volatile unsigned long *addr)</span>
 static __always_inline bool clear_bit_unlock_is_negative_byte(long nr, volatile unsigned long *addr)
 {
 	bool negative;
<span class="p_del">-	asm volatile(LOCK_PREFIX &quot;andb %2,%1\n\t&quot;</span>
<span class="p_add">+	asm volatile(LOCK_PREFIX &quot;andb %2,%1&quot;</span>
 		CC_SET(s)
 		: CC_OUT(s) (negative), ADDR
 		: &quot;ir&quot; ((char) ~(1 &lt;&lt; nr)) : &quot;memory&quot;);
<span class="p_chunk">@@ -246,7 +246,7 @@</span> <span class="p_context"> static __always_inline bool __test_and_set_bit(long nr, volatile unsigned long *</span>
 {
 	bool oldbit;
 
<span class="p_del">-	asm(&quot;bts %2,%1\n\t&quot;</span>
<span class="p_add">+	asm(&quot;bts %2,%1&quot;</span>
 	    CC_SET(c)
 	    : CC_OUT(c) (oldbit), ADDR
 	    : &quot;Ir&quot; (nr));
<span class="p_chunk">@@ -286,7 +286,7 @@</span> <span class="p_context"> static __always_inline bool __test_and_clear_bit(long nr, volatile unsigned long</span>
 {
 	bool oldbit;
 
<span class="p_del">-	asm volatile(&quot;btr %2,%1\n\t&quot;</span>
<span class="p_add">+	asm volatile(&quot;btr %2,%1&quot;</span>
 		     CC_SET(c)
 		     : CC_OUT(c) (oldbit), ADDR
 		     : &quot;Ir&quot; (nr));
<span class="p_chunk">@@ -298,7 +298,7 @@</span> <span class="p_context"> static __always_inline bool __test_and_change_bit(long nr, volatile unsigned lon</span>
 {
 	bool oldbit;
 
<span class="p_del">-	asm volatile(&quot;btc %2,%1\n\t&quot;</span>
<span class="p_add">+	asm volatile(&quot;btc %2,%1&quot;</span>
 		     CC_SET(c)
 		     : CC_OUT(c) (oldbit), ADDR
 		     : &quot;Ir&quot; (nr) : &quot;memory&quot;);
<span class="p_chunk">@@ -329,7 +329,7 @@</span> <span class="p_context"> static __always_inline bool variable_test_bit(long nr, volatile const unsigned l</span>
 {
 	bool oldbit;
 
<span class="p_del">-	asm volatile(&quot;bt %2,%1\n\t&quot;</span>
<span class="p_add">+	asm volatile(&quot;bt %2,%1&quot;</span>
 		     CC_SET(c)
 		     : CC_OUT(c) (oldbit)
 		     : &quot;m&quot; (*(unsigned long *)addr), &quot;Ir&quot; (nr));
<span class="p_header">diff --git a/arch/x86/include/asm/compat.h b/arch/x86/include/asm/compat.h</span>
<span class="p_header">index 70bc1df580b2..2cbd75dd2fd3 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/compat.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/compat.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
  */
 #include &lt;linux/types.h&gt;
 #include &lt;linux/sched.h&gt;
<span class="p_add">+#include &lt;linux/sched/task_stack.h&gt;</span>
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/user32.h&gt;
 #include &lt;asm/unistd.h&gt;
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index 0dfa68438e80..ea9a7dde62e5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -126,16 +126,17 @@</span> <span class="p_context"> extern const char * const x86_bug_flags[NBUGINTS*32];</span>
 #define boot_cpu_has(bit)	cpu_has(&amp;boot_cpu_data, bit)
 
 #define set_cpu_cap(c, bit)	set_bit(bit, (unsigned long *)((c)-&gt;x86_capability))
<span class="p_del">-#define clear_cpu_cap(c, bit)	clear_bit(bit, (unsigned long *)((c)-&gt;x86_capability))</span>
<span class="p_del">-#define setup_clear_cpu_cap(bit) do { \</span>
<span class="p_del">-	clear_cpu_cap(&amp;boot_cpu_data, bit);	\</span>
<span class="p_del">-	set_bit(bit, (unsigned long *)cpu_caps_cleared); \</span>
<span class="p_del">-} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+extern void setup_clear_cpu_cap(unsigned int bit);</span>
<span class="p_add">+extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);</span>
<span class="p_add">+</span>
 #define setup_force_cpu_cap(bit) do { \
 	set_cpu_cap(&amp;boot_cpu_data, bit);	\
 	set_bit(bit, (unsigned long *)cpu_caps_set);	\
 } while (0)
 
<span class="p_add">+#define setup_force_cpu_bug(bit) setup_force_cpu_cap(bit)</span>
<span class="p_add">+</span>
 #if defined(CC_HAVE_ASM_GOTO) &amp;&amp; defined(CONFIG_X86_FAST_FEATURE_TESTS)
 /*
  * Static testing of CPU features.  Used the same as boot_cpu_has().
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 793690fbda36..800104c8a3ed 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -13,173 +13,176 @@</span> <span class="p_context"></span>
 /*
  * Defines x86 CPU feature bits
  */
<span class="p_del">-#define NCAPINTS	18	/* N 32-bit words worth of info */</span>
<span class="p_del">-#define NBUGINTS	1	/* N 32-bit bug flags */</span>
<span class="p_add">+#define NCAPINTS			18	   /* N 32-bit words worth of info */</span>
<span class="p_add">+#define NBUGINTS			1	   /* N 32-bit bug flags */</span>
 
 /*
  * Note: If the comment begins with a quoted string, that string is used
  * in /proc/cpuinfo instead of the macro name.  If the string is &quot;&quot;,
  * this feature bit is not displayed in /proc/cpuinfo at all.
<span class="p_add">+ *</span>
<span class="p_add">+ * When adding new features here that depend on other features,</span>
<span class="p_add">+ * please update the table in kernel/cpu/cpuid-deps.c as well.</span>
  */
 
<span class="p_del">-/* Intel-defined CPU features, CPUID level 0x00000001 (edx), word 0 */</span>
<span class="p_del">-#define X86_FEATURE_FPU		( 0*32+ 0) /* Onboard FPU */</span>
<span class="p_del">-#define X86_FEATURE_VME		( 0*32+ 1) /* Virtual Mode Extensions */</span>
<span class="p_del">-#define X86_FEATURE_DE		( 0*32+ 2) /* Debugging Extensions */</span>
<span class="p_del">-#define X86_FEATURE_PSE		( 0*32+ 3) /* Page Size Extensions */</span>
<span class="p_del">-#define X86_FEATURE_TSC		( 0*32+ 4) /* Time Stamp Counter */</span>
<span class="p_del">-#define X86_FEATURE_MSR		( 0*32+ 5) /* Model-Specific Registers */</span>
<span class="p_del">-#define X86_FEATURE_PAE		( 0*32+ 6) /* Physical Address Extensions */</span>
<span class="p_del">-#define X86_FEATURE_MCE		( 0*32+ 7) /* Machine Check Exception */</span>
<span class="p_del">-#define X86_FEATURE_CX8		( 0*32+ 8) /* CMPXCHG8 instruction */</span>
<span class="p_del">-#define X86_FEATURE_APIC	( 0*32+ 9) /* Onboard APIC */</span>
<span class="p_del">-#define X86_FEATURE_SEP		( 0*32+11) /* SYSENTER/SYSEXIT */</span>
<span class="p_del">-#define X86_FEATURE_MTRR	( 0*32+12) /* Memory Type Range Registers */</span>
<span class="p_del">-#define X86_FEATURE_PGE		( 0*32+13) /* Page Global Enable */</span>
<span class="p_del">-#define X86_FEATURE_MCA		( 0*32+14) /* Machine Check Architecture */</span>
<span class="p_del">-#define X86_FEATURE_CMOV	( 0*32+15) /* CMOV instructions */</span>
<span class="p_del">-					  /* (plus FCMOVcc, FCOMI with FPU) */</span>
<span class="p_del">-#define X86_FEATURE_PAT		( 0*32+16) /* Page Attribute Table */</span>
<span class="p_del">-#define X86_FEATURE_PSE36	( 0*32+17) /* 36-bit PSEs */</span>
<span class="p_del">-#define X86_FEATURE_PN		( 0*32+18) /* Processor serial number */</span>
<span class="p_del">-#define X86_FEATURE_CLFLUSH	( 0*32+19) /* CLFLUSH instruction */</span>
<span class="p_del">-#define X86_FEATURE_DS		( 0*32+21) /* &quot;dts&quot; Debug Store */</span>
<span class="p_del">-#define X86_FEATURE_ACPI	( 0*32+22) /* ACPI via MSR */</span>
<span class="p_del">-#define X86_FEATURE_MMX		( 0*32+23) /* Multimedia Extensions */</span>
<span class="p_del">-#define X86_FEATURE_FXSR	( 0*32+24) /* FXSAVE/FXRSTOR, CR4.OSFXSR */</span>
<span class="p_del">-#define X86_FEATURE_XMM		( 0*32+25) /* &quot;sse&quot; */</span>
<span class="p_del">-#define X86_FEATURE_XMM2	( 0*32+26) /* &quot;sse2&quot; */</span>
<span class="p_del">-#define X86_FEATURE_SELFSNOOP	( 0*32+27) /* &quot;ss&quot; CPU self snoop */</span>
<span class="p_del">-#define X86_FEATURE_HT		( 0*32+28) /* Hyper-Threading */</span>
<span class="p_del">-#define X86_FEATURE_ACC		( 0*32+29) /* &quot;tm&quot; Automatic clock control */</span>
<span class="p_del">-#define X86_FEATURE_IA64	( 0*32+30) /* IA-64 processor */</span>
<span class="p_del">-#define X86_FEATURE_PBE		( 0*32+31) /* Pending Break Enable */</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000001 (EDX), word 0 */</span>
<span class="p_add">+#define X86_FEATURE_FPU			( 0*32+ 0) /* Onboard FPU */</span>
<span class="p_add">+#define X86_FEATURE_VME			( 0*32+ 1) /* Virtual Mode Extensions */</span>
<span class="p_add">+#define X86_FEATURE_DE			( 0*32+ 2) /* Debugging Extensions */</span>
<span class="p_add">+#define X86_FEATURE_PSE			( 0*32+ 3) /* Page Size Extensions */</span>
<span class="p_add">+#define X86_FEATURE_TSC			( 0*32+ 4) /* Time Stamp Counter */</span>
<span class="p_add">+#define X86_FEATURE_MSR			( 0*32+ 5) /* Model-Specific Registers */</span>
<span class="p_add">+#define X86_FEATURE_PAE			( 0*32+ 6) /* Physical Address Extensions */</span>
<span class="p_add">+#define X86_FEATURE_MCE			( 0*32+ 7) /* Machine Check Exception */</span>
<span class="p_add">+#define X86_FEATURE_CX8			( 0*32+ 8) /* CMPXCHG8 instruction */</span>
<span class="p_add">+#define X86_FEATURE_APIC		( 0*32+ 9) /* Onboard APIC */</span>
<span class="p_add">+#define X86_FEATURE_SEP			( 0*32+11) /* SYSENTER/SYSEXIT */</span>
<span class="p_add">+#define X86_FEATURE_MTRR		( 0*32+12) /* Memory Type Range Registers */</span>
<span class="p_add">+#define X86_FEATURE_PGE			( 0*32+13) /* Page Global Enable */</span>
<span class="p_add">+#define X86_FEATURE_MCA			( 0*32+14) /* Machine Check Architecture */</span>
<span class="p_add">+#define X86_FEATURE_CMOV		( 0*32+15) /* CMOV instructions (plus FCMOVcc, FCOMI with FPU) */</span>
<span class="p_add">+#define X86_FEATURE_PAT			( 0*32+16) /* Page Attribute Table */</span>
<span class="p_add">+#define X86_FEATURE_PSE36		( 0*32+17) /* 36-bit PSEs */</span>
<span class="p_add">+#define X86_FEATURE_PN			( 0*32+18) /* Processor serial number */</span>
<span class="p_add">+#define X86_FEATURE_CLFLUSH		( 0*32+19) /* CLFLUSH instruction */</span>
<span class="p_add">+#define X86_FEATURE_DS			( 0*32+21) /* &quot;dts&quot; Debug Store */</span>
<span class="p_add">+#define X86_FEATURE_ACPI		( 0*32+22) /* ACPI via MSR */</span>
<span class="p_add">+#define X86_FEATURE_MMX			( 0*32+23) /* Multimedia Extensions */</span>
<span class="p_add">+#define X86_FEATURE_FXSR		( 0*32+24) /* FXSAVE/FXRSTOR, CR4.OSFXSR */</span>
<span class="p_add">+#define X86_FEATURE_XMM			( 0*32+25) /* &quot;sse&quot; */</span>
<span class="p_add">+#define X86_FEATURE_XMM2		( 0*32+26) /* &quot;sse2&quot; */</span>
<span class="p_add">+#define X86_FEATURE_SELFSNOOP		( 0*32+27) /* &quot;ss&quot; CPU self snoop */</span>
<span class="p_add">+#define X86_FEATURE_HT			( 0*32+28) /* Hyper-Threading */</span>
<span class="p_add">+#define X86_FEATURE_ACC			( 0*32+29) /* &quot;tm&quot; Automatic clock control */</span>
<span class="p_add">+#define X86_FEATURE_IA64		( 0*32+30) /* IA-64 processor */</span>
<span class="p_add">+#define X86_FEATURE_PBE			( 0*32+31) /* Pending Break Enable */</span>
 
 /* AMD-defined CPU features, CPUID level 0x80000001, word 1 */
 /* Don&#39;t duplicate feature flags which are redundant with Intel! */
<span class="p_del">-#define X86_FEATURE_SYSCALL	( 1*32+11) /* SYSCALL/SYSRET */</span>
<span class="p_del">-#define X86_FEATURE_MP		( 1*32+19) /* MP Capable. */</span>
<span class="p_del">-#define X86_FEATURE_NX		( 1*32+20) /* Execute Disable */</span>
<span class="p_del">-#define X86_FEATURE_MMXEXT	( 1*32+22) /* AMD MMX extensions */</span>
<span class="p_del">-#define X86_FEATURE_FXSR_OPT	( 1*32+25) /* FXSAVE/FXRSTOR optimizations */</span>
<span class="p_del">-#define X86_FEATURE_GBPAGES	( 1*32+26) /* &quot;pdpe1gb&quot; GB pages */</span>
<span class="p_del">-#define X86_FEATURE_RDTSCP	( 1*32+27) /* RDTSCP */</span>
<span class="p_del">-#define X86_FEATURE_LM		( 1*32+29) /* Long Mode (x86-64) */</span>
<span class="p_del">-#define X86_FEATURE_3DNOWEXT	( 1*32+30) /* AMD 3DNow! extensions */</span>
<span class="p_del">-#define X86_FEATURE_3DNOW	( 1*32+31) /* 3DNow! */</span>
<span class="p_add">+#define X86_FEATURE_SYSCALL		( 1*32+11) /* SYSCALL/SYSRET */</span>
<span class="p_add">+#define X86_FEATURE_MP			( 1*32+19) /* MP Capable */</span>
<span class="p_add">+#define X86_FEATURE_NX			( 1*32+20) /* Execute Disable */</span>
<span class="p_add">+#define X86_FEATURE_MMXEXT		( 1*32+22) /* AMD MMX extensions */</span>
<span class="p_add">+#define X86_FEATURE_FXSR_OPT		( 1*32+25) /* FXSAVE/FXRSTOR optimizations */</span>
<span class="p_add">+#define X86_FEATURE_GBPAGES		( 1*32+26) /* &quot;pdpe1gb&quot; GB pages */</span>
<span class="p_add">+#define X86_FEATURE_RDTSCP		( 1*32+27) /* RDTSCP */</span>
<span class="p_add">+#define X86_FEATURE_LM			( 1*32+29) /* Long Mode (x86-64, 64-bit support) */</span>
<span class="p_add">+#define X86_FEATURE_3DNOWEXT		( 1*32+30) /* AMD 3DNow extensions */</span>
<span class="p_add">+#define X86_FEATURE_3DNOW		( 1*32+31) /* 3DNow */</span>
 
 /* Transmeta-defined CPU features, CPUID level 0x80860001, word 2 */
<span class="p_del">-#define X86_FEATURE_RECOVERY	( 2*32+ 0) /* CPU in recovery mode */</span>
<span class="p_del">-#define X86_FEATURE_LONGRUN	( 2*32+ 1) /* Longrun power control */</span>
<span class="p_del">-#define X86_FEATURE_LRTI	( 2*32+ 3) /* LongRun table interface */</span>
<span class="p_add">+#define X86_FEATURE_RECOVERY		( 2*32+ 0) /* CPU in recovery mode */</span>
<span class="p_add">+#define X86_FEATURE_LONGRUN		( 2*32+ 1) /* Longrun power control */</span>
<span class="p_add">+#define X86_FEATURE_LRTI		( 2*32+ 3) /* LongRun table interface */</span>
 
 /* Other features, Linux-defined mapping, word 3 */
 /* This range is used for feature bits which conflict or are synthesized */
<span class="p_del">-#define X86_FEATURE_CXMMX	( 3*32+ 0) /* Cyrix MMX extensions */</span>
<span class="p_del">-#define X86_FEATURE_K6_MTRR	( 3*32+ 1) /* AMD K6 nonstandard MTRRs */</span>
<span class="p_del">-#define X86_FEATURE_CYRIX_ARR	( 3*32+ 2) /* Cyrix ARRs (= MTRRs) */</span>
<span class="p_del">-#define X86_FEATURE_CENTAUR_MCR	( 3*32+ 3) /* Centaur MCRs (= MTRRs) */</span>
<span class="p_del">-/* cpu types for specific tunings: */</span>
<span class="p_del">-#define X86_FEATURE_K8		( 3*32+ 4) /* &quot;&quot; Opteron, Athlon64 */</span>
<span class="p_del">-#define X86_FEATURE_K7		( 3*32+ 5) /* &quot;&quot; Athlon */</span>
<span class="p_del">-#define X86_FEATURE_P3		( 3*32+ 6) /* &quot;&quot; P3 */</span>
<span class="p_del">-#define X86_FEATURE_P4		( 3*32+ 7) /* &quot;&quot; P4 */</span>
<span class="p_del">-#define X86_FEATURE_CONSTANT_TSC ( 3*32+ 8) /* TSC ticks at a constant rate */</span>
<span class="p_del">-#define X86_FEATURE_UP		( 3*32+ 9) /* smp kernel running on up */</span>
<span class="p_del">-#define X86_FEATURE_ART		( 3*32+10) /* Platform has always running timer (ART) */</span>
<span class="p_del">-#define X86_FEATURE_ARCH_PERFMON ( 3*32+11) /* Intel Architectural PerfMon */</span>
<span class="p_del">-#define X86_FEATURE_PEBS	( 3*32+12) /* Precise-Event Based Sampling */</span>
<span class="p_del">-#define X86_FEATURE_BTS		( 3*32+13) /* Branch Trace Store */</span>
<span class="p_del">-#define X86_FEATURE_SYSCALL32	( 3*32+14) /* &quot;&quot; syscall in ia32 userspace */</span>
<span class="p_del">-#define X86_FEATURE_SYSENTER32	( 3*32+15) /* &quot;&quot; sysenter in ia32 userspace */</span>
<span class="p_del">-#define X86_FEATURE_REP_GOOD	( 3*32+16) /* rep microcode works well */</span>
<span class="p_del">-#define X86_FEATURE_MFENCE_RDTSC ( 3*32+17) /* &quot;&quot; Mfence synchronizes RDTSC */</span>
<span class="p_del">-#define X86_FEATURE_LFENCE_RDTSC ( 3*32+18) /* &quot;&quot; Lfence synchronizes RDTSC */</span>
<span class="p_del">-#define X86_FEATURE_ACC_POWER	( 3*32+19) /* AMD Accumulated Power Mechanism */</span>
<span class="p_del">-#define X86_FEATURE_NOPL	( 3*32+20) /* The NOPL (0F 1F) instructions */</span>
<span class="p_del">-#define X86_FEATURE_ALWAYS	( 3*32+21) /* &quot;&quot; Always-present feature */</span>
<span class="p_del">-#define X86_FEATURE_XTOPOLOGY	( 3*32+22) /* cpu topology enum extensions */</span>
<span class="p_del">-#define X86_FEATURE_TSC_RELIABLE ( 3*32+23) /* TSC is known to be reliable */</span>
<span class="p_del">-#define X86_FEATURE_NONSTOP_TSC	( 3*32+24) /* TSC does not stop in C states */</span>
<span class="p_del">-#define X86_FEATURE_CPUID	( 3*32+25) /* CPU has CPUID instruction itself */</span>
<span class="p_del">-#define X86_FEATURE_EXTD_APICID	( 3*32+26) /* has extended APICID (8 bits) */</span>
<span class="p_del">-#define X86_FEATURE_AMD_DCM     ( 3*32+27) /* multi-node processor */</span>
<span class="p_del">-#define X86_FEATURE_APERFMPERF	( 3*32+28) /* APERFMPERF */</span>
<span class="p_del">-#define X86_FEATURE_NONSTOP_TSC_S3 ( 3*32+30) /* TSC doesn&#39;t stop in S3 state */</span>
<span class="p_del">-#define X86_FEATURE_TSC_KNOWN_FREQ ( 3*32+31) /* TSC has known frequency */</span>
<span class="p_add">+#define X86_FEATURE_CXMMX		( 3*32+ 0) /* Cyrix MMX extensions */</span>
<span class="p_add">+#define X86_FEATURE_K6_MTRR		( 3*32+ 1) /* AMD K6 nonstandard MTRRs */</span>
<span class="p_add">+#define X86_FEATURE_CYRIX_ARR		( 3*32+ 2) /* Cyrix ARRs (= MTRRs) */</span>
<span class="p_add">+#define X86_FEATURE_CENTAUR_MCR		( 3*32+ 3) /* Centaur MCRs (= MTRRs) */</span>
<span class="p_add">+</span>
<span class="p_add">+/* CPU types for specific tunings: */</span>
<span class="p_add">+#define X86_FEATURE_K8			( 3*32+ 4) /* &quot;&quot; Opteron, Athlon64 */</span>
<span class="p_add">+#define X86_FEATURE_K7			( 3*32+ 5) /* &quot;&quot; Athlon */</span>
<span class="p_add">+#define X86_FEATURE_P3			( 3*32+ 6) /* &quot;&quot; P3 */</span>
<span class="p_add">+#define X86_FEATURE_P4			( 3*32+ 7) /* &quot;&quot; P4 */</span>
<span class="p_add">+#define X86_FEATURE_CONSTANT_TSC	( 3*32+ 8) /* TSC ticks at a constant rate */</span>
<span class="p_add">+#define X86_FEATURE_UP			( 3*32+ 9) /* SMP kernel running on UP */</span>
<span class="p_add">+#define X86_FEATURE_ART			( 3*32+10) /* Always running timer (ART) */</span>
<span class="p_add">+#define X86_FEATURE_ARCH_PERFMON	( 3*32+11) /* Intel Architectural PerfMon */</span>
<span class="p_add">+#define X86_FEATURE_PEBS		( 3*32+12) /* Precise-Event Based Sampling */</span>
<span class="p_add">+#define X86_FEATURE_BTS			( 3*32+13) /* Branch Trace Store */</span>
<span class="p_add">+#define X86_FEATURE_SYSCALL32		( 3*32+14) /* &quot;&quot; syscall in IA32 userspace */</span>
<span class="p_add">+#define X86_FEATURE_SYSENTER32		( 3*32+15) /* &quot;&quot; sysenter in IA32 userspace */</span>
<span class="p_add">+#define X86_FEATURE_REP_GOOD		( 3*32+16) /* REP microcode works well */</span>
<span class="p_add">+#define X86_FEATURE_MFENCE_RDTSC	( 3*32+17) /* &quot;&quot; MFENCE synchronizes RDTSC */</span>
<span class="p_add">+#define X86_FEATURE_LFENCE_RDTSC	( 3*32+18) /* &quot;&quot; LFENCE synchronizes RDTSC */</span>
<span class="p_add">+#define X86_FEATURE_ACC_POWER		( 3*32+19) /* AMD Accumulated Power Mechanism */</span>
<span class="p_add">+#define X86_FEATURE_NOPL		( 3*32+20) /* The NOPL (0F 1F) instructions */</span>
<span class="p_add">+#define X86_FEATURE_ALWAYS		( 3*32+21) /* &quot;&quot; Always-present feature */</span>
<span class="p_add">+#define X86_FEATURE_XTOPOLOGY		( 3*32+22) /* CPU topology enum extensions */</span>
<span class="p_add">+#define X86_FEATURE_TSC_RELIABLE	( 3*32+23) /* TSC is known to be reliable */</span>
<span class="p_add">+#define X86_FEATURE_NONSTOP_TSC		( 3*32+24) /* TSC does not stop in C states */</span>
<span class="p_add">+#define X86_FEATURE_CPUID		( 3*32+25) /* CPU has CPUID instruction itself */</span>
<span class="p_add">+#define X86_FEATURE_EXTD_APICID		( 3*32+26) /* Extended APICID (8 bits) */</span>
<span class="p_add">+#define X86_FEATURE_AMD_DCM		( 3*32+27) /* AMD multi-node processor */</span>
<span class="p_add">+#define X86_FEATURE_APERFMPERF		( 3*32+28) /* P-State hardware coordination feedback capability (APERF/MPERF MSRs) */</span>
<span class="p_add">+#define X86_FEATURE_NONSTOP_TSC_S3	( 3*32+30) /* TSC doesn&#39;t stop in S3 state */</span>
<span class="p_add">+#define X86_FEATURE_TSC_KNOWN_FREQ	( 3*32+31) /* TSC has known frequency */</span>
 
<span class="p_del">-/* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */</span>
<span class="p_del">-#define X86_FEATURE_XMM3	( 4*32+ 0) /* &quot;pni&quot; SSE-3 */</span>
<span class="p_del">-#define X86_FEATURE_PCLMULQDQ	( 4*32+ 1) /* PCLMULQDQ instruction */</span>
<span class="p_del">-#define X86_FEATURE_DTES64	( 4*32+ 2) /* 64-bit Debug Store */</span>
<span class="p_del">-#define X86_FEATURE_MWAIT	( 4*32+ 3) /* &quot;monitor&quot; Monitor/Mwait support */</span>
<span class="p_del">-#define X86_FEATURE_DSCPL	( 4*32+ 4) /* &quot;ds_cpl&quot; CPL Qual. Debug Store */</span>
<span class="p_del">-#define X86_FEATURE_VMX		( 4*32+ 5) /* Hardware virtualization */</span>
<span class="p_del">-#define X86_FEATURE_SMX		( 4*32+ 6) /* Safer mode */</span>
<span class="p_del">-#define X86_FEATURE_EST		( 4*32+ 7) /* Enhanced SpeedStep */</span>
<span class="p_del">-#define X86_FEATURE_TM2		( 4*32+ 8) /* Thermal Monitor 2 */</span>
<span class="p_del">-#define X86_FEATURE_SSSE3	( 4*32+ 9) /* Supplemental SSE-3 */</span>
<span class="p_del">-#define X86_FEATURE_CID		( 4*32+10) /* Context ID */</span>
<span class="p_del">-#define X86_FEATURE_SDBG	( 4*32+11) /* Silicon Debug */</span>
<span class="p_del">-#define X86_FEATURE_FMA		( 4*32+12) /* Fused multiply-add */</span>
<span class="p_del">-#define X86_FEATURE_CX16	( 4*32+13) /* CMPXCHG16B */</span>
<span class="p_del">-#define X86_FEATURE_XTPR	( 4*32+14) /* Send Task Priority Messages */</span>
<span class="p_del">-#define X86_FEATURE_PDCM	( 4*32+15) /* Performance Capabilities */</span>
<span class="p_del">-#define X86_FEATURE_PCID	( 4*32+17) /* Process Context Identifiers */</span>
<span class="p_del">-#define X86_FEATURE_DCA		( 4*32+18) /* Direct Cache Access */</span>
<span class="p_del">-#define X86_FEATURE_XMM4_1	( 4*32+19) /* &quot;sse4_1&quot; SSE-4.1 */</span>
<span class="p_del">-#define X86_FEATURE_XMM4_2	( 4*32+20) /* &quot;sse4_2&quot; SSE-4.2 */</span>
<span class="p_del">-#define X86_FEATURE_X2APIC	( 4*32+21) /* x2APIC */</span>
<span class="p_del">-#define X86_FEATURE_MOVBE	( 4*32+22) /* MOVBE instruction */</span>
<span class="p_del">-#define X86_FEATURE_POPCNT      ( 4*32+23) /* POPCNT instruction */</span>
<span class="p_del">-#define X86_FEATURE_TSC_DEADLINE_TIMER	( 4*32+24) /* Tsc deadline timer */</span>
<span class="p_del">-#define X86_FEATURE_AES		( 4*32+25) /* AES instructions */</span>
<span class="p_del">-#define X86_FEATURE_XSAVE	( 4*32+26) /* XSAVE/XRSTOR/XSETBV/XGETBV */</span>
<span class="p_del">-#define X86_FEATURE_OSXSAVE	( 4*32+27) /* &quot;&quot; XSAVE enabled in the OS */</span>
<span class="p_del">-#define X86_FEATURE_AVX		( 4*32+28) /* Advanced Vector Extensions */</span>
<span class="p_del">-#define X86_FEATURE_F16C	( 4*32+29) /* 16-bit fp conversions */</span>
<span class="p_del">-#define X86_FEATURE_RDRAND	( 4*32+30) /* The RDRAND instruction */</span>
<span class="p_del">-#define X86_FEATURE_HYPERVISOR	( 4*32+31) /* Running on a hypervisor */</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000001 (ECX), word 4 */</span>
<span class="p_add">+#define X86_FEATURE_XMM3		( 4*32+ 0) /* &quot;pni&quot; SSE-3 */</span>
<span class="p_add">+#define X86_FEATURE_PCLMULQDQ		( 4*32+ 1) /* PCLMULQDQ instruction */</span>
<span class="p_add">+#define X86_FEATURE_DTES64		( 4*32+ 2) /* 64-bit Debug Store */</span>
<span class="p_add">+#define X86_FEATURE_MWAIT		( 4*32+ 3) /* &quot;monitor&quot; MONITOR/MWAIT support */</span>
<span class="p_add">+#define X86_FEATURE_DSCPL		( 4*32+ 4) /* &quot;ds_cpl&quot; CPL-qualified (filtered) Debug Store */</span>
<span class="p_add">+#define X86_FEATURE_VMX			( 4*32+ 5) /* Hardware virtualization */</span>
<span class="p_add">+#define X86_FEATURE_SMX			( 4*32+ 6) /* Safer Mode eXtensions */</span>
<span class="p_add">+#define X86_FEATURE_EST			( 4*32+ 7) /* Enhanced SpeedStep */</span>
<span class="p_add">+#define X86_FEATURE_TM2			( 4*32+ 8) /* Thermal Monitor 2 */</span>
<span class="p_add">+#define X86_FEATURE_SSSE3		( 4*32+ 9) /* Supplemental SSE-3 */</span>
<span class="p_add">+#define X86_FEATURE_CID			( 4*32+10) /* Context ID */</span>
<span class="p_add">+#define X86_FEATURE_SDBG		( 4*32+11) /* Silicon Debug */</span>
<span class="p_add">+#define X86_FEATURE_FMA			( 4*32+12) /* Fused multiply-add */</span>
<span class="p_add">+#define X86_FEATURE_CX16		( 4*32+13) /* CMPXCHG16B instruction */</span>
<span class="p_add">+#define X86_FEATURE_XTPR		( 4*32+14) /* Send Task Priority Messages */</span>
<span class="p_add">+#define X86_FEATURE_PDCM		( 4*32+15) /* Perf/Debug Capabilities MSR */</span>
<span class="p_add">+#define X86_FEATURE_PCID		( 4*32+17) /* Process Context Identifiers */</span>
<span class="p_add">+#define X86_FEATURE_DCA			( 4*32+18) /* Direct Cache Access */</span>
<span class="p_add">+#define X86_FEATURE_XMM4_1		( 4*32+19) /* &quot;sse4_1&quot; SSE-4.1 */</span>
<span class="p_add">+#define X86_FEATURE_XMM4_2		( 4*32+20) /* &quot;sse4_2&quot; SSE-4.2 */</span>
<span class="p_add">+#define X86_FEATURE_X2APIC		( 4*32+21) /* X2APIC */</span>
<span class="p_add">+#define X86_FEATURE_MOVBE		( 4*32+22) /* MOVBE instruction */</span>
<span class="p_add">+#define X86_FEATURE_POPCNT		( 4*32+23) /* POPCNT instruction */</span>
<span class="p_add">+#define X86_FEATURE_TSC_DEADLINE_TIMER	( 4*32+24) /* TSC deadline timer */</span>
<span class="p_add">+#define X86_FEATURE_AES			( 4*32+25) /* AES instructions */</span>
<span class="p_add">+#define X86_FEATURE_XSAVE		( 4*32+26) /* XSAVE/XRSTOR/XSETBV/XGETBV instructions */</span>
<span class="p_add">+#define X86_FEATURE_OSXSAVE		( 4*32+27) /* &quot;&quot; XSAVE instruction enabled in the OS */</span>
<span class="p_add">+#define X86_FEATURE_AVX			( 4*32+28) /* Advanced Vector Extensions */</span>
<span class="p_add">+#define X86_FEATURE_F16C		( 4*32+29) /* 16-bit FP conversions */</span>
<span class="p_add">+#define X86_FEATURE_RDRAND		( 4*32+30) /* RDRAND instruction */</span>
<span class="p_add">+#define X86_FEATURE_HYPERVISOR		( 4*32+31) /* Running on a hypervisor */</span>
 
 /* VIA/Cyrix/Centaur-defined CPU features, CPUID level 0xC0000001, word 5 */
<span class="p_del">-#define X86_FEATURE_XSTORE	( 5*32+ 2) /* &quot;rng&quot; RNG present (xstore) */</span>
<span class="p_del">-#define X86_FEATURE_XSTORE_EN	( 5*32+ 3) /* &quot;rng_en&quot; RNG enabled */</span>
<span class="p_del">-#define X86_FEATURE_XCRYPT	( 5*32+ 6) /* &quot;ace&quot; on-CPU crypto (xcrypt) */</span>
<span class="p_del">-#define X86_FEATURE_XCRYPT_EN	( 5*32+ 7) /* &quot;ace_en&quot; on-CPU crypto enabled */</span>
<span class="p_del">-#define X86_FEATURE_ACE2	( 5*32+ 8) /* Advanced Cryptography Engine v2 */</span>
<span class="p_del">-#define X86_FEATURE_ACE2_EN	( 5*32+ 9) /* ACE v2 enabled */</span>
<span class="p_del">-#define X86_FEATURE_PHE		( 5*32+10) /* PadLock Hash Engine */</span>
<span class="p_del">-#define X86_FEATURE_PHE_EN	( 5*32+11) /* PHE enabled */</span>
<span class="p_del">-#define X86_FEATURE_PMM		( 5*32+12) /* PadLock Montgomery Multiplier */</span>
<span class="p_del">-#define X86_FEATURE_PMM_EN	( 5*32+13) /* PMM enabled */</span>
<span class="p_add">+#define X86_FEATURE_XSTORE		( 5*32+ 2) /* &quot;rng&quot; RNG present (xstore) */</span>
<span class="p_add">+#define X86_FEATURE_XSTORE_EN		( 5*32+ 3) /* &quot;rng_en&quot; RNG enabled */</span>
<span class="p_add">+#define X86_FEATURE_XCRYPT		( 5*32+ 6) /* &quot;ace&quot; on-CPU crypto (xcrypt) */</span>
<span class="p_add">+#define X86_FEATURE_XCRYPT_EN		( 5*32+ 7) /* &quot;ace_en&quot; on-CPU crypto enabled */</span>
<span class="p_add">+#define X86_FEATURE_ACE2		( 5*32+ 8) /* Advanced Cryptography Engine v2 */</span>
<span class="p_add">+#define X86_FEATURE_ACE2_EN		( 5*32+ 9) /* ACE v2 enabled */</span>
<span class="p_add">+#define X86_FEATURE_PHE			( 5*32+10) /* PadLock Hash Engine */</span>
<span class="p_add">+#define X86_FEATURE_PHE_EN		( 5*32+11) /* PHE enabled */</span>
<span class="p_add">+#define X86_FEATURE_PMM			( 5*32+12) /* PadLock Montgomery Multiplier */</span>
<span class="p_add">+#define X86_FEATURE_PMM_EN		( 5*32+13) /* PMM enabled */</span>
 
<span class="p_del">-/* More extended AMD flags: CPUID level 0x80000001, ecx, word 6 */</span>
<span class="p_del">-#define X86_FEATURE_LAHF_LM	( 6*32+ 0) /* LAHF/SAHF in long mode */</span>
<span class="p_del">-#define X86_FEATURE_CMP_LEGACY	( 6*32+ 1) /* If yes HyperThreading not valid */</span>
<span class="p_del">-#define X86_FEATURE_SVM		( 6*32+ 2) /* Secure virtual machine */</span>
<span class="p_del">-#define X86_FEATURE_EXTAPIC	( 6*32+ 3) /* Extended APIC space */</span>
<span class="p_del">-#define X86_FEATURE_CR8_LEGACY	( 6*32+ 4) /* CR8 in 32-bit mode */</span>
<span class="p_del">-#define X86_FEATURE_ABM		( 6*32+ 5) /* Advanced bit manipulation */</span>
<span class="p_del">-#define X86_FEATURE_SSE4A	( 6*32+ 6) /* SSE-4A */</span>
<span class="p_del">-#define X86_FEATURE_MISALIGNSSE ( 6*32+ 7) /* Misaligned SSE mode */</span>
<span class="p_del">-#define X86_FEATURE_3DNOWPREFETCH ( 6*32+ 8) /* 3DNow prefetch instructions */</span>
<span class="p_del">-#define X86_FEATURE_OSVW	( 6*32+ 9) /* OS Visible Workaround */</span>
<span class="p_del">-#define X86_FEATURE_IBS		( 6*32+10) /* Instruction Based Sampling */</span>
<span class="p_del">-#define X86_FEATURE_XOP		( 6*32+11) /* extended AVX instructions */</span>
<span class="p_del">-#define X86_FEATURE_SKINIT	( 6*32+12) /* SKINIT/STGI instructions */</span>
<span class="p_del">-#define X86_FEATURE_WDT		( 6*32+13) /* Watchdog timer */</span>
<span class="p_del">-#define X86_FEATURE_LWP		( 6*32+15) /* Light Weight Profiling */</span>
<span class="p_del">-#define X86_FEATURE_FMA4	( 6*32+16) /* 4 operands MAC instructions */</span>
<span class="p_del">-#define X86_FEATURE_TCE		( 6*32+17) /* translation cache extension */</span>
<span class="p_del">-#define X86_FEATURE_NODEID_MSR	( 6*32+19) /* NodeId MSR */</span>
<span class="p_del">-#define X86_FEATURE_TBM		( 6*32+21) /* trailing bit manipulations */</span>
<span class="p_del">-#define X86_FEATURE_TOPOEXT	( 6*32+22) /* topology extensions CPUID leafs */</span>
<span class="p_del">-#define X86_FEATURE_PERFCTR_CORE ( 6*32+23) /* core performance counter extensions */</span>
<span class="p_del">-#define X86_FEATURE_PERFCTR_NB  ( 6*32+24) /* NB performance counter extensions */</span>
<span class="p_del">-#define X86_FEATURE_BPEXT	(6*32+26) /* data breakpoint extension */</span>
<span class="p_del">-#define X86_FEATURE_PTSC	( 6*32+27) /* performance time-stamp counter */</span>
<span class="p_del">-#define X86_FEATURE_PERFCTR_LLC	( 6*32+28) /* Last Level Cache performance counter extensions */</span>
<span class="p_del">-#define X86_FEATURE_MWAITX	( 6*32+29) /* MWAIT extension (MONITORX/MWAITX) */</span>
<span class="p_add">+/* More extended AMD flags: CPUID level 0x80000001, ECX, word 6 */</span>
<span class="p_add">+#define X86_FEATURE_LAHF_LM		( 6*32+ 0) /* LAHF/SAHF in long mode */</span>
<span class="p_add">+#define X86_FEATURE_CMP_LEGACY		( 6*32+ 1) /* If yes HyperThreading not valid */</span>
<span class="p_add">+#define X86_FEATURE_SVM			( 6*32+ 2) /* Secure Virtual Machine */</span>
<span class="p_add">+#define X86_FEATURE_EXTAPIC		( 6*32+ 3) /* Extended APIC space */</span>
<span class="p_add">+#define X86_FEATURE_CR8_LEGACY		( 6*32+ 4) /* CR8 in 32-bit mode */</span>
<span class="p_add">+#define X86_FEATURE_ABM			( 6*32+ 5) /* Advanced bit manipulation */</span>
<span class="p_add">+#define X86_FEATURE_SSE4A		( 6*32+ 6) /* SSE-4A */</span>
<span class="p_add">+#define X86_FEATURE_MISALIGNSSE		( 6*32+ 7) /* Misaligned SSE mode */</span>
<span class="p_add">+#define X86_FEATURE_3DNOWPREFETCH	( 6*32+ 8) /* 3DNow prefetch instructions */</span>
<span class="p_add">+#define X86_FEATURE_OSVW		( 6*32+ 9) /* OS Visible Workaround */</span>
<span class="p_add">+#define X86_FEATURE_IBS			( 6*32+10) /* Instruction Based Sampling */</span>
<span class="p_add">+#define X86_FEATURE_XOP			( 6*32+11) /* extended AVX instructions */</span>
<span class="p_add">+#define X86_FEATURE_SKINIT		( 6*32+12) /* SKINIT/STGI instructions */</span>
<span class="p_add">+#define X86_FEATURE_WDT			( 6*32+13) /* Watchdog timer */</span>
<span class="p_add">+#define X86_FEATURE_LWP			( 6*32+15) /* Light Weight Profiling */</span>
<span class="p_add">+#define X86_FEATURE_FMA4		( 6*32+16) /* 4 operands MAC instructions */</span>
<span class="p_add">+#define X86_FEATURE_TCE			( 6*32+17) /* Translation Cache Extension */</span>
<span class="p_add">+#define X86_FEATURE_NODEID_MSR		( 6*32+19) /* NodeId MSR */</span>
<span class="p_add">+#define X86_FEATURE_TBM			( 6*32+21) /* Trailing Bit Manipulations */</span>
<span class="p_add">+#define X86_FEATURE_TOPOEXT		( 6*32+22) /* Topology extensions CPUID leafs */</span>
<span class="p_add">+#define X86_FEATURE_PERFCTR_CORE	( 6*32+23) /* Core performance counter extensions */</span>
<span class="p_add">+#define X86_FEATURE_PERFCTR_NB		( 6*32+24) /* NB performance counter extensions */</span>
<span class="p_add">+#define X86_FEATURE_BPEXT		( 6*32+26) /* Data breakpoint extension */</span>
<span class="p_add">+#define X86_FEATURE_PTSC		( 6*32+27) /* Performance time-stamp counter */</span>
<span class="p_add">+#define X86_FEATURE_PERFCTR_LLC		( 6*32+28) /* Last Level Cache performance counter extensions */</span>
<span class="p_add">+#define X86_FEATURE_MWAITX		( 6*32+29) /* MWAIT extension (MONITORX/MWAITX instructions) */</span>
 
 /*
  * Auxiliary flags: Linux defined - For features scattered in various
<span class="p_chunk">@@ -187,146 +190,155 @@</span> <span class="p_context"></span>
  *
  * Reuse free bits when adding new feature flags!
  */
<span class="p_del">-#define X86_FEATURE_RING3MWAIT	( 7*32+ 0) /* Ring 3 MONITOR/MWAIT */</span>
<span class="p_del">-#define X86_FEATURE_CPUID_FAULT ( 7*32+ 1) /* Intel CPUID faulting */</span>
<span class="p_del">-#define X86_FEATURE_CPB		( 7*32+ 2) /* AMD Core Performance Boost */</span>
<span class="p_del">-#define X86_FEATURE_EPB		( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */</span>
<span class="p_del">-#define X86_FEATURE_CAT_L3	( 7*32+ 4) /* Cache Allocation Technology L3 */</span>
<span class="p_del">-#define X86_FEATURE_CAT_L2	( 7*32+ 5) /* Cache Allocation Technology L2 */</span>
<span class="p_del">-#define X86_FEATURE_CDP_L3	( 7*32+ 6) /* Code and Data Prioritization L3 */</span>
<span class="p_add">+#define X86_FEATURE_RING3MWAIT		( 7*32+ 0) /* Ring 3 MONITOR/MWAIT instructions */</span>
<span class="p_add">+#define X86_FEATURE_CPUID_FAULT		( 7*32+ 1) /* Intel CPUID faulting */</span>
<span class="p_add">+#define X86_FEATURE_CPB			( 7*32+ 2) /* AMD Core Performance Boost */</span>
<span class="p_add">+#define X86_FEATURE_EPB			( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */</span>
<span class="p_add">+#define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */</span>
<span class="p_add">+#define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */</span>
<span class="p_add">+#define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */</span>
 
<span class="p_del">-#define X86_FEATURE_HW_PSTATE	( 7*32+ 8) /* AMD HW-PState */</span>
<span class="p_del">-#define X86_FEATURE_PROC_FEEDBACK ( 7*32+ 9) /* AMD ProcFeedbackInterface */</span>
<span class="p_del">-#define X86_FEATURE_SME		( 7*32+10) /* AMD Secure Memory Encryption */</span>
<span class="p_add">+#define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */</span>
<span class="p_add">+#define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */</span>
<span class="p_add">+#define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */</span>
 
<span class="p_del">-#define X86_FEATURE_INTEL_PPIN	( 7*32+14) /* Intel Processor Inventory Number */</span>
<span class="p_del">-#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */</span>
<span class="p_del">-#define X86_FEATURE_AVX512_4VNNIW (7*32+16) /* AVX-512 Neural Network Instructions */</span>
<span class="p_del">-#define X86_FEATURE_AVX512_4FMAPS (7*32+17) /* AVX-512 Multiply Accumulation Single precision */</span>
<span class="p_add">+#define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */</span>
<span class="p_add">+#define X86_FEATURE_INTEL_PT		( 7*32+15) /* Intel Processor Trace */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_4VNNIW	( 7*32+16) /* AVX-512 Neural Network Instructions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_4FMAPS	( 7*32+17) /* AVX-512 Multiply Accumulation Single precision */</span>
 
<span class="p_del">-#define X86_FEATURE_MBA         ( 7*32+18) /* Memory Bandwidth Allocation */</span>
<span class="p_add">+#define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */</span>
 
 /* Virtualization flags: Linux defined, word 8 */
<span class="p_del">-#define X86_FEATURE_TPR_SHADOW  ( 8*32+ 0) /* Intel TPR Shadow */</span>
<span class="p_del">-#define X86_FEATURE_VNMI        ( 8*32+ 1) /* Intel Virtual NMI */</span>
<span class="p_del">-#define X86_FEATURE_FLEXPRIORITY ( 8*32+ 2) /* Intel FlexPriority */</span>
<span class="p_del">-#define X86_FEATURE_EPT         ( 8*32+ 3) /* Intel Extended Page Table */</span>
<span class="p_del">-#define X86_FEATURE_VPID        ( 8*32+ 4) /* Intel Virtual Processor ID */</span>
<span class="p_add">+#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */</span>
<span class="p_add">+#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */</span>
<span class="p_add">+#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */</span>
<span class="p_add">+#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */</span>
<span class="p_add">+#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */</span>
 
<span class="p_del">-#define X86_FEATURE_VMMCALL     ( 8*32+15) /* Prefer vmmcall to vmcall */</span>
<span class="p_del">-#define X86_FEATURE_XENPV       ( 8*32+16) /* &quot;&quot; Xen paravirtual guest */</span>
<span class="p_add">+#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */</span>
<span class="p_add">+#define X86_FEATURE_XENPV		( 8*32+16) /* &quot;&quot; Xen paravirtual guest */</span>
 
 
<span class="p_del">-/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */</span>
<span class="p_del">-#define X86_FEATURE_FSGSBASE	( 9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/</span>
<span class="p_del">-#define X86_FEATURE_TSC_ADJUST	( 9*32+ 1) /* TSC adjustment MSR 0x3b */</span>
<span class="p_del">-#define X86_FEATURE_BMI1	( 9*32+ 3) /* 1st group bit manipulation extensions */</span>
<span class="p_del">-#define X86_FEATURE_HLE		( 9*32+ 4) /* Hardware Lock Elision */</span>
<span class="p_del">-#define X86_FEATURE_AVX2	( 9*32+ 5) /* AVX2 instructions */</span>
<span class="p_del">-#define X86_FEATURE_SMEP	( 9*32+ 7) /* Supervisor Mode Execution Protection */</span>
<span class="p_del">-#define X86_FEATURE_BMI2	( 9*32+ 8) /* 2nd group bit manipulation extensions */</span>
<span class="p_del">-#define X86_FEATURE_ERMS	( 9*32+ 9) /* Enhanced REP MOVSB/STOSB */</span>
<span class="p_del">-#define X86_FEATURE_INVPCID	( 9*32+10) /* Invalidate Processor Context ID */</span>
<span class="p_del">-#define X86_FEATURE_RTM		( 9*32+11) /* Restricted Transactional Memory */</span>
<span class="p_del">-#define X86_FEATURE_CQM		( 9*32+12) /* Cache QoS Monitoring */</span>
<span class="p_del">-#define X86_FEATURE_MPX		( 9*32+14) /* Memory Protection Extension */</span>
<span class="p_del">-#define X86_FEATURE_RDT_A	( 9*32+15) /* Resource Director Technology Allocation */</span>
<span class="p_del">-#define X86_FEATURE_AVX512F	( 9*32+16) /* AVX-512 Foundation */</span>
<span class="p_del">-#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */</span>
<span class="p_del">-#define X86_FEATURE_RDSEED	( 9*32+18) /* The RDSEED instruction */</span>
<span class="p_del">-#define X86_FEATURE_ADX		( 9*32+19) /* The ADCX and ADOX instructions */</span>
<span class="p_del">-#define X86_FEATURE_SMAP	( 9*32+20) /* Supervisor Mode Access Prevention */</span>
<span class="p_del">-#define X86_FEATURE_AVX512IFMA  ( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */</span>
<span class="p_del">-#define X86_FEATURE_CLFLUSHOPT	( 9*32+23) /* CLFLUSHOPT instruction */</span>
<span class="p_del">-#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */</span>
<span class="p_del">-#define X86_FEATURE_AVX512PF	( 9*32+26) /* AVX-512 Prefetch */</span>
<span class="p_del">-#define X86_FEATURE_AVX512ER	( 9*32+27) /* AVX-512 Exponential and Reciprocal */</span>
<span class="p_del">-#define X86_FEATURE_AVX512CD	( 9*32+28) /* AVX-512 Conflict Detection */</span>
<span class="p_del">-#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */</span>
<span class="p_del">-#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */</span>
<span class="p_del">-#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */</span>
<span class="p_add">+#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/</span>
<span class="p_add">+#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */</span>
<span class="p_add">+#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */</span>
<span class="p_add">+#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */</span>
<span class="p_add">+#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */</span>
<span class="p_add">+#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */</span>
<span class="p_add">+#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */</span>
<span class="p_add">+#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */</span>
<span class="p_add">+#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */</span>
<span class="p_add">+#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */</span>
<span class="p_add">+#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */</span>
<span class="p_add">+#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */</span>
<span class="p_add">+#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */</span>
<span class="p_add">+#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */</span>
<span class="p_add">+#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */</span>
<span class="p_add">+#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */</span>
<span class="p_add">+#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */</span>
<span class="p_add">+#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */</span>
<span class="p_add">+#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */</span>
<span class="p_add">+#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */</span>
<span class="p_add">+#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */</span>
<span class="p_add">+#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */</span>
<span class="p_add">+#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */</span>
<span class="p_add">+#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */</span>
<span class="p_add">+#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */</span>
 
<span class="p_del">-/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */</span>
<span class="p_del">-#define X86_FEATURE_XSAVEOPT	(10*32+ 0) /* XSAVEOPT */</span>
<span class="p_del">-#define X86_FEATURE_XSAVEC	(10*32+ 1) /* XSAVEC */</span>
<span class="p_del">-#define X86_FEATURE_XGETBV1	(10*32+ 2) /* XGETBV with ECX = 1 */</span>
<span class="p_del">-#define X86_FEATURE_XSAVES	(10*32+ 3) /* XSAVES/XRSTORS */</span>
<span class="p_add">+/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */</span>
<span class="p_add">+#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */</span>
<span class="p_add">+#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */</span>
<span class="p_add">+#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */</span>
<span class="p_add">+#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */</span>
 
<span class="p_del">-/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */</span>
<span class="p_del">-#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */</span>
<span class="p_add">+/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (EDX), word 11 */</span>
<span class="p_add">+#define X86_FEATURE_CQM_LLC		(11*32+ 1) /* LLC QoS if 1 */</span>
 
<span class="p_del">-/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */</span>
<span class="p_del">-#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */</span>
<span class="p_del">-#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */</span>
<span class="p_del">-#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */</span>
<span class="p_add">+/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (EDX), word 12 */</span>
<span class="p_add">+#define X86_FEATURE_CQM_OCCUP_LLC	(12*32+ 0) /* LLC occupancy monitoring */</span>
<span class="p_add">+#define X86_FEATURE_CQM_MBM_TOTAL	(12*32+ 1) /* LLC Total MBM monitoring */</span>
<span class="p_add">+#define X86_FEATURE_CQM_MBM_LOCAL	(12*32+ 2) /* LLC Local MBM monitoring */</span>
 
<span class="p_del">-/* AMD-defined CPU features, CPUID level 0x80000008 (ebx), word 13 */</span>
<span class="p_del">-#define X86_FEATURE_CLZERO	(13*32+0) /* CLZERO instruction */</span>
<span class="p_del">-#define X86_FEATURE_IRPERF	(13*32+1) /* Instructions Retired Count */</span>
<span class="p_add">+/* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */</span>
<span class="p_add">+#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */</span>
<span class="p_add">+#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */</span>
<span class="p_add">+#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */</span>
 
<span class="p_del">-/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */</span>
<span class="p_del">-#define X86_FEATURE_DTHERM	(14*32+ 0) /* Digital Thermal Sensor */</span>
<span class="p_del">-#define X86_FEATURE_IDA		(14*32+ 1) /* Intel Dynamic Acceleration */</span>
<span class="p_del">-#define X86_FEATURE_ARAT	(14*32+ 2) /* Always Running APIC Timer */</span>
<span class="p_del">-#define X86_FEATURE_PLN		(14*32+ 4) /* Intel Power Limit Notification */</span>
<span class="p_del">-#define X86_FEATURE_PTS		(14*32+ 6) /* Intel Package Thermal Status */</span>
<span class="p_del">-#define X86_FEATURE_HWP		(14*32+ 7) /* Intel Hardware P-states */</span>
<span class="p_del">-#define X86_FEATURE_HWP_NOTIFY	(14*32+ 8) /* HWP Notification */</span>
<span class="p_del">-#define X86_FEATURE_HWP_ACT_WINDOW (14*32+ 9) /* HWP Activity Window */</span>
<span class="p_del">-#define X86_FEATURE_HWP_EPP	(14*32+10) /* HWP Energy Perf. Preference */</span>
<span class="p_del">-#define X86_FEATURE_HWP_PKG_REQ (14*32+11) /* HWP Package Level Request */</span>
<span class="p_add">+/* Thermal and Power Management Leaf, CPUID level 0x00000006 (EAX), word 14 */</span>
<span class="p_add">+#define X86_FEATURE_DTHERM		(14*32+ 0) /* Digital Thermal Sensor */</span>
<span class="p_add">+#define X86_FEATURE_IDA			(14*32+ 1) /* Intel Dynamic Acceleration */</span>
<span class="p_add">+#define X86_FEATURE_ARAT		(14*32+ 2) /* Always Running APIC Timer */</span>
<span class="p_add">+#define X86_FEATURE_PLN			(14*32+ 4) /* Intel Power Limit Notification */</span>
<span class="p_add">+#define X86_FEATURE_PTS			(14*32+ 6) /* Intel Package Thermal Status */</span>
<span class="p_add">+#define X86_FEATURE_HWP			(14*32+ 7) /* Intel Hardware P-states */</span>
<span class="p_add">+#define X86_FEATURE_HWP_NOTIFY		(14*32+ 8) /* HWP Notification */</span>
<span class="p_add">+#define X86_FEATURE_HWP_ACT_WINDOW	(14*32+ 9) /* HWP Activity Window */</span>
<span class="p_add">+#define X86_FEATURE_HWP_EPP		(14*32+10) /* HWP Energy Perf. Preference */</span>
<span class="p_add">+#define X86_FEATURE_HWP_PKG_REQ		(14*32+11) /* HWP Package Level Request */</span>
 
<span class="p_del">-/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */</span>
<span class="p_del">-#define X86_FEATURE_NPT		(15*32+ 0) /* Nested Page Table support */</span>
<span class="p_del">-#define X86_FEATURE_LBRV	(15*32+ 1) /* LBR Virtualization support */</span>
<span class="p_del">-#define X86_FEATURE_SVML	(15*32+ 2) /* &quot;svm_lock&quot; SVM locking MSR */</span>
<span class="p_del">-#define X86_FEATURE_NRIPS	(15*32+ 3) /* &quot;nrip_save&quot; SVM next_rip save */</span>
<span class="p_del">-#define X86_FEATURE_TSCRATEMSR  (15*32+ 4) /* &quot;tsc_scale&quot; TSC scaling support */</span>
<span class="p_del">-#define X86_FEATURE_VMCBCLEAN   (15*32+ 5) /* &quot;vmcb_clean&quot; VMCB clean bits support */</span>
<span class="p_del">-#define X86_FEATURE_FLUSHBYASID (15*32+ 6) /* flush-by-ASID support */</span>
<span class="p_del">-#define X86_FEATURE_DECODEASSISTS (15*32+ 7) /* Decode Assists support */</span>
<span class="p_del">-#define X86_FEATURE_PAUSEFILTER (15*32+10) /* filtered pause intercept */</span>
<span class="p_del">-#define X86_FEATURE_PFTHRESHOLD (15*32+12) /* pause filter threshold */</span>
<span class="p_del">-#define X86_FEATURE_AVIC	(15*32+13) /* Virtual Interrupt Controller */</span>
<span class="p_del">-#define X86_FEATURE_V_VMSAVE_VMLOAD (15*32+15) /* Virtual VMSAVE VMLOAD */</span>
<span class="p_del">-#define X86_FEATURE_VGIF	(15*32+16) /* Virtual GIF */</span>
<span class="p_add">+/* AMD SVM Feature Identification, CPUID level 0x8000000a (EDX), word 15 */</span>
<span class="p_add">+#define X86_FEATURE_NPT			(15*32+ 0) /* Nested Page Table support */</span>
<span class="p_add">+#define X86_FEATURE_LBRV		(15*32+ 1) /* LBR Virtualization support */</span>
<span class="p_add">+#define X86_FEATURE_SVML		(15*32+ 2) /* &quot;svm_lock&quot; SVM locking MSR */</span>
<span class="p_add">+#define X86_FEATURE_NRIPS		(15*32+ 3) /* &quot;nrip_save&quot; SVM next_rip save */</span>
<span class="p_add">+#define X86_FEATURE_TSCRATEMSR		(15*32+ 4) /* &quot;tsc_scale&quot; TSC scaling support */</span>
<span class="p_add">+#define X86_FEATURE_VMCBCLEAN		(15*32+ 5) /* &quot;vmcb_clean&quot; VMCB clean bits support */</span>
<span class="p_add">+#define X86_FEATURE_FLUSHBYASID		(15*32+ 6) /* flush-by-ASID support */</span>
<span class="p_add">+#define X86_FEATURE_DECODEASSISTS	(15*32+ 7) /* Decode Assists support */</span>
<span class="p_add">+#define X86_FEATURE_PAUSEFILTER		(15*32+10) /* filtered pause intercept */</span>
<span class="p_add">+#define X86_FEATURE_PFTHRESHOLD		(15*32+12) /* pause filter threshold */</span>
<span class="p_add">+#define X86_FEATURE_AVIC		(15*32+13) /* Virtual Interrupt Controller */</span>
<span class="p_add">+#define X86_FEATURE_V_VMSAVE_VMLOAD	(15*32+15) /* Virtual VMSAVE VMLOAD */</span>
<span class="p_add">+#define X86_FEATURE_VGIF		(15*32+16) /* Virtual GIF */</span>
 
<span class="p_del">-/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */</span>
<span class="p_del">-#define X86_FEATURE_AVX512VBMI  (16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/</span>
<span class="p_del">-#define X86_FEATURE_PKU		(16*32+ 3) /* Protection Keys for Userspace */</span>
<span class="p_del">-#define X86_FEATURE_OSPKE	(16*32+ 4) /* OS Protection Keys Enable */</span>
<span class="p_del">-#define X86_FEATURE_AVX512_VPOPCNTDQ (16*32+14) /* POPCNT for vectors of DW/QW */</span>
<span class="p_del">-#define X86_FEATURE_LA57	(16*32+16) /* 5-level page tables */</span>
<span class="p_del">-#define X86_FEATURE_RDPID	(16*32+22) /* RDPID instruction */</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000007:0 (ECX), word 16 */</span>
<span class="p_add">+#define X86_FEATURE_AVX512VBMI		(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/</span>
<span class="p_add">+#define X86_FEATURE_UMIP		(16*32+ 2) /* User Mode Instruction Protection */</span>
<span class="p_add">+#define X86_FEATURE_PKU			(16*32+ 3) /* Protection Keys for Userspace */</span>
<span class="p_add">+#define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */</span>
<span class="p_add">+#define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */</span>
<span class="p_add">+#define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */</span>
<span class="p_add">+#define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_VNNI		(16*32+11) /* Vector Neural Network Instructions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_BITALG	(16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB instructions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512_VPOPCNTDQ	(16*32+14) /* POPCNT for vectors of DW/QW */</span>
<span class="p_add">+#define X86_FEATURE_LA57		(16*32+16) /* 5-level page tables */</span>
<span class="p_add">+#define X86_FEATURE_RDPID		(16*32+22) /* RDPID instruction */</span>
 
<span class="p_del">-/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */</span>
<span class="p_del">-#define X86_FEATURE_OVERFLOW_RECOV (17*32+0) /* MCA overflow recovery support */</span>
<span class="p_del">-#define X86_FEATURE_SUCCOR	(17*32+1) /* Uncorrectable error containment and recovery */</span>
<span class="p_del">-#define X86_FEATURE_SMCA	(17*32+3) /* Scalable MCA */</span>
<span class="p_add">+/* AMD-defined CPU features, CPUID level 0x80000007 (EBX), word 17 */</span>
<span class="p_add">+#define X86_FEATURE_OVERFLOW_RECOV	(17*32+ 0) /* MCA overflow recovery support */</span>
<span class="p_add">+#define X86_FEATURE_SUCCOR		(17*32+ 1) /* Uncorrectable error containment and recovery */</span>
<span class="p_add">+#define X86_FEATURE_SMCA		(17*32+ 3) /* Scalable MCA */</span>
 
 /*
  * BUG word(s)
  */
<span class="p_del">-#define X86_BUG(x)		(NCAPINTS*32 + (x))</span>
<span class="p_add">+#define X86_BUG(x)			(NCAPINTS*32 + (x))</span>
 
<span class="p_del">-#define X86_BUG_F00F		X86_BUG(0) /* Intel F00F */</span>
<span class="p_del">-#define X86_BUG_FDIV		X86_BUG(1) /* FPU FDIV */</span>
<span class="p_del">-#define X86_BUG_COMA		X86_BUG(2) /* Cyrix 6x86 coma */</span>
<span class="p_del">-#define X86_BUG_AMD_TLB_MMATCH	X86_BUG(3) /* &quot;tlb_mmatch&quot; AMD Erratum 383 */</span>
<span class="p_del">-#define X86_BUG_AMD_APIC_C1E	X86_BUG(4) /* &quot;apic_c1e&quot; AMD Erratum 400 */</span>
<span class="p_del">-#define X86_BUG_11AP		X86_BUG(5) /* Bad local APIC aka 11AP */</span>
<span class="p_del">-#define X86_BUG_FXSAVE_LEAK	X86_BUG(6) /* FXSAVE leaks FOP/FIP/FOP */</span>
<span class="p_del">-#define X86_BUG_CLFLUSH_MONITOR	X86_BUG(7) /* AAI65, CLFLUSH required before MONITOR */</span>
<span class="p_del">-#define X86_BUG_SYSRET_SS_ATTRS	X86_BUG(8) /* SYSRET doesn&#39;t fix up SS attrs */</span>
<span class="p_add">+#define X86_BUG_F00F			X86_BUG(0) /* Intel F00F */</span>
<span class="p_add">+#define X86_BUG_FDIV			X86_BUG(1) /* FPU FDIV */</span>
<span class="p_add">+#define X86_BUG_COMA			X86_BUG(2) /* Cyrix 6x86 coma */</span>
<span class="p_add">+#define X86_BUG_AMD_TLB_MMATCH		X86_BUG(3) /* &quot;tlb_mmatch&quot; AMD Erratum 383 */</span>
<span class="p_add">+#define X86_BUG_AMD_APIC_C1E		X86_BUG(4) /* &quot;apic_c1e&quot; AMD Erratum 400 */</span>
<span class="p_add">+#define X86_BUG_11AP			X86_BUG(5) /* Bad local APIC aka 11AP */</span>
<span class="p_add">+#define X86_BUG_FXSAVE_LEAK		X86_BUG(6) /* FXSAVE leaks FOP/FIP/FOP */</span>
<span class="p_add">+#define X86_BUG_CLFLUSH_MONITOR		X86_BUG(7) /* AAI65, CLFLUSH required before MONITOR */</span>
<span class="p_add">+#define X86_BUG_SYSRET_SS_ATTRS		X86_BUG(8) /* SYSRET doesn&#39;t fix up SS attrs */</span>
 #ifdef CONFIG_X86_32
 /*
  * 64-bit kernels don&#39;t use X86_BUG_ESPFIX.  Make the define conditional
  * to avoid confusion.
  */
<span class="p_del">-#define X86_BUG_ESPFIX		X86_BUG(9) /* &quot;&quot; IRET to 16-bit SS corrupts ESP/RSP high bits */</span>
<span class="p_add">+#define X86_BUG_ESPFIX			X86_BUG(9) /* &quot;&quot; IRET to 16-bit SS corrupts ESP/RSP high bits */</span>
 #endif
<span class="p_del">-#define X86_BUG_NULL_SEG	X86_BUG(10) /* Nulling a selector preserves the base */</span>
<span class="p_del">-#define X86_BUG_SWAPGS_FENCE	X86_BUG(11) /* SWAPGS without input dep on GS */</span>
<span class="p_del">-#define X86_BUG_MONITOR		X86_BUG(12) /* IPI required to wake up remote CPU */</span>
<span class="p_del">-#define X86_BUG_AMD_E400	X86_BUG(13) /* CPU is among the affected by Erratum 400 */</span>
<span class="p_add">+#define X86_BUG_NULL_SEG		X86_BUG(10) /* Nulling a selector preserves the base */</span>
<span class="p_add">+#define X86_BUG_SWAPGS_FENCE		X86_BUG(11) /* SWAPGS without input dep on GS */</span>
<span class="p_add">+#define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */</span>
<span class="p_add">+#define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */</span>
<span class="p_add">+</span>
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index 0a3e808b9123..2ace1f90d138 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -60,17 +60,10 @@</span> <span class="p_context"> static inline struct desc_struct *get_current_gdt_rw(void)</span>
 	return this_cpu_ptr(&amp;gdt_page)-&gt;gdt;
 }
 
<span class="p_del">-/* Get the fixmap index for a specific processor */</span>
<span class="p_del">-static inline unsigned int get_cpu_gdt_ro_index(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return FIX_GDT_REMAP_BEGIN + cpu;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /* Provide the fixmap address of the remapped GDT */
 static inline struct desc_struct *get_cpu_gdt_ro(int cpu)
 {
<span class="p_del">-	unsigned int idx = get_cpu_gdt_ro_index(cpu);</span>
<span class="p_del">-	return (struct desc_struct *)__fix_to_virt(idx);</span>
<span class="p_add">+	return (struct desc_struct *)&amp;get_cpu_entry_area(cpu)-&gt;gdt;</span>
 }
 
 /* Provide the current read-only GDT */
<span class="p_chunk">@@ -185,7 +178,7 @@</span> <span class="p_context"> static inline void set_tssldt_descriptor(void *d, unsigned long addr,</span>
 #endif
 }
 
<span class="p_del">-static inline void __set_tss_desc(unsigned cpu, unsigned int entry, void *addr)</span>
<span class="p_add">+static inline void __set_tss_desc(unsigned cpu, unsigned int entry, struct x86_hw_tss *addr)</span>
 {
 	struct desc_struct *d = get_cpu_gdt_rw(cpu);
 	tss_desc tss;
<span class="p_header">diff --git a/arch/x86/include/asm/fixmap.h b/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">index dcd9fb55e679..94fc4fa14127 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -44,6 +44,45 @@</span> <span class="p_context"> extern unsigned long __FIXADDR_TOP;</span>
 			 PAGE_SIZE)
 #endif
 
<span class="p_add">+/*</span>
<span class="p_add">+ * cpu_entry_area is a percpu region in the fixmap that contains things</span>
<span class="p_add">+ * needed by the CPU and early entry/exit code.  Real types aren&#39;t used</span>
<span class="p_add">+ * for all fields here to avoid circular header dependencies.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Every field is a virtual alias of some other allocated backing store.</span>
<span class="p_add">+ * There is no direct allocation of a struct cpu_entry_area.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct cpu_entry_area {</span>
<span class="p_add">+	char gdt[PAGE_SIZE];</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The GDT is just below SYSENTER_stack and thus serves (on x86_64) as</span>
<span class="p_add">+	 * a a read-only guard page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct SYSENTER_stack_page SYSENTER_stack_page;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * On x86_64, the TSS is mapped RO.  On x86_32, it&#39;s mapped RW because</span>
<span class="p_add">+	 * we need task switches to work, and task switches write to the TSS.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct tss_struct tss;</span>
<span class="p_add">+</span>
<span class="p_add">+	char entry_trampoline[PAGE_SIZE];</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Exception stacks used for IST entries.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * In the future, this should have a separate slot for each stack</span>
<span class="p_add">+	 * with guard pages between them.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	char exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PAGES (sizeof(struct cpu_entry_area) / PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+extern void setup_cpu_entry_areas(void);</span>
 
 /*
  * Here we define all the compile-time &#39;special&#39; virtual
<span class="p_chunk">@@ -101,8 +140,14 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 	FIX_LNW_VRTC,
 #endif
 	/* Fixmap entries to remap the GDTs, one per processor. */
<span class="p_del">-	FIX_GDT_REMAP_BEGIN,</span>
<span class="p_del">-	FIX_GDT_REMAP_END = FIX_GDT_REMAP_BEGIN + NR_CPUS - 1,</span>
<span class="p_add">+	FIX_CPU_ENTRY_AREA_TOP,</span>
<span class="p_add">+	FIX_CPU_ENTRY_AREA_BOTTOM = FIX_CPU_ENTRY_AREA_TOP + (CPU_ENTRY_AREA_PAGES * NR_CPUS) - 1,</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ACPI_APEI_GHES</span>
<span class="p_add">+	/* Used for GHES mapping from assorted contexts */</span>
<span class="p_add">+	FIX_APEI_GHES_IRQ,</span>
<span class="p_add">+	FIX_APEI_GHES_NMI,</span>
<span class="p_add">+#endif</span>
 
 	__end_of_permanent_fixed_addresses,
 
<span class="p_chunk">@@ -185,5 +230,30 @@</span> <span class="p_context"> void __init *early_memremap_decrypted_wp(resource_size_t phys_addr,</span>
 void __early_set_fixmap(enum fixed_addresses idx,
 			phys_addr_t phys, pgprot_t flags);
 
<span class="p_add">+static inline unsigned int __get_cpu_entry_area_page_index(int cpu, int page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return FIX_CPU_ENTRY_AREA_BOTTOM - cpu*CPU_ENTRY_AREA_PAGES - page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __get_cpu_entry_area_offset_index(cpu, offset) ({		\</span>
<span class="p_add">+	BUILD_BUG_ON(offset % PAGE_SIZE != 0);				\</span>
<span class="p_add">+	__get_cpu_entry_area_page_index(cpu, offset / PAGE_SIZE);	\</span>
<span class="p_add">+	})</span>
<span class="p_add">+</span>
<span class="p_add">+#define get_cpu_entry_area_index(cpu, field)				\</span>
<span class="p_add">+	__get_cpu_entry_area_offset_index((cpu), offsetof(struct cpu_entry_area, field))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct cpu_entry_area *get_cpu_entry_area(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (struct cpu_entry_area *)__fix_to_virt(__get_cpu_entry_area_page_index(cpu, 0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct SYSENTER_stack *cpu_SYSENTER_stack(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;get_cpu_entry_area(cpu)-&gt;SYSENTER_stack_page.stack;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* !__ASSEMBLY__ */
 #endif /* _ASM_X86_FIXMAP_H */
<span class="p_header">diff --git a/arch/x86/include/asm/hypervisor.h b/arch/x86/include/asm/hypervisor.h</span>
<span class="p_header">index 0ead9dbb9130..96aa6b9884dc 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hypervisor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hypervisor.h</span>
<span class="p_chunk">@@ -20,14 +20,22 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_HYPERVISOR_H
 #define _ASM_X86_HYPERVISOR_H
 
<span class="p_add">+/* x86 hypervisor types  */</span>
<span class="p_add">+enum x86_hypervisor_type {</span>
<span class="p_add">+	X86_HYPER_NATIVE = 0,</span>
<span class="p_add">+	X86_HYPER_VMWARE,</span>
<span class="p_add">+	X86_HYPER_MS_HYPERV,</span>
<span class="p_add">+	X86_HYPER_XEN_PV,</span>
<span class="p_add">+	X86_HYPER_XEN_HVM,</span>
<span class="p_add">+	X86_HYPER_KVM,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HYPERVISOR_GUEST
 
 #include &lt;asm/kvm_para.h&gt;
<span class="p_add">+#include &lt;asm/x86_init.h&gt;</span>
 #include &lt;asm/xen/hypervisor.h&gt;
 
<span class="p_del">-/*</span>
<span class="p_del">- * x86 hypervisor information</span>
<span class="p_del">- */</span>
 struct hypervisor_x86 {
 	/* Hypervisor name */
 	const char	*name;
<span class="p_chunk">@@ -35,40 +43,27 @@</span> <span class="p_context"> struct hypervisor_x86 {</span>
 	/* Detection routine */
 	uint32_t	(*detect)(void);
 
<span class="p_del">-	/* Platform setup (run once per boot) */</span>
<span class="p_del">-	void		(*init_platform)(void);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* X2APIC detection (run once per boot) */</span>
<span class="p_del">-	bool		(*x2apic_available)(void);</span>
<span class="p_add">+	/* Hypervisor type */</span>
<span class="p_add">+	enum x86_hypervisor_type type;</span>
 
<span class="p_del">-	/* pin current vcpu to specified physical cpu (run rarely) */</span>
<span class="p_del">-	void		(*pin_vcpu)(int);</span>
<span class="p_add">+	/* init time callbacks */</span>
<span class="p_add">+	struct x86_hyper_init init;</span>
 
<span class="p_del">-	/* called during init_mem_mapping() to setup early mappings. */</span>
<span class="p_del">-	void		(*init_mem_mapping)(void);</span>
<span class="p_add">+	/* runtime callbacks */</span>
<span class="p_add">+	struct x86_hyper_runtime runtime;</span>
 };
 
<span class="p_del">-extern const struct hypervisor_x86 *x86_hyper;</span>
<span class="p_del">-</span>
<span class="p_del">-/* Recognized hypervisors */</span>
<span class="p_del">-extern const struct hypervisor_x86 x86_hyper_vmware;</span>
<span class="p_del">-extern const struct hypervisor_x86 x86_hyper_ms_hyperv;</span>
<span class="p_del">-extern const struct hypervisor_x86 x86_hyper_xen_pv;</span>
<span class="p_del">-extern const struct hypervisor_x86 x86_hyper_xen_hvm;</span>
<span class="p_del">-extern const struct hypervisor_x86 x86_hyper_kvm;</span>
<span class="p_del">-</span>
<span class="p_add">+extern enum x86_hypervisor_type x86_hyper_type;</span>
 extern void init_hypervisor_platform(void);
<span class="p_del">-extern bool hypervisor_x2apic_available(void);</span>
<span class="p_del">-extern void hypervisor_pin_vcpu(int cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void hypervisor_init_mem_mapping(void)</span>
<span class="p_add">+static inline bool hypervisor_is_type(enum x86_hypervisor_type type)</span>
 {
<span class="p_del">-	if (x86_hyper &amp;&amp; x86_hyper-&gt;init_mem_mapping)</span>
<span class="p_del">-		x86_hyper-&gt;init_mem_mapping();</span>
<span class="p_add">+	return x86_hyper_type == type;</span>
 }
 #else
 static inline void init_hypervisor_platform(void) { }
<span class="p_del">-static inline bool hypervisor_x2apic_available(void) { return false; }</span>
<span class="p_del">-static inline void hypervisor_init_mem_mapping(void) { }</span>
<span class="p_add">+static inline bool hypervisor_is_type(enum x86_hypervisor_type type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return type == X86_HYPER_NATIVE;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_HYPERVISOR_GUEST */
 #endif /* _ASM_X86_HYPERVISOR_H */
<span class="p_header">diff --git a/arch/x86/include/asm/irqflags.h b/arch/x86/include/asm/irqflags.h</span>
<span class="p_header">index c8ef23f2c28f..89f08955fff7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/irqflags.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/irqflags.h</span>
<span class="p_chunk">@@ -142,6 +142,9 @@</span> <span class="p_context"> static inline notrace unsigned long arch_local_irq_save(void)</span>
 	swapgs;					\
 	sysretl
 
<span class="p_add">+#ifdef CONFIG_DEBUG_ENTRY</span>
<span class="p_add">+#define SAVE_FLAGS(x)		pushfq; popq %rax</span>
<span class="p_add">+#endif</span>
 #else
 #define INTERRUPT_RETURN		iret
 #define ENABLE_INTERRUPTS_SYSEXIT	sti; sysexit
<span class="p_header">diff --git a/arch/x86/include/asm/kdebug.h b/arch/x86/include/asm/kdebug.h</span>
<span class="p_header">index f86a8caa561e..395c9631e000 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kdebug.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kdebug.h</span>
<span class="p_chunk">@@ -26,6 +26,7 @@</span> <span class="p_context"> extern void die(const char *, struct pt_regs *,long);</span>
 extern int __must_check __die(const char *, struct pt_regs *, long);
 extern void show_stack_regs(struct pt_regs *regs);
 extern void __show_regs(struct pt_regs *regs, int all);
<span class="p_add">+extern void show_iret_regs(struct pt_regs *regs);</span>
 extern unsigned long oops_begin(void);
 extern void oops_end(unsigned long, struct pt_regs *, int signr);
 
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 6699fc441644..6d16d15d09a0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -73,8 +73,8 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm_struct *mm)</span>
 #ifdef CONFIG_MODIFY_LDT_SYSCALL
 	struct ldt_struct *ldt;
 
<span class="p_del">-	/* lockless_dereference synchronizes with smp_store_release */</span>
<span class="p_del">-	ldt = lockless_dereference(mm-&gt;context.ldt);</span>
<span class="p_add">+	/* READ_ONCE synchronizes with smp_store_release */</span>
<span class="p_add">+	ldt = READ_ONCE(mm-&gt;context.ldt);</span>
 
 	/*
 	 * Any change to mm-&gt;context.ldt is followed by an IPI to all
<span class="p_header">diff --git a/arch/x86/include/asm/module.h b/arch/x86/include/asm/module.h</span>
<span class="p_header">index 8546fafa21a9..7948a17febb4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/module.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/module.h</span>
<span class="p_chunk">@@ -6,7 +6,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/orc_types.h&gt;
 
 struct mod_arch_specific {
<span class="p_del">-#ifdef CONFIG_ORC_UNWINDER</span>
<span class="p_add">+#ifdef CONFIG_UNWINDER_ORC</span>
 	unsigned int num_orcs;
 	int *orc_unwind_ip;
 	struct orc_entry *orc_unwind;
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index fd81228e8037..892df375b615 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -16,10 +16,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/cpumask.h&gt;
 #include &lt;asm/frame.h&gt;
 
<span class="p_del">-static inline void load_sp0(struct tss_struct *tss,</span>
<span class="p_del">-			     struct thread_struct *thread)</span>
<span class="p_add">+static inline void load_sp0(unsigned long sp0)</span>
 {
<span class="p_del">-	PVOP_VCALL2(pv_cpu_ops.load_sp0, tss, thread);</span>
<span class="p_add">+	PVOP_VCALL1(pv_cpu_ops.load_sp0, sp0);</span>
 }
 
 /* The paravirtualized CPUID instruction. */
<span class="p_chunk">@@ -928,6 +927,15 @@</span> <span class="p_context"> extern void default_banner(void);</span>
 	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_usergs_sysret64),	\
 		  CLBR_NONE,						\
 		  jmp PARA_INDIRECT(pv_cpu_ops+PV_CPU_usergs_sysret64))
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_ENTRY</span>
<span class="p_add">+#define SAVE_FLAGS(clobbers)                                        \</span>
<span class="p_add">+	PARA_SITE(PARA_PATCH(pv_irq_ops, PV_IRQ_save_fl), clobbers, \</span>
<span class="p_add">+		  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);        \</span>
<span class="p_add">+		  call PARA_INDIRECT(pv_irq_ops+PV_IRQ_save_fl);    \</span>
<span class="p_add">+		  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif	/* CONFIG_X86_32 */
 
 #endif /* __ASSEMBLY__ */
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 10cc3b9709fe..6ec54d01972d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -134,7 +134,7 @@</span> <span class="p_context"> struct pv_cpu_ops {</span>
 	void (*alloc_ldt)(struct desc_struct *ldt, unsigned entries);
 	void (*free_ldt)(struct desc_struct *ldt, unsigned entries);
 
<span class="p_del">-	void (*load_sp0)(struct tss_struct *tss, struct thread_struct *t);</span>
<span class="p_add">+	void (*load_sp0)(unsigned long sp0);</span>
 
 	void (*set_iopl_mask)(unsigned mask);
 
<span class="p_header">diff --git a/arch/x86/include/asm/percpu.h b/arch/x86/include/asm/percpu.h</span>
<span class="p_header">index 377f1ffd18be..ba3c523aaf16 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/percpu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/percpu.h</span>
<span class="p_chunk">@@ -526,7 +526,7 @@</span> <span class="p_context"> static inline bool x86_this_cpu_variable_test_bit(int nr,</span>
 {
 	bool oldbit;
 
<span class="p_del">-	asm volatile(&quot;bt &quot;__percpu_arg(2)&quot;,%1\n\t&quot;</span>
<span class="p_add">+	asm volatile(&quot;bt &quot;__percpu_arg(2)&quot;,%1&quot;</span>
 			CC_SET(c)
 			: CC_OUT(c) (oldbit)
 			: &quot;m&quot; (*(unsigned long __percpu *)addr), &quot;Ir&quot; (nr));
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">index 59df7b47a434..9e9b05fc4860 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_chunk">@@ -200,10 +200,9 @@</span> <span class="p_context"> enum page_cache_mode {</span>
 
 #define _PAGE_ENC	(_AT(pteval_t, sme_me_mask))
 
<span class="p_del">-#define _PAGE_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |	\</span>
<span class="p_del">-			 _PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_ENC)</span>
 #define _KERNPG_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED |	\
 			 _PAGE_DIRTY | _PAGE_ENC)
<span class="p_add">+#define _PAGE_TABLE	(_KERNPG_TABLE | _PAGE_USER)</span>
 
 #define __PAGE_KERNEL_ENC	(__PAGE_KERNEL | _PAGE_ENC)
 #define __PAGE_KERNEL_ENC_WP	(__PAGE_KERNEL_WP | _PAGE_ENC)
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index bdac19ab2488..da943411d3d8 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -162,9 +162,9 @@</span> <span class="p_context"> enum cpuid_regs_idx {</span>
 extern struct cpuinfo_x86	boot_cpu_data;
 extern struct cpuinfo_x86	new_cpu_data;
 
<span class="p_del">-extern struct tss_struct	doublefault_tss;</span>
<span class="p_del">-extern __u32			cpu_caps_cleared[NCAPINTS];</span>
<span class="p_del">-extern __u32			cpu_caps_set[NCAPINTS];</span>
<span class="p_add">+extern struct x86_hw_tss	doublefault_tss;</span>
<span class="p_add">+extern __u32			cpu_caps_cleared[NCAPINTS + NBUGINTS];</span>
<span class="p_add">+extern __u32			cpu_caps_set[NCAPINTS + NBUGINTS];</span>
 
 #ifdef CONFIG_SMP
 DECLARE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);
<span class="p_chunk">@@ -252,6 +252,11 @@</span> <span class="p_context"> static inline void load_cr3(pgd_t *pgdir)</span>
 	write_cr3(__sme_pa(pgdir));
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Note that while the legacy &#39;TSS&#39; name comes from &#39;Task State Segment&#39;,</span>
<span class="p_add">+ * on modern x86 CPUs the TSS also holds information important to 64-bit mode,</span>
<span class="p_add">+ * unrelated to the task-switch mechanism:</span>
<span class="p_add">+ */</span>
 #ifdef CONFIG_X86_32
 /* This is the TSS defined by the hardware. */
 struct x86_hw_tss {
<span class="p_chunk">@@ -304,7 +309,13 @@</span> <span class="p_context"> struct x86_hw_tss {</span>
 struct x86_hw_tss {
 	u32			reserved1;
 	u64			sp0;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We store cpu_current_top_of_stack in sp1 so it&#39;s always accessible.</span>
<span class="p_add">+	 * Linux does not use ring 1, so sp1 is not otherwise needed.</span>
<span class="p_add">+	 */</span>
 	u64			sp1;
<span class="p_add">+</span>
 	u64			sp2;
 	u64			reserved2;
 	u64			ist[7];
<span class="p_chunk">@@ -322,12 +333,22 @@</span> <span class="p_context"> struct x86_hw_tss {</span>
 #define IO_BITMAP_BITS			65536
 #define IO_BITMAP_BYTES			(IO_BITMAP_BITS/8)
 #define IO_BITMAP_LONGS			(IO_BITMAP_BYTES/sizeof(long))
<span class="p_del">-#define IO_BITMAP_OFFSET		offsetof(struct tss_struct, io_bitmap)</span>
<span class="p_add">+#define IO_BITMAP_OFFSET		(offsetof(struct tss_struct, io_bitmap) - offsetof(struct tss_struct, x86_tss))</span>
 #define INVALID_IO_BITMAP_OFFSET	0x8000
 
<span class="p_add">+struct SYSENTER_stack {</span>
<span class="p_add">+	unsigned long		words[64];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct SYSENTER_stack_page {</span>
<span class="p_add">+	struct SYSENTER_stack stack;</span>
<span class="p_add">+} __aligned(PAGE_SIZE);</span>
<span class="p_add">+</span>
 struct tss_struct {
 	/*
<span class="p_del">-	 * The hardware state:</span>
<span class="p_add">+	 * The fixed hardware portion.  This must not cross a page boundary</span>
<span class="p_add">+	 * at risk of violating the SDM&#39;s advice and potentially triggering</span>
<span class="p_add">+	 * errata.</span>
 	 */
 	struct x86_hw_tss	x86_tss;
 
<span class="p_chunk">@@ -338,18 +359,9 @@</span> <span class="p_context"> struct tss_struct {</span>
 	 * be within the limit.
 	 */
 	unsigned long		io_bitmap[IO_BITMAP_LONGS + 1];
<span class="p_add">+} __aligned(PAGE_SIZE);</span>
 
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Space for the temporary SYSENTER stack.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	unsigned long		SYSENTER_stack_canary;</span>
<span class="p_del">-	unsigned long		SYSENTER_stack[64];</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-} ____cacheline_aligned;</span>
<span class="p_del">-</span>
<span class="p_del">-DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);</span>
<span class="p_add">+DECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);</span>
 
 /*
  * sizeof(unsigned long) coming from an extra &quot;long&quot; at the end
<span class="p_chunk">@@ -363,6 +375,9 @@</span> <span class="p_context"> DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);</span>
 
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
<span class="p_add">+#else</span>
<span class="p_add">+/* The RO copy can&#39;t be accessed with this_cpu_xyz(), so use the RW copy. */</span>
<span class="p_add">+#define cpu_current_top_of_stack cpu_tss_rw.x86_tss.sp1</span>
 #endif
 
 /*
<span class="p_chunk">@@ -431,7 +446,9 @@</span> <span class="p_context"> typedef struct {</span>
 struct thread_struct {
 	/* Cached TLS descriptors: */
 	struct desc_struct	tls_array[GDT_ENTRY_TLS_ENTRIES];
<span class="p_add">+#ifdef CONFIG_X86_32</span>
 	unsigned long		sp0;
<span class="p_add">+#endif</span>
 	unsigned long		sp;
 #ifdef CONFIG_X86_32
 	unsigned long		sysenter_cs;
<span class="p_chunk">@@ -518,16 +535,9 @@</span> <span class="p_context"> static inline void native_set_iopl_mask(unsigned mask)</span>
 }
 
 static inline void
<span class="p_del">-native_load_sp0(struct tss_struct *tss, struct thread_struct *thread)</span>
<span class="p_add">+native_load_sp0(unsigned long sp0)</span>
 {
<span class="p_del">-	tss-&gt;x86_tss.sp0 = thread-&gt;sp0;</span>
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	/* Only happens when SEP is enabled, no need to test &quot;SEP&quot;arately: */</span>
<span class="p_del">-	if (unlikely(tss-&gt;x86_tss.ss1 != thread-&gt;sysenter_cs)) {</span>
<span class="p_del">-		tss-&gt;x86_tss.ss1 = thread-&gt;sysenter_cs;</span>
<span class="p_del">-		wrmsr(MSR_IA32_SYSENTER_CS, thread-&gt;sysenter_cs, 0);</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	this_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);</span>
 }
 
 static inline void native_swapgs(void)
<span class="p_chunk">@@ -539,12 +549,18 @@</span> <span class="p_context"> static inline void native_swapgs(void)</span>
 
 static inline unsigned long current_top_of_stack(void)
 {
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	return this_cpu_read_stable(cpu_tss.x86_tss.sp0);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	/* sp0 on x86_32 is special in and around vm86 mode. */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 *  We can&#39;t read directly from tss.sp0: sp0 on x86_32 is special in</span>
<span class="p_add">+	 *  and around vm86 mode and sp0 on x86_64 is special because of the</span>
<span class="p_add">+	 *  entry trampoline.</span>
<span class="p_add">+	 */</span>
 	return this_cpu_read_stable(cpu_current_top_of_stack);
<span class="p_del">-#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool on_thread_stack(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long)(current_top_of_stack() -</span>
<span class="p_add">+			       current_stack_pointer) &lt; THREAD_SIZE;</span>
 }
 
 #ifdef CONFIG_PARAVIRT
<span class="p_chunk">@@ -552,10 +568,9 @@</span> <span class="p_context"> static inline unsigned long current_top_of_stack(void)</span>
 #else
 #define __cpuid			native_cpuid
 
<span class="p_del">-static inline void load_sp0(struct tss_struct *tss,</span>
<span class="p_del">-			    struct thread_struct *thread)</span>
<span class="p_add">+static inline void load_sp0(unsigned long sp0)</span>
 {
<span class="p_del">-	native_load_sp0(tss, thread);</span>
<span class="p_add">+	native_load_sp0(sp0);</span>
 }
 
 #define set_iopl_mask native_set_iopl_mask
<span class="p_chunk">@@ -804,6 +819,15 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define TOP_OF_INIT_STACK ((unsigned long)&amp;init_stack + sizeof(init_stack) - \
 			   TOP_OF_KERNEL_STACK_PADDING)
 
<span class="p_add">+#define task_top_of_stack(task) ((unsigned long)(task_pt_regs(task) + 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#define task_pt_regs(task) \</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)task_stack_page(task);	\</span>
<span class="p_add">+	__ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;		\</span>
<span class="p_add">+	((struct pt_regs *)__ptr) - 1;					\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_32
 /*
  * User space process size: 3GB (default).
<span class="p_chunk">@@ -823,23 +847,6 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 	.addr_limit		= KERNEL_DS,				  \
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * TOP_OF_KERNEL_STACK_PADDING reserves 8 bytes on top of the ring0 stack.</span>
<span class="p_del">- * This is necessary to guarantee that the entire &quot;struct pt_regs&quot;</span>
<span class="p_del">- * is accessible even if the CPU haven&#39;t stored the SS/ESP registers</span>
<span class="p_del">- * on the stack (interrupt gate does not save these registers</span>
<span class="p_del">- * when switching to the same priv ring).</span>
<span class="p_del">- * Therefore beware: accessing the ss/esp fields of the</span>
<span class="p_del">- * &quot;struct pt_regs&quot; is possible, but they may contain the</span>
<span class="p_del">- * completely wrong values.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define task_pt_regs(task) \</span>
<span class="p_del">-({									\</span>
<span class="p_del">-	unsigned long __ptr = (unsigned long)task_stack_page(task);	\</span>
<span class="p_del">-	__ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;		\</span>
<span class="p_del">-	((struct pt_regs *)__ptr) - 1;					\</span>
<span class="p_del">-})</span>
<span class="p_del">-</span>
 #define KSTK_ESP(task)		(task_pt_regs(task)-&gt;sp)
 
 #else
<span class="p_chunk">@@ -873,11 +880,9 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
<span class="p_del">-	.sp0			= TOP_OF_INIT_STACK,		\</span>
 	.addr_limit		= KERNEL_DS,			\
 }
 
<span class="p_del">-#define task_pt_regs(tsk)	((struct pt_regs *)(tsk)-&gt;thread.sp0 - 1)</span>
 extern unsigned long KSTK_ESP(struct task_struct *task);
 
 #endif /* CONFIG_X86_64 */
<span class="p_header">diff --git a/arch/x86/include/asm/ptrace.h b/arch/x86/include/asm/ptrace.h</span>
<span class="p_header">index c0e3c45cf6ab..14131dd06b29 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/ptrace.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/ptrace.h</span>
<span class="p_chunk">@@ -136,9 +136,9 @@</span> <span class="p_context"> static inline int v8086_mode(struct pt_regs *regs)</span>
 #endif
 }
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
 static inline bool user_64bit_mode(struct pt_regs *regs)
 {
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 #ifndef CONFIG_PARAVIRT
 	/*
 	 * On non-paravirt systems, this is the only long mode CPL 3
<span class="p_chunk">@@ -149,8 +149,12 @@</span> <span class="p_context"> static inline bool user_64bit_mode(struct pt_regs *regs)</span>
 	/* Headers are too twisted for this to go in paravirt.h. */
 	return regs-&gt;cs == __USER_CS || regs-&gt;cs == pv_info.extra_user_64bit_cs;
 #endif
<span class="p_add">+#else /* !CONFIG_X86_64 */</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+#endif</span>
 }
 
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 #define current_user_stack_pointer()	current_pt_regs()-&gt;sp
 #define compat_user_stack_pointer()	current_pt_regs()-&gt;sp
 #endif
<span class="p_header">diff --git a/arch/x86/include/asm/rmwcc.h b/arch/x86/include/asm/rmwcc.h</span>
<span class="p_header">index d8f3a6ae9f6c..f91c365e57c3 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/rmwcc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/rmwcc.h</span>
<span class="p_chunk">@@ -29,7 +29,7 @@</span> <span class="p_context"> cc_label:								\</span>
 #define __GEN_RMWcc(fullop, var, cc, clobbers, ...)			\
 do {									\
 	bool c;								\
<span class="p_del">-	asm volatile (fullop &quot;;&quot; CC_SET(cc)				\</span>
<span class="p_add">+	asm volatile (fullop CC_SET(cc)					\</span>
 			: [counter] &quot;+m&quot; (var), CC_OUT(cc) (c)		\
 			: __VA_ARGS__ : clobbers);			\
 	return c;							\
<span class="p_header">diff --git a/arch/x86/include/asm/stacktrace.h b/arch/x86/include/asm/stacktrace.h</span>
<span class="p_header">index 8da111b3c342..f8062bfd43a0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/stacktrace.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/stacktrace.h</span>
<span class="p_chunk">@@ -16,6 +16,7 @@</span> <span class="p_context"> enum stack_type {</span>
 	STACK_TYPE_TASK,
 	STACK_TYPE_IRQ,
 	STACK_TYPE_SOFTIRQ,
<span class="p_add">+	STACK_TYPE_SYSENTER,</span>
 	STACK_TYPE_EXCEPTION,
 	STACK_TYPE_EXCEPTION_LAST = STACK_TYPE_EXCEPTION + N_EXCEPTION_STACKS-1,
 };
<span class="p_chunk">@@ -28,6 +29,8 @@</span> <span class="p_context"> struct stack_info {</span>
 bool in_task_stack(unsigned long *stack, struct task_struct *task,
 		   struct stack_info *info);
 
<span class="p_add">+bool in_sysenter_stack(unsigned long *stack, struct stack_info *info);</span>
<span class="p_add">+</span>
 int get_stack_info(unsigned long *stack, struct task_struct *task,
 		   struct stack_info *info, unsigned long *visit_mask);
 
<span class="p_header">diff --git a/arch/x86/include/asm/switch_to.h b/arch/x86/include/asm/switch_to.h</span>
<span class="p_header">index 899084b70412..9b6df68d8fd1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/switch_to.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/switch_to.h</span>
<span class="p_chunk">@@ -2,6 +2,8 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_SWITCH_TO_H
 #define _ASM_X86_SWITCH_TO_H
 
<span class="p_add">+#include &lt;linux/sched/task_stack.h&gt;</span>
<span class="p_add">+</span>
 struct task_struct; /* one of the stranger aspects of C forward declarations */
 
 struct task_struct *__switch_to_asm(struct task_struct *prev,
<span class="p_chunk">@@ -73,4 +75,28 @@</span> <span class="p_context"> do {									\</span>
 	((last) = __switch_to_asm((prev), (next)));			\
 } while (0)
 
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+static inline void refresh_sysenter_cs(struct thread_struct *thread)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* Only happens when SEP is enabled, no need to test &quot;SEP&quot;arately: */</span>
<span class="p_add">+	if (unlikely(this_cpu_read(cpu_tss_rw.x86_tss.ss1) == thread-&gt;sysenter_cs))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	this_cpu_write(cpu_tss_rw.x86_tss.ss1, thread-&gt;sysenter_cs);</span>
<span class="p_add">+	wrmsr(MSR_IA32_SYSENTER_CS, thread-&gt;sysenter_cs, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* This is used when switching tasks or entering/exiting vm86 mode. */</span>
<span class="p_add">+static inline void update_sp0(struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* On x86_64, sp0 always points to the entry trampoline stack, which is constant: */</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+	load_sp0(task-&gt;thread.sp0);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_XENPV))</span>
<span class="p_add">+		load_sp0(task_top_of_stack(task));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* _ASM_X86_SWITCH_TO_H */
<span class="p_header">diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">index 70f425947dc5..00223333821a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -207,7 +207,7 @@</span> <span class="p_context"> static inline int arch_within_stack_frames(const void * const stack,</span>
 #else /* !__ASSEMBLY__ */
 
 #ifdef CONFIG_X86_64
<span class="p_del">-# define cpu_current_top_of_stack (cpu_tss + TSS_sp0)</span>
<span class="p_add">+# define cpu_current_top_of_stack (cpu_tss_rw + TSS_sp1)</span>
 #endif
 
 #endif
<span class="p_header">diff --git a/arch/x86/include/asm/trace/fpu.h b/arch/x86/include/asm/trace/fpu.h</span>
<span class="p_header">index fa60398bbc3a..069c04be1507 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/trace/fpu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/trace/fpu.h</span>
<span class="p_chunk">@@ -34,11 +34,6 @@</span> <span class="p_context"> DECLARE_EVENT_CLASS(x86_fpu,</span>
 	)
 );
 
<span class="p_del">-DEFINE_EVENT(x86_fpu, x86_fpu_state,</span>
<span class="p_del">-	TP_PROTO(struct fpu *fpu),</span>
<span class="p_del">-	TP_ARGS(fpu)</span>
<span class="p_del">-);</span>
<span class="p_del">-</span>
 DEFINE_EVENT(x86_fpu, x86_fpu_before_save,
 	TP_PROTO(struct fpu *fpu),
 	TP_ARGS(fpu)
<span class="p_chunk">@@ -74,11 +69,6 @@</span> <span class="p_context"> DEFINE_EVENT(x86_fpu, x86_fpu_activate_state,</span>
 	TP_ARGS(fpu)
 );
 
<span class="p_del">-DEFINE_EVENT(x86_fpu, x86_fpu_deactivate_state,</span>
<span class="p_del">-	TP_PROTO(struct fpu *fpu),</span>
<span class="p_del">-	TP_ARGS(fpu)</span>
<span class="p_del">-);</span>
<span class="p_del">-</span>
 DEFINE_EVENT(x86_fpu, x86_fpu_init_state,
 	TP_PROTO(struct fpu *fpu),
 	TP_ARGS(fpu)
<span class="p_header">diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h</span>
<span class="p_header">index b0cced97a6ce..31051f35cbb7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/traps.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/traps.h</span>
<span class="p_chunk">@@ -38,9 +38,9 @@</span> <span class="p_context"> asmlinkage void simd_coprocessor_error(void);</span>
 
 #if defined(CONFIG_X86_64) &amp;&amp; defined(CONFIG_XEN_PV)
 asmlinkage void xen_divide_error(void);
<span class="p_add">+asmlinkage void xen_xennmi(void);</span>
 asmlinkage void xen_xendebug(void);
 asmlinkage void xen_xenint3(void);
<span class="p_del">-asmlinkage void xen_nmi(void);</span>
 asmlinkage void xen_overflow(void);
 asmlinkage void xen_bounds(void);
 asmlinkage void xen_invalid_op(void);
<span class="p_chunk">@@ -75,7 +75,6 @@</span> <span class="p_context"> dotraplinkage void do_segment_not_present(struct pt_regs *, long);</span>
 dotraplinkage void do_stack_segment(struct pt_regs *, long);
 #ifdef CONFIG_X86_64
 dotraplinkage void do_double_fault(struct pt_regs *, long);
<span class="p_del">-asmlinkage struct pt_regs *sync_regs(struct pt_regs *);</span>
 #endif
 dotraplinkage void do_general_protection(struct pt_regs *, long);
 dotraplinkage void do_page_fault(struct pt_regs *, unsigned long);
<span class="p_chunk">@@ -145,4 +144,22 @@</span> <span class="p_context"> enum {</span>
 	X86_TRAP_IRET = 32,	/* 32, IRET Exception */
 };
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Page fault error code bits:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   bit 0 ==	 0: no page found	1: protection fault</span>
<span class="p_add">+ *   bit 1 ==	 0: read access		1: write access</span>
<span class="p_add">+ *   bit 2 ==	 0: kernel-mode access	1: user-mode access</span>
<span class="p_add">+ *   bit 3 ==				1: use of reserved bit detected</span>
<span class="p_add">+ *   bit 4 ==				1: fault was an instruction fetch</span>
<span class="p_add">+ *   bit 5 ==				1: protection keys block access</span>
<span class="p_add">+ */</span>
<span class="p_add">+enum x86_pf_error_code {</span>
<span class="p_add">+	X86_PF_PROT	=		1 &lt;&lt; 0,</span>
<span class="p_add">+	X86_PF_WRITE	=		1 &lt;&lt; 1,</span>
<span class="p_add">+	X86_PF_USER	=		1 &lt;&lt; 2,</span>
<span class="p_add">+	X86_PF_RSVD	=		1 &lt;&lt; 3,</span>
<span class="p_add">+	X86_PF_INSTR	=		1 &lt;&lt; 4,</span>
<span class="p_add">+	X86_PF_PK	=		1 &lt;&lt; 5,</span>
<span class="p_add">+};</span>
 #endif /* _ASM_X86_TRAPS_H */
<span class="p_header">diff --git a/arch/x86/include/asm/unwind.h b/arch/x86/include/asm/unwind.h</span>
<span class="p_header">index 87adc0d38c4a..c1688c2d0a12 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/unwind.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/unwind.h</span>
<span class="p_chunk">@@ -7,17 +7,20 @@</span> <span class="p_context"></span>
 #include &lt;asm/ptrace.h&gt;
 #include &lt;asm/stacktrace.h&gt;
 
<span class="p_add">+#define IRET_FRAME_OFFSET (offsetof(struct pt_regs, ip))</span>
<span class="p_add">+#define IRET_FRAME_SIZE   (sizeof(struct pt_regs) - IRET_FRAME_OFFSET)</span>
<span class="p_add">+</span>
 struct unwind_state {
 	struct stack_info stack_info;
 	unsigned long stack_mask;
 	struct task_struct *task;
 	int graph_idx;
 	bool error;
<span class="p_del">-#if defined(CONFIG_ORC_UNWINDER)</span>
<span class="p_add">+#if defined(CONFIG_UNWINDER_ORC)</span>
 	bool signal, full_regs;
 	unsigned long sp, bp, ip;
 	struct pt_regs *regs;
<span class="p_del">-#elif defined(CONFIG_FRAME_POINTER_UNWINDER)</span>
<span class="p_add">+#elif defined(CONFIG_UNWINDER_FRAME_POINTER)</span>
 	bool got_irq;
 	unsigned long *bp, *orig_sp, ip;
 	struct pt_regs *regs;
<span class="p_chunk">@@ -51,7 +54,11 @@</span> <span class="p_context"> void unwind_start(struct unwind_state *state, struct task_struct *task,</span>
 	__unwind_start(state, task, regs, first_frame);
 }
 
<span class="p_del">-#if defined(CONFIG_ORC_UNWINDER) || defined(CONFIG_FRAME_POINTER_UNWINDER)</span>
<span class="p_add">+#if defined(CONFIG_UNWINDER_ORC) || defined(CONFIG_UNWINDER_FRAME_POINTER)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * WARNING: The entire pt_regs may not be safe to dereference.  In some cases,</span>
<span class="p_add">+ * only the iret frame registers are accessible.  Use with caution!</span>
<span class="p_add">+ */</span>
 static inline struct pt_regs *unwind_get_entry_regs(struct unwind_state *state)
 {
 	if (unwind_done(state))
<span class="p_chunk">@@ -66,7 +73,7 @@</span> <span class="p_context"> static inline struct pt_regs *unwind_get_entry_regs(struct unwind_state *state)</span>
 }
 #endif
 
<span class="p_del">-#ifdef CONFIG_ORC_UNWINDER</span>
<span class="p_add">+#ifdef CONFIG_UNWINDER_ORC</span>
 void unwind_init(void);
 void unwind_module_init(struct module *mod, void *orc_ip, size_t orc_ip_size,
 			void *orc, size_t orc_size);
<span class="p_header">diff --git a/arch/x86/include/asm/x86_init.h b/arch/x86/include/asm/x86_init.h</span>
<span class="p_header">index 8a1ebf9540dd..ad15a0fda917 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/x86_init.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/x86_init.h</span>
<span class="p_chunk">@@ -114,6 +114,18 @@</span> <span class="p_context"> struct x86_init_pci {</span>
 	void (*fixup_irqs)(void);
 };
 
<span class="p_add">+/**</span>
<span class="p_add">+ * struct x86_hyper_init - x86 hypervisor init functions</span>
<span class="p_add">+ * @init_platform:		platform setup</span>
<span class="p_add">+ * @x2apic_available:		X2APIC detection</span>
<span class="p_add">+ * @init_mem_mapping:		setup early mappings during init_mem_mapping()</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct x86_hyper_init {</span>
<span class="p_add">+	void (*init_platform)(void);</span>
<span class="p_add">+	bool (*x2apic_available)(void);</span>
<span class="p_add">+	void (*init_mem_mapping)(void);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 /**
  * struct x86_init_ops - functions for platform specific setup
  *
<span class="p_chunk">@@ -127,6 +139,7 @@</span> <span class="p_context"> struct x86_init_ops {</span>
 	struct x86_init_timers		timers;
 	struct x86_init_iommu		iommu;
 	struct x86_init_pci		pci;
<span class="p_add">+	struct x86_hyper_init		hyper;</span>
 };
 
 /**
<span class="p_chunk">@@ -199,6 +212,15 @@</span> <span class="p_context"> struct x86_legacy_features {</span>
 	struct x86_legacy_devices devices;
 };
 
<span class="p_add">+/**</span>
<span class="p_add">+ * struct x86_hyper_runtime - x86 hypervisor specific runtime callbacks</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @pin_vcpu:		pin current vcpu to specified physical cpu (run rarely)</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct x86_hyper_runtime {</span>
<span class="p_add">+	void (*pin_vcpu)(int cpu);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 /**
  * struct x86_platform_ops - platform specific runtime functions
  * @calibrate_cpu:		calibrate CPU
<span class="p_chunk">@@ -218,6 +240,7 @@</span> <span class="p_context"> struct x86_legacy_features {</span>
  * 				possible in x86_early_init_platform_quirks() by
  * 				only using the current x86_hardware_subarch
  * 				semantics.
<span class="p_add">+ * @hyper:			x86 hypervisor specific runtime callbacks</span>
  */
 struct x86_platform_ops {
 	unsigned long (*calibrate_cpu)(void);
<span class="p_chunk">@@ -233,6 +256,7 @@</span> <span class="p_context"> struct x86_platform_ops {</span>
 	void (*apic_post_init)(void);
 	struct x86_legacy_features legacy;
 	void (*set_legacy_features)(void);
<span class="p_add">+	struct x86_hyper_runtime hyper;</span>
 };
 
 struct pci_dev;
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">index 6f3355399665..53b4ca55ebb6 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -152,5 +152,8 @@</span> <span class="p_context"></span>
 #define CX86_ARR_BASE	0xc4
 #define CX86_RCR_BASE	0xdc
 
<span class="p_add">+#define CR0_STATE	(X86_CR0_PE | X86_CR0_MP | X86_CR0_ET | \</span>
<span class="p_add">+			 X86_CR0_NE | X86_CR0_WP | X86_CR0_AM | \</span>
<span class="p_add">+			 X86_CR0_PG)</span>
 
 #endif /* _UAPI_ASM_X86_PROCESSOR_FLAGS_H */
<span class="p_header">diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile</span>
<span class="p_header">index 5f70044340ff..295abaa58add 100644</span>
<span class="p_header">--- a/arch/x86/kernel/Makefile</span>
<span class="p_header">+++ b/arch/x86/kernel/Makefile</span>
<span class="p_chunk">@@ -25,9 +25,9 @@</span> <span class="p_context"> endif</span>
 KASAN_SANITIZE_head$(BITS).o				:= n
 KASAN_SANITIZE_dumpstack.o				:= n
 KASAN_SANITIZE_dumpstack_$(BITS).o			:= n
<span class="p_del">-KASAN_SANITIZE_stacktrace.o := n</span>
<span class="p_add">+KASAN_SANITIZE_stacktrace.o				:= n</span>
<span class="p_add">+KASAN_SANITIZE_paravirt.o				:= n</span>
 
<span class="p_del">-OBJECT_FILES_NON_STANDARD_head_$(BITS).o		:= y</span>
 OBJECT_FILES_NON_STANDARD_relocate_kernel_$(BITS).o	:= y
 OBJECT_FILES_NON_STANDARD_ftrace_$(BITS).o		:= y
 OBJECT_FILES_NON_STANDARD_test_nx.o			:= y
<span class="p_chunk">@@ -128,9 +128,9 @@</span> <span class="p_context"> obj-$(CONFIG_PERF_EVENTS)		+= perf_regs.o</span>
 obj-$(CONFIG_TRACING)			+= tracepoint.o
 obj-$(CONFIG_SCHED_MC_PRIO)		+= itmt.o
 
<span class="p_del">-obj-$(CONFIG_ORC_UNWINDER)		+= unwind_orc.o</span>
<span class="p_del">-obj-$(CONFIG_FRAME_POINTER_UNWINDER)	+= unwind_frame.o</span>
<span class="p_del">-obj-$(CONFIG_GUESS_UNWINDER)		+= unwind_guess.o</span>
<span class="p_add">+obj-$(CONFIG_UNWINDER_ORC)		+= unwind_orc.o</span>
<span class="p_add">+obj-$(CONFIG_UNWINDER_FRAME_POINTER)	+= unwind_frame.o</span>
<span class="p_add">+obj-$(CONFIG_UNWINDER_GUESS)		+= unwind_guess.o</span>
 
 ###
 # 64 bit specific files
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">index ff891772c9f8..89c7c8569e5e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic.c</span>
<span class="p_chunk">@@ -1645,7 +1645,7 @@</span> <span class="p_context"> static __init void try_to_enable_x2apic(int remap_mode)</span>
 		 * under KVM
 		 */
 		if (max_physical_apicid &gt; 255 ||
<span class="p_del">-		    !hypervisor_x2apic_available()) {</span>
<span class="p_add">+		    !x86_init.hyper.x2apic_available()) {</span>
 			pr_info(&quot;x2apic: IRQ remapping doesn&#39;t support X2APIC mode\n&quot;);
 			x2apic_disable();
 			return;
<span class="p_header">diff --git a/arch/x86/kernel/apic/x2apic_uv_x.c b/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_header">index 0d57bb9079c9..c0b694810ff4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_chunk">@@ -920,9 +920,8 @@</span> <span class="p_context"> static __init void uv_rtc_init(void)</span>
 /*
  * percpu heartbeat timer
  */
<span class="p_del">-static void uv_heartbeat(unsigned long ignored)</span>
<span class="p_add">+static void uv_heartbeat(struct timer_list *timer)</span>
 {
<span class="p_del">-	struct timer_list *timer = &amp;uv_scir_info-&gt;timer;</span>
 	unsigned char bits = uv_scir_info-&gt;state;
 
 	/* Flip heartbeat bit: */
<span class="p_chunk">@@ -947,7 +946,7 @@</span> <span class="p_context"> static int uv_heartbeat_enable(unsigned int cpu)</span>
 		struct timer_list *timer = &amp;uv_cpu_scir_info(cpu)-&gt;timer;
 
 		uv_set_cpu_scir_bits(cpu, SCIR_CPU_HEARTBEAT|SCIR_CPU_ACTIVITY);
<span class="p_del">-		setup_pinned_timer(timer, uv_heartbeat, cpu);</span>
<span class="p_add">+		timer_setup(timer, uv_heartbeat, TIMER_PINNED);</span>
 		timer-&gt;expires = jiffies + SCIR_CPU_HB_INTERVAL;
 		add_timer_on(timer, cpu);
 		uv_cpu_scir_info(cpu)-&gt;enabled = 1;
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index 8ea78275480d..cd360a5e0dca 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -93,4 +93,10 @@</span> <span class="p_context"> void common(void) {</span>
 
 	BLANK();
 	DEFINE(PTREGS_SIZE, sizeof(struct pt_regs));
<span class="p_add">+</span>
<span class="p_add">+	/* Layout info for cpu_entry_area */</span>
<span class="p_add">+	OFFSET(CPU_ENTRY_AREA_tss, cpu_entry_area, tss);</span>
<span class="p_add">+	OFFSET(CPU_ENTRY_AREA_entry_trampoline, cpu_entry_area, entry_trampoline);</span>
<span class="p_add">+	OFFSET(CPU_ENTRY_AREA_SYSENTER_stack, cpu_entry_area, SYSENTER_stack_page);</span>
<span class="p_add">+	DEFINE(SIZEOF_SYSENTER_stack, sizeof(struct SYSENTER_stack));</span>
 }
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets_32.c b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">index dedf428b20b6..7d20d9c0b3d6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_chunk">@@ -47,13 +47,8 @@</span> <span class="p_context"> void foo(void)</span>
 	BLANK();
 
 	/* Offset from the sysenter stack to tss.sp0 */
<span class="p_del">-	DEFINE(TSS_sysenter_sp0, offsetof(struct tss_struct, x86_tss.sp0) -</span>
<span class="p_del">-	       offsetofend(struct tss_struct, SYSENTER_stack));</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Offset from cpu_tss to SYSENTER_stack */</span>
<span class="p_del">-	OFFSET(CPU_TSS_SYSENTER_stack, tss_struct, SYSENTER_stack);</span>
<span class="p_del">-	/* Size of SYSENTER_stack */</span>
<span class="p_del">-	DEFINE(SIZEOF_SYSENTER_stack, sizeof(((struct tss_struct *)0)-&gt;SYSENTER_stack));</span>
<span class="p_add">+	DEFINE(TSS_sysenter_sp0, offsetof(struct cpu_entry_area, tss.x86_tss.sp0) -</span>
<span class="p_add">+	       offsetofend(struct cpu_entry_area, SYSENTER_stack_page.stack));</span>
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 	BLANK();
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets_64.c b/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_header">index 630212fa9b9d..bf51e51d808d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_chunk">@@ -23,6 +23,9 @@</span> <span class="p_context"> int main(void)</span>
 #ifdef CONFIG_PARAVIRT
 	OFFSET(PV_CPU_usergs_sysret64, pv_cpu_ops, usergs_sysret64);
 	OFFSET(PV_CPU_swapgs, pv_cpu_ops, swapgs);
<span class="p_add">+#ifdef CONFIG_DEBUG_ENTRY</span>
<span class="p_add">+	OFFSET(PV_IRQ_save_fl, pv_irq_ops, save_fl);</span>
<span class="p_add">+#endif</span>
 	BLANK();
 #endif
 
<span class="p_chunk">@@ -63,6 +66,7 @@</span> <span class="p_context"> int main(void)</span>
 
 	OFFSET(TSS_ist, tss_struct, x86_tss.ist);
 	OFFSET(TSS_sp0, tss_struct, x86_tss.sp0);
<span class="p_add">+	OFFSET(TSS_sp1, tss_struct, x86_tss.sp1);</span>
 	BLANK();
 
 #ifdef CONFIG_CC_STACKPROTECTOR
<span class="p_header">diff --git a/arch/x86/kernel/cpu/Makefile b/arch/x86/kernel/cpu/Makefile</span>
<span class="p_header">index c60922a66385..90cb82dbba57 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/Makefile</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/Makefile</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"> obj-y			+= rdrand.o</span>
 obj-y			+= match.o
 obj-y			+= bugs.o
 obj-$(CONFIG_CPU_FREQ)	+= aperfmperf.o
<span class="p_add">+obj-y			+= cpuid-deps.o</span>
 
 obj-$(CONFIG_PROC_FS)	+= proc.o
 obj-$(CONFIG_X86_FEATURE_NAMES) += capflags.o powerflags.o
<span class="p_header">diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">index d58184b7cd44..bcb75dc97d44 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_chunk">@@ -804,8 +804,11 @@</span> <span class="p_context"> static void init_amd(struct cpuinfo_x86 *c)</span>
 	case 0x17: init_amd_zn(c); break;
 	}
 
<span class="p_del">-	/* Enable workaround for FXSAVE leak */</span>
<span class="p_del">-	if (c-&gt;x86 &gt;= 6)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Enable workaround for FXSAVE leak on CPUs</span>
<span class="p_add">+	 * without a XSaveErPtr feature</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((c-&gt;x86 &gt;= 6) &amp;&amp; (!cpu_has(c, X86_FEATURE_XSAVEERPTR)))</span>
 		set_cpu_bug(c, X86_BUG_FXSAVE_LEAK);
 
 	cpu_detect_cache_sizes(c);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index c9176bae7fd8..034900623adf 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -452,8 +452,8 @@</span> <span class="p_context"> static const char *table_lookup_model(struct cpuinfo_x86 *c)</span>
 	return NULL;		/* Not found */
 }
 
<span class="p_del">-__u32 cpu_caps_cleared[NCAPINTS];</span>
<span class="p_del">-__u32 cpu_caps_set[NCAPINTS];</span>
<span class="p_add">+__u32 cpu_caps_cleared[NCAPINTS + NBUGINTS];</span>
<span class="p_add">+__u32 cpu_caps_set[NCAPINTS + NBUGINTS];</span>
 
 void load_percpu_segment(int cpu)
 {
<span class="p_chunk">@@ -466,27 +466,116 @@</span> <span class="p_context"> void load_percpu_segment(int cpu)</span>
 	load_stack_canary_segment();
 }
 
<span class="p_del">-/* Setup the fixmap mapping only once per-processor */</span>
<span class="p_del">-static inline void setup_fixmap_gdt(int cpu)</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+/* The 32-bit entry code needs to find cpu_entry_area. */</span>
<span class="p_add">+DEFINE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Special IST stacks which the CPU switches to when it calls</span>
<span class="p_add">+ * an IST-marked descriptor entry. Up to 7 stacks (hardware</span>
<span class="p_add">+ * limit), all of them are 4K, except the debug stack which</span>
<span class="p_add">+ * is 8K.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {</span>
<span class="p_add">+	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,</span>
<span class="p_add">+	  [DEBUG_STACK - 1]			= DEBUG_STKSZ</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks</span>
<span class="p_add">+	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_PER_CPU_PAGE_ALIGNED(struct SYSENTER_stack_page,</span>
<span class="p_add">+				   SYSENTER_stack_storage);</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init</span>
<span class="p_add">+set_percpu_fixmap_pages(int idx, void *ptr, int pages, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	for ( ; pages; pages--, idx--, ptr += PAGE_SIZE)</span>
<span class="p_add">+		__set_fixmap(idx, per_cpu_ptr_to_phys(ptr), prot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Setup the fixmap mappings only once per-processor */</span>
<span class="p_add">+static void __init setup_cpu_entry_area(int cpu)</span>
 {
 #ifdef CONFIG_X86_64
<span class="p_del">-	/* On 64-bit systems, we use a read-only fixmap GDT. */</span>
<span class="p_del">-	pgprot_t prot = PAGE_KERNEL_RO;</span>
<span class="p_add">+	extern char _entry_trampoline[];</span>
<span class="p_add">+</span>
<span class="p_add">+	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */</span>
<span class="p_add">+	pgprot_t gdt_prot = PAGE_KERNEL_RO;</span>
<span class="p_add">+	pgprot_t tss_prot = PAGE_KERNEL_RO;</span>
 #else
 	/*
 	 * On native 32-bit systems, the GDT cannot be read-only because
 	 * our double fault handler uses a task gate, and entering through
<span class="p_del">-	 * a task gate needs to change an available TSS to busy.  If the GDT</span>
<span class="p_del">-	 * is read-only, that will triple fault.</span>
<span class="p_add">+	 * a task gate needs to change an available TSS to busy.  If the</span>
<span class="p_add">+	 * GDT is read-only, that will triple fault.  The TSS cannot be</span>
<span class="p_add">+	 * read-only because the CPU writes to it on task switches.</span>
 	 *
<span class="p_del">-	 * On Xen PV, the GDT must be read-only because the hypervisor requires</span>
<span class="p_del">-	 * it.</span>
<span class="p_add">+	 * On Xen PV, the GDT must be read-only because the hypervisor</span>
<span class="p_add">+	 * requires it.</span>
 	 */
<span class="p_del">-	pgprot_t prot = boot_cpu_has(X86_FEATURE_XENPV) ?</span>
<span class="p_add">+	pgprot_t gdt_prot = boot_cpu_has(X86_FEATURE_XENPV) ?</span>
 		PAGE_KERNEL_RO : PAGE_KERNEL;
<span class="p_add">+	pgprot_t tss_prot = PAGE_KERNEL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);</span>
<span class="p_add">+	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, SYSENTER_stack_page),</span>
<span class="p_add">+				per_cpu_ptr(&amp;SYSENTER_stack_storage, cpu), 1,</span>
<span class="p_add">+				PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The Intel SDM says (Volume 3, 7.2.1):</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  Avoid placing a page boundary in the part of the TSS that the</span>
<span class="p_add">+	 *  processor reads during a task switch (the first 104 bytes). The</span>
<span class="p_add">+	 *  processor may not correctly perform address translations if a</span>
<span class="p_add">+	 *  boundary occurs in this area. During a task switch, the processor</span>
<span class="p_add">+	 *  reads and writes into the first 104 bytes of each TSS (using</span>
<span class="p_add">+	 *  contiguous physical addresses beginning with the physical address</span>
<span class="p_add">+	 *  of the first byte of the TSS). So, after TSS access begins, if</span>
<span class="p_add">+	 *  part of the 104 bytes is not physically contiguous, the processor</span>
<span class="p_add">+	 *  will access incorrect information without generating a page-fault</span>
<span class="p_add">+	 *  exception.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * There are also a lot of errata involving the TSS spanning a page</span>
<span class="p_add">+	 * boundary.  Assert that we&#39;re not doing that.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^</span>
<span class="p_add">+		      offsetofend(struct tss_struct, x86_tss)) &amp; PAGE_MASK);</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);</span>
<span class="p_add">+	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, tss),</span>
<span class="p_add">+				&amp;per_cpu(cpu_tss_rw, cpu),</span>
<span class="p_add">+				sizeof(struct tss_struct) / PAGE_SIZE,</span>
<span class="p_add">+				tss_prot);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);</span>
 #endif
 
<span class="p_del">-	__set_fixmap(get_cpu_gdt_ro_index(cpu), get_cpu_gdt_paddr(cpu), prot);</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(exception_stacks) !=</span>
<span class="p_add">+		     sizeof(((struct cpu_entry_area *)0)-&gt;exception_stacks));</span>
<span class="p_add">+	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, exception_stacks),</span>
<span class="p_add">+				&amp;per_cpu(exception_stacks, cpu),</span>
<span class="p_add">+				sizeof(exception_stacks) / PAGE_SIZE,</span>
<span class="p_add">+				PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	__set_fixmap(get_cpu_entry_area_index(cpu, entry_trampoline),</span>
<span class="p_add">+		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init setup_cpu_entry_areas(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu)</span>
<span class="p_add">+		setup_cpu_entry_area(cpu);</span>
 }
 
 /* Load the original GDT from the per-cpu structure */
<span class="p_chunk">@@ -723,7 +812,7 @@</span> <span class="p_context"> static void apply_forced_caps(struct cpuinfo_x86 *c)</span>
 {
 	int i;
 
<span class="p_del">-	for (i = 0; i &lt; NCAPINTS; i++) {</span>
<span class="p_add">+	for (i = 0; i &lt; NCAPINTS + NBUGINTS; i++) {</span>
 		c-&gt;x86_capability[i] &amp;= ~cpu_caps_cleared[i];
 		c-&gt;x86_capability[i] |= cpu_caps_set[i];
 	}
<span class="p_chunk">@@ -1225,7 +1314,7 @@</span> <span class="p_context"> void enable_sep_cpu(void)</span>
 		return;
 
 	cpu = get_cpu();
<span class="p_del">-	tss = &amp;per_cpu(cpu_tss, cpu);</span>
<span class="p_add">+	tss = &amp;per_cpu(cpu_tss_rw, cpu);</span>
 
 	/*
 	 * We cache MSR_IA32_SYSENTER_CS&#39;s value in the TSS&#39;s ss1 field --
<span class="p_chunk">@@ -1234,11 +1323,7 @@</span> <span class="p_context"> void enable_sep_cpu(void)</span>
 
 	tss-&gt;x86_tss.ss1 = __KERNEL_CS;
 	wrmsr(MSR_IA32_SYSENTER_CS, tss-&gt;x86_tss.ss1, 0);
<span class="p_del">-</span>
<span class="p_del">-	wrmsr(MSR_IA32_SYSENTER_ESP,</span>
<span class="p_del">-	      (unsigned long)tss + offsetofend(struct tss_struct, SYSENTER_stack),</span>
<span class="p_del">-	      0);</span>
<span class="p_del">-</span>
<span class="p_add">+	wrmsr(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1), 0);</span>
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
 
 	put_cpu();
<span class="p_chunk">@@ -1301,18 +1386,16 @@</span> <span class="p_context"> void print_cpu_info(struct cpuinfo_x86 *c)</span>
 		pr_cont(&quot;)\n&quot;);
 }
 
<span class="p_del">-static __init int setup_disablecpuid(char *arg)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * clearcpuid= was already parsed in fpu__init_parse_early_param.</span>
<span class="p_add">+ * But we need to keep a dummy __setup around otherwise it would</span>
<span class="p_add">+ * show up as an environment variable for init.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __init int setup_clearcpuid(char *arg)</span>
 {
<span class="p_del">-	int bit;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (get_option(&amp;arg, &amp;bit) &amp;&amp; bit &gt;= 0 &amp;&amp; bit &lt; NCAPINTS * 32)</span>
<span class="p_del">-		setup_clear_cpu_cap(bit);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
 	return 1;
 }
<span class="p_del">-__setup(&quot;clearcpuid=&quot;, setup_disablecpuid);</span>
<span class="p_add">+__setup(&quot;clearcpuid=&quot;, setup_clearcpuid);</span>
 
 #ifdef CONFIG_X86_64
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
<span class="p_chunk">@@ -1334,25 +1417,19 @@</span> <span class="p_context"> DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;</span>
 DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
 EXPORT_PER_CPU_SYMBOL(__preempt_count);
 
<span class="p_del">-/*</span>
<span class="p_del">- * Special IST stacks which the CPU switches to when it calls</span>
<span class="p_del">- * an IST-marked descriptor entry. Up to 7 stacks (hardware</span>
<span class="p_del">- * limit), all of them are 4K, except the debug stack which</span>
<span class="p_del">- * is 8K.</span>
<span class="p_del">- */</span>
<span class="p_del">-static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {</span>
<span class="p_del">-	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,</span>
<span class="p_del">-	  [DEBUG_STACK - 1]			= DEBUG_STKSZ</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks</span>
<span class="p_del">-	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);</span>
<span class="p_del">-</span>
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
<span class="p_add">+	extern char _entry_trampoline[];</span>
<span class="p_add">+	extern char entry_SYSCALL_64_trampoline[];</span>
<span class="p_add">+</span>
<span class="p_add">+	int cpu = smp_processor_id();</span>
<span class="p_add">+	unsigned long SYSCALL64_entry_trampoline =</span>
<span class="p_add">+		(unsigned long)get_cpu_entry_area(cpu)-&gt;entry_trampoline +</span>
<span class="p_add">+		(entry_SYSCALL_64_trampoline - _entry_trampoline);</span>
<span class="p_add">+</span>
 	wrmsr(MSR_STAR, 0, (__USER32_CS &lt;&lt; 16) | __KERNEL_CS);
<span class="p_del">-	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);</span>
<span class="p_add">+	wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);</span>
 
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, (unsigned long)entry_SYSCALL_compat);
<span class="p_chunk">@@ -1363,7 +1440,7 @@</span> <span class="p_context"> void syscall_init(void)</span>
 	 * AMD doesn&#39;t allow SYSENTER in long mode (either 32- or 64-bit).
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
<span class="p_del">-	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, 0ULL);</span>
<span class="p_add">+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1));</span>
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
 	wrmsrl(MSR_CSTAR, (unsigned long)ignore_sysret);
<span class="p_chunk">@@ -1507,7 +1584,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	if (cpu)
 		load_ucode_ap();
 
<span class="p_del">-	t = &amp;per_cpu(cpu_tss, cpu);</span>
<span class="p_add">+	t = &amp;per_cpu(cpu_tss_rw, cpu);</span>
 	oist = &amp;per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
<span class="p_chunk">@@ -1546,7 +1623,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	 * set up and load the per-CPU TSS
 	 */
 	if (!oist-&gt;ist[0]) {
<span class="p_del">-		char *estacks = per_cpu(exception_stacks, cpu);</span>
<span class="p_add">+		char *estacks = get_cpu_entry_area(cpu)-&gt;exception_stacks;</span>
 
 		for (v = 0; v &lt; N_EXCEPTION_STACKS; v++) {
 			estacks += exception_stack_sizes[v];
<span class="p_chunk">@@ -1557,7 +1634,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 		}
 	}
 
<span class="p_del">-	t-&gt;x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);</span>
<span class="p_add">+	t-&gt;x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;</span>
 
 	/*
 	 * &lt;= is required because the CPU will access up to
<span class="p_chunk">@@ -1572,9 +1649,14 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&amp;init_mm, me);
 
<span class="p_del">-	load_sp0(t, &amp;current-&gt;thread);</span>
<span class="p_del">-	set_tss_desc(cpu, t);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Initialize the TSS.  sp0 points to the entry trampoline stack</span>
<span class="p_add">+	 * regardless of what task is running.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	set_tss_desc(cpu, &amp;get_cpu_entry_area(cpu)-&gt;tss.x86_tss);</span>
 	load_TR_desc();
<span class="p_add">+	load_sp0((unsigned long)(cpu_SYSENTER_stack(cpu) + 1));</span>
<span class="p_add">+</span>
 	load_mm_ldt(&amp;init_mm);
 
 	clear_all_debug_regs();
<span class="p_chunk">@@ -1585,7 +1667,6 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	if (is_uv_system())
 		uv_cpu_init();
 
<span class="p_del">-	setup_fixmap_gdt(cpu);</span>
 	load_fixmap_gdt(cpu);
 }
 
<span class="p_chunk">@@ -1595,8 +1676,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 {
 	int cpu = smp_processor_id();
 	struct task_struct *curr = current;
<span class="p_del">-	struct tss_struct *t = &amp;per_cpu(cpu_tss, cpu);</span>
<span class="p_del">-	struct thread_struct *thread = &amp;curr-&gt;thread;</span>
<span class="p_add">+	struct tss_struct *t = &amp;per_cpu(cpu_tss_rw, cpu);</span>
 
 	wait_for_master_cpu(cpu);
 
<span class="p_chunk">@@ -1627,12 +1707,16 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	initialize_tlbstate_and_flush();
 	enter_lazy_tlb(&amp;init_mm, curr);
 
<span class="p_del">-	load_sp0(t, thread);</span>
<span class="p_del">-	set_tss_desc(cpu, t);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Initialize the TSS.  Don&#39;t bother initializing sp0, as the initial</span>
<span class="p_add">+	 * task never enters user mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	set_tss_desc(cpu, &amp;get_cpu_entry_area(cpu)-&gt;tss.x86_tss);</span>
 	load_TR_desc();
<span class="p_add">+</span>
 	load_mm_ldt(&amp;init_mm);
 
<span class="p_del">-	t-&gt;x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);</span>
<span class="p_add">+	t-&gt;x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;</span>
 
 #ifdef CONFIG_DOUBLEFAULT
 	/* Set up doublefault TSS pointer in the GDT */
<span class="p_chunk">@@ -1644,7 +1728,6 @@</span> <span class="p_context"> void cpu_init(void)</span>
 
 	fpu__init_cpu();
 
<span class="p_del">-	setup_fixmap_gdt(cpu);</span>
 	load_fixmap_gdt(cpu);
 }
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/cpu/cpuid-deps.c b/arch/x86/kernel/cpu/cpuid-deps.c</span>
new file mode 100644
<span class="p_header">index 000000000000..904b0a3c4e53</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/cpuid-deps.c</span>
<span class="p_chunk">@@ -0,0 +1,121 @@</span> <span class="p_context"></span>
<span class="p_add">+/* Declare dependencies between CPUIDs */</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpufeature.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct cpuid_dep {</span>
<span class="p_add">+	unsigned int	feature;</span>
<span class="p_add">+	unsigned int	depends;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Table of CPUID features that depend on others.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This only includes dependencies that can be usefully disabled, not</span>
<span class="p_add">+ * features part of the base set (like FPU).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note this all is not __init / __initdata because it can be</span>
<span class="p_add">+ * called from cpu hotplug. It shouldn&#39;t do anything in this case,</span>
<span class="p_add">+ * but it&#39;s difficult to tell that to the init reference checker.</span>
<span class="p_add">+ */</span>
<span class="p_add">+const static struct cpuid_dep cpuid_deps[] = {</span>
<span class="p_add">+	{ X86_FEATURE_XSAVEOPT,		X86_FEATURE_XSAVE     },</span>
<span class="p_add">+	{ X86_FEATURE_XSAVEC,		X86_FEATURE_XSAVE     },</span>
<span class="p_add">+	{ X86_FEATURE_XSAVES,		X86_FEATURE_XSAVE     },</span>
<span class="p_add">+	{ X86_FEATURE_AVX,		X86_FEATURE_XSAVE     },</span>
<span class="p_add">+	{ X86_FEATURE_PKU,		X86_FEATURE_XSAVE     },</span>
<span class="p_add">+	{ X86_FEATURE_MPX,		X86_FEATURE_XSAVE     },</span>
<span class="p_add">+	{ X86_FEATURE_XGETBV1,		X86_FEATURE_XSAVE     },</span>
<span class="p_add">+	{ X86_FEATURE_FXSR_OPT,		X86_FEATURE_FXSR      },</span>
<span class="p_add">+	{ X86_FEATURE_XMM,		X86_FEATURE_FXSR      },</span>
<span class="p_add">+	{ X86_FEATURE_XMM2,		X86_FEATURE_XMM       },</span>
<span class="p_add">+	{ X86_FEATURE_XMM3,		X86_FEATURE_XMM2      },</span>
<span class="p_add">+	{ X86_FEATURE_XMM4_1,		X86_FEATURE_XMM2      },</span>
<span class="p_add">+	{ X86_FEATURE_XMM4_2,		X86_FEATURE_XMM2      },</span>
<span class="p_add">+	{ X86_FEATURE_XMM3,		X86_FEATURE_XMM2      },</span>
<span class="p_add">+	{ X86_FEATURE_PCLMULQDQ,	X86_FEATURE_XMM2      },</span>
<span class="p_add">+	{ X86_FEATURE_SSSE3,		X86_FEATURE_XMM2,     },</span>
<span class="p_add">+	{ X86_FEATURE_F16C,		X86_FEATURE_XMM2,     },</span>
<span class="p_add">+	{ X86_FEATURE_AES,		X86_FEATURE_XMM2      },</span>
<span class="p_add">+	{ X86_FEATURE_SHA_NI,		X86_FEATURE_XMM2      },</span>
<span class="p_add">+	{ X86_FEATURE_FMA,		X86_FEATURE_AVX       },</span>
<span class="p_add">+	{ X86_FEATURE_AVX2,		X86_FEATURE_AVX,      },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512F,		X86_FEATURE_AVX,      },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512IFMA,	X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512PF,		X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512ER,		X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512CD,		X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512DQ,		X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512BW,		X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512VL,		X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512VBMI,	X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512_VBMI2,	X86_FEATURE_AVX512VL  },</span>
<span class="p_add">+	{ X86_FEATURE_GFNI,		X86_FEATURE_AVX512VL  },</span>
<span class="p_add">+	{ X86_FEATURE_VAES,		X86_FEATURE_AVX512VL  },</span>
<span class="p_add">+	{ X86_FEATURE_VPCLMULQDQ,	X86_FEATURE_AVX512VL  },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512_VNNI,	X86_FEATURE_AVX512VL  },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512_BITALG,	X86_FEATURE_AVX512VL  },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512_4VNNIW,	X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512_4FMAPS,	X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{ X86_FEATURE_AVX512_VPOPCNTDQ, X86_FEATURE_AVX512F   },</span>
<span class="p_add">+	{}</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void clear_feature(struct cpuinfo_x86 *c, unsigned int feature)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: This could use the non atomic __*_bit() variants, but the</span>
<span class="p_add">+	 * rest of the cpufeature code uses atomics as well, so keep it for</span>
<span class="p_add">+	 * consistency. Cleanup all of it separately.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!c) {</span>
<span class="p_add">+		clear_cpu_cap(&amp;boot_cpu_data, feature);</span>
<span class="p_add">+		set_bit(feature, (unsigned long *)cpu_caps_cleared);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		clear_bit(feature, (unsigned long *)c-&gt;x86_capability);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Take the capabilities and the BUG bits into account */</span>
<span class="p_add">+#define MAX_FEATURE_BITS ((NCAPINTS + NBUGINTS) * sizeof(u32) * 8)</span>
<span class="p_add">+</span>
<span class="p_add">+static void do_clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int feature)</span>
<span class="p_add">+{</span>
<span class="p_add">+	DECLARE_BITMAP(disable, MAX_FEATURE_BITS);</span>
<span class="p_add">+	const struct cpuid_dep *d;</span>
<span class="p_add">+	bool changed;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON(feature &gt;= MAX_FEATURE_BITS))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	clear_feature(c, feature);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Collect all features to disable, handling dependencies */</span>
<span class="p_add">+	memset(disable, 0, sizeof(disable));</span>
<span class="p_add">+	__set_bit(feature, disable);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Loop until we get a stable state. */</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		changed = false;</span>
<span class="p_add">+		for (d = cpuid_deps; d-&gt;feature; d++) {</span>
<span class="p_add">+			if (!test_bit(d-&gt;depends, disable))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			if (__test_and_set_bit(d-&gt;feature, disable))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			changed = true;</span>
<span class="p_add">+			clear_feature(c, d-&gt;feature);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} while (changed);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int feature)</span>
<span class="p_add">+{</span>
<span class="p_add">+	do_clear_cpu_cap(c, feature);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void setup_clear_cpu_cap(unsigned int feature)</span>
<span class="p_add">+{</span>
<span class="p_add">+	do_clear_cpu_cap(NULL, feature);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/kernel/cpu/hypervisor.c b/arch/x86/kernel/cpu/hypervisor.c</span>
<span class="p_header">index 4fa90006ac68..bea8d3e24f50 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/hypervisor.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/hypervisor.c</span>
<span class="p_chunk">@@ -26,6 +26,12 @@</span> <span class="p_context"></span>
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/hypervisor.h&gt;
 
<span class="p_add">+extern const struct hypervisor_x86 x86_hyper_vmware;</span>
<span class="p_add">+extern const struct hypervisor_x86 x86_hyper_ms_hyperv;</span>
<span class="p_add">+extern const struct hypervisor_x86 x86_hyper_xen_pv;</span>
<span class="p_add">+extern const struct hypervisor_x86 x86_hyper_xen_hvm;</span>
<span class="p_add">+extern const struct hypervisor_x86 x86_hyper_kvm;</span>
<span class="p_add">+</span>
 static const __initconst struct hypervisor_x86 * const hypervisors[] =
 {
 #ifdef CONFIG_XEN_PV
<span class="p_chunk">@@ -41,54 +47,52 @@</span> <span class="p_context"> static const __initconst struct hypervisor_x86 * const hypervisors[] =</span>
 #endif
 };
 
<span class="p_del">-const struct hypervisor_x86 *x86_hyper;</span>
<span class="p_del">-EXPORT_SYMBOL(x86_hyper);</span>
<span class="p_add">+enum x86_hypervisor_type x86_hyper_type;</span>
<span class="p_add">+EXPORT_SYMBOL(x86_hyper_type);</span>
 
<span class="p_del">-static inline void __init</span>
<span class="p_add">+static inline const struct hypervisor_x86 * __init</span>
 detect_hypervisor_vendor(void)
 {
<span class="p_del">-	const struct hypervisor_x86 *h, * const *p;</span>
<span class="p_add">+	const struct hypervisor_x86 *h = NULL, * const *p;</span>
 	uint32_t pri, max_pri = 0;
 
 	for (p = hypervisors; p &lt; hypervisors + ARRAY_SIZE(hypervisors); p++) {
<span class="p_del">-		h = *p;</span>
<span class="p_del">-		pri = h-&gt;detect();</span>
<span class="p_del">-		if (pri != 0 &amp;&amp; pri &gt; max_pri) {</span>
<span class="p_add">+		pri = (*p)-&gt;detect();</span>
<span class="p_add">+		if (pri &gt; max_pri) {</span>
 			max_pri = pri;
<span class="p_del">-			x86_hyper = h;</span>
<span class="p_add">+			h = *p;</span>
 		}
 	}
 
<span class="p_del">-	if (max_pri)</span>
<span class="p_del">-		pr_info(&quot;Hypervisor detected: %s\n&quot;, x86_hyper-&gt;name);</span>
<span class="p_add">+	if (h)</span>
<span class="p_add">+		pr_info(&quot;Hypervisor detected: %s\n&quot;, h-&gt;name);</span>
<span class="p_add">+</span>
<span class="p_add">+	return h;</span>
 }
 
<span class="p_del">-void __init init_hypervisor_platform(void)</span>
<span class="p_add">+static void __init copy_array(const void *src, void *target, unsigned int size)</span>
 {
<span class="p_add">+	unsigned int i, n = size / sizeof(void *);</span>
<span class="p_add">+	const void * const *from = (const void * const *)src;</span>
<span class="p_add">+	const void **to = (const void **)target;</span>
 
<span class="p_del">-	detect_hypervisor_vendor();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!x86_hyper)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (x86_hyper-&gt;init_platform)</span>
<span class="p_del">-		x86_hyper-&gt;init_platform();</span>
<span class="p_add">+	for (i = 0; i &lt; n; i++)</span>
<span class="p_add">+		if (from[i])</span>
<span class="p_add">+			to[i] = from[i];</span>
 }
 
<span class="p_del">-bool __init hypervisor_x2apic_available(void)</span>
<span class="p_add">+void __init init_hypervisor_platform(void)</span>
 {
<span class="p_del">-	return x86_hyper                   &amp;&amp;</span>
<span class="p_del">-	       x86_hyper-&gt;x2apic_available &amp;&amp;</span>
<span class="p_del">-	       x86_hyper-&gt;x2apic_available();</span>
<span class="p_del">-}</span>
<span class="p_add">+	const struct hypervisor_x86 *h;</span>
 
<span class="p_del">-void hypervisor_pin_vcpu(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!x86_hyper)</span>
<span class="p_add">+	h = detect_hypervisor_vendor();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!h)</span>
 		return;
 
<span class="p_del">-	if (x86_hyper-&gt;pin_vcpu)</span>
<span class="p_del">-		x86_hyper-&gt;pin_vcpu(cpu);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		WARN_ONCE(1, &quot;vcpu pinning requested but not supported!\n&quot;);</span>
<span class="p_add">+	copy_array(&amp;h-&gt;init, &amp;x86_init.hyper, sizeof(h-&gt;init));</span>
<span class="p_add">+	copy_array(&amp;h-&gt;runtime, &amp;x86_platform.hyper, sizeof(h-&gt;runtime));</span>
<span class="p_add">+</span>
<span class="p_add">+	x86_hyper_type = h-&gt;type;</span>
<span class="p_add">+	x86_init.hyper.init_platform();</span>
 }
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_header">index 236324e83a3a..85eb5fc180c8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_chunk">@@ -254,9 +254,9 @@</span> <span class="p_context"> static void __init ms_hyperv_init_platform(void)</span>
 #endif
 }
 
<span class="p_del">-const __refconst struct hypervisor_x86 x86_hyper_ms_hyperv = {</span>
<span class="p_add">+const __initconst struct hypervisor_x86 x86_hyper_ms_hyperv = {</span>
 	.name			= &quot;Microsoft Hyper-V&quot;,
 	.detect			= ms_hyperv_platform,
<span class="p_del">-	.init_platform		= ms_hyperv_init_platform,</span>
<span class="p_add">+	.type			= X86_HYPER_MS_HYPERV,</span>
<span class="p_add">+	.init.init_platform	= ms_hyperv_init_platform,</span>
 };
<span class="p_del">-EXPORT_SYMBOL(x86_hyper_ms_hyperv);</span>
<span class="p_header">diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c</span>
<span class="p_header">index 40ed26852ebd..8e005329648b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/vmware.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/vmware.c</span>
<span class="p_chunk">@@ -205,10 +205,10 @@</span> <span class="p_context"> static bool __init vmware_legacy_x2apic_available(void)</span>
 	       (eax &amp; (1 &lt;&lt; VMWARE_PORT_CMD_LEGACY_X2APIC)) != 0;
 }
 
<span class="p_del">-const __refconst struct hypervisor_x86 x86_hyper_vmware = {</span>
<span class="p_add">+const __initconst struct hypervisor_x86 x86_hyper_vmware = {</span>
 	.name			= &quot;VMware&quot;,
 	.detect			= vmware_platform,
<span class="p_del">-	.init_platform		= vmware_platform_setup,</span>
<span class="p_del">-	.x2apic_available	= vmware_legacy_x2apic_available,</span>
<span class="p_add">+	.type			= X86_HYPER_VMWARE,</span>
<span class="p_add">+	.init.init_platform	= vmware_platform_setup,</span>
<span class="p_add">+	.init.x2apic_available	= vmware_legacy_x2apic_available,</span>
 };
<span class="p_del">-EXPORT_SYMBOL(x86_hyper_vmware);</span>
<span class="p_header">diff --git a/arch/x86/kernel/doublefault.c b/arch/x86/kernel/doublefault.c</span>
<span class="p_header">index 0e662c55ae90..0b8cedb20d6d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/doublefault.c</span>
<span class="p_header">+++ b/arch/x86/kernel/doublefault.c</span>
<span class="p_chunk">@@ -50,25 +50,23 @@</span> <span class="p_context"> static void doublefault_fn(void)</span>
 		cpu_relax();
 }
 
<span class="p_del">-struct tss_struct doublefault_tss __cacheline_aligned = {</span>
<span class="p_del">-	.x86_tss = {</span>
<span class="p_del">-		.sp0		= STACK_START,</span>
<span class="p_del">-		.ss0		= __KERNEL_DS,</span>
<span class="p_del">-		.ldt		= 0,</span>
<span class="p_del">-		.io_bitmap_base	= INVALID_IO_BITMAP_OFFSET,</span>
<span class="p_del">-</span>
<span class="p_del">-		.ip		= (unsigned long) doublefault_fn,</span>
<span class="p_del">-		/* 0x2 bit is always set */</span>
<span class="p_del">-		.flags		= X86_EFLAGS_SF | 0x2,</span>
<span class="p_del">-		.sp		= STACK_START,</span>
<span class="p_del">-		.es		= __USER_DS,</span>
<span class="p_del">-		.cs		= __KERNEL_CS,</span>
<span class="p_del">-		.ss		= __KERNEL_DS,</span>
<span class="p_del">-		.ds		= __USER_DS,</span>
<span class="p_del">-		.fs		= __KERNEL_PERCPU,</span>
<span class="p_del">-</span>
<span class="p_del">-		.__cr3		= __pa_nodebug(swapper_pg_dir),</span>
<span class="p_del">-	}</span>
<span class="p_add">+struct x86_hw_tss doublefault_tss __cacheline_aligned = {</span>
<span class="p_add">+	.sp0		= STACK_START,</span>
<span class="p_add">+	.ss0		= __KERNEL_DS,</span>
<span class="p_add">+	.ldt		= 0,</span>
<span class="p_add">+	.io_bitmap_base	= INVALID_IO_BITMAP_OFFSET,</span>
<span class="p_add">+</span>
<span class="p_add">+	.ip		= (unsigned long) doublefault_fn,</span>
<span class="p_add">+	/* 0x2 bit is always set */</span>
<span class="p_add">+	.flags		= X86_EFLAGS_SF | 0x2,</span>
<span class="p_add">+	.sp		= STACK_START,</span>
<span class="p_add">+	.es		= __USER_DS,</span>
<span class="p_add">+	.cs		= __KERNEL_CS,</span>
<span class="p_add">+	.ss		= __KERNEL_DS,</span>
<span class="p_add">+	.ds		= __USER_DS,</span>
<span class="p_add">+	.fs		= __KERNEL_PERCPU,</span>
<span class="p_add">+</span>
<span class="p_add">+	.__cr3		= __pa_nodebug(swapper_pg_dir),</span>
 };
 
 /* dummy for do_double_fault() call */
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">index f13b4c00a5de..bbd6d986e2d0 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack.c</span>
<span class="p_chunk">@@ -43,6 +43,24 @@</span> <span class="p_context"> bool in_task_stack(unsigned long *stack, struct task_struct *task,</span>
 	return true;
 }
 
<span class="p_add">+bool in_sysenter_stack(unsigned long *stack, struct stack_info *info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct SYSENTER_stack *ss = cpu_SYSENTER_stack(smp_processor_id());</span>
<span class="p_add">+</span>
<span class="p_add">+	void *begin = ss;</span>
<span class="p_add">+	void *end = ss + 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((void *)stack &lt; begin || (void *)stack &gt;= end)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	info-&gt;type	= STACK_TYPE_SYSENTER;</span>
<span class="p_add">+	info-&gt;begin	= begin;</span>
<span class="p_add">+	info-&gt;end	= end;</span>
<span class="p_add">+	info-&gt;next_sp	= NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void printk_stack_address(unsigned long address, int reliable,
 				 char *log_lvl)
 {
<span class="p_chunk">@@ -50,6 +68,28 @@</span> <span class="p_context"> static void printk_stack_address(unsigned long address, int reliable,</span>
 	printk(&quot;%s %s%pB\n&quot;, log_lvl, reliable ? &quot;&quot; : &quot;? &quot;, (void *)address);
 }
 
<span class="p_add">+void show_iret_regs(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printk(KERN_DEFAULT &quot;RIP: %04x:%pS\n&quot;, (int)regs-&gt;cs, (void *)regs-&gt;ip);</span>
<span class="p_add">+	printk(KERN_DEFAULT &quot;RSP: %04x:%016lx EFLAGS: %08lx&quot;, (int)regs-&gt;ss,</span>
<span class="p_add">+		regs-&gt;sp, regs-&gt;flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void show_regs_safe(struct stack_info *info, struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (on_stack(info, regs, sizeof(*regs)))</span>
<span class="p_add">+		__show_regs(regs, 0);</span>
<span class="p_add">+	else if (on_stack(info, (void *)regs + IRET_FRAME_OFFSET,</span>
<span class="p_add">+			  IRET_FRAME_SIZE)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * When an interrupt or exception occurs in entry code, the</span>
<span class="p_add">+		 * full pt_regs might not have been saved yet.  In that case</span>
<span class="p_add">+		 * just print the iret frame.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		show_iret_regs(regs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,
 			unsigned long *stack, char *log_lvl)
 {
<span class="p_chunk">@@ -71,31 +111,35 @@</span> <span class="p_context"> void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 	 * - task stack
 	 * - interrupt stack
 	 * - HW exception stacks (double fault, nmi, debug, mce)
<span class="p_add">+	 * - SYSENTER stack</span>
 	 *
<span class="p_del">-	 * x86-32 can have up to three stacks:</span>
<span class="p_add">+	 * x86-32 can have up to four stacks:</span>
 	 * - task stack
 	 * - softirq stack
 	 * - hardirq stack
<span class="p_add">+	 * - SYSENTER stack</span>
 	 */
 	for (regs = NULL; stack; stack = PTR_ALIGN(stack_info.next_sp, sizeof(long))) {
 		const char *stack_name;
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If we overflowed the task stack into a guard page, jump back</span>
<span class="p_del">-		 * to the bottom of the usable stack.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (task_stack_page(task) - (void *)stack &lt; PAGE_SIZE)</span>
<span class="p_del">-			stack = task_stack_page(task);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (get_stack_info(stack, task, &amp;stack_info, &amp;visit_mask))</span>
<span class="p_del">-			break;</span>
<span class="p_add">+		if (get_stack_info(stack, task, &amp;stack_info, &amp;visit_mask)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We weren&#39;t on a valid stack.  It&#39;s possible that</span>
<span class="p_add">+			 * we overflowed a valid stack into a guard page.</span>
<span class="p_add">+			 * See if the next page up is valid so that we can</span>
<span class="p_add">+			 * generate some kind of backtrace if this happens.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			stack = (unsigned long *)PAGE_ALIGN((unsigned long)stack);</span>
<span class="p_add">+			if (get_stack_info(stack, task, &amp;stack_info, &amp;visit_mask))</span>
<span class="p_add">+				break;</span>
<span class="p_add">+		}</span>
 
 		stack_name = stack_type_name(stack_info.type);
 		if (stack_name)
 			printk(&quot;%s &lt;%s&gt;\n&quot;, log_lvl, stack_name);
 
<span class="p_del">-		if (regs &amp;&amp; on_stack(&amp;stack_info, regs, sizeof(*regs)))</span>
<span class="p_del">-			__show_regs(regs, 0);</span>
<span class="p_add">+		if (regs)</span>
<span class="p_add">+			show_regs_safe(&amp;stack_info, regs);</span>
 
 		/*
 		 * Scan the stack, printing any text addresses we find.  At the
<span class="p_chunk">@@ -119,7 +163,7 @@</span> <span class="p_context"> void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 
 			/*
 			 * Don&#39;t print regs-&gt;ip again if it was already printed
<span class="p_del">-			 * by __show_regs() below.</span>
<span class="p_add">+			 * by show_regs_safe() below.</span>
 			 */
 			if (regs &amp;&amp; stack == &amp;regs-&gt;ip)
 				goto next;
<span class="p_chunk">@@ -155,8 +199,8 @@</span> <span class="p_context"> void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 
 			/* if the frame has entry regs, print them */
 			regs = unwind_get_entry_regs(&amp;state);
<span class="p_del">-			if (regs &amp;&amp; on_stack(&amp;stack_info, regs, sizeof(*regs)))</span>
<span class="p_del">-				__show_regs(regs, 0);</span>
<span class="p_add">+			if (regs)</span>
<span class="p_add">+				show_regs_safe(&amp;stack_info, regs);</span>
 		}
 
 		if (stack_name)
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack_32.c b/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_header">index daefae83a3aa..5ff13a6b3680 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_chunk">@@ -26,6 +26,9 @@</span> <span class="p_context"> const char *stack_type_name(enum stack_type type)</span>
 	if (type == STACK_TYPE_SOFTIRQ)
 		return &quot;SOFTIRQ&quot;;
 
<span class="p_add">+	if (type == STACK_TYPE_SYSENTER)</span>
<span class="p_add">+		return &quot;SYSENTER&quot;;</span>
<span class="p_add">+</span>
 	return NULL;
 }
 
<span class="p_chunk">@@ -93,6 +96,9 @@</span> <span class="p_context"> int get_stack_info(unsigned long *stack, struct task_struct *task,</span>
 	if (task != current)
 		goto unknown;
 
<span class="p_add">+	if (in_sysenter_stack(stack, info))</span>
<span class="p_add">+		goto recursion_check;</span>
<span class="p_add">+</span>
 	if (in_hardirq_stack(stack, info))
 		goto recursion_check;
 
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack_64.c b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">index 88ce2ffdb110..abc828f8c297 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_chunk">@@ -37,6 +37,9 @@</span> <span class="p_context"> const char *stack_type_name(enum stack_type type)</span>
 	if (type == STACK_TYPE_IRQ)
 		return &quot;IRQ&quot;;
 
<span class="p_add">+	if (type == STACK_TYPE_SYSENTER)</span>
<span class="p_add">+		return &quot;SYSENTER&quot;;</span>
<span class="p_add">+</span>
 	if (type &gt;= STACK_TYPE_EXCEPTION &amp;&amp; type &lt;= STACK_TYPE_EXCEPTION_LAST)
 		return exception_stack_names[type - STACK_TYPE_EXCEPTION];
 
<span class="p_chunk">@@ -115,6 +118,9 @@</span> <span class="p_context"> int get_stack_info(unsigned long *stack, struct task_struct *task,</span>
 	if (in_irq_stack(stack, info))
 		goto recursion_check;
 
<span class="p_add">+	if (in_sysenter_stack(stack, info))</span>
<span class="p_add">+		goto recursion_check;</span>
<span class="p_add">+</span>
 	goto unknown;
 
 recursion_check:
<span class="p_header">diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">index 7affb7e3d9a5..6abd83572b01 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/init.c</span>
<span class="p_chunk">@@ -249,6 +249,10 @@</span> <span class="p_context"> static void __init fpu__init_system_ctx_switch(void)</span>
  */
 static void __init fpu__init_parse_early_param(void)
 {
<span class="p_add">+	char arg[32];</span>
<span class="p_add">+	char *argptr = arg;</span>
<span class="p_add">+	int bit;</span>
<span class="p_add">+</span>
 	if (cmdline_find_option_bool(boot_command_line, &quot;no387&quot;))
 		setup_clear_cpu_cap(X86_FEATURE_FPU);
 
<span class="p_chunk">@@ -266,6 +270,13 @@</span> <span class="p_context"> static void __init fpu__init_parse_early_param(void)</span>
 
 	if (cmdline_find_option_bool(boot_command_line, &quot;noxsaves&quot;))
 		setup_clear_cpu_cap(X86_FEATURE_XSAVES);
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option(boot_command_line, &quot;clearcpuid&quot;, arg,</span>
<span class="p_add">+				sizeof(arg)) &amp;&amp;</span>
<span class="p_add">+	    get_option(&amp;argptr, &amp;bit) &amp;&amp;</span>
<span class="p_add">+	    bit &gt;= 0 &amp;&amp;</span>
<span class="p_add">+	    bit &lt; NCAPINTS * 32)</span>
<span class="p_add">+		setup_clear_cpu_cap(bit);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">index f1d5476c9022..87a57b7642d3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_chunk">@@ -15,6 +15,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/fpu/xstate.h&gt;
 
 #include &lt;asm/tlbflush.h&gt;
<span class="p_add">+#include &lt;asm/cpufeature.h&gt;</span>
 
 /*
  * Although we spell it out in here, the Processor Trace
<span class="p_chunk">@@ -36,6 +37,19 @@</span> <span class="p_context"> static const char *xfeature_names[] =</span>
 	&quot;unknown xstate feature&quot;	,
 };
 
<span class="p_add">+static short xsave_cpuid_features[] __initdata = {</span>
<span class="p_add">+	X86_FEATURE_FPU,</span>
<span class="p_add">+	X86_FEATURE_XMM,</span>
<span class="p_add">+	X86_FEATURE_AVX,</span>
<span class="p_add">+	X86_FEATURE_MPX,</span>
<span class="p_add">+	X86_FEATURE_MPX,</span>
<span class="p_add">+	X86_FEATURE_AVX512F,</span>
<span class="p_add">+	X86_FEATURE_AVX512F,</span>
<span class="p_add">+	X86_FEATURE_AVX512F,</span>
<span class="p_add">+	X86_FEATURE_INTEL_PT,</span>
<span class="p_add">+	X86_FEATURE_PKU,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 /*
  * Mask of xstate features supported by the CPU and the kernel:
  */
<span class="p_chunk">@@ -59,26 +73,6 @@</span> <span class="p_context"> unsigned int fpu_user_xstate_size;</span>
 void fpu__xstate_clear_all_cpu_caps(void)
 {
 	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_XSAVEC);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_XSAVES);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX2);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512F);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512IFMA);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512PF);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512ER);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512CD);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512DQ);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512BW);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512VL);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_MPX);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_XGETBV1);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512VBMI);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_PKU);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512_4VNNIW);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512_4FMAPS);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_AVX512_VPOPCNTDQ);</span>
 }
 
 /*
<span class="p_chunk">@@ -726,6 +720,7 @@</span> <span class="p_context"> void __init fpu__init_system_xstate(void)</span>
 	unsigned int eax, ebx, ecx, edx;
 	static int on_boot_cpu __initdata = 1;
 	int err;
<span class="p_add">+	int i;</span>
 
 	WARN_ON_FPU(!on_boot_cpu);
 	on_boot_cpu = 0;
<span class="p_chunk">@@ -759,6 +754,14 @@</span> <span class="p_context"> void __init fpu__init_system_xstate(void)</span>
 		goto out_disable;
 	}
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Clear XSAVE features that are disabled in the normal CPUID.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (i = 0; i &lt; ARRAY_SIZE(xsave_cpuid_features); i++) {</span>
<span class="p_add">+		if (!boot_cpu_has(xsave_cpuid_features[i]))</span>
<span class="p_add">+			xfeatures_mask &amp;= ~BIT(i);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	xfeatures_mask &amp;= fpu__get_supported_xfeatures_mask();
 
 	/* Enable xstate instructions to be able to continue with initialization: */
<span class="p_header">diff --git a/arch/x86/kernel/head_32.S b/arch/x86/kernel/head_32.S</span>
<span class="p_header">index f1d528bb66a6..c29020907886 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_32.S</span>
<span class="p_chunk">@@ -212,9 +212,6 @@</span> <span class="p_context"> ENTRY(startup_32_smp)</span>
 #endif
 
 .Ldefault_entry:
<span class="p_del">-#define CR0_STATE	(X86_CR0_PE | X86_CR0_MP | X86_CR0_ET | \</span>
<span class="p_del">-			 X86_CR0_NE | X86_CR0_WP | X86_CR0_AM | \</span>
<span class="p_del">-			 X86_CR0_PG)</span>
 	movl $(CR0_STATE &amp; ~X86_CR0_PG),%eax
 	movl %eax,%cr0
 
<span class="p_chunk">@@ -402,7 +399,7 @@</span> <span class="p_context"> ENTRY(early_idt_handler_array)</span>
 	# 24(%rsp) error code
 	i = 0
 	.rept NUM_EXCEPTION_VECTORS
<span class="p_del">-	.ifeq (EXCEPTION_ERRCODE_MASK &gt;&gt; i) &amp; 1</span>
<span class="p_add">+	.if ((EXCEPTION_ERRCODE_MASK &gt;&gt; i) &amp; 1) == 0</span>
 	pushl $0		# Dummy error code, to make stack frame uniform
 	.endif
 	pushl $i		# 20(%esp) Vector number
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index 6dde3f3fc1f8..7dca675fe78d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -38,11 +38,12 @@</span> <span class="p_context"></span>
  *
  */
 
<span class="p_del">-#define p4d_index(x)	(((x) &gt;&gt; P4D_SHIFT) &amp; (PTRS_PER_P4D-1))</span>
 #define pud_index(x)	(((x) &gt;&gt; PUD_SHIFT) &amp; (PTRS_PER_PUD-1))
 
<span class="p_add">+#if defined(CONFIG_XEN_PV) || defined(CONFIG_XEN_PVH)</span>
 PGD_PAGE_OFFSET = pgd_index(__PAGE_OFFSET_BASE)
 PGD_START_KERNEL = pgd_index(__START_KERNEL_map)
<span class="p_add">+#endif</span>
 L3_START_KERNEL = pud_index(__START_KERNEL_map)
 
 	.text
<span class="p_chunk">@@ -50,6 +51,7 @@</span> <span class="p_context"> L3_START_KERNEL = pud_index(__START_KERNEL_map)</span>
 	.code64
 	.globl startup_64
 startup_64:
<span class="p_add">+	UNWIND_HINT_EMPTY</span>
 	/*
 	 * At this point the CPU runs in 64bit mode CS.L = 1 CS.D = 0,
 	 * and someone has loaded an identity mapped page table
<span class="p_chunk">@@ -89,6 +91,7 @@</span> <span class="p_context"> startup_64:</span>
 	addq	$(early_top_pgt - __START_KERNEL_map), %rax
 	jmp 1f
 ENTRY(secondary_startup_64)
<span class="p_add">+	UNWIND_HINT_EMPTY</span>
 	/*
 	 * At this point the CPU runs in 64bit mode CS.L = 1 CS.D = 0,
 	 * and someone has loaded a mapped page table.
<span class="p_chunk">@@ -133,6 +136,7 @@</span> <span class="p_context"> ENTRY(secondary_startup_64)</span>
 	movq	$1f, %rax
 	jmp	*%rax
 1:
<span class="p_add">+	UNWIND_HINT_EMPTY</span>
 
 	/* Check if nx is implemented */
 	movl	$0x80000001, %eax
<span class="p_chunk">@@ -150,9 +154,6 @@</span> <span class="p_context"> ENTRY(secondary_startup_64)</span>
 1:	wrmsr				/* Make changes effective */
 
 	/* Setup cr0 */
<span class="p_del">-#define CR0_STATE	(X86_CR0_PE | X86_CR0_MP | X86_CR0_ET | \</span>
<span class="p_del">-			 X86_CR0_NE | X86_CR0_WP | X86_CR0_AM | \</span>
<span class="p_del">-			 X86_CR0_PG)</span>
 	movl	$CR0_STATE, %eax
 	/* Make changes effective */
 	movq	%rax, %cr0
<span class="p_chunk">@@ -235,7 +236,7 @@</span> <span class="p_context"> ENTRY(secondary_startup_64)</span>
 	pushq	%rax		# target address in negative space
 	lretq
 .Lafter_lret:
<span class="p_del">-ENDPROC(secondary_startup_64)</span>
<span class="p_add">+END(secondary_startup_64)</span>
 
 #include &quot;verify_cpu.S&quot;
 
<span class="p_chunk">@@ -247,6 +248,7 @@</span> <span class="p_context"> ENDPROC(secondary_startup_64)</span>
  */
 ENTRY(start_cpu0)
 	movq	initial_stack(%rip), %rsp
<span class="p_add">+	UNWIND_HINT_EMPTY</span>
 	jmp	.Ljump_to_C_code
 ENDPROC(start_cpu0)
 #endif
<span class="p_chunk">@@ -266,26 +268,24 @@</span> <span class="p_context"> ENDPROC(start_cpu0)</span>
 	.quad  init_thread_union + THREAD_SIZE - SIZEOF_PTREGS
 	__FINITDATA
 
<span class="p_del">-bad_address:</span>
<span class="p_del">-	jmp bad_address</span>
<span class="p_del">-</span>
 	__INIT
 ENTRY(early_idt_handler_array)
<span class="p_del">-	# 104(%rsp) %rflags</span>
<span class="p_del">-	#  96(%rsp) %cs</span>
<span class="p_del">-	#  88(%rsp) %rip</span>
<span class="p_del">-	#  80(%rsp) error code</span>
 	i = 0
 	.rept NUM_EXCEPTION_VECTORS
<span class="p_del">-	.ifeq (EXCEPTION_ERRCODE_MASK &gt;&gt; i) &amp; 1</span>
<span class="p_del">-	pushq $0		# Dummy error code, to make stack frame uniform</span>
<span class="p_add">+	.if ((EXCEPTION_ERRCODE_MASK &gt;&gt; i) &amp; 1) == 0</span>
<span class="p_add">+		UNWIND_HINT_IRET_REGS</span>
<span class="p_add">+		pushq $0	# Dummy error code, to make stack frame uniform</span>
<span class="p_add">+	.else</span>
<span class="p_add">+		UNWIND_HINT_IRET_REGS offset=8</span>
 	.endif
 	pushq $i		# 72(%rsp) Vector number
 	jmp early_idt_handler_common
<span class="p_add">+	UNWIND_HINT_IRET_REGS</span>
 	i = i + 1
 	.fill early_idt_handler_array + i*EARLY_IDT_HANDLER_SIZE - ., 1, 0xcc
 	.endr
<span class="p_del">-ENDPROC(early_idt_handler_array)</span>
<span class="p_add">+	UNWIND_HINT_IRET_REGS offset=16</span>
<span class="p_add">+END(early_idt_handler_array)</span>
 
 early_idt_handler_common:
 	/*
<span class="p_chunk">@@ -313,6 +313,7 @@</span> <span class="p_context"> early_idt_handler_common:</span>
 	pushq %r13				/* pt_regs-&gt;r13 */
 	pushq %r14				/* pt_regs-&gt;r14 */
 	pushq %r15				/* pt_regs-&gt;r15 */
<span class="p_add">+	UNWIND_HINT_REGS</span>
 
 	cmpq $14,%rsi		/* Page fault? */
 	jnz 10f
<span class="p_chunk">@@ -327,8 +328,8 @@</span> <span class="p_context"> early_idt_handler_common:</span>
 
 20:
 	decl early_recursion_flag(%rip)
<span class="p_del">-	jmp restore_regs_and_iret</span>
<span class="p_del">-ENDPROC(early_idt_handler_common)</span>
<span class="p_add">+	jmp restore_regs_and_return_to_kernel</span>
<span class="p_add">+END(early_idt_handler_common)</span>
 
 	__INITDATA
 
<span class="p_chunk">@@ -362,10 +363,7 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 
 	.data
 
<span class="p_del">-#ifndef CONFIG_XEN</span>
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_del">-	.fill	512,8,0</span>
<span class="p_del">-#else</span>
<span class="p_add">+#if defined(CONFIG_XEN_PV) || defined(CONFIG_XEN_PVH)</span>
 NEXT_PAGE(init_top_pgt)
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
 	.org    init_top_pgt + PGD_PAGE_OFFSET*8, 0
<span class="p_chunk">@@ -382,6 +380,9 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 	 * Don&#39;t set NX because code runs from these pages.
 	 */
 	PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)
<span class="p_add">+#else</span>
<span class="p_add">+NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+	.fill	512,8,0</span>
 #endif
 
 #ifdef CONFIG_X86_5LEVEL
<span class="p_chunk">@@ -435,7 +436,7 @@</span> <span class="p_context"> ENTRY(phys_base)</span>
 EXPORT_SYMBOL(phys_base)
 
 #include &quot;../../x86/xen/xen-head.S&quot;
<span class="p_del">-	</span>
<span class="p_add">+</span>
 	__PAGE_ALIGNED_BSS
 NEXT_PAGE(empty_zero_page)
 	.skip PAGE_SIZE
<span class="p_header">diff --git a/arch/x86/kernel/ioport.c b/arch/x86/kernel/ioport.c</span>
<span class="p_header">index 3feb648781c4..2f723301eb58 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ioport.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ioport.c</span>
<span class="p_chunk">@@ -67,7 +67,7 @@</span> <span class="p_context"> asmlinkage long sys_ioperm(unsigned long from, unsigned long num, int turn_on)</span>
 	 * because the -&gt;io_bitmap_max value must match the bitmap
 	 * contents:
 	 */
<span class="p_del">-	tss = &amp;per_cpu(cpu_tss, get_cpu());</span>
<span class="p_add">+	tss = &amp;per_cpu(cpu_tss_rw, get_cpu());</span>
 
 	if (turn_on)
 		bitmap_clear(t-&gt;io_bitmap_ptr, from, num);
<span class="p_header">diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c</span>
<span class="p_header">index 52089c043160..aa9d51eea9d0 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irq.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irq.c</span>
<span class="p_chunk">@@ -219,18 +219,6 @@</span> <span class="p_context"> __visible unsigned int __irq_entry do_IRQ(struct pt_regs *regs)</span>
 	/* high bit used in ret_from_ code  */
 	unsigned vector = ~regs-&gt;orig_ax;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * NB: Unlike exception entries, IRQ entries do not reliably</span>
<span class="p_del">-	 * handle context tracking in the low-level entry code.  This is</span>
<span class="p_del">-	 * because syscall entries execute briefly with IRQs on before</span>
<span class="p_del">-	 * updating context tracking state, so we can take an IRQ from</span>
<span class="p_del">-	 * kernel mode with CONTEXT_USER.  The low-level entry code only</span>
<span class="p_del">-	 * updates the context if we came from user mode, so we won&#39;t</span>
<span class="p_del">-	 * switch to CONTEXT_KERNEL.  We&#39;ll fix that once the syscall</span>
<span class="p_del">-	 * code is cleaned up enough that we can cleanly defer enabling</span>
<span class="p_del">-	 * IRQs.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-</span>
 	entering_irq();
 
 	/* entering_irq() tells RCU that we&#39;re not quiescent.  Check it. */
<span class="p_header">diff --git a/arch/x86/kernel/irq_64.c b/arch/x86/kernel/irq_64.c</span>
<span class="p_header">index 020efbf5786b..d86e344f5b3d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irq_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irq_64.c</span>
<span class="p_chunk">@@ -57,10 +57,10 @@</span> <span class="p_context"> static inline void stack_overflow_check(struct pt_regs *regs)</span>
 	if (regs-&gt;sp &gt;= estack_top &amp;&amp; regs-&gt;sp &lt;= estack_bottom)
 		return;
 
<span class="p_del">-	WARN_ONCE(1, &quot;do_IRQ(): %s has overflown the kernel stack (cur:%Lx,sp:%lx,irq stk top-bottom:%Lx-%Lx,exception stk top-bottom:%Lx-%Lx)\n&quot;,</span>
<span class="p_add">+	WARN_ONCE(1, &quot;do_IRQ(): %s has overflown the kernel stack (cur:%Lx,sp:%lx,irq stk top-bottom:%Lx-%Lx,exception stk top-bottom:%Lx-%Lx,ip:%pF)\n&quot;,</span>
 		current-&gt;comm, curbase, regs-&gt;sp,
 		irq_stack_top, irq_stack_bottom,
<span class="p_del">-		estack_top, estack_bottom);</span>
<span class="p_add">+		estack_top, estack_bottom, (void *)regs-&gt;ip);</span>
 
 	if (sysctl_panic_on_stackoverflow)
 		panic(&quot;low stack detected by irq handler - check messages\n&quot;);
<span class="p_header">diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="p_header">index 8bb9594d0761..a94de09edbed 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvm.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvm.c</span>
<span class="p_chunk">@@ -544,12 +544,12 @@</span> <span class="p_context"> static uint32_t __init kvm_detect(void)</span>
 	return kvm_cpuid_base();
 }
 
<span class="p_del">-const struct hypervisor_x86 x86_hyper_kvm __refconst = {</span>
<span class="p_add">+const __initconst struct hypervisor_x86 x86_hyper_kvm = {</span>
 	.name			= &quot;KVM&quot;,
 	.detect			= kvm_detect,
<span class="p_del">-	.x2apic_available	= kvm_para_available,</span>
<span class="p_add">+	.type			= X86_HYPER_KVM,</span>
<span class="p_add">+	.init.x2apic_available	= kvm_para_available,</span>
 };
<span class="p_del">-EXPORT_SYMBOL_GPL(x86_hyper_kvm);</span>
 
 static __init int activate_jump_labels(void)
 {
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index ae5615b03def..1c1eae961340 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -103,7 +103,7 @@</span> <span class="p_context"> static void finalize_ldt_struct(struct ldt_struct *ldt)</span>
 static void install_ldt(struct mm_struct *current_mm,
 			struct ldt_struct *ldt)
 {
<span class="p_del">-	/* Synchronizes with lockless_dereference in load_mm_ldt. */</span>
<span class="p_add">+	/* Synchronizes with READ_ONCE in load_mm_ldt. */</span>
 	smp_store_release(&amp;current_mm-&gt;context.ldt, ldt);
 
 	/* Activate the LDT for all CPUs using current_mm. */
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index ac0be8283325..9edadabf04f6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -10,7 +10,6 @@</span> <span class="p_context"> DEF_NATIVE(pv_irq_ops, save_fl, &quot;pushfq; popq %rax&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr2, &quot;movq %cr2, %rax&quot;);
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;movq %cr3, %rax&quot;);
 DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;movq %rdi, %cr3&quot;);
<span class="p_del">-DEF_NATIVE(pv_mmu_ops, flush_tlb_single, &quot;invlpg (%rdi)&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, wbinvd, &quot;wbinvd&quot;);
 
 DEF_NATIVE(pv_cpu_ops, usergs_sysret64, &quot;swapgs; sysretq&quot;);
<span class="p_chunk">@@ -60,7 +59,6 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr2);
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
<span class="p_del">-		PATCH_SITE(pv_mmu_ops, flush_tlb_single);</span>
 		PATCH_SITE(pv_cpu_ops, wbinvd);
 #if defined(CONFIG_PARAVIRT_SPINLOCKS)
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index c67685337c5a..517415978409 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -47,9 +47,25 @@</span> <span class="p_context"></span>
  * section. Since TSS&#39;s are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
<span class="p_del">-__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {</span>
<span class="p_add">+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss_rw) = {</span>
 	.x86_tss = {
<span class="p_del">-		.sp0 = TOP_OF_INIT_STACK,</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * .sp0 is only used when entering ring 0 from a lower</span>
<span class="p_add">+		 * privilege level.  Since the init task never runs anything</span>
<span class="p_add">+		 * but ring 0 code, there is no need for a valid value here.</span>
<span class="p_add">+		 * Poison it.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		.sp0 = (1UL &lt;&lt; (BITS_PER_LONG-1)) + 1,</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * .sp1 is cpu_current_top_of_stack.  The init task never</span>
<span class="p_add">+		 * runs user code, but cpu_current_top_of_stack should still</span>
<span class="p_add">+		 * be well defined before the first context switch.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		.sp1 = TOP_OF_INIT_STACK,</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_32
 		.ss0 = __KERNEL_DS,
 		.ss1 = __KERNEL_CS,
<span class="p_chunk">@@ -65,11 +81,8 @@</span> <span class="p_context"> __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {</span>
 	  */
 	.io_bitmap		= { [0 ... IO_BITMAP_LONGS] = ~0 },
 #endif
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	.SYSENTER_stack_canary	= STACK_END_MAGIC,</span>
<span class="p_del">-#endif</span>
 };
<span class="p_del">-EXPORT_PER_CPU_SYMBOL(cpu_tss);</span>
<span class="p_add">+EXPORT_PER_CPU_SYMBOL(cpu_tss_rw);</span>
 
 DEFINE_PER_CPU(bool, __tss_limit_invalid);
 EXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);
<span class="p_chunk">@@ -98,7 +111,7 @@</span> <span class="p_context"> void exit_thread(struct task_struct *tsk)</span>
 	struct fpu *fpu = &amp;t-&gt;fpu;
 
 	if (bp) {
<span class="p_del">-		struct tss_struct *tss = &amp;per_cpu(cpu_tss, get_cpu());</span>
<span class="p_add">+		struct tss_struct *tss = &amp;per_cpu(cpu_tss_rw, get_cpu());</span>
 
 		t-&gt;io_bitmap_ptr = NULL;
 		clear_thread_flag(TIF_IO_BITMAP);
<span class="p_header">diff --git a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c</span>
<span class="p_header">index 11966251cd42..5224c6099184 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_32.c</span>
<span class="p_chunk">@@ -234,7 +234,7 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	struct fpu *prev_fpu = &amp;prev-&gt;fpu;
 	struct fpu *next_fpu = &amp;next-&gt;fpu;
 	int cpu = smp_processor_id();
<span class="p_del">-	struct tss_struct *tss = &amp;per_cpu(cpu_tss, cpu);</span>
<span class="p_add">+	struct tss_struct *tss = &amp;per_cpu(cpu_tss_rw, cpu);</span>
 
 	/* never put a printk in __switch_to... printk() calls wake_up*() indirectly */
 
<span class="p_chunk">@@ -284,9 +284,11 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 
 	/*
 	 * Reload esp0 and cpu_current_top_of_stack.  This changes
<span class="p_del">-	 * current_thread_info().</span>
<span class="p_add">+	 * current_thread_info().  Refresh the SYSENTER configuration in</span>
<span class="p_add">+	 * case prev or next is vm86.</span>
 	 */
<span class="p_del">-	load_sp0(tss, next);</span>
<span class="p_add">+	update_sp0(next_p);</span>
<span class="p_add">+	refresh_sysenter_cs(next);</span>
 	this_cpu_write(cpu_current_top_of_stack,
 		       (unsigned long)task_stack_page(next_p) +
 		       THREAD_SIZE);
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index 302e7b2572d1..c75466232016 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -69,9 +69,8 @@</span> <span class="p_context"> void __show_regs(struct pt_regs *regs, int all)</span>
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
<span class="p_del">-	printk(KERN_DEFAULT &quot;RIP: %04lx:%pS\n&quot;, regs-&gt;cs, (void *)regs-&gt;ip);</span>
<span class="p_del">-	printk(KERN_DEFAULT &quot;RSP: %04lx:%016lx EFLAGS: %08lx&quot;, regs-&gt;ss,</span>
<span class="p_del">-		regs-&gt;sp, regs-&gt;flags);</span>
<span class="p_add">+	show_iret_regs(regs);</span>
<span class="p_add">+</span>
 	if (regs-&gt;orig_ax != -1)
 		pr_cont(&quot; ORIG_RAX: %016lx\n&quot;, regs-&gt;orig_ax);
 	else
<span class="p_chunk">@@ -88,6 +87,9 @@</span> <span class="p_context"> void __show_regs(struct pt_regs *regs, int all)</span>
 	printk(KERN_DEFAULT &quot;R13: %016lx R14: %016lx R15: %016lx\n&quot;,
 	       regs-&gt;r13, regs-&gt;r14, regs-&gt;r15);
 
<span class="p_add">+	if (!all)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	asm(&quot;movl %%ds,%0&quot; : &quot;=r&quot; (ds));
 	asm(&quot;movl %%cs,%0&quot; : &quot;=r&quot; (cs));
 	asm(&quot;movl %%es,%0&quot; : &quot;=r&quot; (es));
<span class="p_chunk">@@ -98,9 +100,6 @@</span> <span class="p_context"> void __show_regs(struct pt_regs *regs, int all)</span>
 	rdmsrl(MSR_GS_BASE, gs);
 	rdmsrl(MSR_KERNEL_GS_BASE, shadowgs);
 
<span class="p_del">-	if (!all)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = __read_cr3();
<span class="p_chunk">@@ -274,7 +273,6 @@</span> <span class="p_context"> int copy_thread_tls(unsigned long clone_flags, unsigned long sp,</span>
 	struct inactive_task_frame *frame;
 	struct task_struct *me = current;
 
<span class="p_del">-	p-&gt;thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;</span>
 	childregs = task_pt_regs(p);
 	fork_frame = container_of(childregs, struct fork_frame, regs);
 	frame = &amp;fork_frame-&gt;frame;
<span class="p_chunk">@@ -401,7 +399,7 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	struct fpu *prev_fpu = &amp;prev-&gt;fpu;
 	struct fpu *next_fpu = &amp;next-&gt;fpu;
 	int cpu = smp_processor_id();
<span class="p_del">-	struct tss_struct *tss = &amp;per_cpu(cpu_tss, cpu);</span>
<span class="p_add">+	struct tss_struct *tss = &amp;per_cpu(cpu_tss_rw, cpu);</span>
 
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &amp;&amp;
 		     this_cpu_read(irq_count) != -1);
<span class="p_chunk">@@ -463,9 +461,10 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	 * Switch the PDA and FPU contexts.
 	 */
 	this_cpu_write(current_task, next_p);
<span class="p_add">+	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));</span>
 
<span class="p_del">-	/* Reload esp0 and ss1.  This changes current_thread_info(). */</span>
<span class="p_del">-	load_sp0(tss, next);</span>
<span class="p_add">+	/* Reload sp0. */</span>
<span class="p_add">+	update_sp0(next_p);</span>
 
 	/*
 	 * Now maybe reload the debug registers and handle I/O bitmaps
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index 5e0453f18a57..142126ab5aae 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -962,8 +962,7 @@</span> <span class="p_context"> void common_cpu_up(unsigned int cpu, struct task_struct *idle)</span>
 #ifdef CONFIG_X86_32
 	/* Stack for startup_32 can be just as for start_secondary onwards */
 	irq_ctx_init(cpu);
<span class="p_del">-	per_cpu(cpu_current_top_of_stack, cpu) =</span>
<span class="p_del">-		(unsigned long)task_stack_page(idle) + THREAD_SIZE;</span>
<span class="p_add">+	per_cpu(cpu_current_top_of_stack, cpu) = task_top_of_stack(idle);</span>
 #else
 	initial_gs = per_cpu_offset(cpu);
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index 5a6b8f809792..74136fd16f49 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -141,8 +141,7 @@</span> <span class="p_context"> void ist_begin_non_atomic(struct pt_regs *regs)</span>
 	 * will catch asm bugs and any attempt to use ist_preempt_enable
 	 * from double_fault.
 	 */
<span class="p_del">-	BUG_ON((unsigned long)(current_top_of_stack() -</span>
<span class="p_del">-			       current_stack_pointer) &gt;= THREAD_SIZE);</span>
<span class="p_add">+	BUG_ON(!on_thread_stack());</span>
 
 	preempt_enable_no_resched();
 }
<span class="p_chunk">@@ -349,9 +348,15 @@</span> <span class="p_context"> dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)</span>
 
 	/*
 	 * If IRET takes a non-IST fault on the espfix64 stack, then we
<span class="p_del">-	 * end up promoting it to a doublefault.  In that case, modify</span>
<span class="p_del">-	 * the stack to make it look like we just entered the #GP</span>
<span class="p_del">-	 * handler from user space, similar to bad_iret.</span>
<span class="p_add">+	 * end up promoting it to a doublefault.  In that case, take</span>
<span class="p_add">+	 * advantage of the fact that we&#39;re not using the normal (TSS.sp0)</span>
<span class="p_add">+	 * stack right now.  We can write a fake #GP(0) frame at TSS.sp0</span>
<span class="p_add">+	 * and then modify our own IRET frame so that, when we return,</span>
<span class="p_add">+	 * we land directly at the #GP(0) vector with the stack already</span>
<span class="p_add">+	 * set up according to its expectations.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The net result is that our #GP handler will think that we</span>
<span class="p_add">+	 * entered from usermode with the bad user context.</span>
 	 *
 	 * No need for ist_enter here because we don&#39;t use RCU.
 	 */
<span class="p_chunk">@@ -359,13 +364,26 @@</span> <span class="p_context"> dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)</span>
 		regs-&gt;cs == __KERNEL_CS &amp;&amp;
 		regs-&gt;ip == (unsigned long)native_irq_return_iret)
 	{
<span class="p_del">-		struct pt_regs *normal_regs = task_pt_regs(current);</span>
<span class="p_add">+		struct pt_regs *gpregs = (struct pt_regs *)this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;</span>
 
<span class="p_del">-		/* Fake a #GP(0) from userspace. */</span>
<span class="p_del">-		memmove(&amp;normal_regs-&gt;ip, (void *)regs-&gt;sp, 5*8);</span>
<span class="p_del">-		normal_regs-&gt;orig_ax = 0;  /* Missing (lost) #GP error code */</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * regs-&gt;sp points to the failing IRET frame on the</span>
<span class="p_add">+		 * ESPFIX64 stack.  Copy it to the entry stack.  This fills</span>
<span class="p_add">+		 * in gpregs-&gt;ss through gpregs-&gt;ip.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		memmove(&amp;gpregs-&gt;ip, (void *)regs-&gt;sp, 5*8);</span>
<span class="p_add">+		gpregs-&gt;orig_ax = 0;  /* Missing (lost) #GP error code */</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Adjust our frame so that we return straight to the #GP</span>
<span class="p_add">+		 * vector with the expected RSP value.  This is safe because</span>
<span class="p_add">+		 * we won&#39;t enable interupts or schedule before we invoke</span>
<span class="p_add">+		 * general_protection, so nothing will clobber the stack</span>
<span class="p_add">+		 * frame we just set up.</span>
<span class="p_add">+		 */</span>
 		regs-&gt;ip = (unsigned long)general_protection;
<span class="p_del">-		regs-&gt;sp = (unsigned long)&amp;normal_regs-&gt;orig_ax;</span>
<span class="p_add">+		regs-&gt;sp = (unsigned long)&amp;gpregs-&gt;orig_ax;</span>
 
 		return;
 	}
<span class="p_chunk">@@ -390,7 +408,7 @@</span> <span class="p_context"> dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)</span>
 	 *
 	 *   Processors update CR2 whenever a page fault is detected. If a
 	 *   second page fault occurs while an earlier page fault is being
<span class="p_del">-	 *   deliv- ered, the faulting linear address of the second fault will</span>
<span class="p_add">+	 *   delivered, the faulting linear address of the second fault will</span>
 	 *   overwrite the contents of CR2 (replacing the previous
 	 *   address). These updates to CR2 occur even if the page fault
 	 *   results in a double fault or occurs during the delivery of a
<span class="p_chunk">@@ -601,14 +619,15 @@</span> <span class="p_context"> NOKPROBE_SYMBOL(do_int3);</span>
 
 #ifdef CONFIG_X86_64
 /*
<span class="p_del">- * Help handler running on IST stack to switch off the IST stack if the</span>
<span class="p_del">- * interrupted code was in user mode. The actual stack switch is done in</span>
<span class="p_del">- * entry_64.S</span>
<span class="p_add">+ * Help handler running on a per-cpu (IST or entry trampoline) stack</span>
<span class="p_add">+ * to switch to the normal thread stack if the interrupted code was in</span>
<span class="p_add">+ * user mode. The actual stack switch is done in entry_64.S</span>
  */
 asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
<span class="p_del">-	struct pt_regs *regs = task_pt_regs(current);</span>
<span class="p_del">-	*regs = *eregs;</span>
<span class="p_add">+	struct pt_regs *regs = (struct pt_regs *)this_cpu_read(cpu_current_top_of_stack) - 1;</span>
<span class="p_add">+	if (regs != eregs)</span>
<span class="p_add">+		*regs = *eregs;</span>
 	return regs;
 }
 NOKPROBE_SYMBOL(sync_regs);
<span class="p_chunk">@@ -624,13 +643,13 @@</span> <span class="p_context"> struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s)</span>
 	/*
 	 * This is called from entry_64.S early in handling a fault
 	 * caused by a bad iret to user mode.  To handle the fault
<span class="p_del">-	 * correctly, we want move our stack frame to task_pt_regs</span>
<span class="p_del">-	 * and we want to pretend that the exception came from the</span>
<span class="p_del">-	 * iret target.</span>
<span class="p_add">+	 * correctly, we want to move our stack frame to where it would</span>
<span class="p_add">+	 * be had we entered directly on the entry stack (rather than</span>
<span class="p_add">+	 * just below the IRET frame) and we want to pretend that the</span>
<span class="p_add">+	 * exception came from the IRET target.</span>
 	 */
 	struct bad_iret_stack *new_stack =
<span class="p_del">-		container_of(task_pt_regs(current),</span>
<span class="p_del">-			     struct bad_iret_stack, regs);</span>
<span class="p_add">+		(struct bad_iret_stack *)this_cpu_read(cpu_tss_rw.x86_tss.sp0) - 1;</span>
 
 	/* Copy the IRET target to the new stack. */
 	memmove(&amp;new_stack-&gt;regs.ip, (void *)s-&gt;regs.sp, 5*8);
<span class="p_chunk">@@ -795,14 +814,6 @@</span> <span class="p_context"> dotraplinkage void do_debug(struct pt_regs *regs, long error_code)</span>
 	debug_stack_usage_dec();
 
 exit:
<span class="p_del">-#if defined(CONFIG_X86_32)</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This is the most likely code path that involves non-trivial use</span>
<span class="p_del">-	 * of the SYSENTER stack.  Check that we haven&#39;t overrun it.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	WARN(this_cpu_read(cpu_tss.SYSENTER_stack_canary) != STACK_END_MAGIC,</span>
<span class="p_del">-	     &quot;Overran or corrupted SYSENTER stack\n&quot;);</span>
<span class="p_del">-#endif</span>
 	ist_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug);
<span class="p_chunk">@@ -929,6 +940,9 @@</span> <span class="p_context"> dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)</span>
 
 void __init trap_init(void)
 {
<span class="p_add">+	/* Init cpu_entry_area before IST entries are set up */</span>
<span class="p_add">+	setup_cpu_entry_areas();</span>
<span class="p_add">+</span>
 	idt_setup_traps();
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/unwind_orc.c b/arch/x86/kernel/unwind_orc.c</span>
<span class="p_header">index a3f973b2c97a..be86a865087a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/unwind_orc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/unwind_orc.c</span>
<span class="p_chunk">@@ -253,22 +253,15 @@</span> <span class="p_context"> unsigned long *unwind_get_return_address_ptr(struct unwind_state *state)</span>
 	return NULL;
 }
 
<span class="p_del">-static bool stack_access_ok(struct unwind_state *state, unsigned long addr,</span>
<span class="p_add">+static bool stack_access_ok(struct unwind_state *state, unsigned long _addr,</span>
 			    size_t len)
 {
 	struct stack_info *info = &amp;state-&gt;stack_info;
<span class="p_add">+	void *addr = (void *)_addr;</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If the address isn&#39;t on the current stack, switch to the next one.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * We may have to traverse multiple stacks to deal with the possibility</span>
<span class="p_del">-	 * that info-&gt;next_sp could point to an empty stack and the address</span>
<span class="p_del">-	 * could be on a subsequent stack.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	while (!on_stack(info, (void *)addr, len))</span>
<span class="p_del">-		if (get_stack_info(info-&gt;next_sp, state-&gt;task, info,</span>
<span class="p_del">-				   &amp;state-&gt;stack_mask))</span>
<span class="p_del">-			return false;</span>
<span class="p_add">+	if (!on_stack(info, addr, len) &amp;&amp;</span>
<span class="p_add">+	    (get_stack_info(addr, state-&gt;task, info, &amp;state-&gt;stack_mask)))</span>
<span class="p_add">+		return false;</span>
 
 	return true;
 }
<span class="p_chunk">@@ -283,42 +276,32 @@</span> <span class="p_context"> static bool deref_stack_reg(struct unwind_state *state, unsigned long addr,</span>
 	return true;
 }
 
<span class="p_del">-#define REGS_SIZE (sizeof(struct pt_regs))</span>
<span class="p_del">-#define SP_OFFSET (offsetof(struct pt_regs, sp))</span>
<span class="p_del">-#define IRET_REGS_SIZE (REGS_SIZE - offsetof(struct pt_regs, ip))</span>
<span class="p_del">-#define IRET_SP_OFFSET (SP_OFFSET - offsetof(struct pt_regs, ip))</span>
<span class="p_del">-</span>
 static bool deref_stack_regs(struct unwind_state *state, unsigned long addr,
<span class="p_del">-			     unsigned long *ip, unsigned long *sp, bool full)</span>
<span class="p_add">+			     unsigned long *ip, unsigned long *sp)</span>
 {
<span class="p_del">-	size_t regs_size = full ? REGS_SIZE : IRET_REGS_SIZE;</span>
<span class="p_del">-	size_t sp_offset = full ? SP_OFFSET : IRET_SP_OFFSET;</span>
<span class="p_del">-	struct pt_regs *regs = (struct pt_regs *)(addr + regs_size - REGS_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_64)) {</span>
<span class="p_del">-		if (!stack_access_ok(state, addr, regs_size))</span>
<span class="p_del">-			return false;</span>
<span class="p_add">+	struct pt_regs *regs = (struct pt_regs *)addr;</span>
 
<span class="p_del">-		*ip = regs-&gt;ip;</span>
<span class="p_del">-		*sp = regs-&gt;sp;</span>
<span class="p_add">+	/* x86-32 support will be more complicated due to the &amp;regs-&gt;sp hack */</span>
<span class="p_add">+	BUILD_BUG_ON(IS_ENABLED(CONFIG_X86_32));</span>
 
<span class="p_del">-		return true;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!stack_access_ok(state, addr, sp_offset))</span>
<span class="p_add">+	if (!stack_access_ok(state, addr, sizeof(struct pt_regs)))</span>
 		return false;
 
 	*ip = regs-&gt;ip;
<span class="p_add">+	*sp = regs-&gt;sp;</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	if (user_mode(regs)) {</span>
<span class="p_del">-		if (!stack_access_ok(state, addr + sp_offset,</span>
<span class="p_del">-				     REGS_SIZE - SP_OFFSET))</span>
<span class="p_del">-			return false;</span>
<span class="p_add">+static bool deref_stack_iret_regs(struct unwind_state *state, unsigned long addr,</span>
<span class="p_add">+				  unsigned long *ip, unsigned long *sp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pt_regs *regs = (void *)addr - IRET_FRAME_OFFSET;</span>
 
<span class="p_del">-		*sp = regs-&gt;sp;</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		*sp = (unsigned long)&amp;regs-&gt;sp;</span>
<span class="p_add">+	if (!stack_access_ok(state, addr, IRET_FRAME_SIZE))</span>
<span class="p_add">+		return false;</span>
 
<span class="p_add">+	*ip = regs-&gt;ip;</span>
<span class="p_add">+	*sp = regs-&gt;sp;</span>
 	return true;
 }
 
<span class="p_chunk">@@ -327,7 +310,6 @@</span> <span class="p_context"> bool unwind_next_frame(struct unwind_state *state)</span>
 	unsigned long ip_p, sp, orig_ip, prev_sp = state-&gt;sp;
 	enum stack_type prev_type = state-&gt;stack_info.type;
 	struct orc_entry *orc;
<span class="p_del">-	struct pt_regs *ptregs;</span>
 	bool indirect = false;
 
 	if (unwind_done(state))
<span class="p_chunk">@@ -435,7 +417,7 @@</span> <span class="p_context"> bool unwind_next_frame(struct unwind_state *state)</span>
 		break;
 
 	case ORC_TYPE_REGS:
<span class="p_del">-		if (!deref_stack_regs(state, sp, &amp;state-&gt;ip, &amp;state-&gt;sp, true)) {</span>
<span class="p_add">+		if (!deref_stack_regs(state, sp, &amp;state-&gt;ip, &amp;state-&gt;sp)) {</span>
 			orc_warn(&quot;can&#39;t dereference registers at %p for ip %pB\n&quot;,
 				 (void *)sp, (void *)orig_ip);
 			goto done;
<span class="p_chunk">@@ -447,20 +429,14 @@</span> <span class="p_context"> bool unwind_next_frame(struct unwind_state *state)</span>
 		break;
 
 	case ORC_TYPE_REGS_IRET:
<span class="p_del">-		if (!deref_stack_regs(state, sp, &amp;state-&gt;ip, &amp;state-&gt;sp, false)) {</span>
<span class="p_add">+		if (!deref_stack_iret_regs(state, sp, &amp;state-&gt;ip, &amp;state-&gt;sp)) {</span>
 			orc_warn(&quot;can&#39;t dereference iret registers at %p for ip %pB\n&quot;,
 				 (void *)sp, (void *)orig_ip);
 			goto done;
 		}
 
<span class="p_del">-		ptregs = container_of((void *)sp, struct pt_regs, ip);</span>
<span class="p_del">-		if ((unsigned long)ptregs &gt;= prev_sp &amp;&amp;</span>
<span class="p_del">-		    on_stack(&amp;state-&gt;stack_info, ptregs, REGS_SIZE)) {</span>
<span class="p_del">-			state-&gt;regs = ptregs;</span>
<span class="p_del">-			state-&gt;full_regs = false;</span>
<span class="p_del">-		} else</span>
<span class="p_del">-			state-&gt;regs = NULL;</span>
<span class="p_del">-</span>
<span class="p_add">+		state-&gt;regs = (void *)sp - IRET_FRAME_OFFSET;</span>
<span class="p_add">+		state-&gt;full_regs = false;</span>
 		state-&gt;signal = true;
 		break;
 
<span class="p_chunk">@@ -553,8 +529,18 @@</span> <span class="p_context"> void __unwind_start(struct unwind_state *state, struct task_struct *task,</span>
 	}
 
 	if (get_stack_info((unsigned long *)state-&gt;sp, state-&gt;task,
<span class="p_del">-			   &amp;state-&gt;stack_info, &amp;state-&gt;stack_mask))</span>
<span class="p_del">-		return;</span>
<span class="p_add">+			   &amp;state-&gt;stack_info, &amp;state-&gt;stack_mask)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We weren&#39;t on a valid stack.  It&#39;s possible that</span>
<span class="p_add">+		 * we overflowed a valid stack into a guard page.</span>
<span class="p_add">+		 * See if the next page up is valid so that we can</span>
<span class="p_add">+		 * generate some kind of backtrace if this happens.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		void *next_page = (void *)PAGE_ALIGN((unsigned long)state-&gt;sp);</span>
<span class="p_add">+		if (get_stack_info(next_page, state-&gt;task, &amp;state-&gt;stack_info,</span>
<span class="p_add">+				   &amp;state-&gt;stack_mask))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
 
 	/*
 	 * The caller can provide the address of the first frame directly
<span class="p_header">diff --git a/arch/x86/kernel/verify_cpu.S b/arch/x86/kernel/verify_cpu.S</span>
<span class="p_header">index 014ea59aa153..3d3c2f71f617 100644</span>
<span class="p_header">--- a/arch/x86/kernel/verify_cpu.S</span>
<span class="p_header">+++ b/arch/x86/kernel/verify_cpu.S</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/cpufeatures.h&gt;
 #include &lt;asm/msr-index.h&gt;
 
<span class="p_del">-verify_cpu:</span>
<span class="p_add">+ENTRY(verify_cpu)</span>
 	pushf				# Save caller passed flags
 	push	$0			# Kill any dangerous flags
 	popf
<span class="p_chunk">@@ -139,3 +139,4 @@</span> <span class="p_context"> verify_cpu:</span>
 	popf				# Restore caller passed flags
 	xorl %eax, %eax
 	ret
<span class="p_add">+ENDPROC(verify_cpu)</span>
<span class="p_header">diff --git a/arch/x86/kernel/vm86_32.c b/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">index 68244742ecb0..5edb27f1a2c4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/vm86_32.c</span>
<span class="p_chunk">@@ -55,6 +55,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/irq.h&gt;
 #include &lt;asm/traps.h&gt;
 #include &lt;asm/vm86.h&gt;
<span class="p_add">+#include &lt;asm/switch_to.h&gt;</span>
 
 /*
  * Known problems:
<span class="p_chunk">@@ -94,7 +95,6 @@</span> <span class="p_context"></span>
 
 void save_v86_state(struct kernel_vm86_regs *regs, int retval)
 {
<span class="p_del">-	struct tss_struct *tss;</span>
 	struct task_struct *tsk = current;
 	struct vm86plus_struct __user *user;
 	struct vm86 *vm86 = current-&gt;thread.vm86;
<span class="p_chunk">@@ -146,12 +146,13 @@</span> <span class="p_context"> void save_v86_state(struct kernel_vm86_regs *regs, int retval)</span>
 		do_exit(SIGSEGV);
 	}
 
<span class="p_del">-	tss = &amp;per_cpu(cpu_tss, get_cpu());</span>
<span class="p_add">+	preempt_disable();</span>
 	tsk-&gt;thread.sp0 = vm86-&gt;saved_sp0;
 	tsk-&gt;thread.sysenter_cs = __KERNEL_CS;
<span class="p_del">-	load_sp0(tss, &amp;tsk-&gt;thread);</span>
<span class="p_add">+	update_sp0(tsk);</span>
<span class="p_add">+	refresh_sysenter_cs(&amp;tsk-&gt;thread);</span>
 	vm86-&gt;saved_sp0 = 0;
<span class="p_del">-	put_cpu();</span>
<span class="p_add">+	preempt_enable();</span>
 
 	memcpy(&amp;regs-&gt;pt, &amp;vm86-&gt;regs32, sizeof(struct pt_regs));
 
<span class="p_chunk">@@ -237,7 +238,6 @@</span> <span class="p_context"> SYSCALL_DEFINE2(vm86, unsigned long, cmd, unsigned long, arg)</span>
 
 static long do_sys_vm86(struct vm86plus_struct __user *user_vm86, bool plus)
 {
<span class="p_del">-	struct tss_struct *tss;</span>
 	struct task_struct *tsk = current;
 	struct vm86 *vm86 = tsk-&gt;thread.vm86;
 	struct kernel_vm86_regs vm86regs;
<span class="p_chunk">@@ -365,15 +365,17 @@</span> <span class="p_context"> static long do_sys_vm86(struct vm86plus_struct __user *user_vm86, bool plus)</span>
 	vm86-&gt;saved_sp0 = tsk-&gt;thread.sp0;
 	lazy_save_gs(vm86-&gt;regs32.gs);
 
<span class="p_del">-	tss = &amp;per_cpu(cpu_tss, get_cpu());</span>
 	/* make room for real-mode segments */
<span class="p_add">+	preempt_disable();</span>
 	tsk-&gt;thread.sp0 += 16;
 
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_SEP))</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_SEP)) {</span>
 		tsk-&gt;thread.sysenter_cs = 0;
<span class="p_add">+		refresh_sysenter_cs(&amp;tsk-&gt;thread);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	load_sp0(tss, &amp;tsk-&gt;thread);</span>
<span class="p_del">-	put_cpu();</span>
<span class="p_add">+	update_sp0(tsk);</span>
<span class="p_add">+	preempt_enable();</span>
 
 	if (vm86-&gt;flags &amp; VM86_SCREEN_BITMAP)
 		mark_screen_rdonly(tsk-&gt;mm);
<span class="p_header">diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">index a4009fb9be87..d2a8b5a24a44 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">+++ b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_chunk">@@ -107,6 +107,15 @@</span> <span class="p_context"> SECTIONS</span>
 		SOFTIRQENTRY_TEXT
 		*(.fixup)
 		*(.gnu.warning)
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+		. = ALIGN(PAGE_SIZE);</span>
<span class="p_add">+		_entry_trampoline = .;</span>
<span class="p_add">+		*(.entry_trampoline)</span>
<span class="p_add">+		. = ALIGN(PAGE_SIZE);</span>
<span class="p_add">+		ASSERT(. - _entry_trampoline == PAGE_SIZE, &quot;entry trampoline is too big&quot;);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 		/* End of text section */
 		_etext = .;
 	} :text = 0x9090
<span class="p_header">diff --git a/arch/x86/kernel/x86_init.c b/arch/x86/kernel/x86_init.c</span>
<span class="p_header">index a088b2c47f73..5b2d10c1973a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/x86_init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/x86_init.c</span>
<span class="p_chunk">@@ -28,6 +28,8 @@</span> <span class="p_context"> void x86_init_noop(void) { }</span>
 void __init x86_init_uint_noop(unsigned int unused) { }
 int __init iommu_init_noop(void) { return 0; }
 void iommu_shutdown_noop(void) { }
<span class="p_add">+bool __init bool_x86_init_noop(void) { return false; }</span>
<span class="p_add">+void x86_op_int_noop(int cpu) { }</span>
 
 /*
  * The platform setup functions are preset with the default functions
<span class="p_chunk">@@ -81,6 +83,12 @@</span> <span class="p_context"> struct x86_init_ops x86_init __initdata = {</span>
 		.init_irq		= x86_default_pci_init_irq,
 		.fixup_irqs		= x86_default_pci_fixup_irqs,
 	},
<span class="p_add">+</span>
<span class="p_add">+	.hyper = {</span>
<span class="p_add">+		.init_platform		= x86_init_noop,</span>
<span class="p_add">+		.x2apic_available	= bool_x86_init_noop,</span>
<span class="p_add">+		.init_mem_mapping	= x86_init_noop,</span>
<span class="p_add">+	},</span>
 };
 
 struct x86_cpuinit_ops x86_cpuinit = {
<span class="p_chunk">@@ -101,6 +109,7 @@</span> <span class="p_context"> struct x86_platform_ops x86_platform __ro_after_init = {</span>
 	.get_nmi_reason			= default_get_nmi_reason,
 	.save_sched_clock_state 	= tsc_save_sched_clock_state,
 	.restore_sched_clock_state 	= tsc_restore_sched_clock_state,
<span class="p_add">+	.hyper.pin_vcpu			= x86_op_int_noop,</span>
 };
 
 EXPORT_SYMBOL_GPL(x86_platform);
<span class="p_header">diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c</span>
<span class="p_header">index 7a69cf053711..13ebeedcec07 100644</span>
<span class="p_header">--- a/arch/x86/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/x86/kvm/mmu.c</span>
<span class="p_chunk">@@ -5476,13 +5476,13 @@</span> <span class="p_context"> int kvm_mmu_module_init(void)</span>
 
 	pte_list_desc_cache = kmem_cache_create(&quot;pte_list_desc&quot;,
 					    sizeof(struct pte_list_desc),
<span class="p_del">-					    0, 0, NULL);</span>
<span class="p_add">+					    0, SLAB_ACCOUNT, NULL);</span>
 	if (!pte_list_desc_cache)
 		goto nomem;
 
 	mmu_page_header_cache = kmem_cache_create(&quot;kvm_mmu_page_header&quot;,
 						  sizeof(struct kvm_mmu_page),
<span class="p_del">-						  0, 0, NULL);</span>
<span class="p_add">+						  0, SLAB_ACCOUNT, NULL);</span>
 	if (!mmu_page_header_cache)
 		goto nomem;
 
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index bc5921c1e2f2..47d9432756f3 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -2295,7 +2295,7 @@</span> <span class="p_context"> static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 		 * processors.  See 22.2.4.
 		 */
 		vmcs_writel(HOST_TR_BASE,
<span class="p_del">-			    (unsigned long)this_cpu_ptr(&amp;cpu_tss));</span>
<span class="p_add">+			    (unsigned long)&amp;get_cpu_entry_area(cpu)-&gt;tss.x86_tss);</span>
 		vmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   /* 22.2.4 */
 
 		/*
<span class="p_header">diff --git a/arch/x86/lib/delay.c b/arch/x86/lib/delay.c</span>
<span class="p_header">index 553f8fd23cc4..4846eff7e4c8 100644</span>
<span class="p_header">--- a/arch/x86/lib/delay.c</span>
<span class="p_header">+++ b/arch/x86/lib/delay.c</span>
<span class="p_chunk">@@ -107,10 +107,10 @@</span> <span class="p_context"> static void delay_mwaitx(unsigned long __loops)</span>
 		delay = min_t(u64, MWAITX_MAX_LOOPS, loops);
 
 		/*
<span class="p_del">-		 * Use cpu_tss as a cacheline-aligned, seldomly</span>
<span class="p_add">+		 * Use cpu_tss_rw as a cacheline-aligned, seldomly</span>
 		 * accessed per-cpu variable as the monitor target.
 		 */
<span class="p_del">-		__monitorx(raw_cpu_ptr(&amp;cpu_tss), 0, 0);</span>
<span class="p_add">+		__monitorx(raw_cpu_ptr(&amp;cpu_tss_rw), 0, 0);</span>
 
 		/*
 		 * AMD, like Intel, supports the EAX hint and EAX=0xf
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index b0ff378650a9..3109ba6c6ede 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -29,26 +29,6 @@</span> <span class="p_context"></span>
 #define CREATE_TRACE_POINTS
 #include &lt;asm/trace/exceptions.h&gt;
 
<span class="p_del">-/*</span>
<span class="p_del">- * Page fault error code bits:</span>
<span class="p_del">- *</span>
<span class="p_del">- *   bit 0 ==	 0: no page found	1: protection fault</span>
<span class="p_del">- *   bit 1 ==	 0: read access		1: write access</span>
<span class="p_del">- *   bit 2 ==	 0: kernel-mode access	1: user-mode access</span>
<span class="p_del">- *   bit 3 ==				1: use of reserved bit detected</span>
<span class="p_del">- *   bit 4 ==				1: fault was an instruction fetch</span>
<span class="p_del">- *   bit 5 ==				1: protection keys block access</span>
<span class="p_del">- */</span>
<span class="p_del">-enum x86_pf_error_code {</span>
<span class="p_del">-</span>
<span class="p_del">-	PF_PROT		=		1 &lt;&lt; 0,</span>
<span class="p_del">-	PF_WRITE	=		1 &lt;&lt; 1,</span>
<span class="p_del">-	PF_USER		=		1 &lt;&lt; 2,</span>
<span class="p_del">-	PF_RSVD		=		1 &lt;&lt; 3,</span>
<span class="p_del">-	PF_INSTR	=		1 &lt;&lt; 4,</span>
<span class="p_del">-	PF_PK		=		1 &lt;&lt; 5,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
<span class="p_chunk">@@ -150,7 +130,7 @@</span> <span class="p_context"> is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)</span>
 	 * If it was a exec (instruction fetch) fault on NX page, then
 	 * do not ignore the fault:
 	 */
<span class="p_del">-	if (error_code &amp; PF_INSTR)</span>
<span class="p_add">+	if (error_code &amp; X86_PF_INSTR)</span>
 		return 0;
 
 	instr = (void *)convert_ip_to_linear(current, regs);
<span class="p_chunk">@@ -180,7 +160,7 @@</span> <span class="p_context"> is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)</span>
  * siginfo so userspace can discover which protection key was set
  * on the PTE.
  *
<span class="p_del">- * If we get here, we know that the hardware signaled a PF_PK</span>
<span class="p_add">+ * If we get here, we know that the hardware signaled a X86_PF_PK</span>
  * fault and that there was a VMA once we got in the fault
  * handler.  It does *not* guarantee that the VMA we find here
  * was the one that we faulted on.
<span class="p_chunk">@@ -205,7 +185,7 @@</span> <span class="p_context"> static void fill_sig_info_pkey(int si_code, siginfo_t *info, u32 *pkey)</span>
 	/*
 	 * force_sig_info_fault() is called from a number of
 	 * contexts, some of which have a VMA and some of which
<span class="p_del">-	 * do not.  The PF_PK handing happens after we have a</span>
<span class="p_add">+	 * do not.  The X86_PF_PK handing happens after we have a</span>
 	 * valid VMA, so we should never reach this without a
 	 * valid VMA.
 	 */
<span class="p_chunk">@@ -698,7 +678,7 @@</span> <span class="p_context"> show_fault_oops(struct pt_regs *regs, unsigned long error_code,</span>
 	if (!oops_may_print())
 		return;
 
<span class="p_del">-	if (error_code &amp; PF_INSTR) {</span>
<span class="p_add">+	if (error_code &amp; X86_PF_INSTR) {</span>
 		unsigned int level;
 		pgd_t *pgd;
 		pte_t *pte;
<span class="p_chunk">@@ -780,7 +760,7 @@</span> <span class="p_context"> no_context(struct pt_regs *regs, unsigned long error_code,</span>
 		 */
 		if (current-&gt;thread.sig_on_uaccess_err &amp;&amp; signal) {
 			tsk-&gt;thread.trap_nr = X86_TRAP_PF;
<span class="p_del">-			tsk-&gt;thread.error_code = error_code | PF_USER;</span>
<span class="p_add">+			tsk-&gt;thread.error_code = error_code | X86_PF_USER;</span>
 			tsk-&gt;thread.cr2 = address;
 
 			/* XXX: hwpoison faults will set the wrong code. */
<span class="p_chunk">@@ -898,7 +878,7 @@</span> <span class="p_context"> __bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,</span>
 	struct task_struct *tsk = current;
 
 	/* User mode accesses just cause a SIGSEGV */
<span class="p_del">-	if (error_code &amp; PF_USER) {</span>
<span class="p_add">+	if (error_code &amp; X86_PF_USER) {</span>
 		/*
 		 * It&#39;s possible to have interrupts off here:
 		 */
<span class="p_chunk">@@ -919,7 +899,7 @@</span> <span class="p_context"> __bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,</span>
 		 * Instruction fetch faults in the vsyscall page might need
 		 * emulation.
 		 */
<span class="p_del">-		if (unlikely((error_code &amp; PF_INSTR) &amp;&amp;</span>
<span class="p_add">+		if (unlikely((error_code &amp; X86_PF_INSTR) &amp;&amp;</span>
 			     ((address &amp; ~0xfff) == VSYSCALL_ADDR))) {
 			if (emulate_vsyscall(regs, address))
 				return;
<span class="p_chunk">@@ -932,7 +912,7 @@</span> <span class="p_context"> __bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,</span>
 		 * are always protection faults.
 		 */
 		if (address &gt;= TASK_SIZE_MAX)
<span class="p_del">-			error_code |= PF_PROT;</span>
<span class="p_add">+			error_code |= X86_PF_PROT;</span>
 
 		if (likely(show_unhandled_signals))
 			show_signal_msg(regs, error_code, address, tsk);
<span class="p_chunk">@@ -993,11 +973,11 @@</span> <span class="p_context"> static inline bool bad_area_access_from_pkeys(unsigned long error_code,</span>
 
 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
 		return false;
<span class="p_del">-	if (error_code &amp; PF_PK)</span>
<span class="p_add">+	if (error_code &amp; X86_PF_PK)</span>
 		return true;
 	/* this checks permission keys on the VMA: */
<span class="p_del">-	if (!arch_vma_access_permitted(vma, (error_code &amp; PF_WRITE),</span>
<span class="p_del">-				(error_code &amp; PF_INSTR), foreign))</span>
<span class="p_add">+	if (!arch_vma_access_permitted(vma, (error_code &amp; X86_PF_WRITE),</span>
<span class="p_add">+				       (error_code &amp; X86_PF_INSTR), foreign))</span>
 		return true;
 	return false;
 }
<span class="p_chunk">@@ -1025,7 +1005,7 @@</span> <span class="p_context"> do_sigbus(struct pt_regs *regs, unsigned long error_code, unsigned long address,</span>
 	int code = BUS_ADRERR;
 
 	/* Kernel mode? Handle exceptions or die: */
<span class="p_del">-	if (!(error_code &amp; PF_USER)) {</span>
<span class="p_add">+	if (!(error_code &amp; X86_PF_USER)) {</span>
 		no_context(regs, error_code, address, SIGBUS, BUS_ADRERR);
 		return;
 	}
<span class="p_chunk">@@ -1053,14 +1033,14 @@</span> <span class="p_context"> static noinline void</span>
 mm_fault_error(struct pt_regs *regs, unsigned long error_code,
 	       unsigned long address, u32 *pkey, unsigned int fault)
 {
<span class="p_del">-	if (fatal_signal_pending(current) &amp;&amp; !(error_code &amp; PF_USER)) {</span>
<span class="p_add">+	if (fatal_signal_pending(current) &amp;&amp; !(error_code &amp; X86_PF_USER)) {</span>
 		no_context(regs, error_code, address, 0, 0);
 		return;
 	}
 
 	if (fault &amp; VM_FAULT_OOM) {
 		/* Kernel mode? Handle exceptions or die: */
<span class="p_del">-		if (!(error_code &amp; PF_USER)) {</span>
<span class="p_add">+		if (!(error_code &amp; X86_PF_USER)) {</span>
 			no_context(regs, error_code, address,
 				   SIGSEGV, SEGV_MAPERR);
 			return;
<span class="p_chunk">@@ -1085,16 +1065,16 @@</span> <span class="p_context"> mm_fault_error(struct pt_regs *regs, unsigned long error_code,</span>
 
 static int spurious_fault_check(unsigned long error_code, pte_t *pte)
 {
<span class="p_del">-	if ((error_code &amp; PF_WRITE) &amp;&amp; !pte_write(*pte))</span>
<span class="p_add">+	if ((error_code &amp; X86_PF_WRITE) &amp;&amp; !pte_write(*pte))</span>
 		return 0;
 
<span class="p_del">-	if ((error_code &amp; PF_INSTR) &amp;&amp; !pte_exec(*pte))</span>
<span class="p_add">+	if ((error_code &amp; X86_PF_INSTR) &amp;&amp; !pte_exec(*pte))</span>
 		return 0;
 	/*
 	 * Note: We do not do lazy flushing on protection key
<span class="p_del">-	 * changes, so no spurious fault will ever set PF_PK.</span>
<span class="p_add">+	 * changes, so no spurious fault will ever set X86_PF_PK.</span>
 	 */
<span class="p_del">-	if ((error_code &amp; PF_PK))</span>
<span class="p_add">+	if ((error_code &amp; X86_PF_PK))</span>
 		return 1;
 
 	return 1;
<span class="p_chunk">@@ -1140,8 +1120,8 @@</span> <span class="p_context"> spurious_fault(unsigned long error_code, unsigned long address)</span>
 	 * change, so user accesses are not expected to cause spurious
 	 * faults.
 	 */
<span class="p_del">-	if (error_code != (PF_WRITE | PF_PROT)</span>
<span class="p_del">-	    &amp;&amp; error_code != (PF_INSTR | PF_PROT))</span>
<span class="p_add">+	if (error_code != (X86_PF_WRITE | X86_PF_PROT) &amp;&amp;</span>
<span class="p_add">+	    error_code != (X86_PF_INSTR | X86_PF_PROT))</span>
 		return 0;
 
 	pgd = init_mm.pgd + pgd_index(address);
<span class="p_chunk">@@ -1201,19 +1181,19 @@</span> <span class="p_context"> access_error(unsigned long error_code, struct vm_area_struct *vma)</span>
 	 * always an unconditional error and can never result in
 	 * a follow-up action to resolve the fault, like a COW.
 	 */
<span class="p_del">-	if (error_code &amp; PF_PK)</span>
<span class="p_add">+	if (error_code &amp; X86_PF_PK)</span>
 		return 1;
 
 	/*
 	 * Make sure to check the VMA so that we do not perform
<span class="p_del">-	 * faults just to hit a PF_PK as soon as we fill in a</span>
<span class="p_add">+	 * faults just to hit a X86_PF_PK as soon as we fill in a</span>
 	 * page.
 	 */
<span class="p_del">-	if (!arch_vma_access_permitted(vma, (error_code &amp; PF_WRITE),</span>
<span class="p_del">-				(error_code &amp; PF_INSTR), foreign))</span>
<span class="p_add">+	if (!arch_vma_access_permitted(vma, (error_code &amp; X86_PF_WRITE),</span>
<span class="p_add">+				       (error_code &amp; X86_PF_INSTR), foreign))</span>
 		return 1;
 
<span class="p_del">-	if (error_code &amp; PF_WRITE) {</span>
<span class="p_add">+	if (error_code &amp; X86_PF_WRITE) {</span>
 		/* write, present and write, not present: */
 		if (unlikely(!(vma-&gt;vm_flags &amp; VM_WRITE)))
 			return 1;
<span class="p_chunk">@@ -1221,7 +1201,7 @@</span> <span class="p_context"> access_error(unsigned long error_code, struct vm_area_struct *vma)</span>
 	}
 
 	/* read, present: */
<span class="p_del">-	if (unlikely(error_code &amp; PF_PROT))</span>
<span class="p_add">+	if (unlikely(error_code &amp; X86_PF_PROT))</span>
 		return 1;
 
 	/* read, not present: */
<span class="p_chunk">@@ -1244,7 +1224,7 @@</span> <span class="p_context"> static inline bool smap_violation(int error_code, struct pt_regs *regs)</span>
 	if (!static_cpu_has(X86_FEATURE_SMAP))
 		return false;
 
<span class="p_del">-	if (error_code &amp; PF_USER)</span>
<span class="p_add">+	if (error_code &amp; X86_PF_USER)</span>
 		return false;
 
 	if (!user_mode(regs) &amp;&amp; (regs-&gt;flags &amp; X86_EFLAGS_AC))
<span class="p_chunk">@@ -1297,7 +1277,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * protection error (error_code &amp; 9) == 0.
 	 */
 	if (unlikely(fault_in_kernel_space(address))) {
<span class="p_del">-		if (!(error_code &amp; (PF_RSVD | PF_USER | PF_PROT))) {</span>
<span class="p_add">+		if (!(error_code &amp; (X86_PF_RSVD | X86_PF_USER | X86_PF_PROT))) {</span>
 			if (vmalloc_fault(address) &gt;= 0)
 				return;
 
<span class="p_chunk">@@ -1325,7 +1305,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	if (unlikely(kprobes_fault(regs)))
 		return;
 
<span class="p_del">-	if (unlikely(error_code &amp; PF_RSVD))</span>
<span class="p_add">+	if (unlikely(error_code &amp; X86_PF_RSVD))</span>
 		pgtable_bad(regs, error_code, address);
 
 	if (unlikely(smap_violation(error_code, regs))) {
<span class="p_chunk">@@ -1351,7 +1331,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 */
 	if (user_mode(regs)) {
 		local_irq_enable();
<span class="p_del">-		error_code |= PF_USER;</span>
<span class="p_add">+		error_code |= X86_PF_USER;</span>
 		flags |= FAULT_FLAG_USER;
 	} else {
 		if (regs-&gt;flags &amp; X86_EFLAGS_IF)
<span class="p_chunk">@@ -1360,9 +1340,9 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 
<span class="p_del">-	if (error_code &amp; PF_WRITE)</span>
<span class="p_add">+	if (error_code &amp; X86_PF_WRITE)</span>
 		flags |= FAULT_FLAG_WRITE;
<span class="p_del">-	if (error_code &amp; PF_INSTR)</span>
<span class="p_add">+	if (error_code &amp; X86_PF_INSTR)</span>
 		flags |= FAULT_FLAG_INSTRUCTION;
 
 	/*
<span class="p_chunk">@@ -1382,7 +1362,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * space check, thus avoiding the deadlock:
 	 */
 	if (unlikely(!down_read_trylock(&amp;mm-&gt;mmap_sem))) {
<span class="p_del">-		if ((error_code &amp; PF_USER) == 0 &amp;&amp;</span>
<span class="p_add">+		if (!(error_code &amp; X86_PF_USER) &amp;&amp;</span>
 		    !search_exception_tables(regs-&gt;ip)) {
 			bad_area_nosemaphore(regs, error_code, address, NULL);
 			return;
<span class="p_chunk">@@ -1409,7 +1389,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 		bad_area(regs, error_code, address);
 		return;
 	}
<span class="p_del">-	if (error_code &amp; PF_USER) {</span>
<span class="p_add">+	if (error_code &amp; X86_PF_USER) {</span>
 		/*
 		 * Accessing the stack below %sp is always a bug.
 		 * The large cushion allows instructions like enter
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index af5c1ed21d43..a22c2b95e513 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -671,7 +671,7 @@</span> <span class="p_context"> void __init init_mem_mapping(void)</span>
 	load_cr3(swapper_pg_dir);
 	__flush_tlb_all();
 
<span class="p_del">-	hypervisor_init_mem_mapping();</span>
<span class="p_add">+	x86_init.hyper.init_mem_mapping();</span>
 
 	early_memtest(0, max_pfn_mapped &lt;&lt; PAGE_SHIFT);
 }
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index 048fbe8fc274..adcea90a2046 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -1426,16 +1426,16 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)</span>
 
 #if defined(CONFIG_MEMORY_HOTPLUG_SPARSE) &amp;&amp; defined(CONFIG_HAVE_BOOTMEM_INFO_NODE)
 void register_page_bootmem_memmap(unsigned long section_nr,
<span class="p_del">-				  struct page *start_page, unsigned long size)</span>
<span class="p_add">+				  struct page *start_page, unsigned long nr_pages)</span>
 {
 	unsigned long addr = (unsigned long)start_page;
<span class="p_del">-	unsigned long end = (unsigned long)(start_page + size);</span>
<span class="p_add">+	unsigned long end = (unsigned long)(start_page + nr_pages);</span>
 	unsigned long next;
 	pgd_t *pgd;
 	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
<span class="p_del">-	unsigned int nr_pages;</span>
<span class="p_add">+	unsigned int nr_pmd_pages;</span>
 	struct page *page;
 
 	for (; addr &lt; end; addr = next) {
<span class="p_chunk">@@ -1482,9 +1482,9 @@</span> <span class="p_context"> void register_page_bootmem_memmap(unsigned long section_nr,</span>
 			if (pmd_none(*pmd))
 				continue;
 
<span class="p_del">-			nr_pages = 1 &lt;&lt; (get_order(PMD_SIZE));</span>
<span class="p_add">+			nr_pmd_pages = 1 &lt;&lt; get_order(PMD_SIZE);</span>
 			page = pmd_page(*pmd);
<span class="p_del">-			while (nr_pages--)</span>
<span class="p_add">+			while (nr_pmd_pages--)</span>
 				get_page_bootmem(section_nr, page++,
 						 SECTION_INFO);
 		}
<span class="p_header">diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">index 8f5be3eb40dd..9ec70d780f1f 100644</span>
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -4,19 +4,150 @@</span> <span class="p_context"></span>
 #include &lt;linux/bootmem.h&gt;
 #include &lt;linux/kasan.h&gt;
 #include &lt;linux/kdebug.h&gt;
<span class="p_add">+#include &lt;linux/memblock.h&gt;</span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/sched/task.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 
 #include &lt;asm/e820/types.h&gt;
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/sections.h&gt;
 #include &lt;asm/pgtable.h&gt;
 
 extern struct range pfn_mapped[E820_MAX_ENTRIES];
 
<span class="p_del">-static int __init map_range(struct range *range)</span>
<span class="p_add">+static p4d_t tmp_p4d_table[PTRS_PER_P4D] __initdata __aligned(PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+static __init void *early_alloc(size_t size, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return memblock_virt_alloc_try_nid_nopanic(size, size,</span>
<span class="p_add">+		__pa(MAX_DMA_ADDRESS), BOOTMEM_ALLOC_ACCESSIBLE, nid);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_populate_pmd(pmd_t *pmd, unsigned long addr,</span>
<span class="p_add">+				      unsigned long end, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		void *p;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_PSE) &amp;&amp;</span>
<span class="p_add">+		    ((end - addr) == PMD_SIZE) &amp;&amp;</span>
<span class="p_add">+		    IS_ALIGNED(addr, PMD_SIZE)) {</span>
<span class="p_add">+			p = early_alloc(PMD_SIZE, nid);</span>
<span class="p_add">+			if (p &amp;&amp; pmd_set_huge(pmd, __pa(p), PAGE_KERNEL))</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			else if (p)</span>
<span class="p_add">+				memblock_free(__pa(p), PMD_SIZE);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		pmd_populate_kernel(&amp;init_mm, pmd, p);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pte_t entry;</span>
<span class="p_add">+		void *p;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_none(*pte))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		entry = pfn_pte(PFN_DOWN(__pa(p)), PAGE_KERNEL);</span>
<span class="p_add">+		set_pte_at(&amp;init_mm, addr, pte, entry);</span>
<span class="p_add">+	} while (pte++, addr += PAGE_SIZE, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_populate_pud(pud_t *pud, unsigned long addr,</span>
<span class="p_add">+				      unsigned long end, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		void *p;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_GBPAGES) &amp;&amp;</span>
<span class="p_add">+		    ((end - addr) == PUD_SIZE) &amp;&amp;</span>
<span class="p_add">+		    IS_ALIGNED(addr, PUD_SIZE)) {</span>
<span class="p_add">+			p = early_alloc(PUD_SIZE, nid);</span>
<span class="p_add">+			if (p &amp;&amp; pud_set_huge(pud, __pa(p), PAGE_KERNEL))</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			else if (p)</span>
<span class="p_add">+				memblock_free(__pa(p), PUD_SIZE);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		pud_populate(&amp;init_mm, pud, p);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+		if (!pmd_large(*pmd))</span>
<span class="p_add">+			kasan_populate_pmd(pmd, addr, next, nid);</span>
<span class="p_add">+	} while (pmd++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_populate_p4d(p4d_t *p4d, unsigned long addr,</span>
<span class="p_add">+				      unsigned long end, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		void *p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+</span>
<span class="p_add">+		p4d_populate(&amp;init_mm, p4d, p);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pud_addr_end(addr, end);</span>
<span class="p_add">+		if (!pud_large(*pud))</span>
<span class="p_add">+			kasan_populate_pud(pud, addr, next, nid);</span>
<span class="p_add">+	} while (pud++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_populate_pgd(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+				      unsigned long end, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *p;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		pgd_populate(&amp;init_mm, pgd, p);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		kasan_populate_p4d(p4d, addr, next, nid);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_populate_shadow(unsigned long addr, unsigned long end,</span>
<span class="p_add">+					 int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	addr = addr &amp; PAGE_MASK;</span>
<span class="p_add">+	end = round_up(end, PAGE_SIZE);</span>
<span class="p_add">+	pgd = pgd_offset_k(addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		kasan_populate_pgd(pgd, addr, next, nid);</span>
<span class="p_add">+	} while (pgd++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init map_range(struct range *range)</span>
 {
 	unsigned long start;
 	unsigned long end;
<span class="p_chunk">@@ -24,15 +155,17 @@</span> <span class="p_context"> static int __init map_range(struct range *range)</span>
 	start = (unsigned long)kasan_mem_to_shadow(pfn_to_kaddr(range-&gt;start));
 	end = (unsigned long)kasan_mem_to_shadow(pfn_to_kaddr(range-&gt;end));
 
<span class="p_del">-	return vmemmap_populate(start, end, NUMA_NO_NODE);</span>
<span class="p_add">+	kasan_populate_shadow(start, end, early_pfn_to_nid(range-&gt;start));</span>
 }
 
 static void __init clear_pgds(unsigned long start,
 			unsigned long end)
 {
 	pgd_t *pgd;
<span class="p_add">+	/* See comment in kasan_init() */</span>
<span class="p_add">+	unsigned long pgd_end = end &amp; PGDIR_MASK;</span>
 
<span class="p_del">-	for (; start &lt; end; start += PGDIR_SIZE) {</span>
<span class="p_add">+	for (; start &lt; pgd_end; start += PGDIR_SIZE) {</span>
 		pgd = pgd_offset_k(start);
 		/*
 		 * With folded p4d, pgd_clear() is nop, use p4d_clear()
<span class="p_chunk">@@ -43,29 +176,61 @@</span> <span class="p_context"> static void __init clear_pgds(unsigned long start,</span>
 		else
 			pgd_clear(pgd);
 	}
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(start);</span>
<span class="p_add">+	for (; start &lt; end; start += P4D_SIZE)</span>
<span class="p_add">+		p4d_clear(p4d_offset(pgd, start));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline p4d_t *early_p4d_offset(pgd_t *pgd, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long p4d;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_X86_5LEVEL))</span>
<span class="p_add">+		return (p4d_t *)pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = __pa_nodebug(pgd_val(*pgd)) &amp; PTE_PFN_MASK;</span>
<span class="p_add">+	p4d += __START_KERNEL_map - phys_base;</span>
<span class="p_add">+	return (p4d_t *)p4d + p4d_index(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_early_p4d_populate(pgd_t *pgd,</span>
<span class="p_add">+		unsigned long addr,</span>
<span class="p_add">+		unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t pgd_entry;</span>
<span class="p_add">+	p4d_t *p4d, p4d_entry;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		pgd_entry = __pgd(_KERNPG_TABLE | __pa_nodebug(kasan_zero_p4d));</span>
<span class="p_add">+		set_pgd(pgd, pgd_entry);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = early_p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!p4d_none(*p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		p4d_entry = __p4d(_KERNPG_TABLE | __pa_nodebug(kasan_zero_pud));</span>
<span class="p_add">+		set_p4d(p4d, p4d_entry);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end &amp;&amp; p4d_none(*p4d));</span>
 }
 
 static void __init kasan_map_early_shadow(pgd_t *pgd)
 {
<span class="p_del">-	int i;</span>
<span class="p_del">-	unsigned long start = KASAN_SHADOW_START;</span>
<span class="p_add">+	/* See comment in kasan_init() */</span>
<span class="p_add">+	unsigned long addr = KASAN_SHADOW_START &amp; PGDIR_MASK;</span>
 	unsigned long end = KASAN_SHADOW_END;
<span class="p_add">+	unsigned long next;</span>
 
<span class="p_del">-	for (i = pgd_index(start); start &lt; end; i++) {</span>
<span class="p_del">-		switch (CONFIG_PGTABLE_LEVELS) {</span>
<span class="p_del">-		case 4:</span>
<span class="p_del">-			pgd[i] = __pgd(__pa_nodebug(kasan_zero_pud) |</span>
<span class="p_del">-					_KERNPG_TABLE);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		case 5:</span>
<span class="p_del">-			pgd[i] = __pgd(__pa_nodebug(kasan_zero_p4d) |</span>
<span class="p_del">-					_KERNPG_TABLE);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		default:</span>
<span class="p_del">-			BUILD_BUG();</span>
<span class="p_del">-		}</span>
<span class="p_del">-		start += PGDIR_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	pgd += pgd_index(addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		kasan_early_p4d_populate(pgd, addr, next);</span>
<span class="p_add">+	} while (pgd++, addr = next, addr != end);</span>
 }
 
 #ifdef CONFIG_KASAN_INLINE
<span class="p_chunk">@@ -102,7 +267,7 @@</span> <span class="p_context"> void __init kasan_early_init(void)</span>
 	for (i = 0; i &lt; PTRS_PER_PUD; i++)
 		kasan_zero_pud[i] = __pud(pud_val);
 
<span class="p_del">-	for (i = 0; CONFIG_PGTABLE_LEVELS &gt;= 5 &amp;&amp; i &lt; PTRS_PER_P4D; i++)</span>
<span class="p_add">+	for (i = 0; IS_ENABLED(CONFIG_X86_5LEVEL) &amp;&amp; i &lt; PTRS_PER_P4D; i++)</span>
 		kasan_zero_p4d[i] = __p4d(p4d_val);
 
 	kasan_map_early_shadow(early_top_pgt);
<span class="p_chunk">@@ -112,37 +277,76 @@</span> <span class="p_context"> void __init kasan_early_init(void)</span>
 void __init kasan_init(void)
 {
 	int i;
<span class="p_add">+	void *shadow_cpu_entry_begin, *shadow_cpu_entry_end;</span>
 
 #ifdef CONFIG_KASAN_INLINE
 	register_die_notifier(&amp;kasan_die_notifier);
 #endif
 
 	memcpy(early_top_pgt, init_top_pgt, sizeof(early_top_pgt));
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We use the same shadow offset for 4- and 5-level paging to</span>
<span class="p_add">+	 * facilitate boot-time switching between paging modes.</span>
<span class="p_add">+	 * As result in 5-level paging mode KASAN_SHADOW_START and</span>
<span class="p_add">+	 * KASAN_SHADOW_END are not aligned to PGD boundary.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * KASAN_SHADOW_START doesn&#39;t share PGD with anything else.</span>
<span class="p_add">+	 * We claim whole PGD entry to make things easier.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * KASAN_SHADOW_END lands in the last PGD entry and it collides with</span>
<span class="p_add">+	 * bunch of things like kernel code, modules, EFI mapping, etc.</span>
<span class="p_add">+	 * We need to take extra steps to not overwrite them.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_add">+		void *ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+		ptr = (void *)pgd_page_vaddr(*pgd_offset_k(KASAN_SHADOW_END));</span>
<span class="p_add">+		memcpy(tmp_p4d_table, (void *)ptr, sizeof(tmp_p4d_table));</span>
<span class="p_add">+		set_pgd(&amp;early_top_pgt[pgd_index(KASAN_SHADOW_END)],</span>
<span class="p_add">+				__pgd(__pa(tmp_p4d_table) | _KERNPG_TABLE));</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	load_cr3(early_top_pgt);
 	__flush_tlb_all();
 
<span class="p_del">-	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);</span>
<span class="p_add">+	clear_pgds(KASAN_SHADOW_START &amp; PGDIR_MASK, KASAN_SHADOW_END);</span>
 
<span class="p_del">-	kasan_populate_zero_shadow((void *)KASAN_SHADOW_START,</span>
<span class="p_add">+	kasan_populate_zero_shadow((void *)(KASAN_SHADOW_START &amp; PGDIR_MASK),</span>
 			kasan_mem_to_shadow((void *)PAGE_OFFSET));
 
 	for (i = 0; i &lt; E820_MAX_ENTRIES; i++) {
 		if (pfn_mapped[i].end == 0)
 			break;
 
<span class="p_del">-		if (map_range(&amp;pfn_mapped[i]))</span>
<span class="p_del">-			panic(&quot;kasan: unable to allocate shadow!&quot;);</span>
<span class="p_add">+		map_range(&amp;pfn_mapped[i]);</span>
 	}
<span class="p_add">+</span>
 	kasan_populate_zero_shadow(
 		kasan_mem_to_shadow((void *)PAGE_OFFSET + MAXMEM),
 		kasan_mem_to_shadow((void *)__START_KERNEL_map));
 
<span class="p_del">-	vmemmap_populate((unsigned long)kasan_mem_to_shadow(_stext),</span>
<span class="p_del">-			(unsigned long)kasan_mem_to_shadow(_end),</span>
<span class="p_del">-			NUMA_NO_NODE);</span>
<span class="p_add">+	kasan_populate_shadow((unsigned long)kasan_mem_to_shadow(_stext),</span>
<span class="p_add">+			      (unsigned long)kasan_mem_to_shadow(_end),</span>
<span class="p_add">+			      early_pfn_to_nid(__pa(_stext)));</span>
<span class="p_add">+</span>
<span class="p_add">+	shadow_cpu_entry_begin = (void *)__fix_to_virt(FIX_CPU_ENTRY_AREA_BOTTOM);</span>
<span class="p_add">+	shadow_cpu_entry_begin = kasan_mem_to_shadow(shadow_cpu_entry_begin);</span>
<span class="p_add">+	shadow_cpu_entry_begin = (void *)round_down((unsigned long)shadow_cpu_entry_begin,</span>
<span class="p_add">+						PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	shadow_cpu_entry_end = (void *)(__fix_to_virt(FIX_CPU_ENTRY_AREA_TOP) + PAGE_SIZE);</span>
<span class="p_add">+	shadow_cpu_entry_end = kasan_mem_to_shadow(shadow_cpu_entry_end);</span>
<span class="p_add">+	shadow_cpu_entry_end = (void *)round_up((unsigned long)shadow_cpu_entry_end,</span>
<span class="p_add">+					PAGE_SIZE);</span>
 
 	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),
<span class="p_del">-			(void *)KASAN_SHADOW_END);</span>
<span class="p_add">+				   shadow_cpu_entry_begin);</span>
<span class="p_add">+</span>
<span class="p_add">+	kasan_populate_shadow((unsigned long)shadow_cpu_entry_begin,</span>
<span class="p_add">+			      (unsigned long)shadow_cpu_entry_end, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	kasan_populate_zero_shadow(shadow_cpu_entry_end, (void *)KASAN_SHADOW_END);</span>
 
 	load_cr3(init_top_pgt);
 	__flush_tlb_all();
<span class="p_header">diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c</span>
<span class="p_header">index 84fcfde53f8f..04d5157fe7f8 100644</span>
<span class="p_header">--- a/arch/x86/power/cpu.c</span>
<span class="p_header">+++ b/arch/x86/power/cpu.c</span>
<span class="p_chunk">@@ -160,17 +160,19 @@</span> <span class="p_context"> static void do_fpu_end(void)</span>
 static void fix_processor_context(void)
 {
 	int cpu = smp_processor_id();
<span class="p_del">-	struct tss_struct *t = &amp;per_cpu(cpu_tss, cpu);</span>
 #ifdef CONFIG_X86_64
 	struct desc_struct *desc = get_cpu_gdt_rw(cpu);
 	tss_desc tss;
 #endif
<span class="p_del">-	set_tss_desc(cpu, t);	/*</span>
<span class="p_del">-				 * This just modifies memory; should not be</span>
<span class="p_del">-				 * necessary. But... This is necessary, because</span>
<span class="p_del">-				 * 386 hardware has concept of busy TSS or some</span>
<span class="p_del">-				 * similar stupidity.</span>
<span class="p_del">-				 */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We need to reload TR, which requires that we change the</span>
<span class="p_add">+	 * GDT entry to indicate &quot;available&quot; first.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * XXX: This could probably all be replaced by a call to</span>
<span class="p_add">+	 * force_reload_TR().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	set_tss_desc(cpu, &amp;get_cpu_entry_area(cpu)-&gt;tss.x86_tss);</span>
 
 #ifdef CONFIG_X86_64
 	memcpy(&amp;tss, &amp;desc[GDT_ENTRY_TSS], sizeof(tss_desc));
<span class="p_header">diff --git a/arch/x86/xen/enlighten_hvm.c b/arch/x86/xen/enlighten_hvm.c</span>
<span class="p_header">index de503c225ae1..754d5391d9fa 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten_hvm.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten_hvm.c</span>
<span class="p_chunk">@@ -226,12 +226,12 @@</span> <span class="p_context"> static uint32_t __init xen_platform_hvm(void)</span>
 	return xen_cpuid_base();
 }
 
<span class="p_del">-const struct hypervisor_x86 x86_hyper_xen_hvm = {</span>
<span class="p_add">+const __initconst struct hypervisor_x86 x86_hyper_xen_hvm = {</span>
 	.name                   = &quot;Xen HVM&quot;,
 	.detect                 = xen_platform_hvm,
<span class="p_del">-	.init_platform          = xen_hvm_guest_init,</span>
<span class="p_del">-	.pin_vcpu               = xen_pin_vcpu,</span>
<span class="p_del">-	.x2apic_available       = xen_x2apic_para_available,</span>
<span class="p_del">-	.init_mem_mapping	= xen_hvm_init_mem_mapping,</span>
<span class="p_add">+	.type			= X86_HYPER_XEN_HVM,</span>
<span class="p_add">+	.init.init_platform     = xen_hvm_guest_init,</span>
<span class="p_add">+	.init.x2apic_available  = xen_x2apic_para_available,</span>
<span class="p_add">+	.init.init_mem_mapping	= xen_hvm_init_mem_mapping,</span>
<span class="p_add">+	.runtime.pin_vcpu       = xen_pin_vcpu,</span>
 };
<span class="p_del">-EXPORT_SYMBOL(x86_hyper_xen_hvm);</span>
<span class="p_header">diff --git a/arch/x86/xen/enlighten_pv.c b/arch/x86/xen/enlighten_pv.c</span>
<span class="p_header">index d4396e27b1fb..ae3a071e1d0f 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten_pv.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten_pv.c</span>
<span class="p_chunk">@@ -601,7 +601,7 @@</span> <span class="p_context"> static struct trap_array_entry trap_array[] = {</span>
 #ifdef CONFIG_X86_MCE
 	{ machine_check,               xen_machine_check,               true },
 #endif
<span class="p_del">-	{ nmi,                         xen_nmi,                         true },</span>
<span class="p_add">+	{ nmi,                         xen_xennmi,                      true },</span>
 	{ overflow,                    xen_overflow,                    false },
 #ifdef CONFIG_IA32_EMULATION
 	{ entry_INT80_compat,          xen_entry_INT80_compat,          false },
<span class="p_chunk">@@ -811,15 +811,14 @@</span> <span class="p_context"> static void __init xen_write_gdt_entry_boot(struct desc_struct *dt, int entry,</span>
 	}
 }
 
<span class="p_del">-static void xen_load_sp0(struct tss_struct *tss,</span>
<span class="p_del">-			 struct thread_struct *thread)</span>
<span class="p_add">+static void xen_load_sp0(unsigned long sp0)</span>
 {
 	struct multicall_space mcs;
 
 	mcs = xen_mc_entry(0);
<span class="p_del">-	MULTI_stack_switch(mcs.mc, __KERNEL_DS, thread-&gt;sp0);</span>
<span class="p_add">+	MULTI_stack_switch(mcs.mc, __KERNEL_DS, sp0);</span>
 	xen_mc_issue(PARAVIRT_LAZY_CPU);
<span class="p_del">-	tss-&gt;x86_tss.sp0 = thread-&gt;sp0;</span>
<span class="p_add">+	this_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);</span>
 }
 
 void xen_set_iopl_mask(unsigned mask)
<span class="p_chunk">@@ -1460,9 +1459,9 @@</span> <span class="p_context"> static uint32_t __init xen_platform_pv(void)</span>
 	return 0;
 }
 
<span class="p_del">-const struct hypervisor_x86 x86_hyper_xen_pv = {</span>
<span class="p_add">+const __initconst struct hypervisor_x86 x86_hyper_xen_pv = {</span>
 	.name                   = &quot;Xen PV&quot;,
 	.detect                 = xen_platform_pv,
<span class="p_del">-	.pin_vcpu               = xen_pin_vcpu,</span>
<span class="p_add">+	.type			= X86_HYPER_XEN_PV,</span>
<span class="p_add">+	.runtime.pin_vcpu       = xen_pin_vcpu,</span>
 };
<span class="p_del">-EXPORT_SYMBOL(x86_hyper_xen_pv);</span>
<span class="p_header">diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">index 71495f1a86d7..c2454237fa67 100644</span>
<span class="p_header">--- a/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">+++ b/arch/x86/xen/mmu_pv.c</span>
<span class="p_chunk">@@ -449,7 +449,7 @@</span> <span class="p_context"> __visible pmd_t xen_make_pmd(pmdval_t pmd)</span>
 }
 PV_CALLEE_SAVE_REGS_THUNK(xen_make_pmd);
 
<span class="p_del">-#if CONFIG_PGTABLE_LEVELS == 4</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 __visible pudval_t xen_pud_val(pud_t pud)
 {
 	return pte_mfn_to_pfn(pud.pud);
<span class="p_chunk">@@ -538,7 +538,7 @@</span> <span class="p_context"> static void xen_set_p4d(p4d_t *ptr, p4d_t val)</span>
 
 	xen_mc_issue(PARAVIRT_LAZY_MMU);
 }
<span class="p_del">-#endif	/* CONFIG_PGTABLE_LEVELS == 4 */</span>
<span class="p_add">+#endif	/* CONFIG_X86_64 */</span>
 
 static int xen_pmd_walk(struct mm_struct *mm, pmd_t *pmd,
 		int (*func)(struct mm_struct *mm, struct page *, enum pt_level),
<span class="p_chunk">@@ -580,21 +580,17 @@</span> <span class="p_context"> static int xen_p4d_walk(struct mm_struct *mm, p4d_t *p4d,</span>
 		int (*func)(struct mm_struct *mm, struct page *, enum pt_level),
 		bool last, unsigned long limit)
 {
<span class="p_del">-	int i, nr, flush = 0;</span>
<span class="p_add">+	int flush = 0;</span>
<span class="p_add">+	pud_t *pud;</span>
 
<span class="p_del">-	nr = last ? p4d_index(limit) + 1 : PTRS_PER_P4D;</span>
<span class="p_del">-	for (i = 0; i &lt; nr; i++) {</span>
<span class="p_del">-		pud_t *pud;</span>
 
<span class="p_del">-		if (p4d_none(p4d[i]))</span>
<span class="p_del">-			continue;</span>
<span class="p_add">+	if (p4d_none(*p4d))</span>
<span class="p_add">+		return flush;</span>
 
<span class="p_del">-		pud = pud_offset(&amp;p4d[i], 0);</span>
<span class="p_del">-		if (PTRS_PER_PUD &gt; 1)</span>
<span class="p_del">-			flush |= (*func)(mm, virt_to_page(pud), PT_PUD);</span>
<span class="p_del">-		flush |= xen_pud_walk(mm, pud, func,</span>
<span class="p_del">-				last &amp;&amp; i == nr - 1, limit);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	pud = pud_offset(p4d, 0);</span>
<span class="p_add">+	if (PTRS_PER_PUD &gt; 1)</span>
<span class="p_add">+		flush |= (*func)(mm, virt_to_page(pud), PT_PUD);</span>
<span class="p_add">+	flush |= xen_pud_walk(mm, pud, func, last, limit);</span>
 	return flush;
 }
 
<span class="p_chunk">@@ -644,8 +640,6 @@</span> <span class="p_context"> static int __xen_pgd_walk(struct mm_struct *mm, pgd_t *pgd,</span>
 			continue;
 
 		p4d = p4d_offset(&amp;pgd[i], 0);
<span class="p_del">-		if (PTRS_PER_P4D &gt; 1)</span>
<span class="p_del">-			flush |= (*func)(mm, virt_to_page(p4d), PT_P4D);</span>
 		flush |= xen_p4d_walk(mm, p4d, func, i == nr - 1, limit);
 	}
 
<span class="p_chunk">@@ -1176,22 +1170,14 @@</span> <span class="p_context"> static void __init xen_cleanmfnmap(unsigned long vaddr)</span>
 {
 	pgd_t *pgd;
 	p4d_t *p4d;
<span class="p_del">-	unsigned int i;</span>
 	bool unpin;
 
 	unpin = (vaddr == 2 * PGDIR_SIZE);
 	vaddr &amp;= PMD_MASK;
 	pgd = pgd_offset_k(vaddr);
 	p4d = p4d_offset(pgd, 0);
<span class="p_del">-	for (i = 0; i &lt; PTRS_PER_P4D; i++) {</span>
<span class="p_del">-		if (p4d_none(p4d[i]))</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-		xen_cleanmfnmap_p4d(p4d + i, unpin);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-		set_pgd(pgd, __pgd(0));</span>
<span class="p_del">-		xen_cleanmfnmap_free_pgtbl(p4d, unpin);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!p4d_none(*p4d))</span>
<span class="p_add">+		xen_cleanmfnmap_p4d(p4d, unpin);</span>
 }
 
 static void __init xen_pagetable_p2m_free(void)
<span class="p_chunk">@@ -1692,7 +1678,7 @@</span> <span class="p_context"> static void xen_release_pmd(unsigned long pfn)</span>
 	xen_release_ptpage(pfn, PT_PMD);
 }
 
<span class="p_del">-#if CONFIG_PGTABLE_LEVELS &gt;= 4</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 static void xen_alloc_pud(struct mm_struct *mm, unsigned long pfn)
 {
 	xen_alloc_ptpage(mm, pfn, PT_PUD);
<span class="p_chunk">@@ -2029,13 +2015,12 @@</span> <span class="p_context"> static phys_addr_t __init xen_early_virt_to_phys(unsigned long vaddr)</span>
  */
 void __init xen_relocate_p2m(void)
 {
<span class="p_del">-	phys_addr_t size, new_area, pt_phys, pmd_phys, pud_phys, p4d_phys;</span>
<span class="p_add">+	phys_addr_t size, new_area, pt_phys, pmd_phys, pud_phys;</span>
 	unsigned long p2m_pfn, p2m_pfn_end, n_frames, pfn, pfn_end;
<span class="p_del">-	int n_pte, n_pt, n_pmd, n_pud, n_p4d, idx_pte, idx_pt, idx_pmd, idx_pud, idx_p4d;</span>
<span class="p_add">+	int n_pte, n_pt, n_pmd, n_pud, idx_pte, idx_pt, idx_pmd, idx_pud;</span>
 	pte_t *pt;
 	pmd_t *pmd;
 	pud_t *pud;
<span class="p_del">-	p4d_t *p4d = NULL;</span>
 	pgd_t *pgd;
 	unsigned long *new_p2m;
 	int save_pud;
<span class="p_chunk">@@ -2045,11 +2030,7 @@</span> <span class="p_context"> void __init xen_relocate_p2m(void)</span>
 	n_pt = roundup(size, PMD_SIZE) &gt;&gt; PMD_SHIFT;
 	n_pmd = roundup(size, PUD_SIZE) &gt;&gt; PUD_SHIFT;
 	n_pud = roundup(size, P4D_SIZE) &gt;&gt; P4D_SHIFT;
<span class="p_del">-	if (PTRS_PER_P4D &gt; 1)</span>
<span class="p_del">-		n_p4d = roundup(size, PGDIR_SIZE) &gt;&gt; PGDIR_SHIFT;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		n_p4d = 0;</span>
<span class="p_del">-	n_frames = n_pte + n_pt + n_pmd + n_pud + n_p4d;</span>
<span class="p_add">+	n_frames = n_pte + n_pt + n_pmd + n_pud;</span>
 
 	new_area = xen_find_free_area(PFN_PHYS(n_frames));
 	if (!new_area) {
<span class="p_chunk">@@ -2065,76 +2046,56 @@</span> <span class="p_context"> void __init xen_relocate_p2m(void)</span>
 	 * To avoid any possible virtual address collision, just use
 	 * 2 * PUD_SIZE for the new area.
 	 */
<span class="p_del">-	p4d_phys = new_area;</span>
<span class="p_del">-	pud_phys = p4d_phys + PFN_PHYS(n_p4d);</span>
<span class="p_add">+	pud_phys = new_area;</span>
 	pmd_phys = pud_phys + PFN_PHYS(n_pud);
 	pt_phys = pmd_phys + PFN_PHYS(n_pmd);
 	p2m_pfn = PFN_DOWN(pt_phys) + n_pt;
 
 	pgd = __va(read_cr3_pa());
 	new_p2m = (unsigned long *)(2 * PGDIR_SIZE);
<span class="p_del">-	idx_p4d = 0;</span>
 	save_pud = n_pud;
<span class="p_del">-	do {</span>
<span class="p_del">-		if (n_p4d &gt; 0) {</span>
<span class="p_del">-			p4d = early_memremap(p4d_phys, PAGE_SIZE);</span>
<span class="p_del">-			clear_page(p4d);</span>
<span class="p_del">-			n_pud = min(save_pud, PTRS_PER_P4D);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		for (idx_pud = 0; idx_pud &lt; n_pud; idx_pud++) {</span>
<span class="p_del">-			pud = early_memremap(pud_phys, PAGE_SIZE);</span>
<span class="p_del">-			clear_page(pud);</span>
<span class="p_del">-			for (idx_pmd = 0; idx_pmd &lt; min(n_pmd, PTRS_PER_PUD);</span>
<span class="p_del">-				 idx_pmd++) {</span>
<span class="p_del">-				pmd = early_memremap(pmd_phys, PAGE_SIZE);</span>
<span class="p_del">-				clear_page(pmd);</span>
<span class="p_del">-				for (idx_pt = 0; idx_pt &lt; min(n_pt, PTRS_PER_PMD);</span>
<span class="p_del">-					 idx_pt++) {</span>
<span class="p_del">-					pt = early_memremap(pt_phys, PAGE_SIZE);</span>
<span class="p_del">-					clear_page(pt);</span>
<span class="p_del">-					for (idx_pte = 0;</span>
<span class="p_del">-						 idx_pte &lt; min(n_pte, PTRS_PER_PTE);</span>
<span class="p_del">-						 idx_pte++) {</span>
<span class="p_del">-						set_pte(pt + idx_pte,</span>
<span class="p_del">-								pfn_pte(p2m_pfn, PAGE_KERNEL));</span>
<span class="p_del">-						p2m_pfn++;</span>
<span class="p_del">-					}</span>
<span class="p_del">-					n_pte -= PTRS_PER_PTE;</span>
<span class="p_del">-					early_memunmap(pt, PAGE_SIZE);</span>
<span class="p_del">-					make_lowmem_page_readonly(__va(pt_phys));</span>
<span class="p_del">-					pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE,</span>
<span class="p_del">-							PFN_DOWN(pt_phys));</span>
<span class="p_del">-					set_pmd(pmd + idx_pt,</span>
<span class="p_del">-							__pmd(_PAGE_TABLE | pt_phys));</span>
<span class="p_del">-					pt_phys += PAGE_SIZE;</span>
<span class="p_add">+	for (idx_pud = 0; idx_pud &lt; n_pud; idx_pud++) {</span>
<span class="p_add">+		pud = early_memremap(pud_phys, PAGE_SIZE);</span>
<span class="p_add">+		clear_page(pud);</span>
<span class="p_add">+		for (idx_pmd = 0; idx_pmd &lt; min(n_pmd, PTRS_PER_PUD);</span>
<span class="p_add">+				idx_pmd++) {</span>
<span class="p_add">+			pmd = early_memremap(pmd_phys, PAGE_SIZE);</span>
<span class="p_add">+			clear_page(pmd);</span>
<span class="p_add">+			for (idx_pt = 0; idx_pt &lt; min(n_pt, PTRS_PER_PMD);</span>
<span class="p_add">+					idx_pt++) {</span>
<span class="p_add">+				pt = early_memremap(pt_phys, PAGE_SIZE);</span>
<span class="p_add">+				clear_page(pt);</span>
<span class="p_add">+				for (idx_pte = 0;</span>
<span class="p_add">+						idx_pte &lt; min(n_pte, PTRS_PER_PTE);</span>
<span class="p_add">+						idx_pte++) {</span>
<span class="p_add">+					set_pte(pt + idx_pte,</span>
<span class="p_add">+							pfn_pte(p2m_pfn, PAGE_KERNEL));</span>
<span class="p_add">+					p2m_pfn++;</span>
 				}
<span class="p_del">-				n_pt -= PTRS_PER_PMD;</span>
<span class="p_del">-				early_memunmap(pmd, PAGE_SIZE);</span>
<span class="p_del">-				make_lowmem_page_readonly(__va(pmd_phys));</span>
<span class="p_del">-				pin_pagetable_pfn(MMUEXT_PIN_L2_TABLE,</span>
<span class="p_del">-						PFN_DOWN(pmd_phys));</span>
<span class="p_del">-				set_pud(pud + idx_pmd, __pud(_PAGE_TABLE | pmd_phys));</span>
<span class="p_del">-				pmd_phys += PAGE_SIZE;</span>
<span class="p_add">+				n_pte -= PTRS_PER_PTE;</span>
<span class="p_add">+				early_memunmap(pt, PAGE_SIZE);</span>
<span class="p_add">+				make_lowmem_page_readonly(__va(pt_phys));</span>
<span class="p_add">+				pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE,</span>
<span class="p_add">+						PFN_DOWN(pt_phys));</span>
<span class="p_add">+				set_pmd(pmd + idx_pt,</span>
<span class="p_add">+						__pmd(_PAGE_TABLE | pt_phys));</span>
<span class="p_add">+				pt_phys += PAGE_SIZE;</span>
 			}
<span class="p_del">-			n_pmd -= PTRS_PER_PUD;</span>
<span class="p_del">-			early_memunmap(pud, PAGE_SIZE);</span>
<span class="p_del">-			make_lowmem_page_readonly(__va(pud_phys));</span>
<span class="p_del">-			pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(pud_phys));</span>
<span class="p_del">-			if (n_p4d &gt; 0)</span>
<span class="p_del">-				set_p4d(p4d + idx_pud, __p4d(_PAGE_TABLE | pud_phys));</span>
<span class="p_del">-			else</span>
<span class="p_del">-				set_pgd(pgd + 2 + idx_pud, __pgd(_PAGE_TABLE | pud_phys));</span>
<span class="p_del">-			pud_phys += PAGE_SIZE;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (n_p4d &gt; 0) {</span>
<span class="p_del">-			save_pud -= PTRS_PER_P4D;</span>
<span class="p_del">-			early_memunmap(p4d, PAGE_SIZE);</span>
<span class="p_del">-			make_lowmem_page_readonly(__va(p4d_phys));</span>
<span class="p_del">-			pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE, PFN_DOWN(p4d_phys));</span>
<span class="p_del">-			set_pgd(pgd + 2 + idx_p4d, __pgd(_PAGE_TABLE | p4d_phys));</span>
<span class="p_del">-			p4d_phys += PAGE_SIZE;</span>
<span class="p_add">+			n_pt -= PTRS_PER_PMD;</span>
<span class="p_add">+			early_memunmap(pmd, PAGE_SIZE);</span>
<span class="p_add">+			make_lowmem_page_readonly(__va(pmd_phys));</span>
<span class="p_add">+			pin_pagetable_pfn(MMUEXT_PIN_L2_TABLE,</span>
<span class="p_add">+					PFN_DOWN(pmd_phys));</span>
<span class="p_add">+			set_pud(pud + idx_pmd, __pud(_PAGE_TABLE | pmd_phys));</span>
<span class="p_add">+			pmd_phys += PAGE_SIZE;</span>
 		}
<span class="p_del">-	} while (++idx_p4d &lt; n_p4d);</span>
<span class="p_add">+		n_pmd -= PTRS_PER_PUD;</span>
<span class="p_add">+		early_memunmap(pud, PAGE_SIZE);</span>
<span class="p_add">+		make_lowmem_page_readonly(__va(pud_phys));</span>
<span class="p_add">+		pin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(pud_phys));</span>
<span class="p_add">+		set_pgd(pgd + 2 + idx_pud, __pgd(_PAGE_TABLE | pud_phys));</span>
<span class="p_add">+		pud_phys += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
 
 	/* Now copy the old p2m info to the new area. */
 	memcpy(new_p2m, xen_p2m_addr, size);
<span class="p_chunk">@@ -2311,7 +2272,7 @@</span> <span class="p_context"> static void xen_set_fixmap(unsigned idx, phys_addr_t phys, pgprot_t prot)</span>
 #endif
 	case FIX_TEXT_POKE0:
 	case FIX_TEXT_POKE1:
<span class="p_del">-	case FIX_GDT_REMAP_BEGIN ... FIX_GDT_REMAP_END:</span>
<span class="p_add">+	case FIX_CPU_ENTRY_AREA_TOP ... FIX_CPU_ENTRY_AREA_BOTTOM:</span>
 		/* All local page mappings */
 		pte = pfn_pte(phys, prot);
 		break;
<span class="p_chunk">@@ -2361,7 +2322,7 @@</span> <span class="p_context"> static void __init xen_post_allocator_init(void)</span>
 	pv_mmu_ops.set_pte = xen_set_pte;
 	pv_mmu_ops.set_pmd = xen_set_pmd;
 	pv_mmu_ops.set_pud = xen_set_pud;
<span class="p_del">-#if CONFIG_PGTABLE_LEVELS &gt;= 4</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 	pv_mmu_ops.set_p4d = xen_set_p4d;
 #endif
 
<span class="p_chunk">@@ -2371,7 +2332,7 @@</span> <span class="p_context"> static void __init xen_post_allocator_init(void)</span>
 	pv_mmu_ops.alloc_pmd = xen_alloc_pmd;
 	pv_mmu_ops.release_pte = xen_release_pte;
 	pv_mmu_ops.release_pmd = xen_release_pmd;
<span class="p_del">-#if CONFIG_PGTABLE_LEVELS &gt;= 4</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 	pv_mmu_ops.alloc_pud = xen_alloc_pud;
 	pv_mmu_ops.release_pud = xen_release_pud;
 #endif
<span class="p_chunk">@@ -2435,14 +2396,14 @@</span> <span class="p_context"> static const struct pv_mmu_ops xen_mmu_ops __initconst = {</span>
 	.make_pmd = PV_CALLEE_SAVE(xen_make_pmd),
 	.pmd_val = PV_CALLEE_SAVE(xen_pmd_val),
 
<span class="p_del">-#if CONFIG_PGTABLE_LEVELS &gt;= 4</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 	.pud_val = PV_CALLEE_SAVE(xen_pud_val),
 	.make_pud = PV_CALLEE_SAVE(xen_make_pud),
 	.set_p4d = xen_set_p4d_hyper,
 
 	.alloc_pud = xen_alloc_pmd_init,
 	.release_pud = xen_release_pmd_init,
<span class="p_del">-#endif	/* CONFIG_PGTABLE_LEVELS == 4 */</span>
<span class="p_add">+#endif	/* CONFIG_X86_64 */</span>
 
 	.activate_mm = xen_activate_mm,
 	.dup_mmap = xen_dup_mmap,
<span class="p_header">diff --git a/arch/x86/xen/smp_pv.c b/arch/x86/xen/smp_pv.c</span>
<span class="p_header">index 05f91ce9b55e..c0c756c76afe 100644</span>
<span class="p_header">--- a/arch/x86/xen/smp_pv.c</span>
<span class="p_header">+++ b/arch/x86/xen/smp_pv.c</span>
<span class="p_chunk">@@ -14,6 +14,7 @@</span> <span class="p_context"></span>
  * single-threaded.
  */
 #include &lt;linux/sched.h&gt;
<span class="p_add">+#include &lt;linux/sched/task_stack.h&gt;</span>
 #include &lt;linux/err.h&gt;
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/smp.h&gt;
<span class="p_chunk">@@ -294,12 +295,19 @@</span> <span class="p_context"> cpu_initialize_context(unsigned int cpu, struct task_struct *idle)</span>
 #endif
 	memset(&amp;ctxt-&gt;fpu_ctxt, 0, sizeof(ctxt-&gt;fpu_ctxt));
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Bring up the CPU in cpu_bringup_and_idle() with the stack</span>
<span class="p_add">+	 * pointing just below where pt_regs would be if it were a normal</span>
<span class="p_add">+	 * kernel entry.</span>
<span class="p_add">+	 */</span>
 	ctxt-&gt;user_regs.eip = (unsigned long)cpu_bringup_and_idle;
 	ctxt-&gt;flags = VGCF_IN_KERNEL;
 	ctxt-&gt;user_regs.eflags = 0x1000; /* IOPL_RING1 */
 	ctxt-&gt;user_regs.ds = __USER_DS;
 	ctxt-&gt;user_regs.es = __USER_DS;
 	ctxt-&gt;user_regs.ss = __KERNEL_DS;
<span class="p_add">+	ctxt-&gt;user_regs.cs = __KERNEL_CS;</span>
<span class="p_add">+	ctxt-&gt;user_regs.esp = (unsigned long)task_pt_regs(idle);</span>
 
 	xen_copy_trap_info(ctxt-&gt;trap_ctxt);
 
<span class="p_chunk">@@ -314,8 +322,13 @@</span> <span class="p_context"> cpu_initialize_context(unsigned int cpu, struct task_struct *idle)</span>
 	ctxt-&gt;gdt_frames[0] = gdt_mfn;
 	ctxt-&gt;gdt_ents      = GDT_ENTRIES;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Set SS:SP that Xen will use when entering guest kernel mode</span>
<span class="p_add">+	 * from guest user mode.  Subsequent calls to load_sp0() can</span>
<span class="p_add">+	 * change this value.</span>
<span class="p_add">+	 */</span>
 	ctxt-&gt;kernel_ss = __KERNEL_DS;
<span class="p_del">-	ctxt-&gt;kernel_sp = idle-&gt;thread.sp0;</span>
<span class="p_add">+	ctxt-&gt;kernel_sp = task_top_of_stack(idle);</span>
 
 #ifdef CONFIG_X86_32
 	ctxt-&gt;event_callback_cs     = __KERNEL_CS;
<span class="p_chunk">@@ -327,10 +340,8 @@</span> <span class="p_context"> cpu_initialize_context(unsigned int cpu, struct task_struct *idle)</span>
 		(unsigned long)xen_hypervisor_callback;
 	ctxt-&gt;failsafe_callback_eip =
 		(unsigned long)xen_failsafe_callback;
<span class="p_del">-	ctxt-&gt;user_regs.cs = __KERNEL_CS;</span>
 	per_cpu(xen_cr3, cpu) = __pa(swapper_pg_dir);
 
<span class="p_del">-	ctxt-&gt;user_regs.esp = idle-&gt;thread.sp0 - sizeof(struct pt_regs);</span>
 	ctxt-&gt;ctrlreg[3] = xen_pfn_to_cr3(virt_to_gfn(swapper_pg_dir));
 	if (HYPERVISOR_vcpu_op(VCPUOP_initialise, xen_vcpu_nr(cpu), ctxt))
 		BUG();
<span class="p_header">diff --git a/arch/x86/xen/xen-asm_64.S b/arch/x86/xen/xen-asm_64.S</span>
<span class="p_header">index c98a48c861fd..8a10c9a9e2b5 100644</span>
<span class="p_header">--- a/arch/x86/xen/xen-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/xen/xen-asm_64.S</span>
<span class="p_chunk">@@ -30,7 +30,7 @@</span> <span class="p_context"> xen_pv_trap debug</span>
 xen_pv_trap xendebug
 xen_pv_trap int3
 xen_pv_trap xenint3
<span class="p_del">-xen_pv_trap nmi</span>
<span class="p_add">+xen_pv_trap xennmi</span>
 xen_pv_trap overflow
 xen_pv_trap bounds
 xen_pv_trap invalid_op
<span class="p_header">diff --git a/arch/x86/xen/xen-head.S b/arch/x86/xen/xen-head.S</span>
<span class="p_header">index b5b8d7f43557..497cc55a0c16 100644</span>
<span class="p_header">--- a/arch/x86/xen/xen-head.S</span>
<span class="p_header">+++ b/arch/x86/xen/xen-head.S</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/boot.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/page_types.h&gt;
<span class="p_add">+#include &lt;asm/unwind_hints.h&gt;</span>
 
 #include &lt;xen/interface/elfnote.h&gt;
 #include &lt;xen/interface/features.h&gt;
<span class="p_chunk">@@ -20,6 +21,7 @@</span> <span class="p_context"></span>
 #ifdef CONFIG_XEN_PV
 	__INIT
 ENTRY(startup_xen)
<span class="p_add">+	UNWIND_HINT_EMPTY</span>
 	cld
 
 	/* Clear .bss */
<span class="p_chunk">@@ -34,21 +36,24 @@</span> <span class="p_context"> ENTRY(startup_xen)</span>
 	mov $init_thread_union+THREAD_SIZE, %_ASM_SP
 
 	jmp xen_start_kernel
<span class="p_del">-</span>
<span class="p_add">+END(startup_xen)</span>
 	__FINIT
 #endif
 
 .pushsection .text
 	.balign PAGE_SIZE
 ENTRY(hypercall_page)
<span class="p_del">-	.skip PAGE_SIZE</span>
<span class="p_add">+	.rept (PAGE_SIZE / 32)</span>
<span class="p_add">+		UNWIND_HINT_EMPTY</span>
<span class="p_add">+		.skip 32</span>
<span class="p_add">+	.endr</span>
 
 #define HYPERCALL(n) \
 	.equ xen_hypercall_##n, hypercall_page + __HYPERVISOR_##n * 32; \
 	.type xen_hypercall_##n, @function; .size xen_hypercall_##n, 32
 #include &lt;asm/xen-hypercalls.h&gt;
 #undef HYPERCALL
<span class="p_del">-</span>
<span class="p_add">+END(hypercall_page)</span>
 .popsection
 
 	ELFNOTE(Xen, XEN_ELFNOTE_GUEST_OS,       .asciz &quot;linux&quot;)
<span class="p_header">diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c</span>
<span class="p_header">index a4783da90ba8..0f860cf0d56d 100644</span>
<span class="p_header">--- a/block/bfq-iosched.c</span>
<span class="p_header">+++ b/block/bfq-iosched.c</span>
<span class="p_chunk">@@ -108,6 +108,7 @@</span> <span class="p_context"></span>
 #include &quot;blk-mq-tag.h&quot;
 #include &quot;blk-mq-sched.h&quot;
 #include &quot;bfq-iosched.h&quot;
<span class="p_add">+#include &quot;blk-wbt.h&quot;</span>
 
 #define BFQ_BFQQ_FNS(name)						\
 void bfq_mark_bfqq_##name(struct bfq_queue *bfqq)			\
<span class="p_chunk">@@ -4775,7 +4776,7 @@</span> <span class="p_context"> static int bfq_init_queue(struct request_queue *q, struct elevator_type *e)</span>
 	bfq_init_root_group(bfqd-&gt;root_group, bfqd);
 	bfq_init_entity(&amp;bfqd-&gt;oom_bfqq.entity, bfqd-&gt;root_group);
 
<span class="p_del">-</span>
<span class="p_add">+	wbt_disable_default(q);</span>
 	return 0;
 
 out_free:
<span class="p_header">diff --git a/block/blk-wbt.c b/block/blk-wbt.c</span>
<span class="p_header">index 6a9a0f03a67b..e59d59c11ebb 100644</span>
<span class="p_header">--- a/block/blk-wbt.c</span>
<span class="p_header">+++ b/block/blk-wbt.c</span>
<span class="p_chunk">@@ -654,7 +654,7 @@</span> <span class="p_context"> void wbt_set_write_cache(struct rq_wb *rwb, bool write_cache_on)</span>
 }
 
 /*
<span class="p_del">- * Disable wbt, if enabled by default. Only called from CFQ.</span>
<span class="p_add">+ * Disable wbt, if enabled by default.</span>
  */
 void wbt_disable_default(struct request_queue *q)
 {
<span class="p_header">diff --git a/crypto/lrw.c b/crypto/lrw.c</span>
<span class="p_header">index a8bfae4451bf..eb681e9fe574 100644</span>
<span class="p_header">--- a/crypto/lrw.c</span>
<span class="p_header">+++ b/crypto/lrw.c</span>
<span class="p_chunk">@@ -610,8 +610,10 @@</span> <span class="p_context"> static int create(struct crypto_template *tmpl, struct rtattr **tb)</span>
 		ecb_name[len - 1] = 0;
 
 		if (snprintf(inst-&gt;alg.base.cra_name, CRYPTO_MAX_ALG_NAME,
<span class="p_del">-			     &quot;lrw(%s)&quot;, ecb_name) &gt;= CRYPTO_MAX_ALG_NAME)</span>
<span class="p_del">-			return -ENAMETOOLONG;</span>
<span class="p_add">+			     &quot;lrw(%s)&quot;, ecb_name) &gt;= CRYPTO_MAX_ALG_NAME) {</span>
<span class="p_add">+			err = -ENAMETOOLONG;</span>
<span class="p_add">+			goto err_drop_spawn;</span>
<span class="p_add">+		}</span>
 	}
 
 	inst-&gt;alg.base.cra_flags = alg-&gt;base.cra_flags &amp; CRYPTO_ALG_ASYNC;
<span class="p_header">diff --git a/drivers/acpi/apei/ghes.c b/drivers/acpi/apei/ghes.c</span>
<span class="p_header">index 3c3a37b8503b..572b6c7303ed 100644</span>
<span class="p_header">--- a/drivers/acpi/apei/ghes.c</span>
<span class="p_header">+++ b/drivers/acpi/apei/ghes.c</span>
<span class="p_chunk">@@ -51,6 +51,7 @@</span> <span class="p_context"></span>
 #include &lt;acpi/actbl1.h&gt;
 #include &lt;acpi/ghes.h&gt;
 #include &lt;acpi/apei.h&gt;
<span class="p_add">+#include &lt;asm/fixmap.h&gt;</span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;ras/ras_event.h&gt;
 
<span class="p_chunk">@@ -112,7 +113,7 @@</span> <span class="p_context"> static DEFINE_MUTEX(ghes_list_mutex);</span>
  * Because the memory area used to transfer hardware error information
  * from BIOS to Linux can be determined only in NMI, IRQ or timer
  * handler, but general ioremap can not be used in atomic context, so
<span class="p_del">- * a special version of atomic ioremap is implemented for that.</span>
<span class="p_add">+ * the fixmap is used instead.</span>
  */
 
 /*
<span class="p_chunk">@@ -126,8 +127,8 @@</span> <span class="p_context"> static DEFINE_MUTEX(ghes_list_mutex);</span>
 /* virtual memory area for atomic ioremap */
 static struct vm_struct *ghes_ioremap_area;
 /*
<span class="p_del">- * These 2 spinlock is used to prevent atomic ioremap virtual memory</span>
<span class="p_del">- * area from being mapped simultaneously.</span>
<span class="p_add">+ * These 2 spinlocks are used to prevent the fixmap entries from being used</span>
<span class="p_add">+ * simultaneously.</span>
  */
 static DEFINE_RAW_SPINLOCK(ghes_ioremap_lock_nmi);
 static DEFINE_SPINLOCK(ghes_ioremap_lock_irq);
<span class="p_chunk">@@ -159,52 +160,36 @@</span> <span class="p_context"> static void ghes_ioremap_exit(void)</span>
 
 static void __iomem *ghes_ioremap_pfn_nmi(u64 pfn)
 {
<span class="p_del">-	unsigned long vaddr;</span>
 	phys_addr_t paddr;
 	pgprot_t prot;
 
<span class="p_del">-	vaddr = (unsigned long)GHES_IOREMAP_NMI_PAGE(ghes_ioremap_area-&gt;addr);</span>
<span class="p_del">-</span>
 	paddr = pfn &lt;&lt; PAGE_SHIFT;
 	prot = arch_apei_get_mem_attribute(paddr);
<span class="p_del">-	ioremap_page_range(vaddr, vaddr + PAGE_SIZE, paddr, prot);</span>
<span class="p_add">+	__set_fixmap(FIX_APEI_GHES_NMI, paddr, prot);</span>
 
<span class="p_del">-	return (void __iomem *)vaddr;</span>
<span class="p_add">+	return (void __iomem *) fix_to_virt(FIX_APEI_GHES_NMI);</span>
 }
 
 static void __iomem *ghes_ioremap_pfn_irq(u64 pfn)
 {
<span class="p_del">-	unsigned long vaddr, paddr;</span>
<span class="p_add">+	phys_addr_t paddr;</span>
 	pgprot_t prot;
 
<span class="p_del">-	vaddr = (unsigned long)GHES_IOREMAP_IRQ_PAGE(ghes_ioremap_area-&gt;addr);</span>
<span class="p_del">-</span>
 	paddr = pfn &lt;&lt; PAGE_SHIFT;
 	prot = arch_apei_get_mem_attribute(paddr);
<span class="p_add">+	__set_fixmap(FIX_APEI_GHES_IRQ, paddr, prot);</span>
 
<span class="p_del">-	ioremap_page_range(vaddr, vaddr + PAGE_SIZE, paddr, prot);</span>
<span class="p_del">-</span>
<span class="p_del">-	return (void __iomem *)vaddr;</span>
<span class="p_add">+	return (void __iomem *) fix_to_virt(FIX_APEI_GHES_IRQ);</span>
 }
 
<span class="p_del">-static void ghes_iounmap_nmi(void __iomem *vaddr_ptr)</span>
<span class="p_add">+static void ghes_iounmap_nmi(void)</span>
 {
<span class="p_del">-	unsigned long vaddr = (unsigned long __force)vaddr_ptr;</span>
<span class="p_del">-	void *base = ghes_ioremap_area-&gt;addr;</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(vaddr != (unsigned long)GHES_IOREMAP_NMI_PAGE(base));</span>
<span class="p_del">-	unmap_kernel_range_noflush(vaddr, PAGE_SIZE);</span>
<span class="p_del">-	arch_apei_flush_tlb_one(vaddr);</span>
<span class="p_add">+	clear_fixmap(FIX_APEI_GHES_NMI);</span>
 }
 
<span class="p_del">-static void ghes_iounmap_irq(void __iomem *vaddr_ptr)</span>
<span class="p_add">+static void ghes_iounmap_irq(void)</span>
 {
<span class="p_del">-	unsigned long vaddr = (unsigned long __force)vaddr_ptr;</span>
<span class="p_del">-	void *base = ghes_ioremap_area-&gt;addr;</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(vaddr != (unsigned long)GHES_IOREMAP_IRQ_PAGE(base));</span>
<span class="p_del">-	unmap_kernel_range_noflush(vaddr, PAGE_SIZE);</span>
<span class="p_del">-	arch_apei_flush_tlb_one(vaddr);</span>
<span class="p_add">+	clear_fixmap(FIX_APEI_GHES_IRQ);</span>
 }
 
 static int ghes_estatus_pool_init(void)
<span class="p_chunk">@@ -360,10 +345,10 @@</span> <span class="p_context"> static void ghes_copy_tofrom_phys(void *buffer, u64 paddr, u32 len,</span>
 		paddr += trunk;
 		buffer += trunk;
 		if (in_nmi) {
<span class="p_del">-			ghes_iounmap_nmi(vaddr);</span>
<span class="p_add">+			ghes_iounmap_nmi();</span>
 			raw_spin_unlock(&amp;ghes_ioremap_lock_nmi);
 		} else {
<span class="p_del">-			ghes_iounmap_irq(vaddr);</span>
<span class="p_add">+			ghes_iounmap_irq();</span>
 			spin_unlock_irqrestore(&amp;ghes_ioremap_lock_irq, flags);
 		}
 	}
<span class="p_chunk">@@ -851,17 +836,8 @@</span> <span class="p_context"> static void ghes_sea_remove(struct ghes *ghes)</span>
 	synchronize_rcu();
 }
 #else /* CONFIG_ACPI_APEI_SEA */
<span class="p_del">-static inline void ghes_sea_add(struct ghes *ghes)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pr_err(GHES_PFX &quot;ID: %d, trying to add SEA notification which is not supported\n&quot;,</span>
<span class="p_del">-	       ghes-&gt;generic-&gt;header.source_id);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void ghes_sea_remove(struct ghes *ghes)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pr_err(GHES_PFX &quot;ID: %d, trying to remove SEA notification which is not supported\n&quot;,</span>
<span class="p_del">-	       ghes-&gt;generic-&gt;header.source_id);</span>
<span class="p_del">-}</span>
<span class="p_add">+static inline void ghes_sea_add(struct ghes *ghes) { }</span>
<span class="p_add">+static inline void ghes_sea_remove(struct ghes *ghes) { }</span>
 #endif /* CONFIG_ACPI_APEI_SEA */
 
 #ifdef CONFIG_HAVE_ACPI_APEI_NMI
<span class="p_chunk">@@ -1063,23 +1039,9 @@</span> <span class="p_context"> static void ghes_nmi_init_cxt(void)</span>
 	init_irq_work(&amp;ghes_proc_irq_work, ghes_proc_in_irq);
 }
 #else /* CONFIG_HAVE_ACPI_APEI_NMI */
<span class="p_del">-static inline void ghes_nmi_add(struct ghes *ghes)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pr_err(GHES_PFX &quot;ID: %d, trying to add NMI notification which is not supported!\n&quot;,</span>
<span class="p_del">-	       ghes-&gt;generic-&gt;header.source_id);</span>
<span class="p_del">-	BUG();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void ghes_nmi_remove(struct ghes *ghes)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pr_err(GHES_PFX &quot;ID: %d, trying to remove NMI notification which is not supported!\n&quot;,</span>
<span class="p_del">-	       ghes-&gt;generic-&gt;header.source_id);</span>
<span class="p_del">-	BUG();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void ghes_nmi_init_cxt(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_add">+static inline void ghes_nmi_add(struct ghes *ghes) { }</span>
<span class="p_add">+static inline void ghes_nmi_remove(struct ghes *ghes) { }</span>
<span class="p_add">+static inline void ghes_nmi_init_cxt(void) { }</span>
 #endif /* CONFIG_HAVE_ACPI_APEI_NMI */
 
 static int ghes_probe(struct platform_device *ghes_dev)
<span class="p_header">diff --git a/drivers/base/power/opp/core.c b/drivers/base/power/opp/core.c</span>
<span class="p_header">index a6de32530693..0459b1204694 100644</span>
<span class="p_header">--- a/drivers/base/power/opp/core.c</span>
<span class="p_header">+++ b/drivers/base/power/opp/core.c</span>
<span class="p_chunk">@@ -296,7 +296,7 @@</span> <span class="p_context"> int dev_pm_opp_get_opp_count(struct device *dev)</span>
 	opp_table = _find_opp_table(dev);
 	if (IS_ERR(opp_table)) {
 		count = PTR_ERR(opp_table);
<span class="p_del">-		dev_err(dev, &quot;%s: OPP table not found (%d)\n&quot;,</span>
<span class="p_add">+		dev_dbg(dev, &quot;%s: OPP table not found (%d)\n&quot;,</span>
 			__func__, count);
 		return count;
 	}
<span class="p_header">diff --git a/drivers/bluetooth/hci_bcm.c b/drivers/bluetooth/hci_bcm.c</span>
<span class="p_header">index e2540113d0da..73d2d88ddc03 100644</span>
<span class="p_header">--- a/drivers/bluetooth/hci_bcm.c</span>
<span class="p_header">+++ b/drivers/bluetooth/hci_bcm.c</span>
<span class="p_chunk">@@ -68,7 +68,7 @@</span> <span class="p_context"> struct bcm_device {</span>
 	u32			init_speed;
 	u32			oper_speed;
 	int			irq;
<span class="p_del">-	u8			irq_polarity;</span>
<span class="p_add">+	bool			irq_active_low;</span>
 
 #ifdef CONFIG_PM
 	struct hci_uart		*hu;
<span class="p_chunk">@@ -213,7 +213,9 @@</span> <span class="p_context"> static int bcm_request_irq(struct bcm_data *bcm)</span>
 	}
 
 	err = devm_request_irq(&amp;bdev-&gt;pdev-&gt;dev, bdev-&gt;irq, bcm_host_wake,
<span class="p_del">-			       IRQF_TRIGGER_RISING, &quot;host_wake&quot;, bdev);</span>
<span class="p_add">+			       bdev-&gt;irq_active_low ? IRQF_TRIGGER_FALLING :</span>
<span class="p_add">+						      IRQF_TRIGGER_RISING,</span>
<span class="p_add">+			       &quot;host_wake&quot;, bdev);</span>
 	if (err)
 		goto unlock;
 
<span class="p_chunk">@@ -253,7 +255,7 @@</span> <span class="p_context"> static int bcm_setup_sleep(struct hci_uart *hu)</span>
 	struct sk_buff *skb;
 	struct bcm_set_sleep_mode sleep_params = default_sleep_params;
 
<span class="p_del">-	sleep_params.host_wake_active = !bcm-&gt;dev-&gt;irq_polarity;</span>
<span class="p_add">+	sleep_params.host_wake_active = !bcm-&gt;dev-&gt;irq_active_low;</span>
 
 	skb = __hci_cmd_sync(hu-&gt;hdev, 0xfc27, sizeof(sleep_params),
 			     &amp;sleep_params, HCI_INIT_TIMEOUT);
<span class="p_chunk">@@ -690,10 +692,8 @@</span> <span class="p_context"> static const struct acpi_gpio_mapping acpi_bcm_int_first_gpios[] = {</span>
 };
 
 #ifdef CONFIG_ACPI
<span class="p_del">-static u8 acpi_active_low = ACPI_ACTIVE_LOW;</span>
<span class="p_del">-</span>
 /* IRQ polarity of some chipsets are not defined correctly in ACPI table. */
<span class="p_del">-static const struct dmi_system_id bcm_wrong_irq_dmi_table[] = {</span>
<span class="p_add">+static const struct dmi_system_id bcm_active_low_irq_dmi_table[] = {</span>
 	{
 		.ident = &quot;Asus T100TA&quot;,
 		.matches = {
<span class="p_chunk">@@ -701,7 +701,6 @@</span> <span class="p_context"> static const struct dmi_system_id bcm_wrong_irq_dmi_table[] = {</span>
 					&quot;ASUSTeK COMPUTER INC.&quot;),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, &quot;T100TA&quot;),
 		},
<span class="p_del">-		.driver_data = &amp;acpi_active_low,</span>
 	},
 	{
 		.ident = &quot;Asus T100CHI&quot;,
<span class="p_chunk">@@ -710,7 +709,6 @@</span> <span class="p_context"> static const struct dmi_system_id bcm_wrong_irq_dmi_table[] = {</span>
 					&quot;ASUSTeK COMPUTER INC.&quot;),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, &quot;T100CHI&quot;),
 		},
<span class="p_del">-		.driver_data = &amp;acpi_active_low,</span>
 	},
 	{	/* Handle ThinkPad 8 tablets with BCM2E55 chipset ACPI ID */
 		.ident = &quot;Lenovo ThinkPad 8&quot;,
<span class="p_chunk">@@ -718,7 +716,6 @@</span> <span class="p_context"> static const struct dmi_system_id bcm_wrong_irq_dmi_table[] = {</span>
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, &quot;LENOVO&quot;),
 			DMI_EXACT_MATCH(DMI_PRODUCT_VERSION, &quot;ThinkPad 8&quot;),
 		},
<span class="p_del">-		.driver_data = &amp;acpi_active_low,</span>
 	},
 	{ }
 };
<span class="p_chunk">@@ -733,13 +730,13 @@</span> <span class="p_context"> static int bcm_resource(struct acpi_resource *ares, void *data)</span>
 	switch (ares-&gt;type) {
 	case ACPI_RESOURCE_TYPE_EXTENDED_IRQ:
 		irq = &amp;ares-&gt;data.extended_irq;
<span class="p_del">-		dev-&gt;irq_polarity = irq-&gt;polarity;</span>
<span class="p_add">+		dev-&gt;irq_active_low = irq-&gt;polarity == ACPI_ACTIVE_LOW;</span>
 		break;
 
 	case ACPI_RESOURCE_TYPE_GPIO:
 		gpio = &amp;ares-&gt;data.gpio;
 		if (gpio-&gt;connection_type == ACPI_RESOURCE_GPIO_TYPE_INT)
<span class="p_del">-			dev-&gt;irq_polarity = gpio-&gt;polarity;</span>
<span class="p_add">+			dev-&gt;irq_active_low = gpio-&gt;polarity == ACPI_ACTIVE_LOW;</span>
 		break;
 
 	case ACPI_RESOURCE_TYPE_SERIAL_BUS:
<span class="p_chunk">@@ -834,11 +831,11 @@</span> <span class="p_context"> static int bcm_acpi_probe(struct bcm_device *dev)</span>
 		return ret;
 	acpi_dev_free_resource_list(&amp;resources);
 
<span class="p_del">-	dmi_id = dmi_first_match(bcm_wrong_irq_dmi_table);</span>
<span class="p_add">+	dmi_id = dmi_first_match(bcm_active_low_irq_dmi_table);</span>
 	if (dmi_id) {
 		bt_dev_warn(dev, &quot;%s: Overwriting IRQ polarity to active low&quot;,
 			    dmi_id-&gt;ident);
<span class="p_del">-		dev-&gt;irq_polarity = *(u8 *)dmi_id-&gt;driver_data;</span>
<span class="p_add">+		dev-&gt;irq_active_low = true;</span>
 	}
 
 	return 0;
<span class="p_header">diff --git a/drivers/bluetooth/hci_ldisc.c b/drivers/bluetooth/hci_ldisc.c</span>
<span class="p_header">index 6e2403805784..6aef3bde10d7 100644</span>
<span class="p_header">--- a/drivers/bluetooth/hci_ldisc.c</span>
<span class="p_header">+++ b/drivers/bluetooth/hci_ldisc.c</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/ioctl.h&gt;
 #include &lt;linux/skbuff.h&gt;
 #include &lt;linux/firmware.h&gt;
<span class="p_add">+#include &lt;linux/serdev.h&gt;</span>
 
 #include &lt;net/bluetooth/bluetooth.h&gt;
 #include &lt;net/bluetooth/hci_core.h&gt;
<span class="p_chunk">@@ -298,6 +299,12 @@</span> <span class="p_context"> void hci_uart_set_flow_control(struct hci_uart *hu, bool enable)</span>
 	unsigned int set = 0;
 	unsigned int clear = 0;
 
<span class="p_add">+	if (hu-&gt;serdev) {</span>
<span class="p_add">+		serdev_device_set_flow_control(hu-&gt;serdev, !enable);</span>
<span class="p_add">+		serdev_device_set_rts(hu-&gt;serdev, !enable);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (enable) {
 		/* Disable hardware flow control */
 		ktermios = tty-&gt;termios;
<span class="p_header">diff --git a/drivers/clk/sunxi-ng/ccu-sun5i.c b/drivers/clk/sunxi-ng/ccu-sun5i.c</span>
<span class="p_header">index ab9e850b3707..2f385a57cd91 100644</span>
<span class="p_header">--- a/drivers/clk/sunxi-ng/ccu-sun5i.c</span>
<span class="p_header">+++ b/drivers/clk/sunxi-ng/ccu-sun5i.c</span>
<span class="p_chunk">@@ -982,8 +982,8 @@</span> <span class="p_context"> static void __init sun5i_ccu_init(struct device_node *node,</span>
 
 	/* Force the PLL-Audio-1x divider to 4 */
 	val = readl(reg + SUN5I_PLL_AUDIO_REG);
<span class="p_del">-	val &amp;= ~GENMASK(19, 16);</span>
<span class="p_del">-	writel(val | (3 &lt;&lt; 16), reg + SUN5I_PLL_AUDIO_REG);</span>
<span class="p_add">+	val &amp;= ~GENMASK(29, 26);</span>
<span class="p_add">+	writel(val | (3 &lt;&lt; 26), reg + SUN5I_PLL_AUDIO_REG);</span>
 
 	/*
 	 * Use the peripheral PLL as the AHB parent, instead of CPU /
<span class="p_header">diff --git a/drivers/clk/sunxi-ng/ccu-sun6i-a31.c b/drivers/clk/sunxi-ng/ccu-sun6i-a31.c</span>
<span class="p_header">index 8af434815fba..241fb13f1c06 100644</span>
<span class="p_header">--- a/drivers/clk/sunxi-ng/ccu-sun6i-a31.c</span>
<span class="p_header">+++ b/drivers/clk/sunxi-ng/ccu-sun6i-a31.c</span>
<span class="p_chunk">@@ -608,7 +608,7 @@</span> <span class="p_context"> static SUNXI_CCU_M_WITH_MUX_GATE(hdmi_clk, &quot;hdmi&quot;, lcd_ch1_parents,</span>
 				 0x150, 0, 4, 24, 2, BIT(31),
 				 CLK_SET_RATE_PARENT);
 
<span class="p_del">-static SUNXI_CCU_GATE(hdmi_ddc_clk, &quot;hdmi-ddc&quot;, &quot;osc24M&quot;, 0x150, BIT(30), 0);</span>
<span class="p_add">+static SUNXI_CCU_GATE(hdmi_ddc_clk, &quot;ddc&quot;, &quot;osc24M&quot;, 0x150, BIT(30), 0);</span>
 
 static SUNXI_CCU_GATE(ps_clk, &quot;ps&quot;, &quot;lcd1-ch1&quot;, 0x140, BIT(31), 0);
 
<span class="p_header">diff --git a/drivers/clk/sunxi-ng/ccu_nm.c b/drivers/clk/sunxi-ng/ccu_nm.c</span>
<span class="p_header">index a32158e8f2e3..84a5e7f17f6f 100644</span>
<span class="p_header">--- a/drivers/clk/sunxi-ng/ccu_nm.c</span>
<span class="p_header">+++ b/drivers/clk/sunxi-ng/ccu_nm.c</span>
<span class="p_chunk">@@ -99,6 +99,9 @@</span> <span class="p_context"> static long ccu_nm_round_rate(struct clk_hw *hw, unsigned long rate,</span>
 	struct ccu_nm *nm = hw_to_ccu_nm(hw);
 	struct _ccu_nm _nm;
 
<span class="p_add">+	if (ccu_frac_helper_has_rate(&amp;nm-&gt;common, &amp;nm-&gt;frac, rate))</span>
<span class="p_add">+		return rate;</span>
<span class="p_add">+</span>
 	_nm.min_n = nm-&gt;n.min ?: 1;
 	_nm.max_n = nm-&gt;n.max ?: 1 &lt;&lt; nm-&gt;n.width;
 	_nm.min_m = 1;
<span class="p_header">diff --git a/drivers/cpuidle/cpuidle.c b/drivers/cpuidle/cpuidle.c</span>
<span class="p_header">index 484cc8909d5c..ed4df58a855e 100644</span>
<span class="p_header">--- a/drivers/cpuidle/cpuidle.c</span>
<span class="p_header">+++ b/drivers/cpuidle/cpuidle.c</span>
<span class="p_chunk">@@ -208,6 +208,7 @@</span> <span class="p_context"> int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,</span>
 			return -EBUSY;
 		}
 		target_state = &amp;drv-&gt;states[index];
<span class="p_add">+		broadcast = false;</span>
 	}
 
 	/* Take note of the planned idle state. */
<span class="p_header">diff --git a/drivers/crypto/amcc/crypto4xx_core.h b/drivers/crypto/amcc/crypto4xx_core.h</span>
<span class="p_header">index ecfdcfe3698d..4f41d6da5acc 100644</span>
<span class="p_header">--- a/drivers/crypto/amcc/crypto4xx_core.h</span>
<span class="p_header">+++ b/drivers/crypto/amcc/crypto4xx_core.h</span>
<span class="p_chunk">@@ -34,12 +34,12 @@</span> <span class="p_context"></span>
 #define PPC405EX_CE_RESET                       0x00000008
 
 #define CRYPTO4XX_CRYPTO_PRIORITY		300
<span class="p_del">-#define PPC4XX_LAST_PD				63</span>
<span class="p_del">-#define PPC4XX_NUM_PD				64</span>
<span class="p_del">-#define PPC4XX_LAST_GD				1023</span>
<span class="p_add">+#define PPC4XX_NUM_PD				256</span>
<span class="p_add">+#define PPC4XX_LAST_PD				(PPC4XX_NUM_PD - 1)</span>
 #define PPC4XX_NUM_GD				1024
<span class="p_del">-#define PPC4XX_LAST_SD				63</span>
<span class="p_del">-#define PPC4XX_NUM_SD				64</span>
<span class="p_add">+#define PPC4XX_LAST_GD				(PPC4XX_NUM_GD - 1)</span>
<span class="p_add">+#define PPC4XX_NUM_SD				256</span>
<span class="p_add">+#define PPC4XX_LAST_SD				(PPC4XX_NUM_SD - 1)</span>
 #define PPC4XX_SD_BUFFER_SIZE			2048
 
 #define PD_ENTRY_INUSE				1
<span class="p_header">diff --git a/drivers/gpu/drm/drm_dp_dual_mode_helper.c b/drivers/gpu/drm/drm_dp_dual_mode_helper.c</span>
<span class="p_header">index 0ef9011a1856..02a50929af67 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/drm_dp_dual_mode_helper.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/drm_dp_dual_mode_helper.c</span>
<span class="p_chunk">@@ -410,6 +410,7 @@</span> <span class="p_context"> int drm_lspcon_get_mode(struct i2c_adapter *adapter,</span>
 {
 	u8 data;
 	int ret = 0;
<span class="p_add">+	int retry;</span>
 
 	if (!mode) {
 		DRM_ERROR(&quot;NULL input\n&quot;);
<span class="p_chunk">@@ -417,10 +418,19 @@</span> <span class="p_context"> int drm_lspcon_get_mode(struct i2c_adapter *adapter,</span>
 	}
 
 	/* Read Status: i2c over aux */
<span class="p_del">-	ret = drm_dp_dual_mode_read(adapter, DP_DUAL_MODE_LSPCON_CURRENT_MODE,</span>
<span class="p_del">-				    &amp;data, sizeof(data));</span>
<span class="p_add">+	for (retry = 0; retry &lt; 6; retry++) {</span>
<span class="p_add">+		if (retry)</span>
<span class="p_add">+			usleep_range(500, 1000);</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = drm_dp_dual_mode_read(adapter,</span>
<span class="p_add">+					    DP_DUAL_MODE_LSPCON_CURRENT_MODE,</span>
<span class="p_add">+					    &amp;data, sizeof(data));</span>
<span class="p_add">+		if (!ret)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (ret &lt; 0) {
<span class="p_del">-		DRM_ERROR(&quot;LSPCON read(0x80, 0x41) failed\n&quot;);</span>
<span class="p_add">+		DRM_DEBUG_KMS(&quot;LSPCON read(0x80, 0x41) failed\n&quot;);</span>
 		return -EFAULT;
 	}
 
<span class="p_header">diff --git a/drivers/gpu/drm/vc4/vc4_dsi.c b/drivers/gpu/drm/vc4/vc4_dsi.c</span>
<span class="p_header">index d1e0dc908048..04796d7d0fdb 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vc4/vc4_dsi.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vc4/vc4_dsi.c</span>
<span class="p_chunk">@@ -866,7 +866,8 @@</span> <span class="p_context"> static bool vc4_dsi_encoder_mode_fixup(struct drm_encoder *encoder,</span>
 	adjusted_mode-&gt;clock = pixel_clock_hz / 1000 + 1;
 
 	/* Given the new pixel clock, adjust HFP to keep vrefresh the same. */
<span class="p_del">-	adjusted_mode-&gt;htotal = pixel_clock_hz / (mode-&gt;vrefresh * mode-&gt;vtotal);</span>
<span class="p_add">+	adjusted_mode-&gt;htotal = adjusted_mode-&gt;clock * mode-&gt;htotal /</span>
<span class="p_add">+				mode-&gt;clock;</span>
 	adjusted_mode-&gt;hsync_end += adjusted_mode-&gt;htotal - mode-&gt;htotal;
 	adjusted_mode-&gt;hsync_start += adjusted_mode-&gt;htotal - mode-&gt;htotal;
 
<span class="p_header">diff --git a/drivers/hv/vmbus_drv.c b/drivers/hv/vmbus_drv.c</span>
<span class="p_header">index 937801ac2fe0..2cd134dd94d2 100644</span>
<span class="p_header">--- a/drivers/hv/vmbus_drv.c</span>
<span class="p_header">+++ b/drivers/hv/vmbus_drv.c</span>
<span class="p_chunk">@@ -1534,7 +1534,7 @@</span> <span class="p_context"> static int __init hv_acpi_init(void)</span>
 {
 	int ret, t;
 
<span class="p_del">-	if (x86_hyper != &amp;x86_hyper_ms_hyperv)</span>
<span class="p_add">+	if (x86_hyper_type != X86_HYPER_MS_HYPERV)</span>
 		return -ENODEV;
 
 	init_completion(&amp;probe_event);
<span class="p_header">diff --git a/drivers/iio/accel/st_accel_core.c b/drivers/iio/accel/st_accel_core.c</span>
<span class="p_header">index 752856b3a849..379de1829cdb 100644</span>
<span class="p_header">--- a/drivers/iio/accel/st_accel_core.c</span>
<span class="p_header">+++ b/drivers/iio/accel/st_accel_core.c</span>
<span class="p_chunk">@@ -164,7 +164,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_accel_sensors_settings[] = {</span>
 			.mask_int2 = 0x00,
 			.addr_ihl = 0x25,
 			.mask_ihl = 0x02,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.sim = {
 			.addr = 0x23,
<span class="p_chunk">@@ -236,7 +239,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_accel_sensors_settings[] = {</span>
 			.mask_ihl = 0x80,
 			.addr_od = 0x22,
 			.mask_od = 0x40,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.sim = {
 			.addr = 0x23,
<span class="p_chunk">@@ -318,7 +324,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_accel_sensors_settings[] = {</span>
 			.mask_int2 = 0x00,
 			.addr_ihl = 0x23,
 			.mask_ihl = 0x40,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 			.ig1 = {
 				.en_addr = 0x23,
 				.en_mask = 0x08,
<span class="p_chunk">@@ -389,7 +398,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_accel_sensors_settings[] = {</span>
 		.drdy_irq = {
 			.addr = 0x21,
 			.mask_int1 = 0x04,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.sim = {
 			.addr = 0x21,
<span class="p_chunk">@@ -451,7 +463,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_accel_sensors_settings[] = {</span>
 			.mask_ihl = 0x80,
 			.addr_od = 0x22,
 			.mask_od = 0x40,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.sim = {
 			.addr = 0x21,
<span class="p_chunk">@@ -569,7 +584,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_accel_sensors_settings[] = {</span>
 		.drdy_irq = {
 			.addr = 0x21,
 			.mask_int1 = 0x04,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.sim = {
 			.addr = 0x21,
<span class="p_chunk">@@ -640,7 +658,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_accel_sensors_settings[] = {</span>
 			.mask_int2 = 0x00,
 			.addr_ihl = 0x25,
 			.mask_ihl = 0x02,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.sim = {
 			.addr = 0x23,
<span class="p_header">diff --git a/drivers/iio/common/st_sensors/st_sensors_core.c b/drivers/iio/common/st_sensors/st_sensors_core.c</span>
<span class="p_header">index 02e833b14db0..34115f05d5c4 100644</span>
<span class="p_header">--- a/drivers/iio/common/st_sensors/st_sensors_core.c</span>
<span class="p_header">+++ b/drivers/iio/common/st_sensors/st_sensors_core.c</span>
<span class="p_chunk">@@ -470,7 +470,7 @@</span> <span class="p_context"> int st_sensors_set_dataready_irq(struct iio_dev *indio_dev, bool enable)</span>
 		 * different one. Take into account irq status register
 		 * to understand if irq trigger can be properly supported
 		 */
<span class="p_del">-		if (sdata-&gt;sensor_settings-&gt;drdy_irq.addr_stat_drdy)</span>
<span class="p_add">+		if (sdata-&gt;sensor_settings-&gt;drdy_irq.stat_drdy.addr)</span>
 			sdata-&gt;hw_irq_trigger = enable;
 		return 0;
 	}
<span class="p_header">diff --git a/drivers/iio/common/st_sensors/st_sensors_trigger.c b/drivers/iio/common/st_sensors/st_sensors_trigger.c</span>
<span class="p_header">index fa73e6795359..fdcc5a891958 100644</span>
<span class="p_header">--- a/drivers/iio/common/st_sensors/st_sensors_trigger.c</span>
<span class="p_header">+++ b/drivers/iio/common/st_sensors/st_sensors_trigger.c</span>
<span class="p_chunk">@@ -31,7 +31,7 @@</span> <span class="p_context"> static int st_sensors_new_samples_available(struct iio_dev *indio_dev,</span>
 	int ret;
 
 	/* How would I know if I can&#39;t check it? */
<span class="p_del">-	if (!sdata-&gt;sensor_settings-&gt;drdy_irq.addr_stat_drdy)</span>
<span class="p_add">+	if (!sdata-&gt;sensor_settings-&gt;drdy_irq.stat_drdy.addr)</span>
 		return -EINVAL;
 
 	/* No scan mask, no interrupt */
<span class="p_chunk">@@ -39,23 +39,15 @@</span> <span class="p_context"> static int st_sensors_new_samples_available(struct iio_dev *indio_dev,</span>
 		return 0;
 
 	ret = sdata-&gt;tf-&gt;read_byte(&amp;sdata-&gt;tb, sdata-&gt;dev,
<span class="p_del">-			sdata-&gt;sensor_settings-&gt;drdy_irq.addr_stat_drdy,</span>
<span class="p_add">+			sdata-&gt;sensor_settings-&gt;drdy_irq.stat_drdy.addr,</span>
 			&amp;status);
 	if (ret &lt; 0) {
 		dev_err(sdata-&gt;dev,
 			&quot;error checking samples available\n&quot;);
 		return ret;
 	}
<span class="p_del">-	/*</span>
<span class="p_del">-	 * the lower bits of .active_scan_mask[0] is directly mapped</span>
<span class="p_del">-	 * to the channels on the sensor: either bit 0 for</span>
<span class="p_del">-	 * one-dimensional sensors, or e.g. x,y,z for accelerometers,</span>
<span class="p_del">-	 * gyroscopes or magnetometers. No sensor use more than 3</span>
<span class="p_del">-	 * channels, so cut the other status bits here.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	status &amp;= 0x07;</span>
 
<span class="p_del">-	if (status &amp; (u8)indio_dev-&gt;active_scan_mask[0])</span>
<span class="p_add">+	if (status &amp; sdata-&gt;sensor_settings-&gt;drdy_irq.stat_drdy.mask)</span>
 		return 1;
 
 	return 0;
<span class="p_chunk">@@ -212,7 +204,7 @@</span> <span class="p_context"> int st_sensors_allocate_trigger(struct iio_dev *indio_dev,</span>
 	 * it was &quot;our&quot; interrupt.
 	 */
 	if (sdata-&gt;int_pin_open_drain &amp;&amp;
<span class="p_del">-	    sdata-&gt;sensor_settings-&gt;drdy_irq.addr_stat_drdy)</span>
<span class="p_add">+	    sdata-&gt;sensor_settings-&gt;drdy_irq.stat_drdy.addr)</span>
 		irq_trig |= IRQF_SHARED;
 
 	err = request_threaded_irq(sdata-&gt;get_irq_data_ready(indio_dev),
<span class="p_header">diff --git a/drivers/iio/gyro/st_gyro_core.c b/drivers/iio/gyro/st_gyro_core.c</span>
<span class="p_header">index e366422e8512..2536a8400c98 100644</span>
<span class="p_header">--- a/drivers/iio/gyro/st_gyro_core.c</span>
<span class="p_header">+++ b/drivers/iio/gyro/st_gyro_core.c</span>
<span class="p_chunk">@@ -118,7 +118,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_gyro_sensors_settings[] = {</span>
 			 * drain settings, but only for INT1 and not
 			 * for the DRDY line on INT2.
 			 */
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = true,
 		.bootime = 2,
<span class="p_chunk">@@ -188,7 +191,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_gyro_sensors_settings[] = {</span>
 			 * drain settings, but only for INT1 and not
 			 * for the DRDY line on INT2.
 			 */
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = true,
 		.bootime = 2,
<span class="p_chunk">@@ -253,7 +259,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_gyro_sensors_settings[] = {</span>
 			 * drain settings, but only for INT1 and not
 			 * for the DRDY line on INT2.
 			 */
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = true,
 		.bootime = 2,
<span class="p_header">diff --git a/drivers/iio/magnetometer/st_magn_core.c b/drivers/iio/magnetometer/st_magn_core.c</span>
<span class="p_header">index 08aafba4481c..19031a7bce23 100644</span>
<span class="p_header">--- a/drivers/iio/magnetometer/st_magn_core.c</span>
<span class="p_header">+++ b/drivers/iio/magnetometer/st_magn_core.c</span>
<span class="p_chunk">@@ -317,7 +317,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_magn_sensors_settings[] = {</span>
 		},
 		.drdy_irq = {
 			/* drdy line is routed drdy pin */
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = true,
 		.bootime = 2,
<span class="p_chunk">@@ -361,7 +364,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_magn_sensors_settings[] = {</span>
 		.drdy_irq = {
 			.addr = 0x62,
 			.mask_int1 = 0x01,
<span class="p_del">-			.addr_stat_drdy = 0x67,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = 0x67,</span>
<span class="p_add">+				.mask = 0x07,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = false,
 		.bootime = 2,
<span class="p_header">diff --git a/drivers/iio/pressure/st_pressure_core.c b/drivers/iio/pressure/st_pressure_core.c</span>
<span class="p_header">index 34611a8ea2ce..ea075fcd5a6f 100644</span>
<span class="p_header">--- a/drivers/iio/pressure/st_pressure_core.c</span>
<span class="p_header">+++ b/drivers/iio/pressure/st_pressure_core.c</span>
<span class="p_chunk">@@ -287,7 +287,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_press_sensors_settings[] = {</span>
 			.mask_ihl = 0x80,
 			.addr_od = 0x22,
 			.mask_od = 0x40,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x03,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = true,
 		.bootime = 2,
<span class="p_chunk">@@ -395,7 +398,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_press_sensors_settings[] = {</span>
 			.mask_ihl = 0x80,
 			.addr_od = 0x22,
 			.mask_od = 0x40,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x03,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = true,
 		.bootime = 2,
<span class="p_chunk">@@ -454,7 +460,10 @@</span> <span class="p_context"> static const struct st_sensor_settings st_press_sensors_settings[] = {</span>
 			.mask_ihl = 0x80,
 			.addr_od = 0x12,
 			.mask_od = 0x40,
<span class="p_del">-			.addr_stat_drdy = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+			.stat_drdy = {</span>
<span class="p_add">+				.addr = ST_SENSORS_DEFAULT_STAT_ADDR,</span>
<span class="p_add">+				.mask = 0x03,</span>
<span class="p_add">+			},</span>
 		},
 		.multi_read_bit = false,
 		.bootime = 2,
<span class="p_header">diff --git a/drivers/infiniband/hw/hns/hns_roce_hw_v1.c b/drivers/infiniband/hw/hns/hns_roce_hw_v1.c</span>
<span class="p_header">index 747efd1ae5a6..8208c30f03c5 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/hns/hns_roce_hw_v1.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/hns/hns_roce_hw_v1.c</span>
<span class="p_chunk">@@ -1001,6 +1001,11 @@</span> <span class="p_context"> static void hns_roce_v1_mr_free_work_fn(struct work_struct *work)</span>
 		}
 	}
 
<span class="p_add">+	if (!ne) {</span>
<span class="p_add">+		dev_err(dev, &quot;Reseved loop qp is absent!\n&quot;);</span>
<span class="p_add">+		goto free_work;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	do {
 		ret = hns_roce_v1_poll_cq(&amp;mr_free_cq-&gt;ib_cq, ne, wc);
 		if (ret &lt; 0) {
<span class="p_header">diff --git a/drivers/infiniband/sw/rxe/rxe_pool.c b/drivers/infiniband/sw/rxe/rxe_pool.c</span>
<span class="p_header">index c1b5f38f31a5..3b4916680018 100644</span>
<span class="p_header">--- a/drivers/infiniband/sw/rxe/rxe_pool.c</span>
<span class="p_header">+++ b/drivers/infiniband/sw/rxe/rxe_pool.c</span>
<span class="p_chunk">@@ -404,6 +404,8 @@</span> <span class="p_context"> void *rxe_alloc(struct rxe_pool *pool)</span>
 	elem = kmem_cache_zalloc(pool_cache(pool),
 				 (pool-&gt;flags &amp; RXE_POOL_ATOMIC) ?
 				 GFP_ATOMIC : GFP_KERNEL);
<span class="p_add">+	if (!elem)</span>
<span class="p_add">+		return NULL;</span>
 
 	elem-&gt;pool = pool;
 	kref_init(&amp;elem-&gt;ref_cnt);
<span class="p_header">diff --git a/drivers/infiniband/ulp/opa_vnic/opa_vnic_encap.c b/drivers/infiniband/ulp/opa_vnic/opa_vnic_encap.c</span>
<span class="p_header">index afa938bd26d6..a72278e9cd27 100644</span>
<span class="p_header">--- a/drivers/infiniband/ulp/opa_vnic/opa_vnic_encap.c</span>
<span class="p_header">+++ b/drivers/infiniband/ulp/opa_vnic/opa_vnic_encap.c</span>
<span class="p_chunk">@@ -139,6 +139,7 @@</span> <span class="p_context"> void opa_vnic_release_mac_tbl(struct opa_vnic_adapter *adapter)</span>
 	rcu_assign_pointer(adapter-&gt;mactbl, NULL);
 	synchronize_rcu();
 	opa_vnic_free_mac_tbl(mactbl);
<span class="p_add">+	adapter-&gt;info.vport.mac_tbl_digest = 0;</span>
 	mutex_unlock(&amp;adapter-&gt;mactbl_lock);
 }
 
<span class="p_header">diff --git a/drivers/infiniband/ulp/opa_vnic/opa_vnic_vema_iface.c b/drivers/infiniband/ulp/opa_vnic/opa_vnic_vema_iface.c</span>
<span class="p_header">index c2733964379c..9655cc3aa3a0 100644</span>
<span class="p_header">--- a/drivers/infiniband/ulp/opa_vnic/opa_vnic_vema_iface.c</span>
<span class="p_header">+++ b/drivers/infiniband/ulp/opa_vnic/opa_vnic_vema_iface.c</span>
<span class="p_chunk">@@ -348,7 +348,7 @@</span> <span class="p_context"> void opa_vnic_query_mcast_macs(struct opa_vnic_adapter *adapter,</span>
 void opa_vnic_query_ucast_macs(struct opa_vnic_adapter *adapter,
 			       struct opa_veswport_iface_macs *macs)
 {
<span class="p_del">-	u16 start_idx, tot_macs, num_macs, idx = 0, count = 0;</span>
<span class="p_add">+	u16 start_idx, tot_macs, num_macs, idx = 0, count = 0, em_macs = 0;</span>
 	struct netdev_hw_addr *ha;
 
 	start_idx = be16_to_cpu(macs-&gt;start_idx);
<span class="p_chunk">@@ -359,8 +359,10 @@</span> <span class="p_context"> void opa_vnic_query_ucast_macs(struct opa_vnic_adapter *adapter,</span>
 
 		/* Do not include EM specified MAC address */
 		if (!memcmp(adapter-&gt;info.vport.base_mac_addr, ha-&gt;addr,
<span class="p_del">-			    ARRAY_SIZE(adapter-&gt;info.vport.base_mac_addr)))</span>
<span class="p_add">+			    ARRAY_SIZE(adapter-&gt;info.vport.base_mac_addr))) {</span>
<span class="p_add">+			em_macs++;</span>
 			continue;
<span class="p_add">+		}</span>
 
 		if (start_idx &gt; idx++)
 			continue;
<span class="p_chunk">@@ -383,7 +385,7 @@</span> <span class="p_context"> void opa_vnic_query_ucast_macs(struct opa_vnic_adapter *adapter,</span>
 	}
 
 	tot_macs = netdev_hw_addr_list_count(&amp;adapter-&gt;netdev-&gt;dev_addrs) +
<span class="p_del">-		   netdev_uc_count(adapter-&gt;netdev);</span>
<span class="p_add">+		   netdev_uc_count(adapter-&gt;netdev) - em_macs;</span>
 	macs-&gt;tot_macs_in_lst = cpu_to_be16(tot_macs);
 	macs-&gt;num_macs_in_msg = cpu_to_be16(count);
 	macs-&gt;gen_count = cpu_to_be16(adapter-&gt;info.vport.uc_macs_gen_count);
<span class="p_header">diff --git a/drivers/input/mouse/vmmouse.c b/drivers/input/mouse/vmmouse.c</span>
<span class="p_header">index 0f586780ceb4..1ae5c1ef3f5b 100644</span>
<span class="p_header">--- a/drivers/input/mouse/vmmouse.c</span>
<span class="p_header">+++ b/drivers/input/mouse/vmmouse.c</span>
<span class="p_chunk">@@ -316,11 +316,9 @@</span> <span class="p_context"> static int vmmouse_enable(struct psmouse *psmouse)</span>
 /*
  * Array of supported hypervisors.
  */
<span class="p_del">-static const struct hypervisor_x86 *vmmouse_supported_hypervisors[] = {</span>
<span class="p_del">-	&amp;x86_hyper_vmware,</span>
<span class="p_del">-#ifdef CONFIG_KVM_GUEST</span>
<span class="p_del">-	&amp;x86_hyper_kvm,</span>
<span class="p_del">-#endif</span>
<span class="p_add">+static enum x86_hypervisor_type vmmouse_supported_hypervisors[] = {</span>
<span class="p_add">+	X86_HYPER_VMWARE,</span>
<span class="p_add">+	X86_HYPER_KVM,</span>
 };
 
 /**
<span class="p_chunk">@@ -331,7 +329,7 @@</span> <span class="p_context"> static bool vmmouse_check_hypervisor(void)</span>
 	int i;
 
 	for (i = 0; i &lt; ARRAY_SIZE(vmmouse_supported_hypervisors); i++)
<span class="p_del">-		if (vmmouse_supported_hypervisors[i] == x86_hyper)</span>
<span class="p_add">+		if (vmmouse_supported_hypervisors[i] == x86_hyper_type)</span>
 			return true;
 
 	return false;
<span class="p_header">diff --git a/drivers/leds/leds-pca955x.c b/drivers/leds/leds-pca955x.c</span>
<span class="p_header">index 905729191d3e..78183f90820e 100644</span>
<span class="p_header">--- a/drivers/leds/leds-pca955x.c</span>
<span class="p_header">+++ b/drivers/leds/leds-pca955x.c</span>
<span class="p_chunk">@@ -61,6 +61,10 @@</span> <span class="p_context"></span>
 #define PCA955X_LS_BLINK0	0x2	/* Blink at PWM0 rate */
 #define PCA955X_LS_BLINK1	0x3	/* Blink at PWM1 rate */
 
<span class="p_add">+#define PCA955X_GPIO_INPUT	LED_OFF</span>
<span class="p_add">+#define PCA955X_GPIO_HIGH	LED_OFF</span>
<span class="p_add">+#define PCA955X_GPIO_LOW	LED_FULL</span>
<span class="p_add">+</span>
 enum pca955x_type {
 	pca9550,
 	pca9551,
<span class="p_chunk">@@ -329,9 +333,9 @@</span> <span class="p_context"> static int pca955x_set_value(struct gpio_chip *gc, unsigned int offset,</span>
 	struct pca955x_led *led = &amp;pca955x-&gt;leds[offset];
 
 	if (val)
<span class="p_del">-		return pca955x_led_set(&amp;led-&gt;led_cdev, LED_FULL);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		return pca955x_led_set(&amp;led-&gt;led_cdev, LED_OFF);</span>
<span class="p_add">+		return pca955x_led_set(&amp;led-&gt;led_cdev, PCA955X_GPIO_HIGH);</span>
<span class="p_add">+</span>
<span class="p_add">+	return pca955x_led_set(&amp;led-&gt;led_cdev, PCA955X_GPIO_LOW);</span>
 }
 
 static void pca955x_gpio_set_value(struct gpio_chip *gc, unsigned int offset,
<span class="p_chunk">@@ -355,8 +359,11 @@</span> <span class="p_context"> static int pca955x_gpio_get_value(struct gpio_chip *gc, unsigned int offset)</span>
 static int pca955x_gpio_direction_input(struct gpio_chip *gc,
 					unsigned int offset)
 {
<span class="p_del">-	/* To use as input ensure pin is not driven */</span>
<span class="p_del">-	return pca955x_set_value(gc, offset, 0);</span>
<span class="p_add">+	struct pca955x *pca955x = gpiochip_get_data(gc);</span>
<span class="p_add">+	struct pca955x_led *led = &amp;pca955x-&gt;leds[offset];</span>
<span class="p_add">+</span>
<span class="p_add">+	/* To use as input ensure pin is not driven. */</span>
<span class="p_add">+	return pca955x_led_set(&amp;led-&gt;led_cdev, PCA955X_GPIO_INPUT);</span>
 }
 
 static int pca955x_gpio_direction_output(struct gpio_chip *gc,
<span class="p_header">diff --git a/drivers/md/dm-mpath.c b/drivers/md/dm-mpath.c</span>
<span class="p_header">index 35e82b14ded7..ddf0a4341ae2 100644</span>
<span class="p_header">--- a/drivers/md/dm-mpath.c</span>
<span class="p_header">+++ b/drivers/md/dm-mpath.c</span>
<span class="p_chunk">@@ -366,7 +366,7 @@</span> <span class="p_context"> static struct pgpath *choose_path_in_pg(struct multipath *m,</span>
 
 	pgpath = path_to_pgpath(path);
 
<span class="p_del">-	if (unlikely(lockless_dereference(m-&gt;current_pg) != pg)) {</span>
<span class="p_add">+	if (unlikely(READ_ONCE(m-&gt;current_pg) != pg)) {</span>
 		/* Only update current_pgpath if pg changed */
 		spin_lock_irqsave(&amp;m-&gt;lock, flags);
 		m-&gt;current_pgpath = pgpath;
<span class="p_chunk">@@ -390,7 +390,7 @@</span> <span class="p_context"> static struct pgpath *choose_pgpath(struct multipath *m, size_t nr_bytes)</span>
 	}
 
 	/* Were we instructed to switch PG? */
<span class="p_del">-	if (lockless_dereference(m-&gt;next_pg)) {</span>
<span class="p_add">+	if (READ_ONCE(m-&gt;next_pg)) {</span>
 		spin_lock_irqsave(&amp;m-&gt;lock, flags);
 		pg = m-&gt;next_pg;
 		if (!pg) {
<span class="p_chunk">@@ -406,7 +406,7 @@</span> <span class="p_context"> static struct pgpath *choose_pgpath(struct multipath *m, size_t nr_bytes)</span>
 
 	/* Don&#39;t change PG until it has no remaining paths */
 check_current_pg:
<span class="p_del">-	pg = lockless_dereference(m-&gt;current_pg);</span>
<span class="p_add">+	pg = READ_ONCE(m-&gt;current_pg);</span>
 	if (pg) {
 		pgpath = choose_path_in_pg(m, pg, nr_bytes);
 		if (!IS_ERR_OR_NULL(pgpath))
<span class="p_chunk">@@ -473,7 +473,7 @@</span> <span class="p_context"> static int multipath_clone_and_map(struct dm_target *ti, struct request *rq,</span>
 	struct request *clone;
 
 	/* Do we need to select a new pgpath? */
<span class="p_del">-	pgpath = lockless_dereference(m-&gt;current_pgpath);</span>
<span class="p_add">+	pgpath = READ_ONCE(m-&gt;current_pgpath);</span>
 	if (!pgpath || !test_bit(MPATHF_QUEUE_IO, &amp;m-&gt;flags))
 		pgpath = choose_pgpath(m, nr_bytes);
 
<span class="p_chunk">@@ -533,7 +533,7 @@</span> <span class="p_context"> static int __multipath_map_bio(struct multipath *m, struct bio *bio, struct dm_m</span>
 	bool queue_io;
 
 	/* Do we need to select a new pgpath? */
<span class="p_del">-	pgpath = lockless_dereference(m-&gt;current_pgpath);</span>
<span class="p_add">+	pgpath = READ_ONCE(m-&gt;current_pgpath);</span>
 	queue_io = test_bit(MPATHF_QUEUE_IO, &amp;m-&gt;flags);
 	if (!pgpath || !queue_io)
 		pgpath = choose_pgpath(m, nr_bytes);
<span class="p_chunk">@@ -1802,7 +1802,7 @@</span> <span class="p_context"> static int multipath_prepare_ioctl(struct dm_target *ti,</span>
 	struct pgpath *current_pgpath;
 	int r;
 
<span class="p_del">-	current_pgpath = lockless_dereference(m-&gt;current_pgpath);</span>
<span class="p_add">+	current_pgpath = READ_ONCE(m-&gt;current_pgpath);</span>
 	if (!current_pgpath)
 		current_pgpath = choose_pgpath(m, 0);
 
<span class="p_chunk">@@ -1824,7 +1824,7 @@</span> <span class="p_context"> static int multipath_prepare_ioctl(struct dm_target *ti,</span>
 	}
 
 	if (r == -ENOTCONN) {
<span class="p_del">-		if (!lockless_dereference(m-&gt;current_pg)) {</span>
<span class="p_add">+		if (!READ_ONCE(m-&gt;current_pg)) {</span>
 			/* Path status changed, redo selection */
 			(void) choose_pgpath(m, 0);
 		}
<span class="p_chunk">@@ -1893,9 +1893,9 @@</span> <span class="p_context"> static int multipath_busy(struct dm_target *ti)</span>
 		return (m-&gt;queue_mode != DM_TYPE_MQ_REQUEST_BASED);
 
 	/* Guess which priority_group will be used at next mapping time */
<span class="p_del">-	pg = lockless_dereference(m-&gt;current_pg);</span>
<span class="p_del">-	next_pg = lockless_dereference(m-&gt;next_pg);</span>
<span class="p_del">-	if (unlikely(!lockless_dereference(m-&gt;current_pgpath) &amp;&amp; next_pg))</span>
<span class="p_add">+	pg = READ_ONCE(m-&gt;current_pg);</span>
<span class="p_add">+	next_pg = READ_ONCE(m-&gt;next_pg);</span>
<span class="p_add">+	if (unlikely(!READ_ONCE(m-&gt;current_pgpath) &amp;&amp; next_pg))</span>
 		pg = next_pg;
 
 	if (!pg) {
<span class="p_header">diff --git a/drivers/md/md.c b/drivers/md/md.c</span>
<span class="p_header">index 98ea86309ceb..6bf093cef958 100644</span>
<span class="p_header">--- a/drivers/md/md.c</span>
<span class="p_header">+++ b/drivers/md/md.c</span>
<span class="p_chunk">@@ -7468,8 +7468,8 @@</span> <span class="p_context"> void md_wakeup_thread(struct md_thread *thread)</span>
 {
 	if (thread) {
 		pr_debug(&quot;md: waking up MD thread %s.\n&quot;, thread-&gt;tsk-&gt;comm);
<span class="p_del">-		if (!test_and_set_bit(THREAD_WAKEUP, &amp;thread-&gt;flags))</span>
<span class="p_del">-			wake_up(&amp;thread-&gt;wqueue);</span>
<span class="p_add">+		set_bit(THREAD_WAKEUP, &amp;thread-&gt;flags);</span>
<span class="p_add">+		wake_up(&amp;thread-&gt;wqueue);</span>
 	}
 }
 EXPORT_SYMBOL(md_wakeup_thread);
<span class="p_header">diff --git a/drivers/misc/pti.c b/drivers/misc/pti.c</span>
<span class="p_header">index eda38cbe8530..41f2a9f6851d 100644</span>
<span class="p_header">--- a/drivers/misc/pti.c</span>
<span class="p_header">+++ b/drivers/misc/pti.c</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/pci.h&gt;
 #include &lt;linux/mutex.h&gt;
 #include &lt;linux/miscdevice.h&gt;
<span class="p_del">-#include &lt;linux/pti.h&gt;</span>
<span class="p_add">+#include &lt;linux/intel-pti.h&gt;</span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/uaccess.h&gt;
 
<span class="p_header">diff --git a/drivers/misc/vmw_balloon.c b/drivers/misc/vmw_balloon.c</span>
<span class="p_header">index 1e688bfec567..9047c0a529b2 100644</span>
<span class="p_header">--- a/drivers/misc/vmw_balloon.c</span>
<span class="p_header">+++ b/drivers/misc/vmw_balloon.c</span>
<span class="p_chunk">@@ -1271,7 +1271,7 @@</span> <span class="p_context"> static int __init vmballoon_init(void)</span>
 	 * Check if we are running on VMware&#39;s hypervisor and bail out
 	 * if we are not.
 	 */
<span class="p_del">-	if (x86_hyper != &amp;x86_hyper_vmware)</span>
<span class="p_add">+	if (x86_hyper_type != X86_HYPER_VMWARE)</span>
 		return -ENODEV;
 
 	for (is_2m_pages = 0; is_2m_pages &lt; VMW_BALLOON_NUM_PAGE_SIZES;
<span class="p_header">diff --git a/drivers/net/ethernet/ibm/ibmvnic.c b/drivers/net/ethernet/ibm/ibmvnic.c</span>
<span class="p_header">index c66abd476023..3b0db01ead1f 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/ibm/ibmvnic.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/ibm/ibmvnic.c</span>
<span class="p_chunk">@@ -927,6 +927,7 @@</span> <span class="p_context"> static int ibmvnic_open(struct net_device *netdev)</span>
 	}
 
 	rc = __ibmvnic_open(netdev);
<span class="p_add">+	netif_carrier_on(netdev);</span>
 	mutex_unlock(&amp;adapter-&gt;reset_lock);
 
 	return rc;
<span class="p_chunk">@@ -3899,6 +3900,7 @@</span> <span class="p_context"> static int ibmvnic_probe(struct vio_dev *dev, const struct vio_device_id *id)</span>
 	if (rc)
 		goto ibmvnic_init_fail;
 
<span class="p_add">+	netif_carrier_off(netdev);</span>
 	rc = register_netdev(netdev);
 	if (rc) {
 		dev_err(&amp;dev-&gt;dev, &quot;failed to register netdev rc=%d\n&quot;, rc);
<span class="p_header">diff --git a/drivers/net/ethernet/intel/fm10k/fm10k.h b/drivers/net/ethernet/intel/fm10k/fm10k.h</span>
<span class="p_header">index 689c413b7782..d2f9a2dd76a2 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/fm10k/fm10k.h</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/fm10k/fm10k.h</span>
<span class="p_chunk">@@ -526,8 +526,8 @@</span> <span class="p_context"> s32 fm10k_iov_update_pvid(struct fm10k_intfc *interface, u16 glort, u16 pvid);</span>
 int fm10k_ndo_set_vf_mac(struct net_device *netdev, int vf_idx, u8 *mac);
 int fm10k_ndo_set_vf_vlan(struct net_device *netdev,
 			  int vf_idx, u16 vid, u8 qos, __be16 vlan_proto);
<span class="p_del">-int fm10k_ndo_set_vf_bw(struct net_device *netdev, int vf_idx, int rate,</span>
<span class="p_del">-			int unused);</span>
<span class="p_add">+int fm10k_ndo_set_vf_bw(struct net_device *netdev, int vf_idx,</span>
<span class="p_add">+			int __always_unused min_rate, int max_rate);</span>
 int fm10k_ndo_get_vf_config(struct net_device *netdev,
 			    int vf_idx, struct ifla_vf_info *ivi);
 
<span class="p_header">diff --git a/drivers/net/ethernet/intel/fm10k/fm10k_iov.c b/drivers/net/ethernet/intel/fm10k/fm10k_iov.c</span>
<span class="p_header">index 5f4dac0d36ef..e72fd52bacfe 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/fm10k/fm10k_iov.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/fm10k/fm10k_iov.c</span>
<span class="p_chunk">@@ -126,6 +126,9 @@</span> <span class="p_context"> s32 fm10k_iov_mbx(struct fm10k_intfc *interface)</span>
 		struct fm10k_mbx_info *mbx = &amp;vf_info-&gt;mbx;
 		u16 glort = vf_info-&gt;glort;
 
<span class="p_add">+		/* process the SM mailbox first to drain outgoing messages */</span>
<span class="p_add">+		hw-&gt;mbx.ops.process(hw, &amp;hw-&gt;mbx);</span>
<span class="p_add">+</span>
 		/* verify port mapping is valid, if not reset port */
 		if (vf_info-&gt;vf_flags &amp;&amp; !fm10k_glort_valid_pf(hw, glort))
 			hw-&gt;iov.ops.reset_lport(hw, vf_info);
<span class="p_chunk">@@ -482,7 +485,7 @@</span> <span class="p_context"> int fm10k_ndo_set_vf_vlan(struct net_device *netdev, int vf_idx, u16 vid,</span>
 }
 
 int fm10k_ndo_set_vf_bw(struct net_device *netdev, int vf_idx,
<span class="p_del">-			int __always_unused unused, int rate)</span>
<span class="p_add">+			int __always_unused min_rate, int max_rate)</span>
 {
 	struct fm10k_intfc *interface = netdev_priv(netdev);
 	struct fm10k_iov_data *iov_data = interface-&gt;iov_data;
<span class="p_chunk">@@ -493,14 +496,15 @@</span> <span class="p_context"> int fm10k_ndo_set_vf_bw(struct net_device *netdev, int vf_idx,</span>
 		return -EINVAL;
 
 	/* rate limit cannot be less than 10Mbs or greater than link speed */
<span class="p_del">-	if (rate &amp;&amp; ((rate &lt; FM10K_VF_TC_MIN) || rate &gt; FM10K_VF_TC_MAX))</span>
<span class="p_add">+	if (max_rate &amp;&amp;</span>
<span class="p_add">+	    (max_rate &lt; FM10K_VF_TC_MIN || max_rate &gt; FM10K_VF_TC_MAX))</span>
 		return -EINVAL;
 
 	/* store values */
<span class="p_del">-	iov_data-&gt;vf_info[vf_idx].rate = rate;</span>
<span class="p_add">+	iov_data-&gt;vf_info[vf_idx].rate = max_rate;</span>
 
 	/* update hardware configuration */
<span class="p_del">-	hw-&gt;iov.ops.configure_tc(hw, vf_idx, rate);</span>
<span class="p_add">+	hw-&gt;iov.ops.configure_tc(hw, vf_idx, max_rate);</span>
 
 	return 0;
 }
<span class="p_header">diff --git a/drivers/net/ethernet/intel/i40e/i40e_main.c b/drivers/net/ethernet/intel/i40e/i40e_main.c</span>
<span class="p_header">index ea20aacd5e1d..b2cde9b16d82 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/i40e/i40e_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c</span>
<span class="p_chunk">@@ -2874,14 +2874,15 @@</span> <span class="p_context"> static void i40e_vsi_free_rx_resources(struct i40e_vsi *vsi)</span>
 static void i40e_config_xps_tx_ring(struct i40e_ring *ring)
 {
 	struct i40e_vsi *vsi = ring-&gt;vsi;
<span class="p_add">+	int cpu;</span>
 
 	if (!ring-&gt;q_vector || !ring-&gt;netdev)
 		return;
 
 	if ((vsi-&gt;tc_config.numtc &lt;= 1) &amp;&amp;
 	    !test_and_set_bit(__I40E_TX_XPS_INIT_DONE, &amp;ring-&gt;state)) {
<span class="p_del">-		netif_set_xps_queue(ring-&gt;netdev,</span>
<span class="p_del">-				    get_cpu_mask(ring-&gt;q_vector-&gt;v_idx),</span>
<span class="p_add">+		cpu = cpumask_local_spread(ring-&gt;q_vector-&gt;v_idx, -1);</span>
<span class="p_add">+		netif_set_xps_queue(ring-&gt;netdev, get_cpu_mask(cpu),</span>
 				    ring-&gt;queue_index);
 	}
 
<span class="p_chunk">@@ -3471,6 +3472,7 @@</span> <span class="p_context"> static int i40e_vsi_request_irq_msix(struct i40e_vsi *vsi, char *basename)</span>
 	int tx_int_idx = 0;
 	int vector, err;
 	int irq_num;
<span class="p_add">+	int cpu;</span>
 
 	for (vector = 0; vector &lt; q_vectors; vector++) {
 		struct i40e_q_vector *q_vector = vsi-&gt;q_vectors[vector];
<span class="p_chunk">@@ -3506,10 +3508,14 @@</span> <span class="p_context"> static int i40e_vsi_request_irq_msix(struct i40e_vsi *vsi, char *basename)</span>
 		q_vector-&gt;affinity_notify.notify = i40e_irq_affinity_notify;
 		q_vector-&gt;affinity_notify.release = i40e_irq_affinity_release;
 		irq_set_affinity_notifier(irq_num, &amp;q_vector-&gt;affinity_notify);
<span class="p_del">-		/* get_cpu_mask returns a static constant mask with</span>
<span class="p_del">-		 * a permanent lifetime so it&#39;s ok to use here.</span>
<span class="p_add">+		/* Spread affinity hints out across online CPUs.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * get_cpu_mask returns a static constant mask with</span>
<span class="p_add">+		 * a permanent lifetime so it&#39;s ok to pass to</span>
<span class="p_add">+		 * irq_set_affinity_hint without making a copy.</span>
 		 */
<span class="p_del">-		irq_set_affinity_hint(irq_num, get_cpu_mask(q_vector-&gt;v_idx));</span>
<span class="p_add">+		cpu = cpumask_local_spread(q_vector-&gt;v_idx, -1);</span>
<span class="p_add">+		irq_set_affinity_hint(irq_num, get_cpu_mask(cpu));</span>
 	}
 
 	vsi-&gt;irqs_ready = true;
<span class="p_header">diff --git a/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c b/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c</span>
<span class="p_header">index 4d1e670f490e..e368b0237a1b 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/i40e/i40e_virtchnl_pf.c</span>
<span class="p_chunk">@@ -1008,8 +1008,8 @@</span> <span class="p_context"> static void i40e_cleanup_reset_vf(struct i40e_vf *vf)</span>
 		set_bit(I40E_VF_STATE_ACTIVE, &amp;vf-&gt;vf_states);
 		clear_bit(I40E_VF_STATE_DISABLED, &amp;vf-&gt;vf_states);
 		/* Do not notify the client during VF init */
<span class="p_del">-		if (test_and_clear_bit(I40E_VF_STATE_PRE_ENABLE,</span>
<span class="p_del">-				       &amp;vf-&gt;vf_states))</span>
<span class="p_add">+		if (!test_and_clear_bit(I40E_VF_STATE_PRE_ENABLE,</span>
<span class="p_add">+					&amp;vf-&gt;vf_states))</span>
 			i40e_notify_client_of_vf_reset(pf, abs_vf_id);
 		vf-&gt;num_vlan = 0;
 	}
<span class="p_chunk">@@ -2779,6 +2779,7 @@</span> <span class="p_context"> int i40e_ndo_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac)</span>
 	struct i40e_mac_filter *f;
 	struct i40e_vf *vf;
 	int ret = 0;
<span class="p_add">+	struct hlist_node *h;</span>
 	int bkt;
 
 	/* validate the request */
<span class="p_chunk">@@ -2817,7 +2818,7 @@</span> <span class="p_context"> int i40e_ndo_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac)</span>
 	/* Delete all the filters for this VSI - we&#39;re going to kill it
 	 * anyway.
 	 */
<span class="p_del">-	hash_for_each(vsi-&gt;mac_filter_hash, bkt, f, hlist)</span>
<span class="p_add">+	hash_for_each_safe(vsi-&gt;mac_filter_hash, bkt, h, f, hlist)</span>
 		__i40e_del_filter(vsi, f);
 
 	spin_unlock_bh(&amp;vsi-&gt;mac_filter_hash_lock);
<span class="p_header">diff --git a/drivers/net/ethernet/intel/i40evf/i40evf_main.c b/drivers/net/ethernet/intel/i40evf/i40evf_main.c</span>
<span class="p_header">index 1825d956bb00..1ccad6f30ebf 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/i40evf/i40evf_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/i40evf/i40evf_main.c</span>
<span class="p_chunk">@@ -546,6 +546,7 @@</span> <span class="p_context"> i40evf_request_traffic_irqs(struct i40evf_adapter *adapter, char *basename)</span>
 	unsigned int vector, q_vectors;
 	unsigned int rx_int_idx = 0, tx_int_idx = 0;
 	int irq_num, err;
<span class="p_add">+	int cpu;</span>
 
 	i40evf_irq_disable(adapter);
 	/* Decrement for Other and TCP Timer vectors */
<span class="p_chunk">@@ -584,10 +585,12 @@</span> <span class="p_context"> i40evf_request_traffic_irqs(struct i40evf_adapter *adapter, char *basename)</span>
 		q_vector-&gt;affinity_notify.release =
 						   i40evf_irq_affinity_release;
 		irq_set_affinity_notifier(irq_num, &amp;q_vector-&gt;affinity_notify);
<span class="p_del">-		/* get_cpu_mask returns a static constant mask with</span>
<span class="p_del">-		 * a permanent lifetime so it&#39;s ok to use here.</span>
<span class="p_add">+		/* Spread the IRQ affinity hints across online CPUs. Note that</span>
<span class="p_add">+		 * get_cpu_mask returns a mask with a permanent lifetime so</span>
<span class="p_add">+		 * it&#39;s safe to use as a hint for irq_set_affinity_hint.</span>
 		 */
<span class="p_del">-		irq_set_affinity_hint(irq_num, get_cpu_mask(q_vector-&gt;v_idx));</span>
<span class="p_add">+		cpu = cpumask_local_spread(q_vector-&gt;v_idx, -1);</span>
<span class="p_add">+		irq_set_affinity_hint(irq_num, get_cpu_mask(cpu));</span>
 	}
 
 	return 0;
<span class="p_header">diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c</span>
<span class="p_header">index b0031c5ff767..667dbc7d4a4e 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/igb/igb_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/igb/igb_main.c</span>
<span class="p_chunk">@@ -3162,6 +3162,8 @@</span> <span class="p_context"> static int igb_sw_init(struct igb_adapter *adapter)</span>
 	/* Setup and initialize a copy of the hw vlan table array */
 	adapter-&gt;shadow_vfta = kcalloc(E1000_VLAN_FILTER_TBL_SIZE, sizeof(u32),
 				       GFP_ATOMIC);
<span class="p_add">+	if (!adapter-&gt;shadow_vfta)</span>
<span class="p_add">+		return -ENOMEM;</span>
 
 	/* This call may decrease the number of queues */
 	if (igb_init_interrupt_scheme(adapter, true)) {
<span class="p_header">diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_common.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_common.c</span>
<span class="p_header">index 6e6ab6f6875e..64429a14c630 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_common.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_common.c</span>
<span class="p_chunk">@@ -3781,10 +3781,10 @@</span> <span class="p_context"> s32 ixgbe_set_fw_drv_ver_generic(struct ixgbe_hw *hw, u8 maj, u8 min,</span>
 	fw_cmd.ver_build = build;
 	fw_cmd.ver_sub = sub;
 	fw_cmd.hdr.checksum = 0;
<span class="p_del">-	fw_cmd.hdr.checksum = ixgbe_calculate_checksum((u8 *)&amp;fw_cmd,</span>
<span class="p_del">-				(FW_CEM_HDR_LEN + fw_cmd.hdr.buf_len));</span>
 	fw_cmd.pad = 0;
 	fw_cmd.pad2 = 0;
<span class="p_add">+	fw_cmd.hdr.checksum = ixgbe_calculate_checksum((u8 *)&amp;fw_cmd,</span>
<span class="p_add">+				(FW_CEM_HDR_LEN + fw_cmd.hdr.buf_len));</span>
 
 	for (i = 0; i &lt;= FW_CEM_MAX_RETRIES; i++) {
 		ret_val = ixgbe_host_interface_command(hw, &amp;fw_cmd,
<span class="p_header">diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_x550.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_x550.c</span>
<span class="p_header">index 19fbb2f28ea4..8a85217845ae 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_x550.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_x550.c</span>
<span class="p_chunk">@@ -900,6 +900,8 @@</span> <span class="p_context"> static s32 ixgbe_read_ee_hostif_buffer_X550(struct ixgbe_hw *hw,</span>
 		/* convert offset from words to bytes */
 		buffer.address = cpu_to_be32((offset + current_word) * 2);
 		buffer.length = cpu_to_be16(words_to_read * 2);
<span class="p_add">+		buffer.pad2 = 0;</span>
<span class="p_add">+		buffer.pad3 = 0;</span>
 
 		status = ixgbe_hic_unlocked(hw, (u32 *)&amp;buffer, sizeof(buffer),
 					    IXGBE_HI_COMMAND_TIMEOUT);
<span class="p_header">diff --git a/drivers/net/phy/at803x.c b/drivers/net/phy/at803x.c</span>
<span class="p_header">index c1e52b9dc58d..5f93e6add563 100644</span>
<span class="p_header">--- a/drivers/net/phy/at803x.c</span>
<span class="p_header">+++ b/drivers/net/phy/at803x.c</span>
<span class="p_chunk">@@ -167,7 +167,7 @@</span> <span class="p_context"> static int at803x_set_wol(struct phy_device *phydev,</span>
 		mac = (const u8 *) ndev-&gt;dev_addr;
 
 		if (!is_valid_ether_addr(mac))
<span class="p_del">-			return -EFAULT;</span>
<span class="p_add">+			return -EINVAL;</span>
 
 		for (i = 0; i &lt; 3; i++) {
 			phy_write(phydev, AT803X_MMD_ACCESS_CONTROL,
<span class="p_header">diff --git a/drivers/pci/iov.c b/drivers/pci/iov.c</span>
<span class="p_header">index ac41c8be9200..0fd8e164339c 100644</span>
<span class="p_header">--- a/drivers/pci/iov.c</span>
<span class="p_header">+++ b/drivers/pci/iov.c</span>
<span class="p_chunk">@@ -162,7 +162,6 @@</span> <span class="p_context"> int pci_iov_add_virtfn(struct pci_dev *dev, int id, int reset)</span>
 
 	pci_device_add(virtfn, virtfn-&gt;bus);
 
<span class="p_del">-	pci_bus_add_device(virtfn);</span>
 	sprintf(buf, &quot;virtfn%u&quot;, id);
 	rc = sysfs_create_link(&amp;dev-&gt;dev.kobj, &amp;virtfn-&gt;dev.kobj, buf);
 	if (rc)
<span class="p_chunk">@@ -173,6 +172,8 @@</span> <span class="p_context"> int pci_iov_add_virtfn(struct pci_dev *dev, int id, int reset)</span>
 
 	kobject_uevent(&amp;virtfn-&gt;dev.kobj, KOBJ_CHANGE);
 
<span class="p_add">+	pci_bus_add_device(virtfn);</span>
<span class="p_add">+</span>
 	return 0;
 
 failed2:
<span class="p_header">diff --git a/drivers/pci/pci.c b/drivers/pci/pci.c</span>
<span class="p_header">index 6078dfc11b11..74f1c57ab93b 100644</span>
<span class="p_header">--- a/drivers/pci/pci.c</span>
<span class="p_header">+++ b/drivers/pci/pci.c</span>
<span class="p_chunk">@@ -4356,6 +4356,10 @@</span> <span class="p_context"> static bool pci_bus_resetable(struct pci_bus *bus)</span>
 {
 	struct pci_dev *dev;
 
<span class="p_add">+</span>
<span class="p_add">+	if (bus-&gt;self &amp;&amp; (bus-&gt;self-&gt;dev_flags &amp; PCI_DEV_FLAGS_NO_BUS_RESET))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list) {
 		if (dev-&gt;dev_flags &amp; PCI_DEV_FLAGS_NO_BUS_RESET ||
 		    (dev-&gt;subordinate &amp;&amp; !pci_bus_resetable(dev-&gt;subordinate)))
<span class="p_header">diff --git a/drivers/pci/pcie/aer/aerdrv_core.c b/drivers/pci/pcie/aer/aerdrv_core.c</span>
<span class="p_header">index 890efcc574cb..744805232155 100644</span>
<span class="p_header">--- a/drivers/pci/pcie/aer/aerdrv_core.c</span>
<span class="p_header">+++ b/drivers/pci/pcie/aer/aerdrv_core.c</span>
<span class="p_chunk">@@ -390,7 +390,14 @@</span> <span class="p_context"> static pci_ers_result_t broadcast_error_message(struct pci_dev *dev,</span>
 		 * If the error is reported by an end point, we think this
 		 * error is related to the upstream link of the end point.
 		 */
<span class="p_del">-		pci_walk_bus(dev-&gt;bus, cb, &amp;result_data);</span>
<span class="p_add">+		if (state == pci_channel_io_normal)</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * the error is non fatal so the bus is ok, just invoke</span>
<span class="p_add">+			 * the callback for the function that logged the error.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			cb(dev, &amp;result_data);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			pci_walk_bus(dev-&gt;bus, cb, &amp;result_data);</span>
 	}
 
 	return result_data.result;
<span class="p_header">diff --git a/drivers/platform/x86/asus-wireless.c b/drivers/platform/x86/asus-wireless.c</span>
<span class="p_header">index f3796164329e..d4aeac3477f5 100644</span>
<span class="p_header">--- a/drivers/platform/x86/asus-wireless.c</span>
<span class="p_header">+++ b/drivers/platform/x86/asus-wireless.c</span>
<span class="p_chunk">@@ -118,6 +118,7 @@</span> <span class="p_context"> static void asus_wireless_notify(struct acpi_device *adev, u32 event)</span>
 		return;
 	}
 	input_report_key(data-&gt;idev, KEY_RFKILL, 1);
<span class="p_add">+	input_sync(data-&gt;idev);</span>
 	input_report_key(data-&gt;idev, KEY_RFKILL, 0);
 	input_sync(data-&gt;idev);
 }
<span class="p_header">diff --git a/drivers/rtc/interface.c b/drivers/rtc/interface.c</span>
<span class="p_header">index 8cec9a02c0b8..9eb32ead63db 100644</span>
<span class="p_header">--- a/drivers/rtc/interface.c</span>
<span class="p_header">+++ b/drivers/rtc/interface.c</span>
<span class="p_chunk">@@ -779,7 +779,7 @@</span> <span class="p_context"> static int rtc_timer_enqueue(struct rtc_device *rtc, struct rtc_timer *timer)</span>
 	}
 
 	timerqueue_add(&amp;rtc-&gt;timerqueue, &amp;timer-&gt;node);
<span class="p_del">-	if (!next) {</span>
<span class="p_add">+	if (!next || ktime_before(timer-&gt;node.expires, next-&gt;expires)) {</span>
 		struct rtc_wkalrm alarm;
 		int err;
 		alarm.time = rtc_ktime_to_tm(timer-&gt;node.expires);
<span class="p_header">diff --git a/drivers/rtc/rtc-pl031.c b/drivers/rtc/rtc-pl031.c</span>
<span class="p_header">index e1687e19c59f..a30f24cb6c83 100644</span>
<span class="p_header">--- a/drivers/rtc/rtc-pl031.c</span>
<span class="p_header">+++ b/drivers/rtc/rtc-pl031.c</span>
<span class="p_chunk">@@ -308,7 +308,8 @@</span> <span class="p_context"> static int pl031_remove(struct amba_device *adev)</span>
 
 	dev_pm_clear_wake_irq(&amp;adev-&gt;dev);
 	device_init_wakeup(&amp;adev-&gt;dev, false);
<span class="p_del">-	free_irq(adev-&gt;irq[0], ldata);</span>
<span class="p_add">+	if (adev-&gt;irq[0])</span>
<span class="p_add">+		free_irq(adev-&gt;irq[0], ldata);</span>
 	rtc_device_unregister(ldata-&gt;rtc);
 	iounmap(ldata-&gt;base);
 	kfree(ldata);
<span class="p_chunk">@@ -381,12 +382,13 @@</span> <span class="p_context"> static int pl031_probe(struct amba_device *adev, const struct amba_id *id)</span>
 		goto out_no_rtc;
 	}
 
<span class="p_del">-	if (request_irq(adev-&gt;irq[0], pl031_interrupt,</span>
<span class="p_del">-			vendor-&gt;irqflags, &quot;rtc-pl031&quot;, ldata)) {</span>
<span class="p_del">-		ret = -EIO;</span>
<span class="p_del">-		goto out_no_irq;</span>
<span class="p_add">+	if (adev-&gt;irq[0]) {</span>
<span class="p_add">+		ret = request_irq(adev-&gt;irq[0], pl031_interrupt,</span>
<span class="p_add">+				  vendor-&gt;irqflags, &quot;rtc-pl031&quot;, ldata);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto out_no_irq;</span>
<span class="p_add">+		dev_pm_set_wake_irq(&amp;adev-&gt;dev, adev-&gt;irq[0]);</span>
 	}
<span class="p_del">-	dev_pm_set_wake_irq(&amp;adev-&gt;dev, adev-&gt;irq[0]);</span>
 	return 0;
 
 out_no_irq:
<span class="p_header">diff --git a/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c b/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c</span>
<span class="p_header">index 1d02cf9fe06c..30d5f0ef29bb 100644</span>
<span class="p_header">--- a/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c</span>
<span class="p_header">+++ b/drivers/scsi/cxgbi/cxgb4i/cxgb4i.c</span>
<span class="p_chunk">@@ -1575,6 +1575,7 @@</span> <span class="p_context"> static void release_offload_resources(struct cxgbi_sock *csk)</span>
 		csk, csk-&gt;state, csk-&gt;flags, csk-&gt;tid);
 
 	cxgbi_sock_free_cpl_skbs(csk);
<span class="p_add">+	cxgbi_sock_purge_write_queue(csk);</span>
 	if (csk-&gt;wr_cred != csk-&gt;wr_max_cred) {
 		cxgbi_sock_purge_wr_queue(csk);
 		cxgbi_sock_reset_wr_list(csk);
<span class="p_header">diff --git a/drivers/scsi/lpfc/lpfc_hbadisc.c b/drivers/scsi/lpfc/lpfc_hbadisc.c</span>
<span class="p_header">index 499df9d17339..d9a03beb76a4 100644</span>
<span class="p_header">--- a/drivers/scsi/lpfc/lpfc_hbadisc.c</span>
<span class="p_header">+++ b/drivers/scsi/lpfc/lpfc_hbadisc.c</span>
<span class="p_chunk">@@ -4983,7 +4983,8 @@</span> <span class="p_context"> lpfc_nlp_remove(struct lpfc_vport *vport, struct lpfc_nodelist *ndlp)</span>
 	lpfc_cancel_retry_delay_tmo(vport, ndlp);
 	if ((ndlp-&gt;nlp_flag &amp; NLP_DEFER_RM) &amp;&amp;
 	    !(ndlp-&gt;nlp_flag &amp; NLP_REG_LOGIN_SEND) &amp;&amp;
<span class="p_del">-	    !(ndlp-&gt;nlp_flag &amp; NLP_RPI_REGISTERED)) {</span>
<span class="p_add">+	    !(ndlp-&gt;nlp_flag &amp; NLP_RPI_REGISTERED) &amp;&amp;</span>
<span class="p_add">+	    phba-&gt;sli_rev != LPFC_SLI_REV4) {</span>
 		/* For this case we need to cleanup the default rpi
 		 * allocated by the firmware.
 		 */
<span class="p_header">diff --git a/drivers/scsi/lpfc/lpfc_hw4.h b/drivers/scsi/lpfc/lpfc_hw4.h</span>
<span class="p_header">index 1db0a38683f4..2b145966c73f 100644</span>
<span class="p_header">--- a/drivers/scsi/lpfc/lpfc_hw4.h</span>
<span class="p_header">+++ b/drivers/scsi/lpfc/lpfc_hw4.h</span>
<span class="p_chunk">@@ -3636,7 +3636,7 @@</span> <span class="p_context"> struct lpfc_mbx_get_port_name {</span>
 #define MB_CEQ_STATUS_QUEUE_FLUSHING		0x4
 #define MB_CQE_STATUS_DMA_FAILED		0x5
 
<span class="p_del">-#define LPFC_MBX_WR_CONFIG_MAX_BDE		8</span>
<span class="p_add">+#define LPFC_MBX_WR_CONFIG_MAX_BDE		1</span>
 struct lpfc_mbx_wr_object {
 	struct mbox_header header;
 	union {
<span class="p_header">diff --git a/drivers/scsi/lpfc/lpfc_nvmet.c b/drivers/scsi/lpfc/lpfc_nvmet.c</span>
<span class="p_header">index 3c5b054a56ac..7ac1a067d780 100644</span>
<span class="p_header">--- a/drivers/scsi/lpfc/lpfc_nvmet.c</span>
<span class="p_header">+++ b/drivers/scsi/lpfc/lpfc_nvmet.c</span>
<span class="p_chunk">@@ -1464,6 +1464,7 @@</span> <span class="p_context"> static struct lpfc_nvmet_ctxbuf *</span>
 lpfc_nvmet_replenish_context(struct lpfc_hba *phba,
 			     struct lpfc_nvmet_ctx_info *current_infop)
 {
<span class="p_add">+#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))</span>
 	struct lpfc_nvmet_ctxbuf *ctx_buf = NULL;
 	struct lpfc_nvmet_ctx_info *get_infop;
 	int i;
<span class="p_chunk">@@ -1511,6 +1512,7 @@</span> <span class="p_context"> lpfc_nvmet_replenish_context(struct lpfc_hba *phba,</span>
 		get_infop = get_infop-&gt;nvmet_ctx_next_cpu;
 	}
 
<span class="p_add">+#endif</span>
 	/* Nothing found, all contexts for the MRQ are in-flight */
 	return NULL;
 }
<span class="p_header">diff --git a/drivers/scsi/mpt3sas/mpt3sas_scsih.c b/drivers/scsi/mpt3sas/mpt3sas_scsih.c</span>
<span class="p_header">index 22998cbd538f..33ff691878e2 100644</span>
<span class="p_header">--- a/drivers/scsi/mpt3sas/mpt3sas_scsih.c</span>
<span class="p_header">+++ b/drivers/scsi/mpt3sas/mpt3sas_scsih.c</span>
<span class="p_chunk">@@ -4804,6 +4804,11 @@</span> <span class="p_context"> _scsih_io_done(struct MPT3SAS_ADAPTER *ioc, u16 smid, u8 msix_index, u32 reply)</span>
 		} else if (log_info == VIRTUAL_IO_FAILED_RETRY) {
 			scmd-&gt;result = DID_RESET &lt;&lt; 16;
 			break;
<span class="p_add">+		} else if ((scmd-&gt;device-&gt;channel == RAID_CHANNEL) &amp;&amp;</span>
<span class="p_add">+		   (scsi_state == (MPI2_SCSI_STATE_TERMINATED |</span>
<span class="p_add">+		   MPI2_SCSI_STATE_NO_SCSI_STATUS))) {</span>
<span class="p_add">+			scmd-&gt;result = DID_RESET &lt;&lt; 16;</span>
<span class="p_add">+			break;</span>
 		}
 		scmd-&gt;result = DID_SOFT_ERROR &lt;&lt; 16;
 		break;
<span class="p_header">diff --git a/drivers/staging/greybus/light.c b/drivers/staging/greybus/light.c</span>
<span class="p_header">index 3f4148c92308..0f538b8c3a07 100644</span>
<span class="p_header">--- a/drivers/staging/greybus/light.c</span>
<span class="p_header">+++ b/drivers/staging/greybus/light.c</span>
<span class="p_chunk">@@ -925,6 +925,8 @@</span> <span class="p_context"> static void __gb_lights_led_unregister(struct gb_channel *channel)</span>
 		return;
 
 	led_classdev_unregister(cdev);
<span class="p_add">+	kfree(cdev-&gt;name);</span>
<span class="p_add">+	cdev-&gt;name = NULL;</span>
 	channel-&gt;led = NULL;
 }
 
<span class="p_header">diff --git a/drivers/tee/optee/core.c b/drivers/tee/optee/core.c</span>
<span class="p_header">index 7952357df9c8..edb6e4e9ef3a 100644</span>
<span class="p_header">--- a/drivers/tee/optee/core.c</span>
<span class="p_header">+++ b/drivers/tee/optee/core.c</span>
<span class="p_chunk">@@ -590,7 +590,6 @@</span> <span class="p_context"> static int __init optee_driver_init(void)</span>
 		return -ENODEV;
 
 	np = of_find_matching_node(fw_np, optee_match);
<span class="p_del">-	of_node_put(fw_np);</span>
 	if (!np)
 		return -ENODEV;
 
<span class="p_header">diff --git a/drivers/thermal/hisi_thermal.c b/drivers/thermal/hisi_thermal.c</span>
<span class="p_header">index bd3572c41585..6d8906d65476 100644</span>
<span class="p_header">--- a/drivers/thermal/hisi_thermal.c</span>
<span class="p_header">+++ b/drivers/thermal/hisi_thermal.c</span>
<span class="p_chunk">@@ -35,8 +35,9 @@</span> <span class="p_context"></span>
 #define TEMP0_RST_MSK			(0x1C)
 #define TEMP0_VALUE			(0x28)
 
<span class="p_del">-#define HISI_TEMP_BASE			(-60)</span>
<span class="p_add">+#define HISI_TEMP_BASE			(-60000)</span>
 #define HISI_TEMP_RESET			(100000)
<span class="p_add">+#define HISI_TEMP_STEP			(784)</span>
 
 #define HISI_MAX_SENSORS		4
 
<span class="p_chunk">@@ -61,19 +62,38 @@</span> <span class="p_context"> struct hisi_thermal_data {</span>
 	void __iomem *regs;
 };
 
<span class="p_del">-/* in millicelsius */</span>
<span class="p_del">-static inline int _step_to_temp(int step)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The temperature computation on the tsensor is as follow:</span>
<span class="p_add">+ *	Unit: millidegree Celsius</span>
<span class="p_add">+ *	Step: 255/200 (0.7843)</span>
<span class="p_add">+ *	Temperature base: -60C</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The register is programmed in temperature steps, every step is 784</span>
<span class="p_add">+ * millidegree and begins at -60 000 mC</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The temperature from the steps:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	Temp = TempBase + (steps x 784)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * and the steps from the temperature:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	steps = (Temp - TempBase) / 784</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int hisi_thermal_step_to_temp(int step)</span>
 {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Every step equals (1 * 200) / 255 celsius, and finally</span>
<span class="p_del">-	 * need convert to millicelsius.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	return (HISI_TEMP_BASE * 1000 + (step * 200000 / 255));</span>
<span class="p_add">+	return HISI_TEMP_BASE + (step * HISI_TEMP_STEP);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long hisi_thermal_temp_to_step(long temp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (temp - HISI_TEMP_BASE) / HISI_TEMP_STEP;</span>
 }
 
<span class="p_del">-static inline long _temp_to_step(long temp)</span>
<span class="p_add">+static inline long hisi_thermal_round_temp(int temp)</span>
 {
<span class="p_del">-	return ((temp - HISI_TEMP_BASE * 1000) * 255) / 200000;</span>
<span class="p_add">+	return hisi_thermal_step_to_temp(</span>
<span class="p_add">+		hisi_thermal_temp_to_step(temp));</span>
 }
 
 static long hisi_thermal_get_sensor_temp(struct hisi_thermal_data *data,
<span class="p_chunk">@@ -99,7 +119,7 @@</span> <span class="p_context"> static long hisi_thermal_get_sensor_temp(struct hisi_thermal_data *data,</span>
 	usleep_range(3000, 5000);
 
 	val = readl(data-&gt;regs + TEMP0_VALUE);
<span class="p_del">-	val = _step_to_temp(val);</span>
<span class="p_add">+	val = hisi_thermal_step_to_temp(val);</span>
 
 	mutex_unlock(&amp;data-&gt;thermal_lock);
 
<span class="p_chunk">@@ -126,10 +146,11 @@</span> <span class="p_context"> static void hisi_thermal_enable_bind_irq_sensor</span>
 	writel((sensor-&gt;id &lt;&lt; 12), data-&gt;regs + TEMP0_CFG);
 
 	/* enable for interrupt */
<span class="p_del">-	writel(_temp_to_step(sensor-&gt;thres_temp) | 0x0FFFFFF00,</span>
<span class="p_add">+	writel(hisi_thermal_temp_to_step(sensor-&gt;thres_temp) | 0x0FFFFFF00,</span>
 	       data-&gt;regs + TEMP0_TH);
 
<span class="p_del">-	writel(_temp_to_step(HISI_TEMP_RESET), data-&gt;regs + TEMP0_RST_TH);</span>
<span class="p_add">+	writel(hisi_thermal_temp_to_step(HISI_TEMP_RESET),</span>
<span class="p_add">+	       data-&gt;regs + TEMP0_RST_TH);</span>
 
 	/* enable module */
 	writel(0x1, data-&gt;regs + TEMP0_RST_MSK);
<span class="p_chunk">@@ -230,7 +251,7 @@</span> <span class="p_context"> static irqreturn_t hisi_thermal_alarm_irq_thread(int irq, void *dev)</span>
 	sensor = &amp;data-&gt;sensors[data-&gt;irq_bind_sensor];
 
 	dev_crit(&amp;data-&gt;pdev-&gt;dev, &quot;THERMAL ALARM: T &gt; %d\n&quot;,
<span class="p_del">-		 sensor-&gt;thres_temp / 1000);</span>
<span class="p_add">+		 sensor-&gt;thres_temp);</span>
 	mutex_unlock(&amp;data-&gt;thermal_lock);
 
 	for (i = 0; i &lt; HISI_MAX_SENSORS; i++) {
<span class="p_chunk">@@ -269,7 +290,7 @@</span> <span class="p_context"> static int hisi_thermal_register_sensor(struct platform_device *pdev,</span>
 
 	for (i = 0; i &lt; of_thermal_get_ntrips(sensor-&gt;tzd); i++) {
 		if (trip[i].type == THERMAL_TRIP_PASSIVE) {
<span class="p_del">-			sensor-&gt;thres_temp = trip[i].temperature;</span>
<span class="p_add">+			sensor-&gt;thres_temp = hisi_thermal_round_temp(trip[i].temperature);</span>
 			break;
 		}
 	}
<span class="p_chunk">@@ -317,15 +338,6 @@</span> <span class="p_context"> static int hisi_thermal_probe(struct platform_device *pdev)</span>
 	if (data-&gt;irq &lt; 0)
 		return data-&gt;irq;
 
<span class="p_del">-	ret = devm_request_threaded_irq(&amp;pdev-&gt;dev, data-&gt;irq,</span>
<span class="p_del">-					hisi_thermal_alarm_irq,</span>
<span class="p_del">-					hisi_thermal_alarm_irq_thread,</span>
<span class="p_del">-					0, &quot;hisi_thermal&quot;, data);</span>
<span class="p_del">-	if (ret &lt; 0) {</span>
<span class="p_del">-		dev_err(&amp;pdev-&gt;dev, &quot;failed to request alarm irq: %d\n&quot;, ret);</span>
<span class="p_del">-		return ret;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	platform_set_drvdata(pdev, data);
 
 	data-&gt;clk = devm_clk_get(&amp;pdev-&gt;dev, &quot;thermal_clk&quot;);
<span class="p_chunk">@@ -345,8 +357,7 @@</span> <span class="p_context"> static int hisi_thermal_probe(struct platform_device *pdev)</span>
 	}
 
 	hisi_thermal_enable_bind_irq_sensor(data);
<span class="p_del">-	irq_get_irqchip_state(data-&gt;irq, IRQCHIP_STATE_MASKED,</span>
<span class="p_del">-			      &amp;data-&gt;irq_enabled);</span>
<span class="p_add">+	data-&gt;irq_enabled = true;</span>
 
 	for (i = 0; i &lt; HISI_MAX_SENSORS; ++i) {
 		ret = hisi_thermal_register_sensor(pdev, data,
<span class="p_chunk">@@ -358,6 +369,17 @@</span> <span class="p_context"> static int hisi_thermal_probe(struct platform_device *pdev)</span>
 			hisi_thermal_toggle_sensor(&amp;data-&gt;sensors[i], true);
 	}
 
<span class="p_add">+	ret = devm_request_threaded_irq(&amp;pdev-&gt;dev, data-&gt;irq,</span>
<span class="p_add">+					hisi_thermal_alarm_irq,</span>
<span class="p_add">+					hisi_thermal_alarm_irq_thread,</span>
<span class="p_add">+					0, &quot;hisi_thermal&quot;, data);</span>
<span class="p_add">+	if (ret &lt; 0) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev, &quot;failed to request alarm irq: %d\n&quot;, ret);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	enable_irq(data-&gt;irq);</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/drivers/vfio/pci/vfio_pci_config.c b/drivers/vfio/pci/vfio_pci_config.c</span>
<span class="p_header">index 5628fe114347..91335e6de88a 100644</span>
<span class="p_header">--- a/drivers/vfio/pci/vfio_pci_config.c</span>
<span class="p_header">+++ b/drivers/vfio/pci/vfio_pci_config.c</span>
<span class="p_chunk">@@ -849,11 +849,13 @@</span> <span class="p_context"> static int __init init_pci_cap_exp_perm(struct perm_bits *perm)</span>
 
 	/*
 	 * Allow writes to device control fields, except devctl_phantom,
<span class="p_del">-	 * which could confuse IOMMU, and the ARI bit in devctl2, which</span>
<span class="p_add">+	 * which could confuse IOMMU, MPS, which can break communication</span>
<span class="p_add">+	 * with other physical devices, and the ARI bit in devctl2, which</span>
 	 * is set at probe time.  FLR gets virtualized via our writefn.
 	 */
 	p_setw(perm, PCI_EXP_DEVCTL,
<span class="p_del">-	       PCI_EXP_DEVCTL_BCR_FLR, ~PCI_EXP_DEVCTL_PHANTOM);</span>
<span class="p_add">+	       PCI_EXP_DEVCTL_BCR_FLR | PCI_EXP_DEVCTL_PAYLOAD,</span>
<span class="p_add">+	       ~PCI_EXP_DEVCTL_PHANTOM);</span>
 	p_setw(perm, PCI_EXP_DEVCTL2, NO_VIRT, ~PCI_EXP_DEVCTL2_ARI);
 	return 0;
 }
<span class="p_header">diff --git a/drivers/video/backlight/pwm_bl.c b/drivers/video/backlight/pwm_bl.c</span>
<span class="p_header">index 9bd17682655a..1c2289ddd555 100644</span>
<span class="p_header">--- a/drivers/video/backlight/pwm_bl.c</span>
<span class="p_header">+++ b/drivers/video/backlight/pwm_bl.c</span>
<span class="p_chunk">@@ -79,14 +79,17 @@</span> <span class="p_context"> static void pwm_backlight_power_off(struct pwm_bl_data *pb)</span>
 static int compute_duty_cycle(struct pwm_bl_data *pb, int brightness)
 {
 	unsigned int lth = pb-&gt;lth_brightness;
<span class="p_del">-	int duty_cycle;</span>
<span class="p_add">+	u64 duty_cycle;</span>
 
 	if (pb-&gt;levels)
 		duty_cycle = pb-&gt;levels[brightness];
 	else
 		duty_cycle = brightness;
 
<span class="p_del">-	return (duty_cycle * (pb-&gt;period - lth) / pb-&gt;scale) + lth;</span>
<span class="p_add">+	duty_cycle *= pb-&gt;period - lth;</span>
<span class="p_add">+	do_div(duty_cycle, pb-&gt;scale);</span>
<span class="p_add">+</span>
<span class="p_add">+	return duty_cycle + lth;</span>
 }
 
 static int pwm_backlight_update_status(struct backlight_device *bl)
<span class="p_header">diff --git a/fs/dcache.c b/fs/dcache.c</span>
<span class="p_header">index f90141387f01..34c852af215c 100644</span>
<span class="p_header">--- a/fs/dcache.c</span>
<span class="p_header">+++ b/fs/dcache.c</span>
<span class="p_chunk">@@ -231,7 +231,7 @@</span> <span class="p_context"> static inline int dentry_cmp(const struct dentry *dentry, const unsigned char *c</span>
 {
 	/*
 	 * Be careful about RCU walk racing with rename:
<span class="p_del">-	 * use &#39;lockless_dereference&#39; to fetch the name pointer.</span>
<span class="p_add">+	 * use &#39;READ_ONCE&#39; to fetch the name pointer.</span>
 	 *
 	 * NOTE! Even if a rename will mean that the length
 	 * was not loaded atomically, we don&#39;t care. The
<span class="p_chunk">@@ -245,7 +245,7 @@</span> <span class="p_context"> static inline int dentry_cmp(const struct dentry *dentry, const unsigned char *c</span>
 	 * early because the data cannot match (there can
 	 * be no NUL in the ct/tcount data)
 	 */
<span class="p_del">-	const unsigned char *cs = lockless_dereference(dentry-&gt;d_name.name);</span>
<span class="p_add">+	const unsigned char *cs = READ_ONCE(dentry-&gt;d_name.name);</span>
 
 	return dentry_string_cmp(cs, ct, tcount);
 }
<span class="p_header">diff --git a/fs/overlayfs/ovl_entry.h b/fs/overlayfs/ovl_entry.h</span>
<span class="p_header">index 25d9b5adcd42..36b49bd09264 100644</span>
<span class="p_header">--- a/fs/overlayfs/ovl_entry.h</span>
<span class="p_header">+++ b/fs/overlayfs/ovl_entry.h</span>
<span class="p_chunk">@@ -77,5 +77,5 @@</span> <span class="p_context"> static inline struct ovl_inode *OVL_I(struct inode *inode)</span>
 
 static inline struct dentry *ovl_upperdentry_dereference(struct ovl_inode *oi)
 {
<span class="p_del">-	return lockless_dereference(oi-&gt;__upperdentry);</span>
<span class="p_add">+	return READ_ONCE(oi-&gt;__upperdentry);</span>
 }
<span class="p_header">diff --git a/fs/overlayfs/readdir.c b/fs/overlayfs/readdir.c</span>
<span class="p_header">index b2c7f33e08fc..d94a51dc4e32 100644</span>
<span class="p_header">--- a/fs/overlayfs/readdir.c</span>
<span class="p_header">+++ b/fs/overlayfs/readdir.c</span>
<span class="p_chunk">@@ -757,7 +757,7 @@</span> <span class="p_context"> static int ovl_dir_fsync(struct file *file, loff_t start, loff_t end,</span>
 	if (!od-&gt;is_upper &amp;&amp; OVL_TYPE_UPPER(ovl_path_type(dentry))) {
 		struct inode *inode = file_inode(file);
 
<span class="p_del">-		realfile = lockless_dereference(od-&gt;upperfile);</span>
<span class="p_add">+		realfile = READ_ONCE(od-&gt;upperfile);</span>
 		if (!realfile) {
 			struct path upperpath;
 
<span class="p_header">diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">index e549bff87c5b..353f52fdc35e 100644</span>
<span class="p_header">--- a/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">+++ b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_chunk">@@ -688,7 +688,7 @@</span> <span class="p_context"></span>
 #define BUG_TABLE
 #endif
 
<span class="p_del">-#ifdef CONFIG_ORC_UNWINDER</span>
<span class="p_add">+#ifdef CONFIG_UNWINDER_ORC</span>
 #define ORC_UNWIND_TABLE						\
 	. = ALIGN(4);							\
 	.orc_unwind_ip : AT(ADDR(.orc_unwind_ip) - LOAD_OFFSET) {	\
<span class="p_header">diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h</span>
<span class="p_header">index b8d200f60a40..73bec75b74c8 100644</span>
<span class="p_header">--- a/include/linux/bpf_verifier.h</span>
<span class="p_header">+++ b/include/linux/bpf_verifier.h</span>
<span class="p_chunk">@@ -15,11 +15,11 @@</span> <span class="p_context"></span>
  * In practice this is far bigger than any realistic pointer offset; this limit
  * ensures that umax_value + (int)off + (int)size cannot overflow a u64.
  */
<span class="p_del">-#define BPF_MAX_VAR_OFF	(1ULL &lt;&lt; 31)</span>
<span class="p_add">+#define BPF_MAX_VAR_OFF	(1 &lt;&lt; 29)</span>
 /* Maximum variable size permitted for ARG_CONST_SIZE[_OR_ZERO].  This ensures
  * that converting umax_value to int cannot overflow.
  */
<span class="p_del">-#define BPF_MAX_VAR_SIZ	INT_MAX</span>
<span class="p_add">+#define BPF_MAX_VAR_SIZ	(1 &lt;&lt; 29)</span>
 
 /* Liveness marks, used for registers and spilled-regs (in stack slots).
  * Read marks propagate upwards until they find a write mark; they record that
<span class="p_chunk">@@ -110,7 +110,7 @@</span> <span class="p_context"> struct bpf_insn_aux_data {</span>
 		struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
 	};
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
<span class="p_del">-	int converted_op_size; /* the valid value width after perceived conversion */</span>
<span class="p_add">+	bool seen; /* this insn was processed by the verifier */</span>
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
<span class="p_header">diff --git a/include/linux/compiler-clang.h b/include/linux/compiler-clang.h</span>
<span class="p_header">index 780b1242bf24..3b609edffa8f 100644</span>
<span class="p_header">--- a/include/linux/compiler-clang.h</span>
<span class="p_header">+++ b/include/linux/compiler-clang.h</span>
<span class="p_chunk">@@ -1,5 +1,5 @@</span> <span class="p_context"></span>
 /* SPDX-License-Identifier: GPL-2.0 */
<span class="p_del">-#ifndef __LINUX_COMPILER_H</span>
<span class="p_add">+#ifndef __LINUX_COMPILER_TYPES_H</span>
 #error &quot;Please don&#39;t include &lt;linux/compiler-clang.h&gt; directly, include &lt;linux/compiler.h&gt; instead.&quot;
 #endif
 
<span class="p_header">diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h</span>
<span class="p_header">index bb78e5bdff26..2272ded07496 100644</span>
<span class="p_header">--- a/include/linux/compiler-gcc.h</span>
<span class="p_header">+++ b/include/linux/compiler-gcc.h</span>
<span class="p_chunk">@@ -1,5 +1,5 @@</span> <span class="p_context"></span>
 /* SPDX-License-Identifier: GPL-2.0 */
<span class="p_del">-#ifndef __LINUX_COMPILER_H</span>
<span class="p_add">+#ifndef __LINUX_COMPILER_TYPES_H</span>
 #error &quot;Please don&#39;t include &lt;linux/compiler-gcc.h&gt; directly, include &lt;linux/compiler.h&gt; instead.&quot;
 #endif
 
<span class="p_header">diff --git a/include/linux/compiler-intel.h b/include/linux/compiler-intel.h</span>
<span class="p_header">index 523d1b74550f..bfa08160db3a 100644</span>
<span class="p_header">--- a/include/linux/compiler-intel.h</span>
<span class="p_header">+++ b/include/linux/compiler-intel.h</span>
<span class="p_chunk">@@ -1,5 +1,5 @@</span> <span class="p_context"></span>
 /* SPDX-License-Identifier: GPL-2.0 */
<span class="p_del">-#ifndef __LINUX_COMPILER_H</span>
<span class="p_add">+#ifndef __LINUX_COMPILER_TYPES_H</span>
 #error &quot;Please don&#39;t include &lt;linux/compiler-intel.h&gt; directly, include &lt;linux/compiler.h&gt; instead.&quot;
 #endif
 
<span class="p_header">diff --git a/include/linux/compiler.h b/include/linux/compiler.h</span>
<span class="p_header">index 202710420d6d..fab5dc250c61 100644</span>
<span class="p_header">--- a/include/linux/compiler.h</span>
<span class="p_header">+++ b/include/linux/compiler.h</span>
<span class="p_chunk">@@ -2,111 +2,12 @@</span> <span class="p_context"></span>
 #ifndef __LINUX_COMPILER_H
 #define __LINUX_COMPILER_H
 
<span class="p_del">-#ifndef __ASSEMBLY__</span>
<span class="p_add">+#include &lt;linux/compiler_types.h&gt;</span>
 
<span class="p_del">-#ifdef __CHECKER__</span>
<span class="p_del">-# define __user		__attribute__((noderef, address_space(1)))</span>
<span class="p_del">-# define __kernel	__attribute__((address_space(0)))</span>
<span class="p_del">-# define __safe		__attribute__((safe))</span>
<span class="p_del">-# define __force	__attribute__((force))</span>
<span class="p_del">-# define __nocast	__attribute__((nocast))</span>
<span class="p_del">-# define __iomem	__attribute__((noderef, address_space(2)))</span>
<span class="p_del">-# define __must_hold(x)	__attribute__((context(x,1,1)))</span>
<span class="p_del">-# define __acquires(x)	__attribute__((context(x,0,1)))</span>
<span class="p_del">-# define __releases(x)	__attribute__((context(x,1,0)))</span>
<span class="p_del">-# define __acquire(x)	__context__(x,1)</span>
<span class="p_del">-# define __release(x)	__context__(x,-1)</span>
<span class="p_del">-# define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)</span>
<span class="p_del">-# define __percpu	__attribute__((noderef, address_space(3)))</span>
<span class="p_del">-# define __rcu		__attribute__((noderef, address_space(4)))</span>
<span class="p_del">-# define __private	__attribute__((noderef))</span>
<span class="p_del">-extern void __chk_user_ptr(const volatile void __user *);</span>
<span class="p_del">-extern void __chk_io_ptr(const volatile void __iomem *);</span>
<span class="p_del">-# define ACCESS_PRIVATE(p, member) (*((typeof((p)-&gt;member) __force *) &amp;(p)-&gt;member))</span>
<span class="p_del">-#else /* __CHECKER__ */</span>
<span class="p_del">-# ifdef STRUCTLEAK_PLUGIN</span>
<span class="p_del">-#  define __user __attribute__((user))</span>
<span class="p_del">-# else</span>
<span class="p_del">-#  define __user</span>
<span class="p_del">-# endif</span>
<span class="p_del">-# define __kernel</span>
<span class="p_del">-# define __safe</span>
<span class="p_del">-# define __force</span>
<span class="p_del">-# define __nocast</span>
<span class="p_del">-# define __iomem</span>
<span class="p_del">-# define __chk_user_ptr(x) (void)0</span>
<span class="p_del">-# define __chk_io_ptr(x) (void)0</span>
<span class="p_del">-# define __builtin_warning(x, y...) (1)</span>
<span class="p_del">-# define __must_hold(x)</span>
<span class="p_del">-# define __acquires(x)</span>
<span class="p_del">-# define __releases(x)</span>
<span class="p_del">-# define __acquire(x) (void)0</span>
<span class="p_del">-# define __release(x) (void)0</span>
<span class="p_del">-# define __cond_lock(x,c) (c)</span>
<span class="p_del">-# define __percpu</span>
<span class="p_del">-# define __rcu</span>
<span class="p_del">-# define __private</span>
<span class="p_del">-# define ACCESS_PRIVATE(p, member) ((p)-&gt;member)</span>
<span class="p_del">-#endif /* __CHECKER__ */</span>
<span class="p_del">-</span>
<span class="p_del">-/* Indirect macros required for expanded argument pasting, eg. __LINE__. */</span>
<span class="p_del">-#define ___PASTE(a,b) a##b</span>
<span class="p_del">-#define __PASTE(a,b) ___PASTE(a,b)</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
 
 #ifdef __KERNEL__
 
<span class="p_del">-#ifdef __GNUC__</span>
<span class="p_del">-#include &lt;linux/compiler-gcc.h&gt;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#if defined(CC_USING_HOTPATCH) &amp;&amp; !defined(__CHECKER__)</span>
<span class="p_del">-#define notrace __attribute__((hotpatch(0,0)))</span>
<span class="p_del">-#else</span>
<span class="p_del">-#define notrace __attribute__((no_instrument_function))</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/* Intel compiler defines __GNUC__. So we will overwrite implementations</span>
<span class="p_del">- * coming from above header files here</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifdef __INTEL_COMPILER</span>
<span class="p_del">-# include &lt;linux/compiler-intel.h&gt;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/* Clang compiler defines __GNUC__. So we will overwrite implementations</span>
<span class="p_del">- * coming from above header files here</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifdef __clang__</span>
<span class="p_del">-#include &lt;linux/compiler-clang.h&gt;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Generic compiler-dependent macros required for kernel</span>
<span class="p_del">- * build go below this comment. Actual compiler/compiler version</span>
<span class="p_del">- * specific implementations come from the above header files</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-struct ftrace_branch_data {</span>
<span class="p_del">-	const char *func;</span>
<span class="p_del">-	const char *file;</span>
<span class="p_del">-	unsigned line;</span>
<span class="p_del">-	union {</span>
<span class="p_del">-		struct {</span>
<span class="p_del">-			unsigned long correct;</span>
<span class="p_del">-			unsigned long incorrect;</span>
<span class="p_del">-		};</span>
<span class="p_del">-		struct {</span>
<span class="p_del">-			unsigned long miss;</span>
<span class="p_del">-			unsigned long hit;</span>
<span class="p_del">-		};</span>
<span class="p_del">-		unsigned long miss_hit[2];</span>
<span class="p_del">-	};</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-struct ftrace_likely_data {</span>
<span class="p_del">-	struct ftrace_branch_data	data;</span>
<span class="p_del">-	unsigned long			constant;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 /*
  * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
  * to disable branch tracing on a per file basis.
<span class="p_chunk">@@ -333,6 +234,7 @@</span> <span class="p_context"> static __always_inline void __write_once_size(volatile void *p, void *res, int s</span>
  * with an explicit memory barrier or atomic instruction that provides the
  * required ordering.
  */
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
 
 #define __READ_ONCE(x, check)						\
 ({									\
<span class="p_chunk">@@ -341,6 +243,7 @@</span> <span class="p_context"> static __always_inline void __write_once_size(volatile void *p, void *res, int s</span>
 		__read_once_size(&amp;(x), __u.__c, sizeof(x));		\
 	else								\
 		__read_once_size_nocheck(&amp;(x), __u.__c, sizeof(x));	\
<span class="p_add">+	smp_read_barrier_depends(); /* Enforce dependency ordering from x */ \</span>
 	__u.__val;							\
 })
 #define READ_ONCE(x) __READ_ONCE(x, 1)
<span class="p_chunk">@@ -363,167 +266,6 @@</span> <span class="p_context"> static __always_inline void __write_once_size(volatile void *p, void *res, int s</span>
 
 #endif /* __ASSEMBLY__ */
 
<span class="p_del">-#ifdef __KERNEL__</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Allow us to mark functions as &#39;deprecated&#39; and have gcc emit a nice</span>
<span class="p_del">- * warning for each use, in hopes of speeding the functions removal.</span>
<span class="p_del">- * Usage is:</span>
<span class="p_del">- * 		int __deprecated foo(void)</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifndef __deprecated</span>
<span class="p_del">-# define __deprecated		/* unimplemented */</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef MODULE</span>
<span class="p_del">-#define __deprecated_for_modules __deprecated</span>
<span class="p_del">-#else</span>
<span class="p_del">-#define __deprecated_for_modules</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __must_check</span>
<span class="p_del">-#define __must_check</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef CONFIG_ENABLE_MUST_CHECK</span>
<span class="p_del">-#undef __must_check</span>
<span class="p_del">-#define __must_check</span>
<span class="p_del">-#endif</span>
<span class="p_del">-#ifndef CONFIG_ENABLE_WARN_DEPRECATED</span>
<span class="p_del">-#undef __deprecated</span>
<span class="p_del">-#undef __deprecated_for_modules</span>
<span class="p_del">-#define __deprecated</span>
<span class="p_del">-#define __deprecated_for_modules</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __malloc</span>
<span class="p_del">-#define __malloc</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Allow us to avoid &#39;defined but not used&#39; warnings on functions and data,</span>
<span class="p_del">- * as well as force them to be emitted to the assembly file.</span>
<span class="p_del">- *</span>
<span class="p_del">- * As of gcc 3.4, static functions that are not marked with attribute((used))</span>
<span class="p_del">- * may be elided from the assembly file.  As of gcc 3.4, static data not so</span>
<span class="p_del">- * marked will not be elided, but this may change in a future gcc version.</span>
<span class="p_del">- *</span>
<span class="p_del">- * NOTE: Because distributions shipped with a backported unit-at-a-time</span>
<span class="p_del">- * compiler in gcc 3.3, we must define __used to be __attribute__((used))</span>
<span class="p_del">- * for gcc &gt;=3.3 instead of 3.4.</span>
<span class="p_del">- *</span>
<span class="p_del">- * In prior versions of gcc, such functions and data would be emitted, but</span>
<span class="p_del">- * would be warned about except with attribute((unused)).</span>
<span class="p_del">- *</span>
<span class="p_del">- * Mark functions that are referenced only in inline assembly as __used so</span>
<span class="p_del">- * the code is emitted even though it appears to be unreferenced.</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifndef __used</span>
<span class="p_del">-# define __used			/* unimplemented */</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __maybe_unused</span>
<span class="p_del">-# define __maybe_unused		/* unimplemented */</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __always_unused</span>
<span class="p_del">-# define __always_unused	/* unimplemented */</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef noinline</span>
<span class="p_del">-#define noinline</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Rather then using noinline to prevent stack consumption, use</span>
<span class="p_del">- * noinline_for_stack instead.  For documentation reasons.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define noinline_for_stack noinline</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __always_inline</span>
<span class="p_del">-#define __always_inline inline</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* __KERNEL__ */</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * From the GCC manual:</span>
<span class="p_del">- *</span>
<span class="p_del">- * Many functions do not examine any values except their arguments,</span>
<span class="p_del">- * and have no effects except the return value.  Basically this is</span>
<span class="p_del">- * just slightly more strict class than the `pure&#39; attribute above,</span>
<span class="p_del">- * since function is not allowed to read global memory.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Note that a function that has pointer arguments and examines the</span>
<span class="p_del">- * data pointed to must _not_ be declared `const&#39;.  Likewise, a</span>
<span class="p_del">- * function that calls a non-`const&#39; function usually must not be</span>
<span class="p_del">- * `const&#39;.  It does not make sense for a `const&#39; function to return</span>
<span class="p_del">- * `void&#39;.</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifndef __attribute_const__</span>
<span class="p_del">-# define __attribute_const__	/* unimplemented */</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __designated_init</span>
<span class="p_del">-# define __designated_init</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __latent_entropy</span>
<span class="p_del">-# define __latent_entropy</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __randomize_layout</span>
<span class="p_del">-# define __randomize_layout __designated_init</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __no_randomize_layout</span>
<span class="p_del">-# define __no_randomize_layout</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef randomized_struct_fields_start</span>
<span class="p_del">-# define randomized_struct_fields_start</span>
<span class="p_del">-# define randomized_struct_fields_end</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Tell gcc if a function is cold. The compiler will assume any path</span>
<span class="p_del">- * directly leading to the call is unlikely.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __cold</span>
<span class="p_del">-#define __cold</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/* Simple shorthand for a section definition */</span>
<span class="p_del">-#ifndef __section</span>
<span class="p_del">-# define __section(S) __attribute__ ((__section__(#S)))</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __visible</span>
<span class="p_del">-#define __visible</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef __nostackprotector</span>
<span class="p_del">-# define __nostackprotector</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Assume alignment of return value.</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifndef __assume_aligned</span>
<span class="p_del">-#define __assume_aligned(a, ...)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-/* Are two types/vars the same type (ignoring qualifiers)? */</span>
<span class="p_del">-#ifndef __same_type</span>
<span class="p_del">-# define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is this type a native word size -- useful for atomic operations */</span>
<span class="p_del">-#ifndef __native_word</span>
<span class="p_del">-# define __native_word(t) (sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 /* Compile time object size, -1 for unknown */
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1
<span class="p_header">diff --git a/include/linux/compiler_types.h b/include/linux/compiler_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..6b79a9bba9a7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/compiler_types.h</span>
<span class="p_chunk">@@ -0,0 +1,274 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef __LINUX_COMPILER_TYPES_H</span>
<span class="p_add">+#define __LINUX_COMPILER_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __CHECKER__</span>
<span class="p_add">+# define __user		__attribute__((noderef, address_space(1)))</span>
<span class="p_add">+# define __kernel	__attribute__((address_space(0)))</span>
<span class="p_add">+# define __safe		__attribute__((safe))</span>
<span class="p_add">+# define __force	__attribute__((force))</span>
<span class="p_add">+# define __nocast	__attribute__((nocast))</span>
<span class="p_add">+# define __iomem	__attribute__((noderef, address_space(2)))</span>
<span class="p_add">+# define __must_hold(x)	__attribute__((context(x,1,1)))</span>
<span class="p_add">+# define __acquires(x)	__attribute__((context(x,0,1)))</span>
<span class="p_add">+# define __releases(x)	__attribute__((context(x,1,0)))</span>
<span class="p_add">+# define __acquire(x)	__context__(x,1)</span>
<span class="p_add">+# define __release(x)	__context__(x,-1)</span>
<span class="p_add">+# define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)</span>
<span class="p_add">+# define __percpu	__attribute__((noderef, address_space(3)))</span>
<span class="p_add">+# define __rcu		__attribute__((noderef, address_space(4)))</span>
<span class="p_add">+# define __private	__attribute__((noderef))</span>
<span class="p_add">+extern void __chk_user_ptr(const volatile void __user *);</span>
<span class="p_add">+extern void __chk_io_ptr(const volatile void __iomem *);</span>
<span class="p_add">+# define ACCESS_PRIVATE(p, member) (*((typeof((p)-&gt;member) __force *) &amp;(p)-&gt;member))</span>
<span class="p_add">+#else /* __CHECKER__ */</span>
<span class="p_add">+# ifdef STRUCTLEAK_PLUGIN</span>
<span class="p_add">+#  define __user __attribute__((user))</span>
<span class="p_add">+# else</span>
<span class="p_add">+#  define __user</span>
<span class="p_add">+# endif</span>
<span class="p_add">+# define __kernel</span>
<span class="p_add">+# define __safe</span>
<span class="p_add">+# define __force</span>
<span class="p_add">+# define __nocast</span>
<span class="p_add">+# define __iomem</span>
<span class="p_add">+# define __chk_user_ptr(x) (void)0</span>
<span class="p_add">+# define __chk_io_ptr(x) (void)0</span>
<span class="p_add">+# define __builtin_warning(x, y...) (1)</span>
<span class="p_add">+# define __must_hold(x)</span>
<span class="p_add">+# define __acquires(x)</span>
<span class="p_add">+# define __releases(x)</span>
<span class="p_add">+# define __acquire(x) (void)0</span>
<span class="p_add">+# define __release(x) (void)0</span>
<span class="p_add">+# define __cond_lock(x,c) (c)</span>
<span class="p_add">+# define __percpu</span>
<span class="p_add">+# define __rcu</span>
<span class="p_add">+# define __private</span>
<span class="p_add">+# define ACCESS_PRIVATE(p, member) ((p)-&gt;member)</span>
<span class="p_add">+#endif /* __CHECKER__ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Indirect macros required for expanded argument pasting, eg. __LINE__. */</span>
<span class="p_add">+#define ___PASTE(a,b) a##b</span>
<span class="p_add">+#define __PASTE(a,b) ___PASTE(a,b)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __GNUC__</span>
<span class="p_add">+#include &lt;linux/compiler-gcc.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CC_USING_HOTPATCH) &amp;&amp; !defined(__CHECKER__)</span>
<span class="p_add">+#define notrace __attribute__((hotpatch(0,0)))</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define notrace __attribute__((no_instrument_function))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel compiler defines __GNUC__. So we will overwrite implementations</span>
<span class="p_add">+ * coming from above header files here</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef __INTEL_COMPILER</span>
<span class="p_add">+# include &lt;linux/compiler-intel.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* Clang compiler defines __GNUC__. So we will overwrite implementations</span>
<span class="p_add">+ * coming from above header files here</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef __clang__</span>
<span class="p_add">+#include &lt;linux/compiler-clang.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Generic compiler-dependent macros required for kernel</span>
<span class="p_add">+ * build go below this comment. Actual compiler/compiler version</span>
<span class="p_add">+ * specific implementations come from the above header files</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+struct ftrace_branch_data {</span>
<span class="p_add">+	const char *func;</span>
<span class="p_add">+	const char *file;</span>
<span class="p_add">+	unsigned line;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			unsigned long correct;</span>
<span class="p_add">+			unsigned long incorrect;</span>
<span class="p_add">+		};</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			unsigned long miss;</span>
<span class="p_add">+			unsigned long hit;</span>
<span class="p_add">+		};</span>
<span class="p_add">+		unsigned long miss_hit[2];</span>
<span class="p_add">+	};</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct ftrace_likely_data {</span>
<span class="p_add">+	struct ftrace_branch_data	data;</span>
<span class="p_add">+	unsigned long			constant;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allow us to mark functions as &#39;deprecated&#39; and have gcc emit a nice</span>
<span class="p_add">+ * warning for each use, in hopes of speeding the functions removal.</span>
<span class="p_add">+ * Usage is:</span>
<span class="p_add">+ * 		int __deprecated foo(void)</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __deprecated</span>
<span class="p_add">+# define __deprecated		/* unimplemented */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef MODULE</span>
<span class="p_add">+#define __deprecated_for_modules __deprecated</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __deprecated_for_modules</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __must_check</span>
<span class="p_add">+#define __must_check</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_ENABLE_MUST_CHECK</span>
<span class="p_add">+#undef __must_check</span>
<span class="p_add">+#define __must_check</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifndef CONFIG_ENABLE_WARN_DEPRECATED</span>
<span class="p_add">+#undef __deprecated</span>
<span class="p_add">+#undef __deprecated_for_modules</span>
<span class="p_add">+#define __deprecated</span>
<span class="p_add">+#define __deprecated_for_modules</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __malloc</span>
<span class="p_add">+#define __malloc</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allow us to avoid &#39;defined but not used&#39; warnings on functions and data,</span>
<span class="p_add">+ * as well as force them to be emitted to the assembly file.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * As of gcc 3.4, static functions that are not marked with attribute((used))</span>
<span class="p_add">+ * may be elided from the assembly file.  As of gcc 3.4, static data not so</span>
<span class="p_add">+ * marked will not be elided, but this may change in a future gcc version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * NOTE: Because distributions shipped with a backported unit-at-a-time</span>
<span class="p_add">+ * compiler in gcc 3.3, we must define __used to be __attribute__((used))</span>
<span class="p_add">+ * for gcc &gt;=3.3 instead of 3.4.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * In prior versions of gcc, such functions and data would be emitted, but</span>
<span class="p_add">+ * would be warned about except with attribute((unused)).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Mark functions that are referenced only in inline assembly as __used so</span>
<span class="p_add">+ * the code is emitted even though it appears to be unreferenced.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __used</span>
<span class="p_add">+# define __used			/* unimplemented */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __maybe_unused</span>
<span class="p_add">+# define __maybe_unused		/* unimplemented */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __always_unused</span>
<span class="p_add">+# define __always_unused	/* unimplemented */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef noinline</span>
<span class="p_add">+#define noinline</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Rather then using noinline to prevent stack consumption, use</span>
<span class="p_add">+ * noinline_for_stack instead.  For documentation reasons.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define noinline_for_stack noinline</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __always_inline</span>
<span class="p_add">+#define __always_inline inline</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * From the GCC manual:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Many functions do not examine any values except their arguments,</span>
<span class="p_add">+ * and have no effects except the return value.  Basically this is</span>
<span class="p_add">+ * just slightly more strict class than the `pure&#39; attribute above,</span>
<span class="p_add">+ * since function is not allowed to read global memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that a function that has pointer arguments and examines the</span>
<span class="p_add">+ * data pointed to must _not_ be declared `const&#39;.  Likewise, a</span>
<span class="p_add">+ * function that calls a non-`const&#39; function usually must not be</span>
<span class="p_add">+ * `const&#39;.  It does not make sense for a `const&#39; function to return</span>
<span class="p_add">+ * `void&#39;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __attribute_const__</span>
<span class="p_add">+# define __attribute_const__	/* unimplemented */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __designated_init</span>
<span class="p_add">+# define __designated_init</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __latent_entropy</span>
<span class="p_add">+# define __latent_entropy</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __randomize_layout</span>
<span class="p_add">+# define __randomize_layout __designated_init</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __no_randomize_layout</span>
<span class="p_add">+# define __no_randomize_layout</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef randomized_struct_fields_start</span>
<span class="p_add">+# define randomized_struct_fields_start</span>
<span class="p_add">+# define randomized_struct_fields_end</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Tell gcc if a function is cold. The compiler will assume any path</span>
<span class="p_add">+ * directly leading to the call is unlikely.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __cold</span>
<span class="p_add">+#define __cold</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* Simple shorthand for a section definition */</span>
<span class="p_add">+#ifndef __section</span>
<span class="p_add">+# define __section(S) __attribute__ ((__section__(#S)))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __visible</span>
<span class="p_add">+#define __visible</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __nostackprotector</span>
<span class="p_add">+# define __nostackprotector</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Assume alignment of return value.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __assume_aligned</span>
<span class="p_add">+#define __assume_aligned(a, ...)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Are two types/vars the same type (ignoring qualifiers)? */</span>
<span class="p_add">+#ifndef __same_type</span>
<span class="p_add">+# define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* Is this type a native word size -- useful for atomic operations */</span>
<span class="p_add">+#ifndef __native_word</span>
<span class="p_add">+# define __native_word(t) (sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __LINUX_COMPILER_TYPES_H */</span>
<span class="p_header">diff --git a/include/linux/hypervisor.h b/include/linux/hypervisor.h</span>
<span class="p_header">index b4054fd5b6f6..b19563f9a8eb 100644</span>
<span class="p_header">--- a/include/linux/hypervisor.h</span>
<span class="p_header">+++ b/include/linux/hypervisor.h</span>
<span class="p_chunk">@@ -7,8 +7,12 @@</span> <span class="p_context"></span>
  *		Juergen Gross &lt;jgross@suse.com&gt;
  */
 
<span class="p_del">-#ifdef CONFIG_HYPERVISOR_GUEST</span>
<span class="p_del">-#include &lt;asm/hypervisor.h&gt;</span>
<span class="p_add">+#ifdef CONFIG_X86</span>
<span class="p_add">+#include &lt;asm/x86_init.h&gt;</span>
<span class="p_add">+static inline void hypervisor_pin_vcpu(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	x86_platform.hyper.pin_vcpu(cpu);</span>
<span class="p_add">+}</span>
 #else
 static inline void hypervisor_pin_vcpu(int cpu)
 {
<span class="p_header">diff --git a/include/linux/iio/common/st_sensors.h b/include/linux/iio/common/st_sensors.h</span>
<span class="p_header">index 7b0fa8b5c120..ce0ef1c0a30a 100644</span>
<span class="p_header">--- a/include/linux/iio/common/st_sensors.h</span>
<span class="p_header">+++ b/include/linux/iio/common/st_sensors.h</span>
<span class="p_chunk">@@ -139,7 +139,7 @@</span> <span class="p_context"> struct st_sensor_das {</span>
  * @mask_ihl: mask to enable/disable active low on the INT lines.
  * @addr_od: address to enable/disable Open Drain on the INT lines.
  * @mask_od: mask to enable/disable Open Drain on the INT lines.
<span class="p_del">- * @addr_stat_drdy: address to read status of DRDY (data ready) interrupt</span>
<span class="p_add">+ * struct stat_drdy - status register of DRDY (data ready) interrupt.</span>
  * struct ig1 - represents the Interrupt Generator 1 of sensors.
  * @en_addr: address of the enable ig1 register.
  * @en_mask: mask to write the on/off value for enable.
<span class="p_chunk">@@ -152,7 +152,10 @@</span> <span class="p_context"> struct st_sensor_data_ready_irq {</span>
 	u8 mask_ihl;
 	u8 addr_od;
 	u8 mask_od;
<span class="p_del">-	u8 addr_stat_drdy;</span>
<span class="p_add">+	struct {</span>
<span class="p_add">+		u8 addr;</span>
<span class="p_add">+		u8 mask;</span>
<span class="p_add">+	} stat_drdy;</span>
 	struct {
 		u8 en_addr;
 		u8 en_mask;
<span class="p_header">diff --git a/include/linux/intel-pti.h b/include/linux/intel-pti.h</span>
new file mode 100644
<span class="p_header">index 000000000000..2710d72de3c9</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/intel-pti.h</span>
<span class="p_chunk">@@ -0,0 +1,43 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ *  Copyright (C) Intel 2011</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The PTI (Parallel Trace Interface) driver directs trace data routed from</span>
<span class="p_add">+ * various parts in the system out through the Intel Penwell PTI port and</span>
<span class="p_add">+ * out of the mobile device for analysis with a debugging tool</span>
<span class="p_add">+ * (Lauterbach, Fido). This is part of a solution for the MIPI P1149.7,</span>
<span class="p_add">+ * compact JTAG, standard.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This header file will allow other parts of the OS to use the</span>
<span class="p_add">+ * interface to write out it&#39;s contents for debugging a mobile system.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef LINUX_INTEL_PTI_H_</span>
<span class="p_add">+#define LINUX_INTEL_PTI_H_</span>
<span class="p_add">+</span>
<span class="p_add">+/* offset for last dword of any PTI message. Part of MIPI P1149.7 */</span>
<span class="p_add">+#define PTI_LASTDWORD_DTS	0x30</span>
<span class="p_add">+</span>
<span class="p_add">+/* basic structure used as a write address to the PTI HW */</span>
<span class="p_add">+struct pti_masterchannel {</span>
<span class="p_add">+	u8 master;</span>
<span class="p_add">+	u8 channel;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* the following functions are defined in misc/pti.c */</span>
<span class="p_add">+void pti_writedata(struct pti_masterchannel *mc, u8 *buf, int count);</span>
<span class="p_add">+struct pti_masterchannel *pti_request_masterchannel(u8 type,</span>
<span class="p_add">+						    const char *thread_name);</span>
<span class="p_add">+void pti_release_masterchannel(struct pti_masterchannel *mc);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* LINUX_INTEL_PTI_H_ */</span>
<span class="p_header">diff --git a/include/linux/linkage.h b/include/linux/linkage.h</span>
<span class="p_header">index 2e6f90bd52aa..f68db9e450eb 100644</span>
<span class="p_header">--- a/include/linux/linkage.h</span>
<span class="p_header">+++ b/include/linux/linkage.h</span>
<span class="p_chunk">@@ -2,7 +2,7 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_LINKAGE_H
 #define _LINUX_LINKAGE_H
 
<span class="p_del">-#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/compiler_types.h&gt;</span>
 #include &lt;linux/stringify.h&gt;
 #include &lt;linux/export.h&gt;
 #include &lt;asm/linkage.h&gt;
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index db647d428100..f50deada0f5c 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2510,7 +2510,7 @@</span> <span class="p_context"> void vmemmap_populate_print_last(void);</span>
 void vmemmap_free(unsigned long start, unsigned long end);
 #endif
 void register_page_bootmem_memmap(unsigned long section_nr, struct page *map,
<span class="p_del">-				  unsigned long size);</span>
<span class="p_add">+				  unsigned long nr_pages);</span>
 
 enum mf_flags {
 	MF_COUNT_INCREASED = 1 &lt;&lt; 0,
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index 18b06983131a..f0938257ee6d 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -1152,13 +1152,17 @@</span> <span class="p_context"> struct mem_section {</span>
 #define SECTION_ROOT_MASK	(SECTIONS_PER_ROOT - 1)
 
 #ifdef CONFIG_SPARSEMEM_EXTREME
<span class="p_del">-extern struct mem_section *mem_section[NR_SECTION_ROOTS];</span>
<span class="p_add">+extern struct mem_section **mem_section;</span>
 #else
 extern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];
 #endif
 
 static inline struct mem_section *__nr_to_section(unsigned long nr)
 {
<span class="p_add">+#ifdef CONFIG_SPARSEMEM_EXTREME</span>
<span class="p_add">+	if (!mem_section)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+#endif</span>
 	if (!mem_section[SECTION_NR_TO_ROOT(nr)])
 		return NULL;
 	return &amp;mem_section[SECTION_NR_TO_ROOT(nr)][nr &amp; SECTION_ROOT_MASK];
<span class="p_header">diff --git a/include/linux/pti.h b/include/linux/pti.h</span>
deleted file mode 100644
<span class="p_header">index b3ea01a3197e..000000000000</span>
<span class="p_header">--- a/include/linux/pti.h</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,43 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-/*</span>
<span class="p_del">- *  Copyright (C) Intel 2011</span>
<span class="p_del">- *</span>
<span class="p_del">- * This program is free software; you can redistribute it and/or modify</span>
<span class="p_del">- * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_del">- * published by the Free Software Foundation.</span>
<span class="p_del">- *</span>
<span class="p_del">- * This program is distributed in the hope that it will be useful,</span>
<span class="p_del">- * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_del">- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_del">- * GNU General Public License for more details.</span>
<span class="p_del">- *</span>
<span class="p_del">- * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="p_del">- *</span>
<span class="p_del">- * The PTI (Parallel Trace Interface) driver directs trace data routed from</span>
<span class="p_del">- * various parts in the system out through the Intel Penwell PTI port and</span>
<span class="p_del">- * out of the mobile device for analysis with a debugging tool</span>
<span class="p_del">- * (Lauterbach, Fido). This is part of a solution for the MIPI P1149.7,</span>
<span class="p_del">- * compact JTAG, standard.</span>
<span class="p_del">- *</span>
<span class="p_del">- * This header file will allow other parts of the OS to use the</span>
<span class="p_del">- * interface to write out it&#39;s contents for debugging a mobile system.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef PTI_H_</span>
<span class="p_del">-#define PTI_H_</span>
<span class="p_del">-</span>
<span class="p_del">-/* offset for last dword of any PTI message. Part of MIPI P1149.7 */</span>
<span class="p_del">-#define PTI_LASTDWORD_DTS	0x30</span>
<span class="p_del">-</span>
<span class="p_del">-/* basic structure used as a write address to the PTI HW */</span>
<span class="p_del">-struct pti_masterchannel {</span>
<span class="p_del">-	u8 master;</span>
<span class="p_del">-	u8 channel;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/* the following functions are defined in misc/pti.c */</span>
<span class="p_del">-void pti_writedata(struct pti_masterchannel *mc, u8 *buf, int count);</span>
<span class="p_del">-struct pti_masterchannel *pti_request_masterchannel(u8 type,</span>
<span class="p_del">-						    const char *thread_name);</span>
<span class="p_del">-void pti_release_masterchannel(struct pti_masterchannel *mc);</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /*PTI_H_*/</span>
<span class="p_header">diff --git a/include/linux/rculist.h b/include/linux/rculist.h</span>
<span class="p_header">index c2cdd45a880a..127f534fec94 100644</span>
<span class="p_header">--- a/include/linux/rculist.h</span>
<span class="p_header">+++ b/include/linux/rculist.h</span>
<span class="p_chunk">@@ -275,7 +275,7 @@</span> <span class="p_context"> static inline void list_splice_tail_init_rcu(struct list_head *list,</span>
  * primitives such as list_add_rcu() as long as it&#39;s guarded by rcu_read_lock().
  */
 #define list_entry_rcu(ptr, type, member) \
<span class="p_del">-	container_of(lockless_dereference(ptr), type, member)</span>
<span class="p_add">+	container_of(READ_ONCE(ptr), type, member)</span>
 
 /*
  * Where are list_empty_rcu() and list_first_entry_rcu()?
<span class="p_chunk">@@ -368,7 +368,7 @@</span> <span class="p_context"> static inline void list_splice_tail_init_rcu(struct list_head *list,</span>
  * example is when items are added to the list, but never deleted.
  */
 #define list_entry_lockless(ptr, type, member) \
<span class="p_del">-	container_of((typeof(ptr))lockless_dereference(ptr), type, member)</span>
<span class="p_add">+	container_of((typeof(ptr))READ_ONCE(ptr), type, member)</span>
 
 /**
  * list_for_each_entry_lockless - iterate over rcu list of given type
<span class="p_header">diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h</span>
<span class="p_header">index 1a9f70d44af9..a6ddc42f87a5 100644</span>
<span class="p_header">--- a/include/linux/rcupdate.h</span>
<span class="p_header">+++ b/include/linux/rcupdate.h</span>
<span class="p_chunk">@@ -346,7 +346,7 @@</span> <span class="p_context"> static inline void rcu_preempt_sleep_check(void) { }</span>
 #define __rcu_dereference_check(p, c, space) \
 ({ \
 	/* Dependency order vs. p above. */ \
<span class="p_del">-	typeof(*p) *________p1 = (typeof(*p) *__force)lockless_dereference(p); \</span>
<span class="p_add">+	typeof(*p) *________p1 = (typeof(*p) *__force)READ_ONCE(p); \</span>
 	RCU_LOCKDEP_WARN(!(c), &quot;suspicious rcu_dereference_check() usage&quot;); \
 	rcu_dereference_sparse(p, space); \
 	((typeof(*p) __force __kernel *)(________p1)); \
<span class="p_chunk">@@ -360,7 +360,7 @@</span> <span class="p_context"> static inline void rcu_preempt_sleep_check(void) { }</span>
 #define rcu_dereference_raw(p) \
 ({ \
 	/* Dependency order vs. p above. */ \
<span class="p_del">-	typeof(p) ________p1 = lockless_dereference(p); \</span>
<span class="p_add">+	typeof(p) ________p1 = READ_ONCE(p); \</span>
 	((typeof(*p) __force __kernel *)(________p1)); \
 })
 
<span class="p_header">diff --git a/include/uapi/linux/stddef.h b/include/uapi/linux/stddef.h</span>
<span class="p_header">index f65b92e0e1f9..ee8220f8dcf5 100644</span>
<span class="p_header">--- a/include/uapi/linux/stddef.h</span>
<span class="p_header">+++ b/include/uapi/linux/stddef.h</span>
<span class="p_chunk">@@ -1,5 +1,5 @@</span> <span class="p_context"></span>
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
<span class="p_del">-#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/compiler_types.h&gt;</span>
 
 #ifndef __always_inline
 #define __always_inline inline
<span class="p_header">diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c</span>
<span class="p_header">index c48ca2a34b5e..c5ff809e86d0 100644</span>
<span class="p_header">--- a/kernel/bpf/verifier.c</span>
<span class="p_header">+++ b/kernel/bpf/verifier.c</span>
<span class="p_chunk">@@ -1061,6 +1061,11 @@</span> <span class="p_context"> static int check_ptr_alignment(struct bpf_verifier_env *env,</span>
 		break;
 	case PTR_TO_STACK:
 		pointer_desc = &quot;stack &quot;;
<span class="p_add">+		/* The stack spill tracking logic in check_stack_write()</span>
<span class="p_add">+		 * and check_stack_read() relies on stack accesses being</span>
<span class="p_add">+		 * aligned.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		strict = true;</span>
 		break;
 	default:
 		break;
<span class="p_chunk">@@ -1068,6 +1073,29 @@</span> <span class="p_context"> static int check_ptr_alignment(struct bpf_verifier_env *env,</span>
 	return check_generic_ptr_alignment(reg, pointer_desc, off, size, strict);
 }
 
<span class="p_add">+/* truncate register to smaller size (in bytes)</span>
<span class="p_add">+ * must be called with size &lt; BPF_REG_SIZE</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void coerce_reg_to_size(struct bpf_reg_state *reg, int size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* clear high bits in bit representation */</span>
<span class="p_add">+	reg-&gt;var_off = tnum_cast(reg-&gt;var_off, size);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* fix arithmetic bounds */</span>
<span class="p_add">+	mask = ((u64)1 &lt;&lt; (size * 8)) - 1;</span>
<span class="p_add">+	if ((reg-&gt;umin_value &amp; ~mask) == (reg-&gt;umax_value &amp; ~mask)) {</span>
<span class="p_add">+		reg-&gt;umin_value &amp;= mask;</span>
<span class="p_add">+		reg-&gt;umax_value &amp;= mask;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		reg-&gt;umin_value = 0;</span>
<span class="p_add">+		reg-&gt;umax_value = mask;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	reg-&gt;smin_value = reg-&gt;umin_value;</span>
<span class="p_add">+	reg-&gt;smax_value = reg-&gt;umax_value;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* check whether memory at (regno + off) is accessible for t = (read | write)
  * if t==write, value_regno is a register which value is stored into memory
  * if t==read, value_regno is a register which will receive the value from memory
<span class="p_chunk">@@ -1200,9 +1228,7 @@</span> <span class="p_context"> static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regn</span>
 	if (!err &amp;&amp; size &lt; BPF_REG_SIZE &amp;&amp; value_regno &gt;= 0 &amp;&amp; t == BPF_READ &amp;&amp;
 	    state-&gt;regs[value_regno].type == SCALAR_VALUE) {
 		/* b/h/w load zero-extends, mark upper bits as known 0 */
<span class="p_del">-		state-&gt;regs[value_regno].var_off = tnum_cast(</span>
<span class="p_del">-					state-&gt;regs[value_regno].var_off, size);</span>
<span class="p_del">-		__update_reg_bounds(&amp;state-&gt;regs[value_regno]);</span>
<span class="p_add">+		coerce_reg_to_size(&amp;state-&gt;regs[value_regno], size);</span>
 	}
 	return err;
 }
<span class="p_chunk">@@ -1282,6 +1308,7 @@</span> <span class="p_context"> static int check_stack_boundary(struct bpf_verifier_env *env, int regno,</span>
 		tnum_strn(tn_buf, sizeof(tn_buf), regs[regno].var_off);
 		verbose(&quot;invalid variable stack read R%d var_off=%s\n&quot;,
 			regno, tn_buf);
<span class="p_add">+		return -EACCES;</span>
 	}
 	off = regs[regno].off + regs[regno].var_off.value;
 	if (off &gt;= 0 || off &lt; -MAX_BPF_STACK || off + access_size &gt; 0 ||
<span class="p_chunk">@@ -1742,14 +1769,6 @@</span> <span class="p_context"> static int check_call(struct bpf_verifier_env *env, int func_id, int insn_idx)</span>
 	return 0;
 }
 
<span class="p_del">-static void coerce_reg_to_32(struct bpf_reg_state *reg)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* clear high 32 bits */</span>
<span class="p_del">-	reg-&gt;var_off = tnum_cast(reg-&gt;var_off, 4);</span>
<span class="p_del">-	/* Update bounds */</span>
<span class="p_del">-	__update_reg_bounds(reg);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static bool signed_add_overflows(s64 a, s64 b)
 {
 	/* Do the add in u64, where overflow is well-defined */
<span class="p_chunk">@@ -1770,6 +1789,41 @@</span> <span class="p_context"> static bool signed_sub_overflows(s64 a, s64 b)</span>
 	return res &gt; a;
 }
 
<span class="p_add">+static bool check_reg_sane_offset(struct bpf_verifier_env *env,</span>
<span class="p_add">+				  const struct bpf_reg_state *reg,</span>
<span class="p_add">+				  enum bpf_reg_type type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool known = tnum_is_const(reg-&gt;var_off);</span>
<span class="p_add">+	s64 val = reg-&gt;var_off.value;</span>
<span class="p_add">+	s64 smin = reg-&gt;smin_value;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (known &amp;&amp; (val &gt;= BPF_MAX_VAR_OFF || val &lt;= -BPF_MAX_VAR_OFF)) {</span>
<span class="p_add">+		verbose(&quot;math between %s pointer and %lld is not allowed\n&quot;,</span>
<span class="p_add">+			reg_type_str[type], val);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (reg-&gt;off &gt;= BPF_MAX_VAR_OFF || reg-&gt;off &lt;= -BPF_MAX_VAR_OFF) {</span>
<span class="p_add">+		verbose(&quot;%s pointer offset %d is not allowed\n&quot;,</span>
<span class="p_add">+			reg_type_str[type], reg-&gt;off);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (smin == S64_MIN) {</span>
<span class="p_add">+		verbose(&quot;math between %s pointer and register with unbounded min value is not allowed\n&quot;,</span>
<span class="p_add">+			reg_type_str[type]);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (smin &gt;= BPF_MAX_VAR_OFF || smin &lt;= -BPF_MAX_VAR_OFF) {</span>
<span class="p_add">+		verbose(&quot;value %lld makes %s pointer be out of bounds\n&quot;,</span>
<span class="p_add">+			smin, reg_type_str[type]);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.
  * Caller should also handle BPF_MOV case separately.
  * If we return -EACCES, caller may want to try again treating pointer as a
<span class="p_chunk">@@ -1835,6 +1889,10 @@</span> <span class="p_context"> static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,</span>
 	dst_reg-&gt;type = ptr_reg-&gt;type;
 	dst_reg-&gt;id = ptr_reg-&gt;id;
 
<span class="p_add">+	if (!check_reg_sane_offset(env, off_reg, ptr_reg-&gt;type) ||</span>
<span class="p_add">+	    !check_reg_sane_offset(env, ptr_reg, ptr_reg-&gt;type))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	switch (opcode) {
 	case BPF_ADD:
 		/* We can take a fixed offset as long as it doesn&#39;t overflow
<span class="p_chunk">@@ -1965,12 +2023,19 @@</span> <span class="p_context"> static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,</span>
 		return -EACCES;
 	}
 
<span class="p_add">+	if (!check_reg_sane_offset(env, dst_reg, ptr_reg-&gt;type))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	__update_reg_bounds(dst_reg);
 	__reg_deduce_bounds(dst_reg);
 	__reg_bound_offset(dst_reg);
 	return 0;
 }
 
<span class="p_add">+/* WARNING: This function does calculations on 64-bit values, but the actual</span>
<span class="p_add">+ * execution may occur on 32-bit values. Therefore, things like bitshifts</span>
<span class="p_add">+ * need extra checks in the 32-bit case.</span>
<span class="p_add">+ */</span>
 static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,
 				      struct bpf_insn *insn,
 				      struct bpf_reg_state *dst_reg,
<span class="p_chunk">@@ -1981,12 +2046,8 @@</span> <span class="p_context"> static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,</span>
 	bool src_known, dst_known;
 	s64 smin_val, smax_val;
 	u64 umin_val, umax_val;
<span class="p_add">+	u64 insn_bitness = (BPF_CLASS(insn-&gt;code) == BPF_ALU64) ? 64 : 32;</span>
 
<span class="p_del">-	if (BPF_CLASS(insn-&gt;code) != BPF_ALU64) {</span>
<span class="p_del">-		/* 32-bit ALU ops are (32,32)-&gt;64 */</span>
<span class="p_del">-		coerce_reg_to_32(dst_reg);</span>
<span class="p_del">-		coerce_reg_to_32(&amp;src_reg);</span>
<span class="p_del">-	}</span>
 	smin_val = src_reg.smin_value;
 	smax_val = src_reg.smax_value;
 	umin_val = src_reg.umin_value;
<span class="p_chunk">@@ -1994,6 +2055,12 @@</span> <span class="p_context"> static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,</span>
 	src_known = tnum_is_const(src_reg.var_off);
 	dst_known = tnum_is_const(dst_reg-&gt;var_off);
 
<span class="p_add">+	if (!src_known &amp;&amp;</span>
<span class="p_add">+	    opcode != BPF_ADD &amp;&amp; opcode != BPF_SUB &amp;&amp; opcode != BPF_AND) {</span>
<span class="p_add">+		__mark_reg_unknown(dst_reg);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	switch (opcode) {
 	case BPF_ADD:
 		if (signed_add_overflows(dst_reg-&gt;smin_value, smin_val) ||
<span class="p_chunk">@@ -2122,9 +2189,9 @@</span> <span class="p_context"> static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,</span>
 		__update_reg_bounds(dst_reg);
 		break;
 	case BPF_LSH:
<span class="p_del">-		if (umax_val &gt; 63) {</span>
<span class="p_del">-			/* Shifts greater than 63 are undefined.  This includes</span>
<span class="p_del">-			 * shifts by a negative number.</span>
<span class="p_add">+		if (umax_val &gt;= insn_bitness) {</span>
<span class="p_add">+			/* Shifts greater than 31 or 63 are undefined.</span>
<span class="p_add">+			 * This includes shifts by a negative number.</span>
 			 */
 			mark_reg_unknown(regs, insn-&gt;dst_reg);
 			break;
<span class="p_chunk">@@ -2150,27 +2217,29 @@</span> <span class="p_context"> static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,</span>
 		__update_reg_bounds(dst_reg);
 		break;
 	case BPF_RSH:
<span class="p_del">-		if (umax_val &gt; 63) {</span>
<span class="p_del">-			/* Shifts greater than 63 are undefined.  This includes</span>
<span class="p_del">-			 * shifts by a negative number.</span>
<span class="p_add">+		if (umax_val &gt;= insn_bitness) {</span>
<span class="p_add">+			/* Shifts greater than 31 or 63 are undefined.</span>
<span class="p_add">+			 * This includes shifts by a negative number.</span>
 			 */
 			mark_reg_unknown(regs, insn-&gt;dst_reg);
 			break;
 		}
<span class="p_del">-		/* BPF_RSH is an unsigned shift, so make the appropriate casts */</span>
<span class="p_del">-		if (dst_reg-&gt;smin_value &lt; 0) {</span>
<span class="p_del">-			if (umin_val) {</span>
<span class="p_del">-				/* Sign bit will be cleared */</span>
<span class="p_del">-				dst_reg-&gt;smin_value = 0;</span>
<span class="p_del">-			} else {</span>
<span class="p_del">-				/* Lost sign bit information */</span>
<span class="p_del">-				dst_reg-&gt;smin_value = S64_MIN;</span>
<span class="p_del">-				dst_reg-&gt;smax_value = S64_MAX;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			dst_reg-&gt;smin_value =</span>
<span class="p_del">-				(u64)(dst_reg-&gt;smin_value) &gt;&gt; umax_val;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		/* BPF_RSH is an unsigned shift.  If the value in dst_reg might</span>
<span class="p_add">+		 * be negative, then either:</span>
<span class="p_add">+		 * 1) src_reg might be zero, so the sign bit of the result is</span>
<span class="p_add">+		 *    unknown, so we lose our signed bounds</span>
<span class="p_add">+		 * 2) it&#39;s known negative, thus the unsigned bounds capture the</span>
<span class="p_add">+		 *    signed bounds</span>
<span class="p_add">+		 * 3) the signed bounds cross zero, so they tell us nothing</span>
<span class="p_add">+		 *    about the result</span>
<span class="p_add">+		 * If the value in dst_reg is known nonnegative, then again the</span>
<span class="p_add">+		 * unsigned bounts capture the signed bounds.</span>
<span class="p_add">+		 * Thus, in all cases it suffices to blow away our signed bounds</span>
<span class="p_add">+		 * and rely on inferring new ones from the unsigned bounds and</span>
<span class="p_add">+		 * var_off of the result.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		dst_reg-&gt;smin_value = S64_MIN;</span>
<span class="p_add">+		dst_reg-&gt;smax_value = S64_MAX;</span>
 		if (src_known)
 			dst_reg-&gt;var_off = tnum_rshift(dst_reg-&gt;var_off,
 						       umin_val);
<span class="p_chunk">@@ -2186,6 +2255,12 @@</span> <span class="p_context"> static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,</span>
 		break;
 	}
 
<span class="p_add">+	if (BPF_CLASS(insn-&gt;code) != BPF_ALU64) {</span>
<span class="p_add">+		/* 32-bit ALU ops are (32,32)-&gt;32 */</span>
<span class="p_add">+		coerce_reg_to_size(dst_reg, 4);</span>
<span class="p_add">+		coerce_reg_to_size(&amp;src_reg, 4);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	__reg_deduce_bounds(dst_reg);
 	__reg_bound_offset(dst_reg);
 	return 0;
<span class="p_chunk">@@ -2362,17 +2437,20 @@</span> <span class="p_context"> static int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)</span>
 					return -EACCES;
 				}
 				mark_reg_unknown(regs, insn-&gt;dst_reg);
<span class="p_del">-				/* high 32 bits are known zero. */</span>
<span class="p_del">-				regs[insn-&gt;dst_reg].var_off = tnum_cast(</span>
<span class="p_del">-						regs[insn-&gt;dst_reg].var_off, 4);</span>
<span class="p_del">-				__update_reg_bounds(&amp;regs[insn-&gt;dst_reg]);</span>
<span class="p_add">+				coerce_reg_to_size(&amp;regs[insn-&gt;dst_reg], 4);</span>
 			}
 		} else {
 			/* case: R = imm
 			 * remember the value we stored into this reg
 			 */
 			regs[insn-&gt;dst_reg].type = SCALAR_VALUE;
<span class="p_del">-			__mark_reg_known(regs + insn-&gt;dst_reg, insn-&gt;imm);</span>
<span class="p_add">+			if (BPF_CLASS(insn-&gt;code) == BPF_ALU64) {</span>
<span class="p_add">+				__mark_reg_known(regs + insn-&gt;dst_reg,</span>
<span class="p_add">+						 insn-&gt;imm);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				__mark_reg_known(regs + insn-&gt;dst_reg,</span>
<span class="p_add">+						 (u32)insn-&gt;imm);</span>
<span class="p_add">+			}</span>
 		}
 
 	} else if (opcode &gt; BPF_END) {
<span class="p_chunk">@@ -3307,15 +3385,14 @@</span> <span class="p_context"> static bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,</span>
 			return range_within(rold, rcur) &amp;&amp;
 			       tnum_in(rold-&gt;var_off, rcur-&gt;var_off);
 		} else {
<span class="p_del">-			/* if we knew anything about the old value, we&#39;re not</span>
<span class="p_del">-			 * equal, because we can&#39;t know anything about the</span>
<span class="p_del">-			 * scalar value of the pointer in the new value.</span>
<span class="p_add">+			/* We&#39;re trying to use a pointer in place of a scalar.</span>
<span class="p_add">+			 * Even if the scalar was unbounded, this could lead to</span>
<span class="p_add">+			 * pointer leaks because scalars are allowed to leak</span>
<span class="p_add">+			 * while pointers are not. We could make this safe in</span>
<span class="p_add">+			 * special cases if root is calling us, but it&#39;s</span>
<span class="p_add">+			 * probably not worth the hassle.</span>
 			 */
<span class="p_del">-			return rold-&gt;umin_value == 0 &amp;&amp;</span>
<span class="p_del">-			       rold-&gt;umax_value == U64_MAX &amp;&amp;</span>
<span class="p_del">-			       rold-&gt;smin_value == S64_MIN &amp;&amp;</span>
<span class="p_del">-			       rold-&gt;smax_value == S64_MAX &amp;&amp;</span>
<span class="p_del">-			       tnum_is_unknown(rold-&gt;var_off);</span>
<span class="p_add">+			return false;</span>
 		}
 	case PTR_TO_MAP_VALUE:
 		/* If the new min/max/var_off satisfy the old ones and
<span class="p_chunk">@@ -3665,6 +3742,7 @@</span> <span class="p_context"> static int do_check(struct bpf_verifier_env *env)</span>
 		if (err)
 			return err;
 
<span class="p_add">+		env-&gt;insn_aux_data[insn_idx].seen = true;</span>
 		if (class == BPF_ALU || class == BPF_ALU64) {
 			err = check_alu_op(env, insn);
 			if (err)
<span class="p_chunk">@@ -3855,6 +3933,7 @@</span> <span class="p_context"> static int do_check(struct bpf_verifier_env *env)</span>
 					return err;
 
 				insn_idx++;
<span class="p_add">+				env-&gt;insn_aux_data[insn_idx].seen = true;</span>
 			} else {
 				verbose(&quot;invalid BPF_LD mode\n&quot;);
 				return -EINVAL;
<span class="p_chunk">@@ -4035,6 +4114,7 @@</span> <span class="p_context"> static int adjust_insn_aux_data(struct bpf_verifier_env *env, u32 prog_len,</span>
 				u32 off, u32 cnt)
 {
 	struct bpf_insn_aux_data *new_data, *old_data = env-&gt;insn_aux_data;
<span class="p_add">+	int i;</span>
 
 	if (cnt == 1)
 		return 0;
<span class="p_chunk">@@ -4044,6 +4124,8 @@</span> <span class="p_context"> static int adjust_insn_aux_data(struct bpf_verifier_env *env, u32 prog_len,</span>
 	memcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);
 	memcpy(new_data + off + cnt - 1, old_data + off,
 	       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));
<span class="p_add">+	for (i = off; i &lt; off + cnt - 1; i++)</span>
<span class="p_add">+		new_data[i].seen = true;</span>
 	env-&gt;insn_aux_data = new_data;
 	vfree(old_data);
 	return 0;
<span class="p_chunk">@@ -4062,6 +4144,25 @@</span> <span class="p_context"> static struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 of</span>
 	return new_prog;
 }
 
<span class="p_add">+/* The verifier does more data flow analysis than llvm and will not explore</span>
<span class="p_add">+ * branches that are dead at run time. Malicious programs can have dead code</span>
<span class="p_add">+ * too. Therefore replace all dead at-run-time code with nops.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void sanitize_dead_code(struct bpf_verifier_env *env)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct bpf_insn_aux_data *aux_data = env-&gt;insn_aux_data;</span>
<span class="p_add">+	struct bpf_insn nop = BPF_MOV64_REG(BPF_REG_0, BPF_REG_0);</span>
<span class="p_add">+	struct bpf_insn *insn = env-&gt;prog-&gt;insnsi;</span>
<span class="p_add">+	const int insn_cnt = env-&gt;prog-&gt;len;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; insn_cnt; i++) {</span>
<span class="p_add">+		if (aux_data[i].seen)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		memcpy(insn + i, &amp;nop, sizeof(nop));</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* convert load instructions that access fields of &#39;struct __sk_buff&#39;
  * into sequence of instructions that access fields of &#39;struct sk_buff&#39;
  */
<span class="p_chunk">@@ -4378,6 +4479,9 @@</span> <span class="p_context"> int bpf_check(struct bpf_prog **prog, union bpf_attr *attr)</span>
 	while (pop_stack(env, NULL) &gt;= 0);
 	free_states(env);
 
<span class="p_add">+	if (ret == 0)</span>
<span class="p_add">+		sanitize_dead_code(env);</span>
<span class="p_add">+</span>
 	if (ret == 0)
 		/* program is valid, convert *(u32*)(ctx + off) accesses */
 		ret = convert_ctx_accesses(env);
<span class="p_header">diff --git a/kernel/events/core.c b/kernel/events/core.c</span>
<span class="p_header">index 4f1d4bfc607a..24ebad5567b4 100644</span>
<span class="p_header">--- a/kernel/events/core.c</span>
<span class="p_header">+++ b/kernel/events/core.c</span>
<span class="p_chunk">@@ -4233,7 +4233,7 @@</span> <span class="p_context"> static void perf_remove_from_owner(struct perf_event *event)</span>
 	 * indeed free this event, otherwise we need to serialize on
 	 * owner-&gt;perf_event_mutex.
 	 */
<span class="p_del">-	owner = lockless_dereference(event-&gt;owner);</span>
<span class="p_add">+	owner = READ_ONCE(event-&gt;owner);</span>
 	if (owner) {
 		/*
 		 * Since delayed_put_task_struct() also drops the last
<span class="p_chunk">@@ -4330,7 +4330,7 @@</span> <span class="p_context"> int perf_event_release_kernel(struct perf_event *event)</span>
 		 * Cannot change, child events are not migrated, see the
 		 * comment with perf_event_ctx_lock_nested().
 		 */
<span class="p_del">-		ctx = lockless_dereference(child-&gt;ctx);</span>
<span class="p_add">+		ctx = READ_ONCE(child-&gt;ctx);</span>
 		/*
 		 * Since child_mutex nests inside ctx::mutex, we must jump
 		 * through hoops. We start by grabbing a reference on the ctx.
<span class="p_header">diff --git a/kernel/seccomp.c b/kernel/seccomp.c</span>
<span class="p_header">index 418a1c045933..5f0dfb2abb8d 100644</span>
<span class="p_header">--- a/kernel/seccomp.c</span>
<span class="p_header">+++ b/kernel/seccomp.c</span>
<span class="p_chunk">@@ -190,7 +190,7 @@</span> <span class="p_context"> static u32 seccomp_run_filters(const struct seccomp_data *sd,</span>
 	u32 ret = SECCOMP_RET_ALLOW;
 	/* Make sure cross-thread synced filter points somewhere sane. */
 	struct seccomp_filter *f =
<span class="p_del">-			lockless_dereference(current-&gt;seccomp.filter);</span>
<span class="p_add">+			READ_ONCE(current-&gt;seccomp.filter);</span>
 
 	/* Ensure unexpected behavior doesn&#39;t result in failing open. */
 	if (unlikely(WARN_ON(f == NULL)))
<span class="p_header">diff --git a/kernel/task_work.c b/kernel/task_work.c</span>
<span class="p_header">index 5718b3ea202a..0fef395662a6 100644</span>
<span class="p_header">--- a/kernel/task_work.c</span>
<span class="p_header">+++ b/kernel/task_work.c</span>
<span class="p_chunk">@@ -68,7 +68,7 @@</span> <span class="p_context"> task_work_cancel(struct task_struct *task, task_work_func_t func)</span>
 	 * we raced with task_work_run(), *pprev == NULL/exited.
 	 */
 	raw_spin_lock_irqsave(&amp;task-&gt;pi_lock, flags);
<span class="p_del">-	while ((work = lockless_dereference(*pprev))) {</span>
<span class="p_add">+	while ((work = READ_ONCE(*pprev))) {</span>
 		if (work-&gt;func != func)
 			pprev = &amp;work-&gt;next;
 		else if (cmpxchg(pprev, work, work-&gt;next) == work)
<span class="p_header">diff --git a/kernel/trace/bpf_trace.c b/kernel/trace/bpf_trace.c</span>
<span class="p_header">index dc498b605d5d..6350f64d5aa4 100644</span>
<span class="p_header">--- a/kernel/trace/bpf_trace.c</span>
<span class="p_header">+++ b/kernel/trace/bpf_trace.c</span>
<span class="p_chunk">@@ -293,14 +293,13 @@</span> <span class="p_context"> static const struct bpf_func_proto bpf_perf_event_read_proto = {</span>
 	.arg2_type	= ARG_ANYTHING,
 };
 
<span class="p_del">-static DEFINE_PER_CPU(struct perf_sample_data, bpf_sd);</span>
<span class="p_add">+static DEFINE_PER_CPU(struct perf_sample_data, bpf_trace_sd);</span>
 
 static __always_inline u64
 __bpf_perf_event_output(struct pt_regs *regs, struct bpf_map *map,
<span class="p_del">-			u64 flags, struct perf_raw_record *raw)</span>
<span class="p_add">+			u64 flags, struct perf_sample_data *sd)</span>
 {
 	struct bpf_array *array = container_of(map, struct bpf_array, map);
<span class="p_del">-	struct perf_sample_data *sd = this_cpu_ptr(&amp;bpf_sd);</span>
 	unsigned int cpu = smp_processor_id();
 	u64 index = flags &amp; BPF_F_INDEX_MASK;
 	struct bpf_event_entry *ee;
<span class="p_chunk">@@ -323,8 +322,6 @@</span> <span class="p_context"> __bpf_perf_event_output(struct pt_regs *regs, struct bpf_map *map,</span>
 	if (unlikely(event-&gt;oncpu != cpu))
 		return -EOPNOTSUPP;
 
<span class="p_del">-	perf_sample_data_init(sd, 0, 0);</span>
<span class="p_del">-	sd-&gt;raw = raw;</span>
 	perf_event_output(event, sd, regs);
 	return 0;
 }
<span class="p_chunk">@@ -332,6 +329,7 @@</span> <span class="p_context"> __bpf_perf_event_output(struct pt_regs *regs, struct bpf_map *map,</span>
 BPF_CALL_5(bpf_perf_event_output, struct pt_regs *, regs, struct bpf_map *, map,
 	   u64, flags, void *, data, u64, size)
 {
<span class="p_add">+	struct perf_sample_data *sd = this_cpu_ptr(&amp;bpf_trace_sd);</span>
 	struct perf_raw_record raw = {
 		.frag = {
 			.size = size,
<span class="p_chunk">@@ -342,7 +340,10 @@</span> <span class="p_context"> BPF_CALL_5(bpf_perf_event_output, struct pt_regs *, regs, struct bpf_map *, map,</span>
 	if (unlikely(flags &amp; ~(BPF_F_INDEX_MASK)))
 		return -EINVAL;
 
<span class="p_del">-	return __bpf_perf_event_output(regs, map, flags, &amp;raw);</span>
<span class="p_add">+	perf_sample_data_init(sd, 0, 0);</span>
<span class="p_add">+	sd-&gt;raw = &amp;raw;</span>
<span class="p_add">+</span>
<span class="p_add">+	return __bpf_perf_event_output(regs, map, flags, sd);</span>
 }
 
 static const struct bpf_func_proto bpf_perf_event_output_proto = {
<span class="p_chunk">@@ -357,10 +358,12 @@</span> <span class="p_context"> static const struct bpf_func_proto bpf_perf_event_output_proto = {</span>
 };
 
 static DEFINE_PER_CPU(struct pt_regs, bpf_pt_regs);
<span class="p_add">+static DEFINE_PER_CPU(struct perf_sample_data, bpf_misc_sd);</span>
 
 u64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,
 		     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy)
 {
<span class="p_add">+	struct perf_sample_data *sd = this_cpu_ptr(&amp;bpf_misc_sd);</span>
 	struct pt_regs *regs = this_cpu_ptr(&amp;bpf_pt_regs);
 	struct perf_raw_frag frag = {
 		.copy		= ctx_copy,
<span class="p_chunk">@@ -378,8 +381,10 @@</span> <span class="p_context"> u64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,</span>
 	};
 
 	perf_fetch_caller_regs(regs);
<span class="p_add">+	perf_sample_data_init(sd, 0, 0);</span>
<span class="p_add">+	sd-&gt;raw = &amp;raw;</span>
 
<span class="p_del">-	return __bpf_perf_event_output(regs, map, flags, &amp;raw);</span>
<span class="p_add">+	return __bpf_perf_event_output(regs, map, flags, sd);</span>
 }
 
 BPF_CALL_0(bpf_get_current_task)
<span class="p_header">diff --git a/kernel/trace/trace_events_hist.c b/kernel/trace/trace_events_hist.c</span>
<span class="p_header">index 1c21d0e2a145..7eb975a2d0e1 100644</span>
<span class="p_header">--- a/kernel/trace/trace_events_hist.c</span>
<span class="p_header">+++ b/kernel/trace/trace_events_hist.c</span>
<span class="p_chunk">@@ -450,7 +450,7 @@</span> <span class="p_context"> static int create_val_field(struct hist_trigger_data *hist_data,</span>
 	}
 
 	field = trace_find_event_field(file-&gt;event_call, field_name);
<span class="p_del">-	if (!field) {</span>
<span class="p_add">+	if (!field || !field-&gt;size) {</span>
 		ret = -EINVAL;
 		goto out;
 	}
<span class="p_chunk">@@ -548,7 +548,7 @@</span> <span class="p_context"> static int create_key_field(struct hist_trigger_data *hist_data,</span>
 		}
 
 		field = trace_find_event_field(file-&gt;event_call, field_name);
<span class="p_del">-		if (!field) {</span>
<span class="p_add">+		if (!field || !field-&gt;size) {</span>
 			ret = -EINVAL;
 			goto out;
 		}
<span class="p_header">diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug</span>
<span class="p_header">index dfdad67d8f6c..ff21b4dbb392 100644</span>
<span class="p_header">--- a/lib/Kconfig.debug</span>
<span class="p_header">+++ b/lib/Kconfig.debug</span>
<span class="p_chunk">@@ -376,7 +376,7 @@</span> <span class="p_context"> config STACK_VALIDATION</span>
 	  that runtime stack traces are more reliable.
 
 	  This is also a prerequisite for generation of ORC unwind data, which
<span class="p_del">-	  is needed for CONFIG_ORC_UNWINDER.</span>
<span class="p_add">+	  is needed for CONFIG_UNWINDER_ORC.</span>
 
 	  For more information, see
 	  tools/objtool/Documentation/stack-validation.txt.
<span class="p_header">diff --git a/mm/slab.h b/mm/slab.h</span>
<span class="p_header">index 028cdc7df67e..86d7c7d860f9 100644</span>
<span class="p_header">--- a/mm/slab.h</span>
<span class="p_header">+++ b/mm/slab.h</span>
<span class="p_chunk">@@ -259,7 +259,7 @@</span> <span class="p_context"> cache_from_memcg_idx(struct kmem_cache *s, int idx)</span>
 	 * memcg_caches issues a write barrier to match this (see
 	 * memcg_create_kmem_cache()).
 	 */
<span class="p_del">-	cachep = lockless_dereference(arr-&gt;entries[idx]);</span>
<span class="p_add">+	cachep = READ_ONCE(arr-&gt;entries[idx]);</span>
 	rcu_read_unlock();
 
 	return cachep;
<span class="p_header">diff --git a/mm/sparse.c b/mm/sparse.c</span>
<span class="p_header">index 4900707ae146..60805abf98af 100644</span>
<span class="p_header">--- a/mm/sparse.c</span>
<span class="p_header">+++ b/mm/sparse.c</span>
<span class="p_chunk">@@ -23,8 +23,7 @@</span> <span class="p_context"></span>
  * 1) mem_section	- memory sections, mem_map&#39;s for valid memory
  */
 #ifdef CONFIG_SPARSEMEM_EXTREME
<span class="p_del">-struct mem_section *mem_section[NR_SECTION_ROOTS]</span>
<span class="p_del">-	____cacheline_internodealigned_in_smp;</span>
<span class="p_add">+struct mem_section **mem_section;</span>
 #else
 struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]
 	____cacheline_internodealigned_in_smp;
<span class="p_chunk">@@ -101,7 +100,7 @@</span> <span class="p_context"> static inline int sparse_index_init(unsigned long section_nr, int nid)</span>
 int __section_nr(struct mem_section* ms)
 {
 	unsigned long root_nr;
<span class="p_del">-	struct mem_section* root;</span>
<span class="p_add">+	struct mem_section *root = NULL;</span>
 
 	for (root_nr = 0; root_nr &lt; NR_SECTION_ROOTS; root_nr++) {
 		root = __nr_to_section(root_nr * SECTIONS_PER_ROOT);
<span class="p_chunk">@@ -112,7 +111,7 @@</span> <span class="p_context"> int __section_nr(struct mem_section* ms)</span>
 		     break;
 	}
 
<span class="p_del">-	VM_BUG_ON(root_nr == NR_SECTION_ROOTS);</span>
<span class="p_add">+	VM_BUG_ON(!root);</span>
 
 	return (root_nr * SECTIONS_PER_ROOT) + (ms - root);
 }
<span class="p_chunk">@@ -208,6 +207,16 @@</span> <span class="p_context"> void __init memory_present(int nid, unsigned long start, unsigned long end)</span>
 {
 	unsigned long pfn;
 
<span class="p_add">+#ifdef CONFIG_SPARSEMEM_EXTREME</span>
<span class="p_add">+	if (unlikely(!mem_section)) {</span>
<span class="p_add">+		unsigned long size, align;</span>
<span class="p_add">+</span>
<span class="p_add">+		size = sizeof(struct mem_section) * NR_SECTION_ROOTS;</span>
<span class="p_add">+		align = 1 &lt;&lt; (INTERNODE_CACHE_SHIFT);</span>
<span class="p_add">+		mem_section = memblock_virt_alloc(size, align);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	start &amp;= PAGE_SECTION_MASK;
 	mminit_validate_memmodel_limits(&amp;start, &amp;end);
 	for (pfn = start; pfn &lt; end; pfn += PAGES_PER_SECTION) {
<span class="p_chunk">@@ -330,11 +339,17 @@</span> <span class="p_context"> sparse_early_usemaps_alloc_pgdat_section(struct pglist_data *pgdat,</span>
 static void __init check_usemap_section_nr(int nid, unsigned long *usemap)
 {
 	unsigned long usemap_snr, pgdat_snr;
<span class="p_del">-	static unsigned long old_usemap_snr = NR_MEM_SECTIONS;</span>
<span class="p_del">-	static unsigned long old_pgdat_snr = NR_MEM_SECTIONS;</span>
<span class="p_add">+	static unsigned long old_usemap_snr;</span>
<span class="p_add">+	static unsigned long old_pgdat_snr;</span>
 	struct pglist_data *pgdat = NODE_DATA(nid);
 	int usemap_nid;
 
<span class="p_add">+	/* First call */</span>
<span class="p_add">+	if (!old_usemap_snr) {</span>
<span class="p_add">+		old_usemap_snr = NR_MEM_SECTIONS;</span>
<span class="p_add">+		old_pgdat_snr = NR_MEM_SECTIONS;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	usemap_snr = pfn_to_section_nr(__pa(usemap) &gt;&gt; PAGE_SHIFT);
 	pgdat_snr = pfn_to_section_nr(__pa(pgdat) &gt;&gt; PAGE_SHIFT);
 	if (usemap_snr == pgdat_snr)
<span class="p_header">diff --git a/net/ipv4/ip_gre.c b/net/ipv4/ip_gre.c</span>
<span class="p_header">index 467e44d7587d..045331204097 100644</span>
<span class="p_header">--- a/net/ipv4/ip_gre.c</span>
<span class="p_header">+++ b/net/ipv4/ip_gre.c</span>
<span class="p_chunk">@@ -579,8 +579,8 @@</span> <span class="p_context"> static void erspan_fb_xmit(struct sk_buff *skb, struct net_device *dev,</span>
 	if (gre_handle_offloads(skb, false))
 		goto err_free_rt;
 
<span class="p_del">-	if (skb-&gt;len &gt; dev-&gt;mtu) {</span>
<span class="p_del">-		pskb_trim(skb, dev-&gt;mtu);</span>
<span class="p_add">+	if (skb-&gt;len &gt; dev-&gt;mtu + dev-&gt;hard_header_len) {</span>
<span class="p_add">+		pskb_trim(skb, dev-&gt;mtu + dev-&gt;hard_header_len);</span>
 		truncate = true;
 	}
 
<span class="p_chunk">@@ -731,8 +731,8 @@</span> <span class="p_context"> static netdev_tx_t erspan_xmit(struct sk_buff *skb,</span>
 	if (skb_cow_head(skb, dev-&gt;needed_headroom))
 		goto free_skb;
 
<span class="p_del">-	if (skb-&gt;len - dev-&gt;hard_header_len &gt; dev-&gt;mtu) {</span>
<span class="p_del">-		pskb_trim(skb, dev-&gt;mtu);</span>
<span class="p_add">+	if (skb-&gt;len &gt; dev-&gt;mtu + dev-&gt;hard_header_len) {</span>
<span class="p_add">+		pskb_trim(skb, dev-&gt;mtu + dev-&gt;hard_header_len);</span>
 		truncate = true;
 	}
 
<span class="p_header">diff --git a/net/ipv4/tcp_vegas.c b/net/ipv4/tcp_vegas.c</span>
<span class="p_header">index 218cfcc77650..ee113ff15fd0 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_vegas.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_vegas.c</span>
<span class="p_chunk">@@ -158,7 +158,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(tcp_vegas_cwnd_event);</span>
 
 static inline u32 tcp_vegas_ssthresh(struct tcp_sock *tp)
 {
<span class="p_del">-	return  min(tp-&gt;snd_ssthresh, tp-&gt;snd_cwnd-1);</span>
<span class="p_add">+	return  min(tp-&gt;snd_ssthresh, tp-&gt;snd_cwnd);</span>
 }
 
 static void tcp_vegas_cong_avoid(struct sock *sk, u32 ack, u32 acked)
<span class="p_header">diff --git a/net/ipv6/addrconf.c b/net/ipv6/addrconf.c</span>
<span class="p_header">index 8a1c846d3df9..2ec39404c449 100644</span>
<span class="p_header">--- a/net/ipv6/addrconf.c</span>
<span class="p_header">+++ b/net/ipv6/addrconf.c</span>
<span class="p_chunk">@@ -303,10 +303,10 @@</span> <span class="p_context"> static struct ipv6_devconf ipv6_devconf_dflt __read_mostly = {</span>
 	.disable_policy		= 0,
 };
 
<span class="p_del">-/* Check if a valid qdisc is available */</span>
<span class="p_del">-static inline bool addrconf_qdisc_ok(const struct net_device *dev)</span>
<span class="p_add">+/* Check if link is ready: is it up and is a valid qdisc available */</span>
<span class="p_add">+static inline bool addrconf_link_ready(const struct net_device *dev)</span>
 {
<span class="p_del">-	return !qdisc_tx_is_noop(dev);</span>
<span class="p_add">+	return netif_oper_up(dev) &amp;&amp; !qdisc_tx_is_noop(dev);</span>
 }
 
 static void addrconf_del_rs_timer(struct inet6_dev *idev)
<span class="p_chunk">@@ -451,7 +451,7 @@</span> <span class="p_context"> static struct inet6_dev *ipv6_add_dev(struct net_device *dev)</span>
 
 	ndev-&gt;token = in6addr_any;
 
<span class="p_del">-	if (netif_running(dev) &amp;&amp; addrconf_qdisc_ok(dev))</span>
<span class="p_add">+	if (netif_running(dev) &amp;&amp; addrconf_link_ready(dev))</span>
 		ndev-&gt;if_flags |= IF_READY;
 
 	ipv6_mc_init_dev(ndev);
<span class="p_chunk">@@ -3404,7 +3404,7 @@</span> <span class="p_context"> static int addrconf_notify(struct notifier_block *this, unsigned long event,</span>
 			/* restore routes for permanent addresses */
 			addrconf_permanent_addr(dev);
 
<span class="p_del">-			if (!addrconf_qdisc_ok(dev)) {</span>
<span class="p_add">+			if (!addrconf_link_ready(dev)) {</span>
 				/* device is not ready yet. */
 				pr_info(&quot;ADDRCONF(NETDEV_UP): %s: link is not ready\n&quot;,
 					dev-&gt;name);
<span class="p_chunk">@@ -3419,7 +3419,7 @@</span> <span class="p_context"> static int addrconf_notify(struct notifier_block *this, unsigned long event,</span>
 				run_pending = 1;
 			}
 		} else if (event == NETDEV_CHANGE) {
<span class="p_del">-			if (!addrconf_qdisc_ok(dev)) {</span>
<span class="p_add">+			if (!addrconf_link_ready(dev)) {</span>
 				/* device is still not ready. */
 				break;
 			}
<span class="p_header">diff --git a/net/ipv6/route.c b/net/ipv6/route.c</span>
<span class="p_header">index 598efa8cfe25..76b47682f77f 100644</span>
<span class="p_header">--- a/net/ipv6/route.c</span>
<span class="p_header">+++ b/net/ipv6/route.c</span>
<span class="p_chunk">@@ -1055,7 +1055,6 @@</span> <span class="p_context"> static struct rt6_info *rt6_get_pcpu_route(struct rt6_info *rt)</span>
 
 static struct rt6_info *rt6_make_pcpu_route(struct rt6_info *rt)
 {
<span class="p_del">-	struct fib6_table *table = rt-&gt;rt6i_table;</span>
 	struct rt6_info *pcpu_rt, *prev, **p;
 
 	pcpu_rt = ip6_rt_pcpu_alloc(rt);
<span class="p_chunk">@@ -1066,28 +1065,20 @@</span> <span class="p_context"> static struct rt6_info *rt6_make_pcpu_route(struct rt6_info *rt)</span>
 		return net-&gt;ipv6.ip6_null_entry;
 	}
 
<span class="p_del">-	read_lock_bh(&amp;table-&gt;tb6_lock);</span>
<span class="p_del">-	if (rt-&gt;rt6i_pcpu) {</span>
<span class="p_del">-		p = this_cpu_ptr(rt-&gt;rt6i_pcpu);</span>
<span class="p_del">-		prev = cmpxchg(p, NULL, pcpu_rt);</span>
<span class="p_del">-		if (prev) {</span>
<span class="p_del">-			/* If someone did it before us, return prev instead */</span>
<span class="p_del">-			dst_release_immediate(&amp;pcpu_rt-&gt;dst);</span>
<span class="p_del">-			pcpu_rt = prev;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/* rt has been removed from the fib6 tree</span>
<span class="p_del">-		 * before we have a chance to acquire the read_lock.</span>
<span class="p_del">-		 * In this case, don&#39;t brother to create a pcpu rt</span>
<span class="p_del">-		 * since rt is going away anyway.  The next</span>
<span class="p_del">-		 * dst_check() will trigger a re-lookup.</span>
<span class="p_del">-		 */</span>
<span class="p_add">+	dst_hold(&amp;pcpu_rt-&gt;dst);</span>
<span class="p_add">+	p = this_cpu_ptr(rt-&gt;rt6i_pcpu);</span>
<span class="p_add">+	prev = cmpxchg(p, NULL, pcpu_rt);</span>
<span class="p_add">+	if (prev) {</span>
<span class="p_add">+		/* If someone did it before us, return prev instead */</span>
<span class="p_add">+		/* release refcnt taken by ip6_rt_pcpu_alloc() */</span>
 		dst_release_immediate(&amp;pcpu_rt-&gt;dst);
<span class="p_del">-		pcpu_rt = rt;</span>
<span class="p_add">+		/* release refcnt taken by above dst_hold() */</span>
<span class="p_add">+		dst_release_immediate(&amp;pcpu_rt-&gt;dst);</span>
<span class="p_add">+		dst_hold(&amp;prev-&gt;dst);</span>
<span class="p_add">+		pcpu_rt = prev;</span>
 	}
<span class="p_del">-	dst_hold(&amp;pcpu_rt-&gt;dst);</span>
<span class="p_add">+</span>
 	rt6_dst_from_metrics_check(pcpu_rt);
<span class="p_del">-	read_unlock_bh(&amp;table-&gt;tb6_lock);</span>
 	return pcpu_rt;
 }
 
<span class="p_chunk">@@ -1177,19 +1168,28 @@</span> <span class="p_context"> struct rt6_info *ip6_pol_route(struct net *net, struct fib6_table *table,</span>
 		if (pcpu_rt) {
 			read_unlock_bh(&amp;table-&gt;tb6_lock);
 		} else {
<span class="p_del">-			/* We have to do the read_unlock first</span>
<span class="p_del">-			 * because rt6_make_pcpu_route() may trigger</span>
<span class="p_del">-			 * ip6_dst_gc() which will take the write_lock.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			dst_hold(&amp;rt-&gt;dst);</span>
<span class="p_del">-			read_unlock_bh(&amp;table-&gt;tb6_lock);</span>
<span class="p_del">-			pcpu_rt = rt6_make_pcpu_route(rt);</span>
<span class="p_del">-			dst_release(&amp;rt-&gt;dst);</span>
<span class="p_add">+			/* atomic_inc_not_zero() is needed when using rcu */</span>
<span class="p_add">+			if (atomic_inc_not_zero(&amp;rt-&gt;rt6i_ref)) {</span>
<span class="p_add">+				/* We have to do the read_unlock first</span>
<span class="p_add">+				 * because rt6_make_pcpu_route() may trigger</span>
<span class="p_add">+				 * ip6_dst_gc() which will take the write_lock.</span>
<span class="p_add">+				 *</span>
<span class="p_add">+				 * No dst_hold() on rt is needed because grabbing</span>
<span class="p_add">+				 * rt-&gt;rt6i_ref makes sure rt can&#39;t be released.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				read_unlock_bh(&amp;table-&gt;tb6_lock);</span>
<span class="p_add">+				pcpu_rt = rt6_make_pcpu_route(rt);</span>
<span class="p_add">+				rt6_release(rt);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/* rt is already removed from tree */</span>
<span class="p_add">+				read_unlock_bh(&amp;table-&gt;tb6_lock);</span>
<span class="p_add">+				pcpu_rt = net-&gt;ipv6.ip6_null_entry;</span>
<span class="p_add">+				dst_hold(&amp;pcpu_rt-&gt;dst);</span>
<span class="p_add">+			}</span>
 		}
 
 		trace_fib6_table_lookup(net, pcpu_rt, table-&gt;tb6_id, fl6);
 		return pcpu_rt;
<span class="p_del">-</span>
 	}
 }
 EXPORT_SYMBOL_GPL(ip6_pol_route);
<span class="p_header">diff --git a/net/sctp/stream.c b/net/sctp/stream.c</span>
<span class="p_header">index fa8371ff05c4..724adf2786a2 100644</span>
<span class="p_header">--- a/net/sctp/stream.c</span>
<span class="p_header">+++ b/net/sctp/stream.c</span>
<span class="p_chunk">@@ -40,9 +40,14 @@</span> <span class="p_context"> int sctp_stream_init(struct sctp_stream *stream, __u16 outcnt, __u16 incnt,</span>
 {
 	int i;
 
<span class="p_add">+	gfp |= __GFP_NOWARN;</span>
<span class="p_add">+</span>
 	/* Initial stream-&gt;out size may be very big, so free it and alloc
<span class="p_del">-	 * a new one with new outcnt to save memory.</span>
<span class="p_add">+	 * a new one with new outcnt to save memory if needed.</span>
 	 */
<span class="p_add">+	if (outcnt == stream-&gt;outcnt)</span>
<span class="p_add">+		goto in;</span>
<span class="p_add">+</span>
 	kfree(stream-&gt;out);
 
 	stream-&gt;out = kcalloc(outcnt, sizeof(*stream-&gt;out), gfp);
<span class="p_chunk">@@ -53,6 +58,7 @@</span> <span class="p_context"> int sctp_stream_init(struct sctp_stream *stream, __u16 outcnt, __u16 incnt,</span>
 	for (i = 0; i &lt; stream-&gt;outcnt; i++)
 		stream-&gt;out[i].state = SCTP_STREAM_OPEN;
 
<span class="p_add">+in:</span>
 	if (!incnt)
 		return 0;
 
<span class="p_header">diff --git a/scripts/Makefile.build b/scripts/Makefile.build</span>
<span class="p_header">index bb831d49bcfd..e63af4e19382 100644</span>
<span class="p_header">--- a/scripts/Makefile.build</span>
<span class="p_header">+++ b/scripts/Makefile.build</span>
<span class="p_chunk">@@ -259,7 +259,7 @@</span> <span class="p_context"> ifneq ($(SKIP_STACK_VALIDATION),1)</span>
 
 __objtool_obj := $(objtree)/tools/objtool/objtool
 
<span class="p_del">-objtool_args = $(if $(CONFIG_ORC_UNWINDER),orc generate,check)</span>
<span class="p_add">+objtool_args = $(if $(CONFIG_UNWINDER_ORC),orc generate,check)</span>
 
 ifndef CONFIG_FRAME_POINTER
 objtool_args += --no-fp
<span class="p_header">diff --git a/scripts/headers_install.sh b/scripts/headers_install.sh</span>
<span class="p_header">index 4d1ea96e8794..a18bca720995 100755</span>
<span class="p_header">--- a/scripts/headers_install.sh</span>
<span class="p_header">+++ b/scripts/headers_install.sh</span>
<span class="p_chunk">@@ -34,7 +34,7 @@</span> <span class="p_context"> do</span>
 	sed -r \
 		-e &#39;s/([ \t(])(__user|__force|__iomem)[ \t]/\1/g&#39; \
 		-e &#39;s/__attribute_const__([ \t]|$)/\1/g&#39; \
<span class="p_del">-		-e &#39;s@^#include &lt;linux/compiler.h&gt;@@&#39; \</span>
<span class="p_add">+		-e &#39;s@^#include &lt;linux/compiler(|_types).h&gt;@@&#39; \</span>
 		-e &#39;s/(^|[^a-zA-Z0-9])__packed([^a-zA-Z0-9_]|$)/\1__attribute__((packed))\2/g&#39; \
 		-e &#39;s/(^|[ \t(])(inline|asm|volatile)([ \t(]|$)/\1__\2__\3/g&#39; \
 		-e &#39;s@#(ifndef|define|endif[ \t]*/[*])[ \t]*_UAPI@#\1 @&#39; \
<span class="p_header">diff --git a/sound/soc/codecs/msm8916-wcd-analog.c b/sound/soc/codecs/msm8916-wcd-analog.c</span>
<span class="p_header">index 549c269acc7d..18933bf6473f 100644</span>
<span class="p_header">--- a/sound/soc/codecs/msm8916-wcd-analog.c</span>
<span class="p_header">+++ b/sound/soc/codecs/msm8916-wcd-analog.c</span>
<span class="p_chunk">@@ -104,7 +104,7 @@</span> <span class="p_context"></span>
 #define CDC_A_MICB_1_VAL		(0xf141)
 #define MICB_MIN_VAL 1600
 #define MICB_STEP_SIZE 50
<span class="p_del">-#define MICB_VOLTAGE_REGVAL(v)		((v - MICB_MIN_VAL)/MICB_STEP_SIZE)</span>
<span class="p_add">+#define MICB_VOLTAGE_REGVAL(v)		(((v - MICB_MIN_VAL)/MICB_STEP_SIZE) &lt;&lt; 3)</span>
 #define MICB_1_VAL_MICB_OUT_VAL_MASK	GENMASK(7, 3)
 #define MICB_1_VAL_MICB_OUT_VAL_V2P70V	((0x16)  &lt;&lt; 3)
 #define MICB_1_VAL_MICB_OUT_VAL_V1P80V	((0x4)  &lt;&lt; 3)
<span class="p_chunk">@@ -349,8 +349,9 @@</span> <span class="p_context"> static void pm8916_wcd_analog_micbias_enable(struct snd_soc_codec *codec)</span>
 			    | MICB_1_CTL_EXT_PRECHARG_EN_ENABLE);
 
 	if (wcd-&gt;micbias_mv) {
<span class="p_del">-		snd_soc_write(codec, CDC_A_MICB_1_VAL,</span>
<span class="p_del">-			      MICB_VOLTAGE_REGVAL(wcd-&gt;micbias_mv));</span>
<span class="p_add">+		snd_soc_update_bits(codec, CDC_A_MICB_1_VAL,</span>
<span class="p_add">+				    MICB_1_VAL_MICB_OUT_VAL_MASK,</span>
<span class="p_add">+				    MICB_VOLTAGE_REGVAL(wcd-&gt;micbias_mv));</span>
 		/*
 		 * Special headset needs MICBIAS as 2.7V so wait for
 		 * 50 msec for the MICBIAS to reach 2.7 volts.
<span class="p_chunk">@@ -1241,6 +1242,8 @@</span> <span class="p_context"> static const struct of_device_id pm8916_wcd_analog_spmi_match_table[] = {</span>
 	{ }
 };
 
<span class="p_add">+MODULE_DEVICE_TABLE(of, pm8916_wcd_analog_spmi_match_table);</span>
<span class="p_add">+</span>
 static struct platform_driver pm8916_wcd_analog_spmi_driver = {
 	.driver = {
 		   .name = &quot;qcom,pm8916-wcd-spmi-codec&quot;,
<span class="p_header">diff --git a/sound/soc/img/img-parallel-out.c b/sound/soc/img/img-parallel-out.c</span>
<span class="p_header">index 23b0f0f6ec9c..2fc8a6372206 100644</span>
<span class="p_header">--- a/sound/soc/img/img-parallel-out.c</span>
<span class="p_header">+++ b/sound/soc/img/img-parallel-out.c</span>
<span class="p_chunk">@@ -164,9 +164,11 @@</span> <span class="p_context"> static int img_prl_out_set_fmt(struct snd_soc_dai *dai, unsigned int fmt)</span>
 		return -EINVAL;
 	}
 
<span class="p_add">+	pm_runtime_get_sync(prl-&gt;dev);</span>
 	reg = img_prl_out_readl(prl, IMG_PRL_OUT_CTL);
 	reg = (reg &amp; ~IMG_PRL_OUT_CTL_EDGE_MASK) | control_set;
 	img_prl_out_writel(prl, reg, IMG_PRL_OUT_CTL);
<span class="p_add">+	pm_runtime_put(prl-&gt;dev);</span>
 
 	return 0;
 }
<span class="p_header">diff --git a/tools/objtool/check.c b/tools/objtool/check.c</span>
<span class="p_header">index c0e26ad1fa7e..9b341584eb1b 100644</span>
<span class="p_header">--- a/tools/objtool/check.c</span>
<span class="p_header">+++ b/tools/objtool/check.c</span>
<span class="p_chunk">@@ -1757,11 +1757,14 @@</span> <span class="p_context"> static int validate_branch(struct objtool_file *file, struct instruction *first,</span>
 		if (insn-&gt;dead_end)
 			return 0;
 
<span class="p_del">-		insn = next_insn;</span>
<span class="p_del">-		if (!insn) {</span>
<span class="p_add">+		if (!next_insn) {</span>
<span class="p_add">+			if (state.cfa.base == CFI_UNDEFINED)</span>
<span class="p_add">+				return 0;</span>
 			WARN(&quot;%s: unexpected end of section&quot;, sec-&gt;name);
 			return 1;
 		}
<span class="p_add">+</span>
<span class="p_add">+		insn = next_insn;</span>
 	}
 
 	return 0;
<span class="p_header">diff --git a/tools/objtool/objtool.c b/tools/objtool/objtool.c</span>
<span class="p_header">index 31e0f9143840..07f329919828 100644</span>
<span class="p_header">--- a/tools/objtool/objtool.c</span>
<span class="p_header">+++ b/tools/objtool/objtool.c</span>
<span class="p_chunk">@@ -70,7 +70,7 @@</span> <span class="p_context"> static void cmd_usage(void)</span>
 
 	printf(&quot;\n&quot;);
 
<span class="p_del">-	exit(1);</span>
<span class="p_add">+	exit(129);</span>
 }
 
 static void handle_options(int *argc, const char ***argv)
<span class="p_chunk">@@ -86,9 +86,7 @@</span> <span class="p_context"> static void handle_options(int *argc, const char ***argv)</span>
 			break;
 		} else {
 			fprintf(stderr, &quot;Unknown option: %s\n&quot;, cmd);
<span class="p_del">-			fprintf(stderr, &quot;\n Usage: %s\n&quot;,</span>
<span class="p_del">-				objtool_usage_string);</span>
<span class="p_del">-			exit(1);</span>
<span class="p_add">+			cmd_usage();</span>
 		}
 
 		(*argv)++;
<span class="p_header">diff --git a/tools/testing/selftests/bpf/test_verifier.c b/tools/testing/selftests/bpf/test_verifier.c</span>
<span class="p_header">index 64ae21f64489..7a2d221c4702 100644</span>
<span class="p_header">--- a/tools/testing/selftests/bpf/test_verifier.c</span>
<span class="p_header">+++ b/tools/testing/selftests/bpf/test_verifier.c</span>
<span class="p_chunk">@@ -606,7 +606,6 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 		},
 		.errstr = &quot;misaligned stack access&quot;,
 		.result = REJECT,
<span class="p_del">-		.flags = F_LOAD_WITH_STRICT_ALIGNMENT,</span>
 	},
 	{
 		&quot;invalid map_fd for function call&quot;,
<span class="p_chunk">@@ -1797,7 +1796,6 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 		},
 		.result = REJECT,
 		.errstr = &quot;misaligned stack access off (0x0; 0x0)+-8+2 size 8&quot;,
<span class="p_del">-		.flags = F_LOAD_WITH_STRICT_ALIGNMENT,</span>
 	},
 	{
 		&quot;PTR_TO_STACK store/load - bad alignment on reg&quot;,
<span class="p_chunk">@@ -1810,7 +1808,6 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 		},
 		.result = REJECT,
 		.errstr = &quot;misaligned stack access off (0x0; 0x0)+-10+8 size 8&quot;,
<span class="p_del">-		.flags = F_LOAD_WITH_STRICT_ALIGNMENT,</span>
 	},
 	{
 		&quot;PTR_TO_STACK store/load - out of bounds low&quot;,
<span class="p_chunk">@@ -6115,7 +6112,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6139,7 +6136,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6165,7 +6162,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R8 invalid mem access &#39;inv&#39;&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6190,7 +6187,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R8 invalid mem access &#39;inv&#39;&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6238,7 +6235,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6309,7 +6306,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6360,7 +6357,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6387,7 +6384,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6413,7 +6410,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6442,7 +6439,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6472,7 +6469,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_JMP_IMM(BPF_JA, 0, 0, -7),
 		},
 		.fixup_map1 = { 4 },
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 	},
 	{
<span class="p_chunk">@@ -6500,8 +6497,7 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 			BPF_EXIT_INSN(),
 		},
 		.fixup_map1 = { 3 },
<span class="p_del">-		.errstr_unpriv = &quot;R0 pointer comparison prohibited&quot;,</span>
<span class="p_del">-		.errstr = &quot;R0 min value is negative&quot;,</span>
<span class="p_add">+		.errstr = &quot;unbounded min value&quot;,</span>
 		.result = REJECT,
 		.result_unpriv = REJECT,
 	},
<span class="p_chunk">@@ -6556,6 +6552,462 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 		.errstr = &quot;R0 min value is negative, either use unsigned index or do a if (index &gt;=0) check.&quot;,
 		.result = REJECT,
 	},
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check based on zero-extended MOV&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 4),</span>
<span class="p_add">+			/* r2 = 0x0000&#39;0000&#39;ffff&#39;ffff */</span>
<span class="p_add">+			BPF_MOV32_IMM(BPF_REG_2, 0xffffffff),</span>
<span class="p_add">+			/* r2 = 0 */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_2, 32),</span>
<span class="p_add">+			/* no-op */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_2),</span>
<span class="p_add">+			/* access at offset 0 */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.result = ACCEPT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check based on sign-extended MOV. test1&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 4),</span>
<span class="p_add">+			/* r2 = 0xffff&#39;ffff&#39;ffff&#39;ffff */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_2, 0xffffffff),</span>
<span class="p_add">+			/* r2 = 0xffff&#39;ffff */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_2, 32),</span>
<span class="p_add">+			/* r0 = &lt;oob pointer&gt; */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_2),</span>
<span class="p_add">+			/* access to OOB pointer */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;map_value pointer and 4294967295&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check based on sign-extended MOV. test2&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 4),</span>
<span class="p_add">+			/* r2 = 0xffff&#39;ffff&#39;ffff&#39;ffff */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_2, 0xffffffff),</span>
<span class="p_add">+			/* r2 = 0xfff&#39;ffff */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_2, 36),</span>
<span class="p_add">+			/* r0 = &lt;oob pointer&gt; */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_2),</span>
<span class="p_add">+			/* access to OOB pointer */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;R0 min value is outside of the array range&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check based on reg_off + var_off + insn_off. test1&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_W, BPF_REG_6, BPF_REG_1,</span>
<span class="p_add">+				    offsetof(struct __sk_buff, mark)),</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 4),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_AND, BPF_REG_6, 1),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_6, (1 &lt;&lt; 29) - 1),</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_6),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, (1 &lt;&lt; 29) - 1),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 3),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 4 },</span>
<span class="p_add">+		.errstr = &quot;value_size=8 off=1073741825&quot;,</span>
<span class="p_add">+		.result = REJECT,</span>
<span class="p_add">+		.prog_type = BPF_PROG_TYPE_SCHED_CLS,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check based on reg_off + var_off + insn_off. test2&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_W, BPF_REG_6, BPF_REG_1,</span>
<span class="p_add">+				    offsetof(struct __sk_buff, mark)),</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 4),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_AND, BPF_REG_6, 1),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_6, (1 &lt;&lt; 30) - 1),</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_6),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, (1 &lt;&lt; 29) - 1),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 3),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 4 },</span>
<span class="p_add">+		.errstr = &quot;value 1073741823&quot;,</span>
<span class="p_add">+		.result = REJECT,</span>
<span class="p_add">+		.prog_type = BPF_PROG_TYPE_SCHED_CLS,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check after truncation of non-boundary-crossing range&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 9),</span>
<span class="p_add">+			/* r1 = [0x00, 0xff] */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_1, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_2, 1),</span>
<span class="p_add">+			/* r2 = 0x10&#39;0000&#39;0000 */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_LSH, BPF_REG_2, 36),</span>
<span class="p_add">+			/* r1 = [0x10&#39;0000&#39;0000, 0x10&#39;0000&#39;00ff] */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_1, BPF_REG_2),</span>
<span class="p_add">+			/* r1 = [0x10&#39;7fff&#39;ffff, 0x10&#39;8000&#39;00fe] */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x7fffffff),</span>
<span class="p_add">+			/* r1 = [0x00, 0xff] */</span>
<span class="p_add">+			BPF_ALU32_IMM(BPF_SUB, BPF_REG_1, 0x7fffffff),</span>
<span class="p_add">+			/* r1 = 0 */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_1, 8),</span>
<span class="p_add">+			/* no-op */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			/* access at offset 0 */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.result = ACCEPT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check after truncation of boundary-crossing range (1)&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 9),</span>
<span class="p_add">+			/* r1 = [0x00, 0xff] */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_1, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = [0xffff&#39;ff80, 0x1&#39;0000&#39;007f] */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = [0xffff&#39;ff80, 0xffff&#39;ffff] or</span>
<span class="p_add">+			 *      [0x0000&#39;0000, 0x0000&#39;007f]</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			BPF_ALU32_IMM(BPF_ADD, BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = [0x00, 0xff] or</span>
<span class="p_add">+			 *      [0xffff&#39;ffff&#39;0000&#39;0080, 0xffff&#39;ffff&#39;ffff&#39;ffff]</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = 0 or</span>
<span class="p_add">+			 *      [0x00ff&#39;ffff&#39;ff00&#39;0000, 0x00ff&#39;ffff&#39;ffff&#39;ffff]</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_1, 8),</span>
<span class="p_add">+			/* no-op or OOB pointer computation */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			/* potentially OOB access */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		/* not actually fully unbounded, but the bound is very high */</span>
<span class="p_add">+		.errstr = &quot;R0 unbounded memory access&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check after truncation of boundary-crossing range (2)&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 9),</span>
<span class="p_add">+			/* r1 = [0x00, 0xff] */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_1, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = [0xffff&#39;ff80, 0x1&#39;0000&#39;007f] */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = [0xffff&#39;ff80, 0xffff&#39;ffff] or</span>
<span class="p_add">+			 *      [0x0000&#39;0000, 0x0000&#39;007f]</span>
<span class="p_add">+			 * difference to previous test: truncation via MOV32</span>
<span class="p_add">+			 * instead of ALU32.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			BPF_MOV32_REG(BPF_REG_1, BPF_REG_1),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = [0x00, 0xff] or</span>
<span class="p_add">+			 *      [0xffff&#39;ffff&#39;0000&#39;0080, 0xffff&#39;ffff&#39;ffff&#39;ffff]</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_1, 0xffffff80 &gt;&gt; 1),</span>
<span class="p_add">+			/* r1 = 0 or</span>
<span class="p_add">+			 *      [0x00ff&#39;ffff&#39;ff00&#39;0000, 0x00ff&#39;ffff&#39;ffff&#39;ffff]</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_1, 8),</span>
<span class="p_add">+			/* no-op or OOB pointer computation */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			/* potentially OOB access */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		/* not actually fully unbounded, but the bound is very high */</span>
<span class="p_add">+		.errstr = &quot;R0 unbounded memory access&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check after wrapping 32-bit addition&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 5),</span>
<span class="p_add">+			/* r1 = 0x7fff&#39;ffff */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_1, 0x7fffffff),</span>
<span class="p_add">+			/* r1 = 0xffff&#39;fffe */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x7fffffff),</span>
<span class="p_add">+			/* r1 = 0 */</span>
<span class="p_add">+			BPF_ALU32_IMM(BPF_ADD, BPF_REG_1, 2),</span>
<span class="p_add">+			/* no-op */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			/* access at offset 0 */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.result = ACCEPT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check after shift with oversized count operand&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 6),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_2, 32),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_1, 1),</span>
<span class="p_add">+			/* r1 = (u32)1 &lt;&lt; (u32)32 = ? */</span>
<span class="p_add">+			BPF_ALU32_REG(BPF_LSH, BPF_REG_1, BPF_REG_2),</span>
<span class="p_add">+			/* r1 = [0x0000, 0xffff] */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_AND, BPF_REG_1, 0xffff),</span>
<span class="p_add">+			/* computes unknown pointer, potentially OOB */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			/* potentially OOB access */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;R0 max value is outside of the array range&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check after right shift of maybe-negative number&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 6),</span>
<span class="p_add">+			/* r1 = [0x00, 0xff] */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_1, BPF_REG_0, 0),</span>
<span class="p_add">+			/* r1 = [-0x01, 0xfe] */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_1, 1),</span>
<span class="p_add">+			/* r1 = 0 or 0xff&#39;ffff&#39;ffff&#39;ffff */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_1, 8),</span>
<span class="p_add">+			/* r1 = 0 or 0xffff&#39;ffff&#39;ffff */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_RSH, BPF_REG_1, 8),</span>
<span class="p_add">+			/* computes unknown pointer, potentially OOB */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			/* potentially OOB access */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			/* exit */</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;R0 unbounded memory access&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check map access with off+size signed 32bit overflow. test1&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, 0x7ffffffe),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_JMP_A(0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;map_value pointer and 2147483646&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check map access with off+size signed 32bit overflow. test2&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, 0x1fffffff),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, 0x1fffffff),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, 0x1fffffff),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_JMP_A(0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;pointer offset 1073741822&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check map access with off+size signed 32bit overflow. test3&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_0, 0x1fffffff),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_0, 0x1fffffff),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 2),</span>
<span class="p_add">+			BPF_JMP_A(0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;pointer offset -1073741822&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;bounds check map access with off+size signed 32bit overflow. test4&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_1, 1000000),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_MUL, BPF_REG_1, 1000000),</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 2),</span>
<span class="p_add">+			BPF_JMP_A(0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.errstr = &quot;map_value pointer and 1000000000000&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;pointer/scalar confusion in state equality check (way 1)&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 2),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_JMP_A(1),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_0, BPF_REG_10),</span>
<span class="p_add">+			BPF_JMP_A(0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.result = ACCEPT,</span>
<span class="p_add">+		.result_unpriv = REJECT,</span>
<span class="p_add">+		.errstr_unpriv = &quot;R0 leaks addr as return value&quot;</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;pointer/scalar confusion in state equality check (way 2)&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 2),</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_0, BPF_REG_10),</span>
<span class="p_add">+			BPF_JMP_A(1),</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 3 },</span>
<span class="p_add">+		.result = ACCEPT,</span>
<span class="p_add">+		.result_unpriv = REJECT,</span>
<span class="p_add">+		.errstr_unpriv = &quot;R0 leaks addr as return value&quot;</span>
<span class="p_add">+	},</span>
 	{
 		&quot;variable-offset ctx access&quot;,
 		.insns = {
<span class="p_chunk">@@ -6597,6 +7049,71 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 		.result = REJECT,
 		.prog_type = BPF_PROG_TYPE_LWT_IN,
 	},
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;indirect variable-offset stack access&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			/* Fill the top 8 bytes of the stack */</span>
<span class="p_add">+			BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),</span>
<span class="p_add">+			/* Get an unknown value */</span>
<span class="p_add">+			BPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_1, 0),</span>
<span class="p_add">+			/* Make it small and 4-byte aligned */</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_AND, BPF_REG_2, 4),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_SUB, BPF_REG_2, 8),</span>
<span class="p_add">+			/* add it to fp.  We now have either fp-4 or fp-8, but</span>
<span class="p_add">+			 * we don&#39;t know which</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ADD, BPF_REG_2, BPF_REG_10),</span>
<span class="p_add">+			/* dereference it indirectly */</span>
<span class="p_add">+			BPF_LD_MAP_FD(BPF_REG_1, 0),</span>
<span class="p_add">+			BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,</span>
<span class="p_add">+				     BPF_FUNC_map_lookup_elem),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.fixup_map1 = { 5 },</span>
<span class="p_add">+		.errstr = &quot;variable stack read R2&quot;,</span>
<span class="p_add">+		.result = REJECT,</span>
<span class="p_add">+		.prog_type = BPF_PROG_TYPE_LWT_IN,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;direct stack access with 32-bit wraparound. test1&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x7fffffff),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x7fffffff),</span>
<span class="p_add">+			BPF_MOV32_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN()</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.errstr = &quot;fp pointer and 2147483647&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;direct stack access with 32-bit wraparound. test2&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x3fffffff),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x3fffffff),</span>
<span class="p_add">+			BPF_MOV32_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN()</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.errstr = &quot;fp pointer and 1073741823&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;direct stack access with 32-bit wraparound. test3&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x1fffffff),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 0x1fffffff),</span>
<span class="p_add">+			BPF_MOV32_IMM(BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0, 0),</span>
<span class="p_add">+			BPF_EXIT_INSN()</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.errstr = &quot;fp pointer offset 1073741822&quot;,</span>
<span class="p_add">+		.result = REJECT</span>
<span class="p_add">+	},</span>
 	{
 		&quot;liveness pruning and write screening&quot;,
 		.insns = {
<span class="p_header">diff --git a/tools/testing/selftests/x86/ldt_gdt.c b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">index 2afc41a3730f..66e5ce5b91f0 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_chunk">@@ -137,30 +137,51 @@</span> <span class="p_context"> static void check_valid_segment(uint16_t index, int ldt,</span>
 	}
 }
 
<span class="p_del">-static bool install_valid_mode(const struct user_desc *desc, uint32_t ar,</span>
<span class="p_del">-			       bool oldmode)</span>
<span class="p_add">+static bool install_valid_mode(const struct user_desc *d, uint32_t ar,</span>
<span class="p_add">+			       bool oldmode, bool ldt)</span>
 {
<span class="p_del">-	int ret = syscall(SYS_modify_ldt, oldmode ? 1 : 0x11,</span>
<span class="p_del">-			  desc, sizeof(*desc));</span>
<span class="p_del">-	if (ret &lt; -1)</span>
<span class="p_del">-		errno = -ret;</span>
<span class="p_add">+	struct user_desc desc = *d;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!ldt) {</span>
<span class="p_add">+#ifndef __i386__</span>
<span class="p_add">+		/* No point testing set_thread_area in a 64-bit build */</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		if (!gdt_entry_num)</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		desc.entry_number = gdt_entry_num;</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = syscall(SYS_set_thread_area, &amp;desc);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		ret = syscall(SYS_modify_ldt, oldmode ? 1 : 0x11,</span>
<span class="p_add">+			      &amp;desc, sizeof(desc));</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ret &lt; -1)</span>
<span class="p_add">+			errno = -ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ret != 0 &amp;&amp; errno == ENOSYS) {</span>
<span class="p_add">+			printf(&quot;[OK]\tmodify_ldt returned -ENOSYS\n&quot;);</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (ret == 0) {
<span class="p_del">-		uint32_t limit = desc-&gt;limit;</span>
<span class="p_del">-		if (desc-&gt;limit_in_pages)</span>
<span class="p_add">+		uint32_t limit = desc.limit;</span>
<span class="p_add">+		if (desc.limit_in_pages)</span>
 			limit = (limit &lt;&lt; 12) + 4095;
<span class="p_del">-		check_valid_segment(desc-&gt;entry_number, 1, ar, limit, true);</span>
<span class="p_add">+		check_valid_segment(desc.entry_number, ldt, ar, limit, true);</span>
 		return true;
<span class="p_del">-	} else if (errno == ENOSYS) {</span>
<span class="p_del">-		printf(&quot;[OK]\tmodify_ldt returned -ENOSYS\n&quot;);</span>
<span class="p_del">-		return false;</span>
 	} else {
<span class="p_del">-		if (desc-&gt;seg_32bit) {</span>
<span class="p_del">-			printf(&quot;[FAIL]\tUnexpected modify_ldt failure %d\n&quot;,</span>
<span class="p_add">+		if (desc.seg_32bit) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tUnexpected %s failure %d\n&quot;,</span>
<span class="p_add">+			       ldt ? &quot;modify_ldt&quot; : &quot;set_thread_area&quot;,</span>
 			       errno);
 			nerrs++;
 			return false;
 		} else {
<span class="p_del">-			printf(&quot;[OK]\tmodify_ldt rejected 16 bit segment\n&quot;);</span>
<span class="p_add">+			printf(&quot;[OK]\t%s rejected 16 bit segment\n&quot;,</span>
<span class="p_add">+			       ldt ? &quot;modify_ldt&quot; : &quot;set_thread_area&quot;);</span>
 			return false;
 		}
 	}
<span class="p_chunk">@@ -168,7 +189,15 @@</span> <span class="p_context"> static bool install_valid_mode(const struct user_desc *desc, uint32_t ar,</span>
 
 static bool install_valid(const struct user_desc *desc, uint32_t ar)
 {
<span class="p_del">-	return install_valid_mode(desc, ar, false);</span>
<span class="p_add">+	bool ret = install_valid_mode(desc, ar, false, true);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (desc-&gt;contents &lt;= 1 &amp;&amp; desc-&gt;seg_32bit &amp;&amp;</span>
<span class="p_add">+	    !desc-&gt;seg_not_present) {</span>
<span class="p_add">+		/* Should work in the GDT, too. */</span>
<span class="p_add">+		install_valid_mode(desc, ar, false, false);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
 }
 
 static void install_invalid(const struct user_desc *desc, bool oldmode)
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 484e8820c382..2447d7c017e7 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -4018,7 +4018,7 @@</span> <span class="p_context"> int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,</span>
 	if (!vcpu_align)
 		vcpu_align = __alignof__(struct kvm_vcpu);
 	kvm_vcpu_cache = kmem_cache_create(&quot;kvm_vcpu&quot;, vcpu_size, vcpu_align,
<span class="p_del">-					   0, NULL);</span>
<span class="p_add">+					   SLAB_ACCOUNT, NULL);</span>
 	if (!kvm_vcpu_cache) {
 		r = -ENOMEM;
 		goto out_free_3;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



