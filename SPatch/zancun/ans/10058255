
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v3,for,4.15,08/24] Provide cpu_opv system call - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v3,for,4.15,08/24] Provide cpu_opv system call</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 14, 2017, 8:03 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171114200414.2188-9-mathieu.desnoyers@efficios.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10058255/mbox/"
   >mbox</a>
|
   <a href="/patch/10058255/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10058255/">/patch/10058255/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7450E601D3 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Nov 2017 20:11:03 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 64E6728CF7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Nov 2017 20:11:03 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 591B029969; Tue, 14 Nov 2017 20:11:03 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 10CEC29968
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Nov 2017 20:11:01 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932080AbdKNUK7 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 14 Nov 2017 15:10:59 -0500
Received: from mail.efficios.com ([167.114.142.141]:56655 &quot;EHLO
	mail.efficios.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1754839AbdKNUE6 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 14 Nov 2017 15:04:58 -0500
Received: from localhost (localhost.localdomain [127.0.0.1])
	by mail.efficios.com (Postfix) with ESMTP id A4B9734024F;
	Tue, 14 Nov 2017 20:05:42 +0000 (UTC)
Received: from mail.efficios.com ([127.0.0.1])
	by localhost (evm-mail-1.efficios.com [127.0.0.1]) (amavisd-new,
	port 10032)
	with ESMTP id qXHtL7RqLgq6; Tue, 14 Nov 2017 20:05:28 +0000 (UTC)
Received: from localhost (localhost.localdomain [127.0.0.1])
	by mail.efficios.com (Postfix) with ESMTP id 5DF50340199;
	Tue, 14 Nov 2017 20:05:19 +0000 (UTC)
X-Virus-Scanned: amavisd-new at efficios.com
Received: from mail.efficios.com ([127.0.0.1])
	by localhost (evm-mail-1.efficios.com [127.0.0.1]) (amavisd-new,
	port 10026)
	with ESMTP id uwP1ekhSsJnG; Tue, 14 Nov 2017 20:05:19 +0000 (UTC)
Received: from thinkos.internal.efficios.com
	(192-222-157-41.qc.cable.ebox.net [192.222.157.41])
	by mail.efficios.com (Postfix) with ESMTPSA id EFF153401CF;
	Tue, 14 Nov 2017 20:05:18 +0000 (UTC)
From: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;
To: Peter Zijlstra &lt;peterz@infradead.org&gt;,
	&quot;Paul E . McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;,
	Boqun Feng &lt;boqun.feng@gmail.com&gt;, Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Dave Watson &lt;davejwatson@fb.com&gt;
Cc: linux-kernel@vger.kernel.org, linux-api@vger.kernel.org,
	Paul Turner &lt;pjt@google.com&gt;, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Russell King &lt;linux@arm.linux.org.uk&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H . Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Andrew Hunter &lt;ahh@google.com&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, Chris Lameter &lt;cl@linux.com&gt;,
	Ben Maurer &lt;bmaurer@fb.com&gt;, Steven Rostedt &lt;rostedt@goodmis.org&gt;,
	Josh Triplett &lt;josh@joshtriplett.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;,
	Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;
Subject: [RFC PATCH v3 for 4.15 08/24] Provide cpu_opv system call
Date: Tue, 14 Nov 2017 15:03:58 -0500
Message-Id: &lt;20171114200414.2188-9-mathieu.desnoyers@efficios.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20171114200414.2188-1-mathieu.desnoyers@efficios.com&gt;
References: &lt;20171114200414.2188-1-mathieu.desnoyers@efficios.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 14, 2017, 8:03 p.m.</div>
<pre class="content">
This new cpu_opv system call executes a vector of operations on behalf
of user-space on a specific CPU with preemption disabled. It is inspired
from readv() and writev() system calls which take a &quot;struct iovec&quot; array
as argument.

The operations available are: comparison, memcpy, add, or, and, xor,
left shift, right shift, and mb. The system call receives a CPU number
from user-space as argument, which is the CPU on which those operations
need to be performed. All preparation steps such as loading pointers,
and applying offsets to arrays, need to be performed by user-space
before invoking the system call. The &quot;comparison&quot; operation can be used
to check that the data used in the preparation step did not change
between preparation of system call inputs and operation execution within
the preempt-off critical section.

The reason why we require all pointer offsets to be calculated by
user-space beforehand is because we need to use get_user_pages_fast() to
first pin all pages touched by each operation. This takes care of
faulting-in the pages. Then, preemption is disabled, and the operations
are performed atomically with respect to other thread execution on that
CPU, without generating any page fault.

A maximum limit of 16 operations per cpu_opv syscall invocation is
enforced, so user-space cannot generate a too long preempt-off critical
section. Each operation is also limited a length of PAGE_SIZE bytes,
meaning that an operation can touch a maximum of 4 pages (memcpy: 2
pages for source, 2 pages for destination if addresses are not aligned
on page boundaries). Moreover, a total limit of 4216 bytes is applied
to operation lengths.

If the thread is not running on the requested CPU, a new
push_task_to_cpu() is invoked to migrate the task to the requested CPU.
If the requested CPU is not part of the cpus allowed mask of the thread,
the system call fails with EINVAL. After the migration has been
performed, preemption is disabled, and the current CPU number is checked
again and compared to the requested CPU number. If it still differs, it
means the scheduler migrated us away from that CPU. Return EAGAIN to
user-space in that case, and let user-space retry (either requesting the
same CPU number, or a different one, depending on the user-space
algorithm constraints).
<span class="signed-off-by">
Signed-off-by: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
CC: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;
CC: Peter Zijlstra &lt;peterz@infradead.org&gt;
CC: Paul Turner &lt;pjt@google.com&gt;
CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;
CC: Andrew Hunter &lt;ahh@google.com&gt;
CC: Andy Lutomirski &lt;luto@amacapital.net&gt;
CC: Andi Kleen &lt;andi@firstfloor.org&gt;
CC: Dave Watson &lt;davejwatson@fb.com&gt;
CC: Chris Lameter &lt;cl@linux.com&gt;
CC: Ingo Molnar &lt;mingo@redhat.com&gt;
CC: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
CC: Ben Maurer &lt;bmaurer@fb.com&gt;
CC: Steven Rostedt &lt;rostedt@goodmis.org&gt;
CC: Josh Triplett &lt;josh@joshtriplett.org&gt;
CC: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
CC: Andrew Morton &lt;akpm@linux-foundation.org&gt;
CC: Russell King &lt;linux@arm.linux.org.uk&gt;
CC: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
CC: Will Deacon &lt;will.deacon@arm.com&gt;
CC: Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;
CC: Boqun Feng &lt;boqun.feng@gmail.com&gt;
CC: linux-api@vger.kernel.org
---

Changes since v1:
- handle CPU hotplug,
- cleanup implementation using function pointers: We can use function
  pointers to implement the operations rather than duplicating all the
  user-access code.
- refuse device pages: Performing cpu_opv operations on io map&#39;d pages
  with preemption disabled could generate long preempt-off critical
  sections, which leads to unwanted scheduler latency. Return EFAULT if
  a device page is received as parameter
- restrict op vector to 4216 bytes length sum: Restrict the operation
  vector to length sum of:
  - 4096 bytes (typical page size on most architectures, should be
    enough for a string, or structures)
  - 15 * 8 bytes (typical operations on integers or pointers).
  The goal here is to keep the duration of preempt off critical section
  short, so we don&#39;t add significant scheduler latency.
- Add INIT_ONSTACK macro: Introduce the
  CPU_OP_FIELD_u32_u64_INIT_ONSTACK() macros to ensure that users
  correctly initialize the upper bits of CPU_OP_FIELD_u32_u64() on their
  stack to 0 on 32-bit architectures.
- Add CPU_MB_OP operation:
  Use-cases with:
  - two consecutive stores,
  - a mempcy followed by a store,
  require a memory barrier before the final store operation. A typical
  use-case is a store-release on the final store. Given that this is a
  slow path, just providing an explicit full barrier instruction should
  be sufficient.
- Add expect fault field:
  The use-case of list_pop brings interesting challenges. With rseq, we
  can use rseq_cmpnev_storeoffp_load(), and therefore load a pointer,
  compare it against NULL, add an offset, and load the target &quot;next&quot;
  pointer from the object, all within a single req critical section.

  Life is not so easy for cpu_opv in this use-case, mainly because we
  need to pin all pages we are going to touch in the preempt-off
  critical section beforehand. So we need to know the target object (in
  which we apply an offset to fetch the next pointer) when we pin pages
  before disabling preemption.

  So the approach is to load the head pointer and compare it against
  NULL in user-space, before doing the cpu_opv syscall. User-space can
  then compute the address of the head-&gt;next field, *without loading it*.

  The cpu_opv system call will first need to pin all pages associated
  with input data. This includes the page backing the head-&gt;next object,
  which may have been concurrently deallocated and unmapped. Therefore,
  in this case, getting -EFAULT when trying to pin those pages may
  happen: it just means they have been concurrently unmapped. This is
  an expected situation, and should just return -EAGAIN to user-space,
  to user-space can distinguish between &quot;should retry&quot; type of
  situations and actual errors that should be handled with extreme
  prejudice to the program (e.g. abort()).

  Therefore, add &quot;expect_fault&quot; fields along with op input address
  pointers, so user-space can identify whether a fault when getting a
  field should return EAGAIN rather than EFAULT.
- Add compiler barrier between operations: Adding a compiler barrier
  between store operations in a cpu_opv sequence can be useful when
  paired with membarrier system call.

  An algorithm with a paired slow path and fast path can use
  sys_membarrier on the slow path to replace fast-path memory barriers
  by compiler barrier.

  Adding an explicit compiler barrier between operations allows
  cpu_opv to be used as fallback for operations meant to match
  the membarrier system call.

Changes since v2:

- Fix memory leak by introducing struct cpu_opv_pinned_pages.
  Suggested by Boqun Feng.
- Cast argument 1 passed to access_ok from integer to void __user *,
  fixing sparse warning.
---
 MAINTAINERS                  |   7 +
 include/uapi/linux/cpu_opv.h | 117 ++++++
 init/Kconfig                 |  14 +
 kernel/Makefile              |   1 +
 kernel/cpu_opv.c             | 968 +++++++++++++++++++++++++++++++++++++++++++
 kernel/sched/core.c          |  37 ++
 kernel/sched/sched.h         |   2 +
 kernel/sys_ni.c              |   1 +
 8 files changed, 1147 insertions(+)
 create mode 100644 include/uapi/linux/cpu_opv.h
 create mode 100644 kernel/cpu_opv.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 15, 2017, 1:34 a.m.</div>
<pre class="content">
----- On Nov 14, 2017, at 3:03 PM, Mathieu Desnoyers mathieu.desnoyers@efficios.com wrote:
[...]
<span class="quote">&gt; diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="quote">&gt; index 3b448ba82225..cab256c1720a 100644</span>
<span class="quote">&gt; --- a/kernel/sched/sched.h</span>
<span class="quote">&gt; +++ b/kernel/sched/sched.h</span>
<span class="quote">&gt; @@ -1209,6 +1209,8 @@ static inline void __set_task_cpu(struct task_struct *p,</span>
<span class="quote">&gt; unsigned int cpu)</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu);</span>

Testing on CONFIG_SMP=n showed that I needed to add empty static inline
(returning 0) for !SMP case.

Mathieu
<span class="quote">

&gt; +</span>
<span class="quote">&gt; /*</span>
<span class="quote">&gt;  * Tunables that become constants when CONFIG_SCHED_DEBUG is off:</span>
<span class="quote">&gt;  */</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=466">Michael Kerrisk</a> - Nov. 15, 2017, 7:44 a.m.</div>
<pre class="content">
Hi Matthieu

On 14 November 2017 at 21:03, Mathieu Desnoyers
&lt;mathieu.desnoyers@efficios.com&gt; wrote:
<span class="quote">&gt; This new cpu_opv system call executes a vector of operations on behalf</span>
<span class="quote">&gt; of user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="quote">&gt; from readv() and writev() system calls which take a &quot;struct iovec&quot; array</span>
<span class="quote">&gt; as argument.</span>

Do you have a man page spfr this syscall already?

Thanks,

Michael
<span class="quote">

&gt; The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt; left shift, right shift, and mb. The system call receives a CPU number</span>
<span class="quote">&gt; from user-space as argument, which is the CPU on which those operations</span>
<span class="quote">&gt; need to be performed. All preparation steps such as loading pointers,</span>
<span class="quote">&gt; and applying offsets to arrays, need to be performed by user-space</span>
<span class="quote">&gt; before invoking the system call. The &quot;comparison&quot; operation can be used</span>
<span class="quote">&gt; to check that the data used in the preparation step did not change</span>
<span class="quote">&gt; between preparation of system call inputs and operation execution within</span>
<span class="quote">&gt; the preempt-off critical section.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt; user-space beforehand is because we need to use get_user_pages_fast() to</span>
<span class="quote">&gt; first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt; faulting-in the pages. Then, preemption is disabled, and the operations</span>
<span class="quote">&gt; are performed atomically with respect to other thread execution on that</span>
<span class="quote">&gt; CPU, without generating any page fault.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt; enforced, so user-space cannot generate a too long preempt-off critical</span>
<span class="quote">&gt; section. Each operation is also limited a length of PAGE_SIZE bytes,</span>
<span class="quote">&gt; meaning that an operation can touch a maximum of 4 pages (memcpy: 2</span>
<span class="quote">&gt; pages for source, 2 pages for destination if addresses are not aligned</span>
<span class="quote">&gt; on page boundaries). Moreover, a total limit of 4216 bytes is applied</span>
<span class="quote">&gt; to operation lengths.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If the thread is not running on the requested CPU, a new</span>
<span class="quote">&gt; push_task_to_cpu() is invoked to migrate the task to the requested CPU.</span>
<span class="quote">&gt; If the requested CPU is not part of the cpus allowed mask of the thread,</span>
<span class="quote">&gt; the system call fails with EINVAL. After the migration has been</span>
<span class="quote">&gt; performed, preemption is disabled, and the current CPU number is checked</span>
<span class="quote">&gt; again and compared to the requested CPU number. If it still differs, it</span>
<span class="quote">&gt; means the scheduler migrated us away from that CPU. Return EAGAIN to</span>
<span class="quote">&gt; user-space in that case, and let user-space retry (either requesting the</span>
<span class="quote">&gt; same CPU number, or a different one, depending on the user-space</span>
<span class="quote">&gt; algorithm constraints).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt; CC: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; CC: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt; CC: Paul Turner &lt;pjt@google.com&gt;</span>
<span class="quote">&gt; CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt; CC: Andrew Hunter &lt;ahh@google.com&gt;</span>
<span class="quote">&gt; CC: Andy Lutomirski &lt;luto@amacapital.net&gt;</span>
<span class="quote">&gt; CC: Andi Kleen &lt;andi@firstfloor.org&gt;</span>
<span class="quote">&gt; CC: Dave Watson &lt;davejwatson@fb.com&gt;</span>
<span class="quote">&gt; CC: Chris Lameter &lt;cl@linux.com&gt;</span>
<span class="quote">&gt; CC: Ingo Molnar &lt;mingo@redhat.com&gt;</span>
<span class="quote">&gt; CC: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;</span>
<span class="quote">&gt; CC: Ben Maurer &lt;bmaurer@fb.com&gt;</span>
<span class="quote">&gt; CC: Steven Rostedt &lt;rostedt@goodmis.org&gt;</span>
<span class="quote">&gt; CC: Josh Triplett &lt;josh@joshtriplett.org&gt;</span>
<span class="quote">&gt; CC: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
<span class="quote">&gt; CC: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; CC: Russell King &lt;linux@arm.linux.org.uk&gt;</span>
<span class="quote">&gt; CC: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt; CC: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt; CC: Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;</span>
<span class="quote">&gt; CC: Boqun Feng &lt;boqun.feng@gmail.com&gt;</span>
<span class="quote">&gt; CC: linux-api@vger.kernel.org</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Changes since v1:</span>
<span class="quote">&gt; - handle CPU hotplug,</span>
<span class="quote">&gt; - cleanup implementation using function pointers: We can use function</span>
<span class="quote">&gt;   pointers to implement the operations rather than duplicating all the</span>
<span class="quote">&gt;   user-access code.</span>
<span class="quote">&gt; - refuse device pages: Performing cpu_opv operations on io map&#39;d pages</span>
<span class="quote">&gt;   with preemption disabled could generate long preempt-off critical</span>
<span class="quote">&gt;   sections, which leads to unwanted scheduler latency. Return EFAULT if</span>
<span class="quote">&gt;   a device page is received as parameter</span>
<span class="quote">&gt; - restrict op vector to 4216 bytes length sum: Restrict the operation</span>
<span class="quote">&gt;   vector to length sum of:</span>
<span class="quote">&gt;   - 4096 bytes (typical page size on most architectures, should be</span>
<span class="quote">&gt;     enough for a string, or structures)</span>
<span class="quote">&gt;   - 15 * 8 bytes (typical operations on integers or pointers).</span>
<span class="quote">&gt;   The goal here is to keep the duration of preempt off critical section</span>
<span class="quote">&gt;   short, so we don&#39;t add significant scheduler latency.</span>
<span class="quote">&gt; - Add INIT_ONSTACK macro: Introduce the</span>
<span class="quote">&gt;   CPU_OP_FIELD_u32_u64_INIT_ONSTACK() macros to ensure that users</span>
<span class="quote">&gt;   correctly initialize the upper bits of CPU_OP_FIELD_u32_u64() on their</span>
<span class="quote">&gt;   stack to 0 on 32-bit architectures.</span>
<span class="quote">&gt; - Add CPU_MB_OP operation:</span>
<span class="quote">&gt;   Use-cases with:</span>
<span class="quote">&gt;   - two consecutive stores,</span>
<span class="quote">&gt;   - a mempcy followed by a store,</span>
<span class="quote">&gt;   require a memory barrier before the final store operation. A typical</span>
<span class="quote">&gt;   use-case is a store-release on the final store. Given that this is a</span>
<span class="quote">&gt;   slow path, just providing an explicit full barrier instruction should</span>
<span class="quote">&gt;   be sufficient.</span>
<span class="quote">&gt; - Add expect fault field:</span>
<span class="quote">&gt;   The use-case of list_pop brings interesting challenges. With rseq, we</span>
<span class="quote">&gt;   can use rseq_cmpnev_storeoffp_load(), and therefore load a pointer,</span>
<span class="quote">&gt;   compare it against NULL, add an offset, and load the target &quot;next&quot;</span>
<span class="quote">&gt;   pointer from the object, all within a single req critical section.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   Life is not so easy for cpu_opv in this use-case, mainly because we</span>
<span class="quote">&gt;   need to pin all pages we are going to touch in the preempt-off</span>
<span class="quote">&gt;   critical section beforehand. So we need to know the target object (in</span>
<span class="quote">&gt;   which we apply an offset to fetch the next pointer) when we pin pages</span>
<span class="quote">&gt;   before disabling preemption.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   So the approach is to load the head pointer and compare it against</span>
<span class="quote">&gt;   NULL in user-space, before doing the cpu_opv syscall. User-space can</span>
<span class="quote">&gt;   then compute the address of the head-&gt;next field, *without loading it*.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   The cpu_opv system call will first need to pin all pages associated</span>
<span class="quote">&gt;   with input data. This includes the page backing the head-&gt;next object,</span>
<span class="quote">&gt;   which may have been concurrently deallocated and unmapped. Therefore,</span>
<span class="quote">&gt;   in this case, getting -EFAULT when trying to pin those pages may</span>
<span class="quote">&gt;   happen: it just means they have been concurrently unmapped. This is</span>
<span class="quote">&gt;   an expected situation, and should just return -EAGAIN to user-space,</span>
<span class="quote">&gt;   to user-space can distinguish between &quot;should retry&quot; type of</span>
<span class="quote">&gt;   situations and actual errors that should be handled with extreme</span>
<span class="quote">&gt;   prejudice to the program (e.g. abort()).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   Therefore, add &quot;expect_fault&quot; fields along with op input address</span>
<span class="quote">&gt;   pointers, so user-space can identify whether a fault when getting a</span>
<span class="quote">&gt;   field should return EAGAIN rather than EFAULT.</span>
<span class="quote">&gt; - Add compiler barrier between operations: Adding a compiler barrier</span>
<span class="quote">&gt;   between store operations in a cpu_opv sequence can be useful when</span>
<span class="quote">&gt;   paired with membarrier system call.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   An algorithm with a paired slow path and fast path can use</span>
<span class="quote">&gt;   sys_membarrier on the slow path to replace fast-path memory barriers</span>
<span class="quote">&gt;   by compiler barrier.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   Adding an explicit compiler barrier between operations allows</span>
<span class="quote">&gt;   cpu_opv to be used as fallback for operations meant to match</span>
<span class="quote">&gt;   the membarrier system call.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Changes since v2:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; - Fix memory leak by introducing struct cpu_opv_pinned_pages.</span>
<span class="quote">&gt;   Suggested by Boqun Feng.</span>
<span class="quote">&gt; - Cast argument 1 passed to access_ok from integer to void __user *,</span>
<span class="quote">&gt;   fixing sparse warning.</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  MAINTAINERS                  |   7 +</span>
<span class="quote">&gt;  include/uapi/linux/cpu_opv.h | 117 ++++++</span>
<span class="quote">&gt;  init/Kconfig                 |  14 +</span>
<span class="quote">&gt;  kernel/Makefile              |   1 +</span>
<span class="quote">&gt;  kernel/cpu_opv.c             | 968 +++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  kernel/sched/core.c          |  37 ++</span>
<span class="quote">&gt;  kernel/sched/sched.h         |   2 +</span>
<span class="quote">&gt;  kernel/sys_ni.c              |   1 +</span>
<span class="quote">&gt;  8 files changed, 1147 insertions(+)</span>
<span class="quote">&gt;  create mode 100644 include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt;  create mode 100644 kernel/cpu_opv.c</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="quote">&gt; index c9f95f8b07ed..45a1bbdaa287 100644</span>
<span class="quote">&gt; --- a/MAINTAINERS</span>
<span class="quote">&gt; +++ b/MAINTAINERS</span>
<span class="quote">&gt; @@ -3675,6 +3675,13 @@ B:       https://bugzilla.kernel.org</span>
<span class="quote">&gt;  F:     drivers/cpuidle/*</span>
<span class="quote">&gt;  F:     include/linux/cpuidle.h</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +CPU NON-PREEMPTIBLE OPERATION VECTOR SUPPORT</span>
<span class="quote">&gt; +M:     Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt; +L:     linux-kernel@vger.kernel.org</span>
<span class="quote">&gt; +S:     Supported</span>
<span class="quote">&gt; +F:     kernel/cpu_opv.c</span>
<span class="quote">&gt; +F:     include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  CRAMFS FILESYSTEM</span>
<span class="quote">&gt;  W:     http://sourceforge.net/projects/cramfs/</span>
<span class="quote">&gt;  S:     Orphan / Obsolete</span>
<span class="quote">&gt; diff --git a/include/uapi/linux/cpu_opv.h b/include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..17f7d46e053b</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt; @@ -0,0 +1,117 @@</span>
<span class="quote">&gt; +#ifndef _UAPI_LINUX_CPU_OPV_H</span>
<span class="quote">&gt; +#define _UAPI_LINUX_CPU_OPV_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * linux/cpu_opv.h</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * CPU preempt-off operation vector system call API</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Copyright (c) 2017 Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="quote">&gt; + * of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="quote">&gt; + * in the Software without restriction, including without limitation the rights</span>
<span class="quote">&gt; + * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="quote">&gt; + * copies of the Software, and to permit persons to whom the Software is</span>
<span class="quote">&gt; + * furnished to do so, subject to the following conditions:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The above copyright notice and this permission notice shall be included in</span>
<span class="quote">&gt; + * all copies or substantial portions of the Software.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="quote">&gt; + * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="quote">&gt; + * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="quote">&gt; + * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="quote">&gt; + * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="quote">&gt; + * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="quote">&gt; + * SOFTWARE.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt; +# include &lt;linux/types.h&gt;</span>
<span class="quote">&gt; +#else  /* #ifdef __KERNEL__ */</span>
<span class="quote">&gt; +# include &lt;stdint.h&gt;</span>
<span class="quote">&gt; +#endif /* #else #ifdef __KERNEL__ */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/byteorder.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef __LP64__</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64(field)                   uint64_t field</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)   field = (intptr_t)v</span>
<span class="quote">&gt; +#elif defined(__BYTE_ORDER) ? \</span>
<span class="quote">&gt; +       __BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64(field)   uint32_t field ## _padding, field</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)   \</span>
<span class="quote">&gt; +       field ## _padding = 0, field = (intptr_t)v</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64(field)   uint32_t field, field ## _padding</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)   \</span>
<span class="quote">&gt; +       field = (intptr_t)v, field ## _padding = 0</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define CPU_OP_VEC_LEN_MAX             16</span>
<span class="quote">&gt; +#define CPU_OP_ARG_LEN_MAX             24</span>
<span class="quote">&gt; +/* Max. data len per operation. */</span>
<span class="quote">&gt; +#define CPU_OP_DATA_LEN_MAX            PAGE_SIZE</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Max. data len for overall vector. We to restrict the amount of</span>
<span class="quote">&gt; + * user-space data touched by the kernel in non-preemptible context so</span>
<span class="quote">&gt; + * we do not introduce long scheduler latencies.</span>
<span class="quote">&gt; + * This allows one copy of up to 4096 bytes, and 15 operations touching</span>
<span class="quote">&gt; + * 8 bytes each.</span>
<span class="quote">&gt; + * This limit is applied to the sum of length specified for all</span>
<span class="quote">&gt; + * operations in a vector.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define CPU_OP_VEC_DATA_LEN_MAX                (4096 + 15*8)</span>
<span class="quote">&gt; +#define CPU_OP_MAX_PAGES               4       /* Max. pages per op. */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +enum cpu_op_type {</span>
<span class="quote">&gt; +       CPU_COMPARE_EQ_OP,      /* compare */</span>
<span class="quote">&gt; +       CPU_COMPARE_NE_OP,      /* compare */</span>
<span class="quote">&gt; +       CPU_MEMCPY_OP,          /* memcpy */</span>
<span class="quote">&gt; +       CPU_ADD_OP,             /* arithmetic */</span>
<span class="quote">&gt; +       CPU_OR_OP,              /* bitwise */</span>
<span class="quote">&gt; +       CPU_AND_OP,             /* bitwise */</span>
<span class="quote">&gt; +       CPU_XOR_OP,             /* bitwise */</span>
<span class="quote">&gt; +       CPU_LSHIFT_OP,          /* shift */</span>
<span class="quote">&gt; +       CPU_RSHIFT_OP,          /* shift */</span>
<span class="quote">&gt; +       CPU_MB_OP,              /* memory barrier */</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Vector of operations to perform. Limited to 16. */</span>
<span class="quote">&gt; +struct cpu_op {</span>
<span class="quote">&gt; +       int32_t op;     /* enum cpu_op_type. */</span>
<span class="quote">&gt; +       uint32_t len;   /* data length, in bytes. */</span>
<span class="quote">&gt; +       union {</span>
<span class="quote">&gt; +               struct {</span>
<span class="quote">&gt; +                       CPU_OP_FIELD_u32_u64(a);</span>
<span class="quote">&gt; +                       CPU_OP_FIELD_u32_u64(b);</span>
<span class="quote">&gt; +                       uint8_t expect_fault_a;</span>
<span class="quote">&gt; +                       uint8_t expect_fault_b;</span>
<span class="quote">&gt; +               } compare_op;</span>
<span class="quote">&gt; +               struct {</span>
<span class="quote">&gt; +                       CPU_OP_FIELD_u32_u64(dst);</span>
<span class="quote">&gt; +                       CPU_OP_FIELD_u32_u64(src);</span>
<span class="quote">&gt; +                       uint8_t expect_fault_dst;</span>
<span class="quote">&gt; +                       uint8_t expect_fault_src;</span>
<span class="quote">&gt; +               } memcpy_op;</span>
<span class="quote">&gt; +               struct {</span>
<span class="quote">&gt; +                       CPU_OP_FIELD_u32_u64(p);</span>
<span class="quote">&gt; +                       int64_t count;</span>
<span class="quote">&gt; +                       uint8_t expect_fault_p;</span>
<span class="quote">&gt; +               } arithmetic_op;</span>
<span class="quote">&gt; +               struct {</span>
<span class="quote">&gt; +                       CPU_OP_FIELD_u32_u64(p);</span>
<span class="quote">&gt; +                       uint64_t mask;</span>
<span class="quote">&gt; +                       uint8_t expect_fault_p;</span>
<span class="quote">&gt; +               } bitwise_op;</span>
<span class="quote">&gt; +               struct {</span>
<span class="quote">&gt; +                       CPU_OP_FIELD_u32_u64(p);</span>
<span class="quote">&gt; +                       uint32_t bits;</span>
<span class="quote">&gt; +                       uint8_t expect_fault_p;</span>
<span class="quote">&gt; +               } shift_op;</span>
<span class="quote">&gt; +               char __padding[CPU_OP_ARG_LEN_MAX];</span>
<span class="quote">&gt; +       } u;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* _UAPI_LINUX_CPU_OPV_H */</span>
<span class="quote">&gt; diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="quote">&gt; index cbedfb91b40a..e4fbb5dd6a24 100644</span>
<span class="quote">&gt; --- a/init/Kconfig</span>
<span class="quote">&gt; +++ b/init/Kconfig</span>
<span class="quote">&gt; @@ -1404,6 +1404,7 @@ config RSEQ</span>
<span class="quote">&gt;         bool &quot;Enable rseq() system call&quot; if EXPERT</span>
<span class="quote">&gt;         default y</span>
<span class="quote">&gt;         depends on HAVE_RSEQ</span>
<span class="quote">&gt; +       select CPU_OPV</span>
<span class="quote">&gt;         select MEMBARRIER</span>
<span class="quote">&gt;         help</span>
<span class="quote">&gt;           Enable the restartable sequences system call. It provides a</span>
<span class="quote">&gt; @@ -1414,6 +1415,19 @@ config RSEQ</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;           If unsure, say Y.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +config CPU_OPV</span>
<span class="quote">&gt; +       bool &quot;Enable cpu_opv() system call&quot; if EXPERT</span>
<span class="quote">&gt; +       default y</span>
<span class="quote">&gt; +       help</span>
<span class="quote">&gt; +         Enable the CPU preempt-off operation vector system call.</span>
<span class="quote">&gt; +         It allows user-space to perform a sequence of operations on</span>
<span class="quote">&gt; +         per-cpu data with preemption disabled. Useful as</span>
<span class="quote">&gt; +         single-stepping fall-back for restartable sequences, and for</span>
<span class="quote">&gt; +         performing more complex operations on per-cpu data that would</span>
<span class="quote">&gt; +         not be otherwise possible to do with restartable sequences.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +         If unsure, say Y.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config EMBEDDED</span>
<span class="quote">&gt;         bool &quot;Embedded system&quot;</span>
<span class="quote">&gt;         option allnoconfig_y</span>
<span class="quote">&gt; diff --git a/kernel/Makefile b/kernel/Makefile</span>
<span class="quote">&gt; index 3574669dafd9..cac8855196ff 100644</span>
<span class="quote">&gt; --- a/kernel/Makefile</span>
<span class="quote">&gt; +++ b/kernel/Makefile</span>
<span class="quote">&gt; @@ -113,6 +113,7 @@ obj-$(CONFIG_TORTURE_TEST) += torture.o</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  obj-$(CONFIG_HAS_IOMEM) += memremap.o</span>
<span class="quote">&gt;  obj-$(CONFIG_RSEQ) += rseq.o</span>
<span class="quote">&gt; +obj-$(CONFIG_CPU_OPV) += cpu_opv.o</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  $(obj)/configs.o: $(obj)/config_data.h</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/kernel/cpu_opv.c b/kernel/cpu_opv.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..a81837a14b17</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/kernel/cpu_opv.c</span>
<span class="quote">&gt; @@ -0,0 +1,968 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * CPU preempt-off operation vector system call</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * It allows user-space to perform a sequence of operations on per-cpu</span>
<span class="quote">&gt; + * data with preemption disabled. Useful as single-stepping fall-back</span>
<span class="quote">&gt; + * for restartable sequences, and for performing more complex operations</span>
<span class="quote">&gt; + * on per-cpu data that would not be otherwise possible to do with</span>
<span class="quote">&gt; + * restartable sequences.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; + * it under the terms of the GNU General Public License as published by</span>
<span class="quote">&gt; + * the Free Software Foundation; either version 2 of the License, or</span>
<span class="quote">&gt; + * (at your option) any later version.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Copyright (C) 2017, EfficiOS Inc.,</span>
<span class="quote">&gt; + * Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/syscalls.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/cpu_opv.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/types.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/mutex.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/pagemap.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/ptrace.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/byteorder.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &quot;sched/sched.h&quot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define TMP_BUFLEN                     64</span>
<span class="quote">&gt; +#define NR_PINNED_PAGES_ON_STACK       8</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +union op_fn_data {</span>
<span class="quote">&gt; +       uint8_t _u8;</span>
<span class="quote">&gt; +       uint16_t _u16;</span>
<span class="quote">&gt; +       uint32_t _u32;</span>
<span class="quote">&gt; +       uint64_t _u64;</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt; +       uint32_t _u64_split[2];</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct cpu_opv_pinned_pages {</span>
<span class="quote">&gt; +       struct page **pages;</span>
<span class="quote">&gt; +       size_t nr;</span>
<span class="quote">&gt; +       bool is_kmalloc;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +typedef int (*op_fn_t)(union op_fn_data *data, uint64_t v, uint32_t len);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static DEFINE_MUTEX(cpu_opv_offline_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * The cpu_opv system call executes a vector of operations on behalf of</span>
<span class="quote">&gt; + * user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="quote">&gt; + * from readv() and writev() system calls which take a &quot;struct iovec&quot;</span>
<span class="quote">&gt; + * array as argument.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt; + * left shift, and right shift. The system call receives a CPU number</span>
<span class="quote">&gt; + * from user-space as argument, which is the CPU on which those</span>
<span class="quote">&gt; + * operations need to be performed. All preparation steps such as</span>
<span class="quote">&gt; + * loading pointers, and applying offsets to arrays, need to be</span>
<span class="quote">&gt; + * performed by user-space before invoking the system call. The</span>
<span class="quote">&gt; + * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="quote">&gt; + * preparation step did not change between preparation of system call</span>
<span class="quote">&gt; + * inputs and operation execution within the preempt-off critical</span>
<span class="quote">&gt; + * section.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt; + * to first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt; + * faulting-in the pages. Then, preemption is disabled, and the</span>
<span class="quote">&gt; + * operations are performed atomically with respect to other thread</span>
<span class="quote">&gt; + * execution on that CPU, without generating any page fault.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt; + * enforced, and a overall maximum length sum, so user-space cannot</span>
<span class="quote">&gt; + * generate a too long preempt-off critical section. Each operation is</span>
<span class="quote">&gt; + * also limited a length of PAGE_SIZE bytes, meaning that an operation</span>
<span class="quote">&gt; + * can touch a maximum of 4 pages (memcpy: 2 pages for source, 2 pages</span>
<span class="quote">&gt; + * for destination if addresses are not aligned on page boundaries).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * If the thread is not running on the requested CPU, a new</span>
<span class="quote">&gt; + * push_task_to_cpu() is invoked to migrate the task to the requested</span>
<span class="quote">&gt; + * CPU.  If the requested CPU is not part of the cpus allowed mask of</span>
<span class="quote">&gt; + * the thread, the system call fails with EINVAL. After the migration</span>
<span class="quote">&gt; + * has been performed, preemption is disabled, and the current CPU</span>
<span class="quote">&gt; + * number is checked again and compared to the requested CPU number. If</span>
<span class="quote">&gt; + * it still differs, it means the scheduler migrated us away from that</span>
<span class="quote">&gt; + * CPU. Return EAGAIN to user-space in that case, and let user-space</span>
<span class="quote">&gt; + * retry (either requesting the same CPU number, or a different one,</span>
<span class="quote">&gt; + * depending on the user-space algorithm constraints).</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Check operation types and length parameters.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static int cpu_opv_check(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int i;</span>
<span class="quote">&gt; +       uint32_t sum = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt; +               struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               default:</span>
<span class="quote">&gt; +                       sum += op-&gt;len;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt; +               case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt; +               case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt; +               case CPU_MEMCPY_OP:</span>
<span class="quote">&gt; +                       if (op-&gt;len &gt; CPU_OP_DATA_LEN_MAX)</span>
<span class="quote">&gt; +                               return -EINVAL;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_ADD_OP:</span>
<span class="quote">&gt; +               case CPU_OR_OP:</span>
<span class="quote">&gt; +               case CPU_AND_OP:</span>
<span class="quote">&gt; +               case CPU_XOR_OP:</span>
<span class="quote">&gt; +                       switch (op-&gt;len) {</span>
<span class="quote">&gt; +                       case 1:</span>
<span class="quote">&gt; +                       case 2:</span>
<span class="quote">&gt; +                       case 4:</span>
<span class="quote">&gt; +                       case 8:</span>
<span class="quote">&gt; +                               break;</span>
<span class="quote">&gt; +                       default:</span>
<span class="quote">&gt; +                               return -EINVAL;</span>
<span class="quote">&gt; +                       }</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_LSHIFT_OP:</span>
<span class="quote">&gt; +               case CPU_RSHIFT_OP:</span>
<span class="quote">&gt; +                       switch (op-&gt;len) {</span>
<span class="quote">&gt; +                       case 1:</span>
<span class="quote">&gt; +                               if (op-&gt;u.shift_op.bits &gt; 7)</span>
<span class="quote">&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt; +                               break;</span>
<span class="quote">&gt; +                       case 2:</span>
<span class="quote">&gt; +                               if (op-&gt;u.shift_op.bits &gt; 15)</span>
<span class="quote">&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt; +                               break;</span>
<span class="quote">&gt; +                       case 4:</span>
<span class="quote">&gt; +                               if (op-&gt;u.shift_op.bits &gt; 31)</span>
<span class="quote">&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt; +                               break;</span>
<span class="quote">&gt; +                       case 8:</span>
<span class="quote">&gt; +                               if (op-&gt;u.shift_op.bits &gt; 63)</span>
<span class="quote">&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt; +                               break;</span>
<span class="quote">&gt; +                       default:</span>
<span class="quote">&gt; +                               return -EINVAL;</span>
<span class="quote">&gt; +                       }</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               default:</span>
<span class="quote">&gt; +                       return -EINVAL;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       if (sum &gt; CPU_OP_VEC_DATA_LEN_MAX)</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="quote">&gt; +               unsigned long len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return ((addr + len - 1) &gt;&gt; PAGE_SHIFT) - (addr &gt;&gt; PAGE_SHIFT) + 1;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpu_op_check_page(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct address_space *mapping;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (is_zone_device_page(page))</span>
<span class="quote">&gt; +               return -EFAULT;</span>
<span class="quote">&gt; +       page = compound_head(page);</span>
<span class="quote">&gt; +       mapping = READ_ONCE(page-&gt;mapping);</span>
<span class="quote">&gt; +       if (!mapping) {</span>
<span class="quote">&gt; +               int shmem_swizzled;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               /*</span>
<span class="quote">&gt; +                * Check again with page lock held to guard against</span>
<span class="quote">&gt; +                * memory pressure making shmem_writepage move the page</span>
<span class="quote">&gt; +                * from filecache to swapcache.</span>
<span class="quote">&gt; +                */</span>
<span class="quote">&gt; +               lock_page(page);</span>
<span class="quote">&gt; +               shmem_swizzled = PageSwapCache(page) || page-&gt;mapping;</span>
<span class="quote">&gt; +               unlock_page(page);</span>
<span class="quote">&gt; +               if (shmem_swizzled)</span>
<span class="quote">&gt; +                       return -EAGAIN;</span>
<span class="quote">&gt; +               return -EFAULT;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Refusing device pages, the zero page, pages in the gate area, and</span>
<span class="quote">&gt; + * special mappings. Inspired from futex.c checks.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static int cpu_op_check_pages(struct page **pages,</span>
<span class="quote">&gt; +               unsigned long nr_pages)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       unsigned long i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       for (i = 0; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt; +               int ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               ret = cpu_op_check_page(pages[i]);</span>
<span class="quote">&gt; +               if (ret)</span>
<span class="quote">&gt; +                       return ret;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt; +               struct cpu_opv_pinned_pages *pin_pages, int write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct page *pages[2];</span>
<span class="quote">&gt; +       int ret, nr_pages;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!len)</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt; +       nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="quote">&gt; +       BUG_ON(nr_pages &gt; 2);</span>
<span class="quote">&gt; +       if (!pin_pages-&gt;is_kmalloc &amp;&amp; pin_pages-&gt;nr + nr_pages</span>
<span class="quote">&gt; +                       &gt; NR_PINNED_PAGES_ON_STACK) {</span>
<span class="quote">&gt; +               struct page **pinned_pages =</span>
<span class="quote">&gt; +                       kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="quote">&gt; +                               * sizeof(struct page *), GFP_KERNEL);</span>
<span class="quote">&gt; +               if (!pinned_pages)</span>
<span class="quote">&gt; +                       return -ENOMEM;</span>
<span class="quote">&gt; +               memcpy(pinned_pages, pin_pages-&gt;pages,</span>
<span class="quote">&gt; +                       pin_pages-&gt;nr * sizeof(struct page *));</span>
<span class="quote">&gt; +               pin_pages-&gt;pages = pinned_pages;</span>
<span class="quote">&gt; +               pin_pages-&gt;is_kmalloc = true;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +       ret = get_user_pages_fast(addr, nr_pages, write, pages);</span>
<span class="quote">&gt; +       if (ret &lt; nr_pages) {</span>
<span class="quote">&gt; +               if (ret &gt; 0)</span>
<span class="quote">&gt; +                       put_page(pages[0]);</span>
<span class="quote">&gt; +               return -EFAULT;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * Refuse device pages, the zero page, pages in the gate area,</span>
<span class="quote">&gt; +        * and special mappings.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       ret = cpu_op_check_pages(pages, nr_pages);</span>
<span class="quote">&gt; +       if (ret == -EAGAIN) {</span>
<span class="quote">&gt; +               put_page(pages[0]);</span>
<span class="quote">&gt; +               if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +                       put_page(pages[1]);</span>
<span class="quote">&gt; +               goto again;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       if (ret)</span>
<span class="quote">&gt; +               goto error;</span>
<span class="quote">&gt; +       pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[0];</span>
<span class="quote">&gt; +       if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +               pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[1];</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +error:</span>
<span class="quote">&gt; +       put_page(pages[0]);</span>
<span class="quote">&gt; +       if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +               put_page(pages[1]);</span>
<span class="quote">&gt; +       return -EFAULT;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="quote">&gt; +               struct cpu_opv_pinned_pages *pin_pages)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret, i;</span>
<span class="quote">&gt; +       bool expect_fault = false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /* Check access, pin pages. */</span>
<span class="quote">&gt; +       for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt; +               struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt; +               case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt; +               case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt; +                       expect_fault = op-&gt;u.compare_op.expect_fault_a;</span>
<span class="quote">&gt; +                       if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +                                       (unsigned long)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +                                       op-&gt;len, pin_pages, 0);</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt; +                       expect_fault = op-&gt;u.compare_op.expect_fault_b;</span>
<span class="quote">&gt; +                       if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +                                       (unsigned long)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt; +                                       op-&gt;len, pin_pages, 0);</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_MEMCPY_OP:</span>
<span class="quote">&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt; +                       expect_fault = op-&gt;u.memcpy_op.expect_fault_dst;</span>
<span class="quote">&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +                                       (unsigned long)op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt; +                       expect_fault = op-&gt;u.memcpy_op.expect_fault_src;</span>
<span class="quote">&gt; +                       if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +                                       (unsigned long)op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt; +                                       op-&gt;len, pin_pages, 0);</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_ADD_OP:</span>
<span class="quote">&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt; +                       expect_fault = op-&gt;u.arithmetic_op.expect_fault_p;</span>
<span class="quote">&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +                                       (unsigned long)op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_OR_OP:</span>
<span class="quote">&gt; +               case CPU_AND_OP:</span>
<span class="quote">&gt; +               case CPU_XOR_OP:</span>
<span class="quote">&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt; +                       expect_fault = op-&gt;u.bitwise_op.expect_fault_p;</span>
<span class="quote">&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +                                       (unsigned long)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_LSHIFT_OP:</span>
<span class="quote">&gt; +               case CPU_RSHIFT_OP:</span>
<span class="quote">&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt; +                       expect_fault = op-&gt;u.shift_op.expect_fault_p;</span>
<span class="quote">&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +                                       (unsigned long)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               goto error;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               default:</span>
<span class="quote">&gt; +                       return -EINVAL;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +error:</span>
<span class="quote">&gt; +       for (i = 0; i &lt; pin_pages-&gt;nr; i++)</span>
<span class="quote">&gt; +               put_page(pin_pages-&gt;pages[i]);</span>
<span class="quote">&gt; +       pin_pages-&gt;nr = 0;</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * If faulting access is expected, return EAGAIN to user-space.</span>
<span class="quote">&gt; +        * It allows user-space to distinguish between a fault caused by</span>
<span class="quote">&gt; +        * an access which is expect to fault (e.g. due to concurrent</span>
<span class="quote">&gt; +        * unmapping of underlying memory) from an unexpected fault from</span>
<span class="quote">&gt; +        * which a retry would not recover.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       if (ret == -EFAULT &amp;&amp; expect_fault)</span>
<span class="quote">&gt; +               return -EAGAIN;</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt; +static int do_cpu_op_compare_iter(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       char bufa[TMP_BUFLEN], bufb[TMP_BUFLEN];</span>
<span class="quote">&gt; +       uint32_t compared = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       while (compared != len) {</span>
<span class="quote">&gt; +               unsigned long to_compare;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               to_compare = min_t(uint32_t, TMP_BUFLEN, len - compared);</span>
<span class="quote">&gt; +               if (__copy_from_user_inatomic(bufa, a + compared, to_compare))</span>
<span class="quote">&gt; +                       return -EFAULT;</span>
<span class="quote">&gt; +               if (__copy_from_user_inatomic(bufb, b + compared, to_compare))</span>
<span class="quote">&gt; +                       return -EFAULT;</span>
<span class="quote">&gt; +               if (memcmp(bufa, bufb, to_compare))</span>
<span class="quote">&gt; +                       return 1;       /* different */</span>
<span class="quote">&gt; +               compared += to_compare;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return 0;       /* same */</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt; +static int do_cpu_op_compare(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = -EFAULT;</span>
<span class="quote">&gt; +       union {</span>
<span class="quote">&gt; +               uint8_t _u8;</span>
<span class="quote">&gt; +               uint16_t _u16;</span>
<span class="quote">&gt; +               uint32_t _u32;</span>
<span class="quote">&gt; +               uint64_t _u64;</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt; +               uint32_t _u64_split[2];</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +       } tmp[2];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pagefault_disable();</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               if (__get_user(tmp[0]._u8, (uint8_t __user *)a))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp[1]._u8, (uint8_t __user *)b))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               ret = !!(tmp[0]._u8 != tmp[1]._u8);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               if (__get_user(tmp[0]._u16, (uint16_t __user *)a))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp[1]._u16, (uint16_t __user *)b))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               ret = !!(tmp[0]._u16 != tmp[1]._u16);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               if (__get_user(tmp[0]._u32, (uint32_t __user *)a))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp[1]._u32, (uint32_t __user *)b))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               ret = !!(tmp[0]._u32 != tmp[1]._u32);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt; +               if (__get_user(tmp[0]._u64, (uint64_t __user *)a))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp[1]._u64, (uint64_t __user *)b))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +               if (__get_user(tmp[0]._u64_split[0], (uint32_t __user *)a))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp[0]._u64_split[1], (uint32_t __user *)a + 1))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp[1]._u64_split[0], (uint32_t __user *)b))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp[1]._u64_split[1], (uint32_t __user *)b + 1))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +               ret = !!(tmp[0]._u64 != tmp[1]._u64);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               pagefault_enable();</span>
<span class="quote">&gt; +               return do_cpu_op_compare_iter(a, b, len);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +       pagefault_enable();</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Return 0 on success, &lt; 0 on error. */</span>
<span class="quote">&gt; +static int do_cpu_op_memcpy_iter(void __user *dst, void __user *src,</span>
<span class="quote">&gt; +               uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       char buf[TMP_BUFLEN];</span>
<span class="quote">&gt; +       uint32_t copied = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       while (copied != len) {</span>
<span class="quote">&gt; +               unsigned long to_copy;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               to_copy = min_t(uint32_t, TMP_BUFLEN, len - copied);</span>
<span class="quote">&gt; +               if (__copy_from_user_inatomic(buf, src + copied, to_copy))</span>
<span class="quote">&gt; +                       return -EFAULT;</span>
<span class="quote">&gt; +               if (__copy_to_user_inatomic(dst + copied, buf, to_copy))</span>
<span class="quote">&gt; +                       return -EFAULT;</span>
<span class="quote">&gt; +               copied += to_copy;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Return 0 on success, &lt; 0 on error. */</span>
<span class="quote">&gt; +static int do_cpu_op_memcpy(void __user *dst, void __user *src, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = -EFAULT;</span>
<span class="quote">&gt; +       union {</span>
<span class="quote">&gt; +               uint8_t _u8;</span>
<span class="quote">&gt; +               uint16_t _u16;</span>
<span class="quote">&gt; +               uint32_t _u32;</span>
<span class="quote">&gt; +               uint64_t _u64;</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt; +               uint32_t _u64_split[2];</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +       } tmp;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pagefault_disable();</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               if (__get_user(tmp._u8, (uint8_t __user *)src))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u8, (uint8_t __user *)dst))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               if (__get_user(tmp._u16, (uint16_t __user *)src))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u16, (uint16_t __user *)dst))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               if (__get_user(tmp._u32, (uint32_t __user *)src))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u32, (uint32_t __user *)dst))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt; +               if (__get_user(tmp._u64, (uint64_t __user *)src))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u64, (uint64_t __user *)dst))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +               if (__get_user(tmp._u64_split[0], (uint32_t __user *)src))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp._u64_split[1], (uint32_t __user *)src + 1))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u64_split[0], (uint32_t __user *)dst))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u64_split[1], (uint32_t __user *)dst + 1))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               pagefault_enable();</span>
<span class="quote">&gt; +               return do_cpu_op_memcpy_iter(dst, src, len);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       ret = 0;</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +       pagefault_enable();</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int op_add_fn(union op_fn_data *data, uint64_t count, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               data-&gt;_u8 += (uint8_t)count;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               data-&gt;_u16 += (uint16_t)count;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               data-&gt;_u32 += (uint32_t)count;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +               data-&gt;_u64 += (uint64_t)count;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int op_or_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               data-&gt;_u8 |= (uint8_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               data-&gt;_u16 |= (uint16_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               data-&gt;_u32 |= (uint32_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +               data-&gt;_u64 |= (uint64_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int op_and_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               data-&gt;_u8 &amp;= (uint8_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               data-&gt;_u16 &amp;= (uint16_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               data-&gt;_u32 &amp;= (uint32_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +               data-&gt;_u64 &amp;= (uint64_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int op_xor_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               data-&gt;_u8 ^= (uint8_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               data-&gt;_u16 ^= (uint16_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               data-&gt;_u32 ^= (uint32_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +               data-&gt;_u64 ^= (uint64_t)mask;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int op_lshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               data-&gt;_u8 &lt;&lt;= (uint8_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               data-&gt;_u16 &lt;&lt;= (uint16_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               data-&gt;_u32 &lt;&lt;= (uint32_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +               data-&gt;_u64 &lt;&lt;= (uint64_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int op_rshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               data-&gt;_u8 &gt;&gt;= (uint8_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               data-&gt;_u16 &gt;&gt;= (uint16_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               data-&gt;_u32 &gt;&gt;= (uint32_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +               data-&gt;_u64 &gt;&gt;= (uint64_t)bits;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Return 0 on success, &lt; 0 on error. */</span>
<span class="quote">&gt; +static int do_cpu_op_fn(op_fn_t op_fn, void __user *p, uint64_t v,</span>
<span class="quote">&gt; +               uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret = -EFAULT;</span>
<span class="quote">&gt; +       union op_fn_data tmp;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pagefault_disable();</span>
<span class="quote">&gt; +       switch (len) {</span>
<span class="quote">&gt; +       case 1:</span>
<span class="quote">&gt; +               if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 2:</span>
<span class="quote">&gt; +               if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 4:</span>
<span class="quote">&gt; +               if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case 8:</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt; +               if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +               if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt; +               if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +               if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +               if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="quote">&gt; +                       goto end;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               goto end;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       ret = 0;</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +       pagefault_enable();</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int __do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int i, ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt; +               struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               /* Guarantee a compiler barrier between each operation. */</span>
<span class="quote">&gt; +               barrier();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt; +               case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_compare(</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt; +                                       op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret &lt; 0)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       /*</span>
<span class="quote">&gt; +                        * Stop execution, return op index + 1 if comparison</span>
<span class="quote">&gt; +                        * differs.</span>
<span class="quote">&gt; +                        */</span>
<span class="quote">&gt; +                       if (ret &gt; 0)</span>
<span class="quote">&gt; +                               return i + 1;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_compare(</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt; +                                       op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret &lt; 0)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       /*</span>
<span class="quote">&gt; +                        * Stop execution, return op index + 1 if comparison</span>
<span class="quote">&gt; +                        * is identical.</span>
<span class="quote">&gt; +                        */</span>
<span class="quote">&gt; +                       if (ret == 0)</span>
<span class="quote">&gt; +                               return i + 1;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_MEMCPY_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_memcpy(</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt; +                                       op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_ADD_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_fn(op_add_fn,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;u.arithmetic_op.count, op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_OR_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_fn(op_or_fn,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_AND_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_fn(op_and_fn,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_XOR_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_fn(op_xor_fn,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_LSHIFT_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_fn(op_lshift_fn,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_RSHIFT_OP:</span>
<span class="quote">&gt; +                       ret = do_cpu_op_fn(op_rshift_fn,</span>
<span class="quote">&gt; +                                       (void __user *)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt; +                                       op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="quote">&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt; +                       if (ret)</span>
<span class="quote">&gt; +                               return ret;</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt; +                       smp_mb();</span>
<span class="quote">&gt; +                       break;</span>
<span class="quote">&gt; +               default:</span>
<span class="quote">&gt; +                       return -EINVAL;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt, int cpu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (cpu != raw_smp_processor_id()) {</span>
<span class="quote">&gt; +               ret = push_task_to_cpu(current, cpu);</span>
<span class="quote">&gt; +               if (ret)</span>
<span class="quote">&gt; +                       goto check_online;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       preempt_disable();</span>
<span class="quote">&gt; +       if (cpu != smp_processor_id()) {</span>
<span class="quote">&gt; +               ret = -EAGAIN;</span>
<span class="quote">&gt; +               goto end;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +       preempt_enable();</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +check_online:</span>
<span class="quote">&gt; +       if (!cpu_possible(cpu))</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +       get_online_cpus();</span>
<span class="quote">&gt; +       if (cpu_online(cpu)) {</span>
<span class="quote">&gt; +               ret = -EAGAIN;</span>
<span class="quote">&gt; +               goto put_online_cpus;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * CPU is offline. Perform operation from the current CPU with</span>
<span class="quote">&gt; +        * cpu_online read lock held, preventing that CPU from coming online,</span>
<span class="quote">&gt; +        * and with mutex held, providing mutual exclusion against other</span>
<span class="quote">&gt; +        * CPUs also finding out about an offline CPU.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       mutex_lock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt; +       ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt; +       mutex_unlock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt; +put_online_cpus:</span>
<span class="quote">&gt; +       put_online_cpus();</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * cpu_opv - execute operation vector on a given CPU with preempt off.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Userspace should pass current CPU number as parameter. May fail with</span>
<span class="quote">&gt; + * -EAGAIN if currently executing on the wrong CPU.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +SYSCALL_DEFINE4(cpu_opv, struct cpu_op __user *, ucpuopv, int, cpuopcnt,</span>
<span class="quote">&gt; +               int, cpu, int, flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct cpu_op cpuopv[CPU_OP_VEC_LEN_MAX];</span>
<span class="quote">&gt; +       struct page *pinned_pages_on_stack[NR_PINNED_PAGES_ON_STACK];</span>
<span class="quote">&gt; +       struct cpu_opv_pinned_pages pin_pages = {</span>
<span class="quote">&gt; +               .pages = pinned_pages_on_stack,</span>
<span class="quote">&gt; +               .nr = 0,</span>
<span class="quote">&gt; +               .is_kmalloc = false,</span>
<span class="quote">&gt; +       };</span>
<span class="quote">&gt; +       int ret, i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (unlikely(flags))</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +       if (unlikely(cpu &lt; 0))</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +       if (cpuopcnt &lt; 0 || cpuopcnt &gt; CPU_OP_VEC_LEN_MAX)</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +       if (copy_from_user(cpuopv, ucpuopv, cpuopcnt * sizeof(struct cpu_op)))</span>
<span class="quote">&gt; +               return -EFAULT;</span>
<span class="quote">&gt; +       ret = cpu_opv_check(cpuopv, cpuopcnt);</span>
<span class="quote">&gt; +       if (ret)</span>
<span class="quote">&gt; +               return ret;</span>
<span class="quote">&gt; +       ret = cpu_opv_pin_pages(cpuopv, cpuopcnt, &amp;pin_pages);</span>
<span class="quote">&gt; +       if (ret)</span>
<span class="quote">&gt; +               goto end;</span>
<span class="quote">&gt; +       ret = do_cpu_opv(cpuopv, cpuopcnt, cpu);</span>
<span class="quote">&gt; +       for (i = 0; i &lt; pin_pages.nr; i++)</span>
<span class="quote">&gt; +               put_page(pin_pages.pages[i]);</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +       if (pin_pages.is_kmalloc)</span>
<span class="quote">&gt; +               kfree(pin_pages.pages);</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="quote">&gt; index 6bba05f47e51..e547f93a46c2 100644</span>
<span class="quote">&gt; --- a/kernel/sched/core.c</span>
<span class="quote">&gt; +++ b/kernel/sched/core.c</span>
<span class="quote">&gt; @@ -1052,6 +1052,43 @@ void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)</span>
<span class="quote">&gt;                 set_curr_task(rq, p);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct rq_flags rf;</span>
<span class="quote">&gt; +       struct rq *rq;</span>
<span class="quote">&gt; +       int ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       rq = task_rq_lock(p, &amp;rf);</span>
<span class="quote">&gt; +       update_rq_clock(rq);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!cpumask_test_cpu(dest_cpu, &amp;p-&gt;cpus_allowed)) {</span>
<span class="quote">&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt; +               goto out;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (task_cpu(p) == dest_cpu)</span>
<span class="quote">&gt; +               goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (task_running(rq, p) || p-&gt;state == TASK_WAKING) {</span>
<span class="quote">&gt; +               struct migration_arg arg = { p, dest_cpu };</span>
<span class="quote">&gt; +               /* Need help from migration thread: drop lock and wait. */</span>
<span class="quote">&gt; +               task_rq_unlock(rq, p, &amp;rf);</span>
<span class="quote">&gt; +               stop_one_cpu(cpu_of(rq), migration_cpu_stop, &amp;arg);</span>
<span class="quote">&gt; +               tlb_migrate_finish(p-&gt;mm);</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt; +       } else if (task_on_rq_queued(p)) {</span>
<span class="quote">&gt; +               /*</span>
<span class="quote">&gt; +                * OK, since we&#39;re going to drop the lock immediately</span>
<span class="quote">&gt; +                * afterwards anyway.</span>
<span class="quote">&gt; +                */</span>
<span class="quote">&gt; +               rq = move_queued_task(rq, &amp;rf, p, dest_cpu);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +       task_rq_unlock(rq, p, &amp;rf);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Change a given task&#39;s CPU affinity. Migrate the thread to a</span>
<span class="quote">&gt;   * proper CPU and schedule it away if the CPU it&#39;s executing on</span>
<span class="quote">&gt; diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="quote">&gt; index 3b448ba82225..cab256c1720a 100644</span>
<span class="quote">&gt; --- a/kernel/sched/sched.h</span>
<span class="quote">&gt; +++ b/kernel/sched/sched.h</span>
<span class="quote">&gt; @@ -1209,6 +1209,8 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Tunables that become constants when CONFIG_SCHED_DEBUG is off:</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; diff --git a/kernel/sys_ni.c b/kernel/sys_ni.c</span>
<span class="quote">&gt; index bfa1ee1bf669..59e622296dc3 100644</span>
<span class="quote">&gt; --- a/kernel/sys_ni.c</span>
<span class="quote">&gt; +++ b/kernel/sys_ni.c</span>
<span class="quote">&gt; @@ -262,3 +262,4 @@ cond_syscall(sys_pkey_free);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /* restartable sequence */</span>
<span class="quote">&gt;  cond_syscall(sys_rseq);</span>
<span class="quote">&gt; +cond_syscall(sys_cpu_opv);</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 15, 2017, 2:30 p.m.</div>
<pre class="content">
----- On Nov 15, 2017, at 2:44 AM, Michael Kerrisk mtk.manpages@gmail.com wrote:
<span class="quote">
&gt; Hi Matthieu</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 14 November 2017 at 21:03, Mathieu Desnoyers</span>
<span class="quote">&gt; &lt;mathieu.desnoyers@efficios.com&gt; wrote:</span>
<span class="quote">&gt;&gt; This new cpu_opv system call executes a vector of operations on behalf</span>
<span class="quote">&gt;&gt; of user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="quote">&gt;&gt; from readv() and writev() system calls which take a &quot;struct iovec&quot; array</span>
<span class="quote">&gt;&gt; as argument.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you have a man page for this syscall already?</span>

Hi Michael,

It&#39;s the next thing on my roadmap when the syscall reaches mainline.
That and membarrier commands man pages updates.

Thanks,

Mathieu
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Michael</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt;&gt; left shift, right shift, and mb. The system call receives a CPU number</span>
<span class="quote">&gt;&gt; from user-space as argument, which is the CPU on which those operations</span>
<span class="quote">&gt;&gt; need to be performed. All preparation steps such as loading pointers,</span>
<span class="quote">&gt;&gt; and applying offsets to arrays, need to be performed by user-space</span>
<span class="quote">&gt;&gt; before invoking the system call. The &quot;comparison&quot; operation can be used</span>
<span class="quote">&gt;&gt; to check that the data used in the preparation step did not change</span>
<span class="quote">&gt;&gt; between preparation of system call inputs and operation execution within</span>
<span class="quote">&gt;&gt; the preempt-off critical section.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt;&gt; user-space beforehand is because we need to use get_user_pages_fast() to</span>
<span class="quote">&gt;&gt; first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt;&gt; faulting-in the pages. Then, preemption is disabled, and the operations</span>
<span class="quote">&gt;&gt; are performed atomically with respect to other thread execution on that</span>
<span class="quote">&gt;&gt; CPU, without generating any page fault.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt;&gt; enforced, so user-space cannot generate a too long preempt-off critical</span>
<span class="quote">&gt;&gt; section. Each operation is also limited a length of PAGE_SIZE bytes,</span>
<span class="quote">&gt;&gt; meaning that an operation can touch a maximum of 4 pages (memcpy: 2</span>
<span class="quote">&gt;&gt; pages for source, 2 pages for destination if addresses are not aligned</span>
<span class="quote">&gt;&gt; on page boundaries). Moreover, a total limit of 4216 bytes is applied</span>
<span class="quote">&gt;&gt; to operation lengths.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If the thread is not running on the requested CPU, a new</span>
<span class="quote">&gt;&gt; push_task_to_cpu() is invoked to migrate the task to the requested CPU.</span>
<span class="quote">&gt;&gt; If the requested CPU is not part of the cpus allowed mask of the thread,</span>
<span class="quote">&gt;&gt; the system call fails with EINVAL. After the migration has been</span>
<span class="quote">&gt;&gt; performed, preemption is disabled, and the current CPU number is checked</span>
<span class="quote">&gt;&gt; again and compared to the requested CPU number. If it still differs, it</span>
<span class="quote">&gt;&gt; means the scheduler migrated us away from that CPU. Return EAGAIN to</span>
<span class="quote">&gt;&gt; user-space in that case, and let user-space retry (either requesting the</span>
<span class="quote">&gt;&gt; same CPU number, or a different one, depending on the user-space</span>
<span class="quote">&gt;&gt; algorithm constraints).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt;&gt; CC: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;&gt; CC: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt;&gt; CC: Paul Turner &lt;pjt@google.com&gt;</span>
<span class="quote">&gt;&gt; CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt;&gt; CC: Andrew Hunter &lt;ahh@google.com&gt;</span>
<span class="quote">&gt;&gt; CC: Andy Lutomirski &lt;luto@amacapital.net&gt;</span>
<span class="quote">&gt;&gt; CC: Andi Kleen &lt;andi@firstfloor.org&gt;</span>
<span class="quote">&gt;&gt; CC: Dave Watson &lt;davejwatson@fb.com&gt;</span>
<span class="quote">&gt;&gt; CC: Chris Lameter &lt;cl@linux.com&gt;</span>
<span class="quote">&gt;&gt; CC: Ingo Molnar &lt;mingo@redhat.com&gt;</span>
<span class="quote">&gt;&gt; CC: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;</span>
<span class="quote">&gt;&gt; CC: Ben Maurer &lt;bmaurer@fb.com&gt;</span>
<span class="quote">&gt;&gt; CC: Steven Rostedt &lt;rostedt@goodmis.org&gt;</span>
<span class="quote">&gt;&gt; CC: Josh Triplett &lt;josh@joshtriplett.org&gt;</span>
<span class="quote">&gt;&gt; CC: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
<span class="quote">&gt;&gt; CC: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt;&gt; CC: Russell King &lt;linux@arm.linux.org.uk&gt;</span>
<span class="quote">&gt;&gt; CC: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt;&gt; CC: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt;&gt; CC: Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;</span>
<span class="quote">&gt;&gt; CC: Boqun Feng &lt;boqun.feng@gmail.com&gt;</span>
<span class="quote">&gt;&gt; CC: linux-api@vger.kernel.org</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Changes since v1:</span>
<span class="quote">&gt;&gt; - handle CPU hotplug,</span>
<span class="quote">&gt;&gt; - cleanup implementation using function pointers: We can use function</span>
<span class="quote">&gt;&gt;   pointers to implement the operations rather than duplicating all the</span>
<span class="quote">&gt;&gt;   user-access code.</span>
<span class="quote">&gt;&gt; - refuse device pages: Performing cpu_opv operations on io map&#39;d pages</span>
<span class="quote">&gt;&gt;   with preemption disabled could generate long preempt-off critical</span>
<span class="quote">&gt;&gt;   sections, which leads to unwanted scheduler latency. Return EFAULT if</span>
<span class="quote">&gt;&gt;   a device page is received as parameter</span>
<span class="quote">&gt;&gt; - restrict op vector to 4216 bytes length sum: Restrict the operation</span>
<span class="quote">&gt;&gt;   vector to length sum of:</span>
<span class="quote">&gt;&gt;   - 4096 bytes (typical page size on most architectures, should be</span>
<span class="quote">&gt;&gt;     enough for a string, or structures)</span>
<span class="quote">&gt;&gt;   - 15 * 8 bytes (typical operations on integers or pointers).</span>
<span class="quote">&gt;&gt;   The goal here is to keep the duration of preempt off critical section</span>
<span class="quote">&gt;&gt;   short, so we don&#39;t add significant scheduler latency.</span>
<span class="quote">&gt;&gt; - Add INIT_ONSTACK macro: Introduce the</span>
<span class="quote">&gt;&gt;   CPU_OP_FIELD_u32_u64_INIT_ONSTACK() macros to ensure that users</span>
<span class="quote">&gt;&gt;   correctly initialize the upper bits of CPU_OP_FIELD_u32_u64() on their</span>
<span class="quote">&gt;&gt;   stack to 0 on 32-bit architectures.</span>
<span class="quote">&gt;&gt; - Add CPU_MB_OP operation:</span>
<span class="quote">&gt;&gt;   Use-cases with:</span>
<span class="quote">&gt;&gt;   - two consecutive stores,</span>
<span class="quote">&gt;&gt;   - a mempcy followed by a store,</span>
<span class="quote">&gt;&gt;   require a memory barrier before the final store operation. A typical</span>
<span class="quote">&gt;&gt;   use-case is a store-release on the final store. Given that this is a</span>
<span class="quote">&gt;&gt;   slow path, just providing an explicit full barrier instruction should</span>
<span class="quote">&gt;&gt;   be sufficient.</span>
<span class="quote">&gt;&gt; - Add expect fault field:</span>
<span class="quote">&gt;&gt;   The use-case of list_pop brings interesting challenges. With rseq, we</span>
<span class="quote">&gt;&gt;   can use rseq_cmpnev_storeoffp_load(), and therefore load a pointer,</span>
<span class="quote">&gt;&gt;   compare it against NULL, add an offset, and load the target &quot;next&quot;</span>
<span class="quote">&gt;&gt;   pointer from the object, all within a single req critical section.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   Life is not so easy for cpu_opv in this use-case, mainly because we</span>
<span class="quote">&gt;&gt;   need to pin all pages we are going to touch in the preempt-off</span>
<span class="quote">&gt;&gt;   critical section beforehand. So we need to know the target object (in</span>
<span class="quote">&gt;&gt;   which we apply an offset to fetch the next pointer) when we pin pages</span>
<span class="quote">&gt;&gt;   before disabling preemption.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   So the approach is to load the head pointer and compare it against</span>
<span class="quote">&gt;&gt;   NULL in user-space, before doing the cpu_opv syscall. User-space can</span>
<span class="quote">&gt;&gt;   then compute the address of the head-&gt;next field, *without loading it*.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   The cpu_opv system call will first need to pin all pages associated</span>
<span class="quote">&gt;&gt;   with input data. This includes the page backing the head-&gt;next object,</span>
<span class="quote">&gt;&gt;   which may have been concurrently deallocated and unmapped. Therefore,</span>
<span class="quote">&gt;&gt;   in this case, getting -EFAULT when trying to pin those pages may</span>
<span class="quote">&gt;&gt;   happen: it just means they have been concurrently unmapped. This is</span>
<span class="quote">&gt;&gt;   an expected situation, and should just return -EAGAIN to user-space,</span>
<span class="quote">&gt;&gt;   to user-space can distinguish between &quot;should retry&quot; type of</span>
<span class="quote">&gt;&gt;   situations and actual errors that should be handled with extreme</span>
<span class="quote">&gt;&gt;   prejudice to the program (e.g. abort()).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   Therefore, add &quot;expect_fault&quot; fields along with op input address</span>
<span class="quote">&gt;&gt;   pointers, so user-space can identify whether a fault when getting a</span>
<span class="quote">&gt;&gt;   field should return EAGAIN rather than EFAULT.</span>
<span class="quote">&gt;&gt; - Add compiler barrier between operations: Adding a compiler barrier</span>
<span class="quote">&gt;&gt;   between store operations in a cpu_opv sequence can be useful when</span>
<span class="quote">&gt;&gt;   paired with membarrier system call.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   An algorithm with a paired slow path and fast path can use</span>
<span class="quote">&gt;&gt;   sys_membarrier on the slow path to replace fast-path memory barriers</span>
<span class="quote">&gt;&gt;   by compiler barrier.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   Adding an explicit compiler barrier between operations allows</span>
<span class="quote">&gt;&gt;   cpu_opv to be used as fallback for operations meant to match</span>
<span class="quote">&gt;&gt;   the membarrier system call.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Changes since v2:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; - Fix memory leak by introducing struct cpu_opv_pinned_pages.</span>
<span class="quote">&gt;&gt;   Suggested by Boqun Feng.</span>
<span class="quote">&gt;&gt; - Cast argument 1 passed to access_ok from integer to void __user *,</span>
<span class="quote">&gt;&gt;   fixing sparse warning.</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  MAINTAINERS                  |   7 +</span>
<span class="quote">&gt;&gt;  include/uapi/linux/cpu_opv.h | 117 ++++++</span>
<span class="quote">&gt;&gt;  init/Kconfig                 |  14 +</span>
<span class="quote">&gt;&gt;  kernel/Makefile              |   1 +</span>
<span class="quote">&gt;&gt;  kernel/cpu_opv.c             | 968 +++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;  kernel/sched/core.c          |  37 ++</span>
<span class="quote">&gt;&gt;  kernel/sched/sched.h         |   2 +</span>
<span class="quote">&gt;&gt;  kernel/sys_ni.c              |   1 +</span>
<span class="quote">&gt;&gt;  8 files changed, 1147 insertions(+)</span>
<span class="quote">&gt;&gt;  create mode 100644 include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt;&gt;  create mode 100644 kernel/cpu_opv.c</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="quote">&gt;&gt; index c9f95f8b07ed..45a1bbdaa287 100644</span>
<span class="quote">&gt;&gt; --- a/MAINTAINERS</span>
<span class="quote">&gt;&gt; +++ b/MAINTAINERS</span>
<span class="quote">&gt;&gt; @@ -3675,6 +3675,13 @@ B:       https://bugzilla.kernel.org</span>
<span class="quote">&gt;&gt;  F:     drivers/cpuidle/*</span>
<span class="quote">&gt;&gt;  F:     include/linux/cpuidle.h</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +CPU NON-PREEMPTIBLE OPERATION VECTOR SUPPORT</span>
<span class="quote">&gt;&gt; +M:     Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt;&gt; +L:     linux-kernel@vger.kernel.org</span>
<span class="quote">&gt;&gt; +S:     Supported</span>
<span class="quote">&gt;&gt; +F:     kernel/cpu_opv.c</span>
<span class="quote">&gt;&gt; +F:     include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  CRAMFS FILESYSTEM</span>
<span class="quote">&gt;&gt;  W:     http://sourceforge.net/projects/cramfs/</span>
<span class="quote">&gt;&gt;  S:     Orphan / Obsolete</span>
<span class="quote">&gt;&gt; diff --git a/include/uapi/linux/cpu_opv.h b/include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..17f7d46e053b</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/include/uapi/linux/cpu_opv.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,117 @@</span>
<span class="quote">&gt;&gt; +#ifndef _UAPI_LINUX_CPU_OPV_H</span>
<span class="quote">&gt;&gt; +#define _UAPI_LINUX_CPU_OPV_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * linux/cpu_opv.h</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * CPU preempt-off operation vector system call API</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Copyright (c) 2017 Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="quote">&gt;&gt; + * of this software and associated documentation files (the &quot;Software&quot;), to</span>
<span class="quote">&gt;&gt; deal</span>
<span class="quote">&gt;&gt; + * in the Software without restriction, including without limitation the rights</span>
<span class="quote">&gt;&gt; + * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="quote">&gt;&gt; + * copies of the Software, and to permit persons to whom the Software is</span>
<span class="quote">&gt;&gt; + * furnished to do so, subject to the following conditions:</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * The above copyright notice and this permission notice shall be included in</span>
<span class="quote">&gt;&gt; + * all copies or substantial portions of the Software.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="quote">&gt;&gt; + * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="quote">&gt;&gt; + * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="quote">&gt;&gt; + * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="quote">&gt;&gt; + * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING</span>
<span class="quote">&gt;&gt; FROM,</span>
<span class="quote">&gt;&gt; + * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN</span>
<span class="quote">&gt;&gt; THE</span>
<span class="quote">&gt;&gt; + * SOFTWARE.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt;&gt; +# include &lt;linux/types.h&gt;</span>
<span class="quote">&gt;&gt; +#else  /* #ifdef __KERNEL__ */</span>
<span class="quote">&gt;&gt; +# include &lt;stdint.h&gt;</span>
<span class="quote">&gt;&gt; +#endif /* #else #ifdef __KERNEL__ */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm/byteorder.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef __LP64__</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64(field)                   uint64_t field</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)   field = (intptr_t)v</span>
<span class="quote">&gt;&gt; +#elif defined(__BYTE_ORDER) ? \</span>
<span class="quote">&gt;&gt; +       __BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64(field)   uint32_t field ## _padding, field</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)   \</span>
<span class="quote">&gt;&gt; +       field ## _padding = 0, field = (intptr_t)v</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64(field)   uint32_t field, field ## _padding</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)   \</span>
<span class="quote">&gt;&gt; +       field = (intptr_t)v, field ## _padding = 0</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define CPU_OP_VEC_LEN_MAX             16</span>
<span class="quote">&gt;&gt; +#define CPU_OP_ARG_LEN_MAX             24</span>
<span class="quote">&gt;&gt; +/* Max. data len per operation. */</span>
<span class="quote">&gt;&gt; +#define CPU_OP_DATA_LEN_MAX            PAGE_SIZE</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Max. data len for overall vector. We to restrict the amount of</span>
<span class="quote">&gt;&gt; + * user-space data touched by the kernel in non-preemptible context so</span>
<span class="quote">&gt;&gt; + * we do not introduce long scheduler latencies.</span>
<span class="quote">&gt;&gt; + * This allows one copy of up to 4096 bytes, and 15 operations touching</span>
<span class="quote">&gt;&gt; + * 8 bytes each.</span>
<span class="quote">&gt;&gt; + * This limit is applied to the sum of length specified for all</span>
<span class="quote">&gt;&gt; + * operations in a vector.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define CPU_OP_VEC_DATA_LEN_MAX                (4096 + 15*8)</span>
<span class="quote">&gt;&gt; +#define CPU_OP_MAX_PAGES               4       /* Max. pages per op. */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +enum cpu_op_type {</span>
<span class="quote">&gt;&gt; +       CPU_COMPARE_EQ_OP,      /* compare */</span>
<span class="quote">&gt;&gt; +       CPU_COMPARE_NE_OP,      /* compare */</span>
<span class="quote">&gt;&gt; +       CPU_MEMCPY_OP,          /* memcpy */</span>
<span class="quote">&gt;&gt; +       CPU_ADD_OP,             /* arithmetic */</span>
<span class="quote">&gt;&gt; +       CPU_OR_OP,              /* bitwise */</span>
<span class="quote">&gt;&gt; +       CPU_AND_OP,             /* bitwise */</span>
<span class="quote">&gt;&gt; +       CPU_XOR_OP,             /* bitwise */</span>
<span class="quote">&gt;&gt; +       CPU_LSHIFT_OP,          /* shift */</span>
<span class="quote">&gt;&gt; +       CPU_RSHIFT_OP,          /* shift */</span>
<span class="quote">&gt;&gt; +       CPU_MB_OP,              /* memory barrier */</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Vector of operations to perform. Limited to 16. */</span>
<span class="quote">&gt;&gt; +struct cpu_op {</span>
<span class="quote">&gt;&gt; +       int32_t op;     /* enum cpu_op_type. */</span>
<span class="quote">&gt;&gt; +       uint32_t len;   /* data length, in bytes. */</span>
<span class="quote">&gt;&gt; +       union {</span>
<span class="quote">&gt;&gt; +               struct {</span>
<span class="quote">&gt;&gt; +                       CPU_OP_FIELD_u32_u64(a);</span>
<span class="quote">&gt;&gt; +                       CPU_OP_FIELD_u32_u64(b);</span>
<span class="quote">&gt;&gt; +                       uint8_t expect_fault_a;</span>
<span class="quote">&gt;&gt; +                       uint8_t expect_fault_b;</span>
<span class="quote">&gt;&gt; +               } compare_op;</span>
<span class="quote">&gt;&gt; +               struct {</span>
<span class="quote">&gt;&gt; +                       CPU_OP_FIELD_u32_u64(dst);</span>
<span class="quote">&gt;&gt; +                       CPU_OP_FIELD_u32_u64(src);</span>
<span class="quote">&gt;&gt; +                       uint8_t expect_fault_dst;</span>
<span class="quote">&gt;&gt; +                       uint8_t expect_fault_src;</span>
<span class="quote">&gt;&gt; +               } memcpy_op;</span>
<span class="quote">&gt;&gt; +               struct {</span>
<span class="quote">&gt;&gt; +                       CPU_OP_FIELD_u32_u64(p);</span>
<span class="quote">&gt;&gt; +                       int64_t count;</span>
<span class="quote">&gt;&gt; +                       uint8_t expect_fault_p;</span>
<span class="quote">&gt;&gt; +               } arithmetic_op;</span>
<span class="quote">&gt;&gt; +               struct {</span>
<span class="quote">&gt;&gt; +                       CPU_OP_FIELD_u32_u64(p);</span>
<span class="quote">&gt;&gt; +                       uint64_t mask;</span>
<span class="quote">&gt;&gt; +                       uint8_t expect_fault_p;</span>
<span class="quote">&gt;&gt; +               } bitwise_op;</span>
<span class="quote">&gt;&gt; +               struct {</span>
<span class="quote">&gt;&gt; +                       CPU_OP_FIELD_u32_u64(p);</span>
<span class="quote">&gt;&gt; +                       uint32_t bits;</span>
<span class="quote">&gt;&gt; +                       uint8_t expect_fault_p;</span>
<span class="quote">&gt;&gt; +               } shift_op;</span>
<span class="quote">&gt;&gt; +               char __padding[CPU_OP_ARG_LEN_MAX];</span>
<span class="quote">&gt;&gt; +       } u;</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#endif /* _UAPI_LINUX_CPU_OPV_H */</span>
<span class="quote">&gt;&gt; diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="quote">&gt;&gt; index cbedfb91b40a..e4fbb5dd6a24 100644</span>
<span class="quote">&gt;&gt; --- a/init/Kconfig</span>
<span class="quote">&gt;&gt; +++ b/init/Kconfig</span>
<span class="quote">&gt;&gt; @@ -1404,6 +1404,7 @@ config RSEQ</span>
<span class="quote">&gt;&gt;         bool &quot;Enable rseq() system call&quot; if EXPERT</span>
<span class="quote">&gt;&gt;         default y</span>
<span class="quote">&gt;&gt;         depends on HAVE_RSEQ</span>
<span class="quote">&gt;&gt; +       select CPU_OPV</span>
<span class="quote">&gt;&gt;         select MEMBARRIER</span>
<span class="quote">&gt;&gt;         help</span>
<span class="quote">&gt;&gt;           Enable the restartable sequences system call. It provides a</span>
<span class="quote">&gt;&gt; @@ -1414,6 +1415,19 @@ config RSEQ</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;           If unsure, say Y.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +config CPU_OPV</span>
<span class="quote">&gt;&gt; +       bool &quot;Enable cpu_opv() system call&quot; if EXPERT</span>
<span class="quote">&gt;&gt; +       default y</span>
<span class="quote">&gt;&gt; +       help</span>
<span class="quote">&gt;&gt; +         Enable the CPU preempt-off operation vector system call.</span>
<span class="quote">&gt;&gt; +         It allows user-space to perform a sequence of operations on</span>
<span class="quote">&gt;&gt; +         per-cpu data with preemption disabled. Useful as</span>
<span class="quote">&gt;&gt; +         single-stepping fall-back for restartable sequences, and for</span>
<span class="quote">&gt;&gt; +         performing more complex operations on per-cpu data that would</span>
<span class="quote">&gt;&gt; +         not be otherwise possible to do with restartable sequences.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +         If unsure, say Y.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  config EMBEDDED</span>
<span class="quote">&gt;&gt;         bool &quot;Embedded system&quot;</span>
<span class="quote">&gt;&gt;         option allnoconfig_y</span>
<span class="quote">&gt;&gt; diff --git a/kernel/Makefile b/kernel/Makefile</span>
<span class="quote">&gt;&gt; index 3574669dafd9..cac8855196ff 100644</span>
<span class="quote">&gt;&gt; --- a/kernel/Makefile</span>
<span class="quote">&gt;&gt; +++ b/kernel/Makefile</span>
<span class="quote">&gt;&gt; @@ -113,6 +113,7 @@ obj-$(CONFIG_TORTURE_TEST) += torture.o</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  obj-$(CONFIG_HAS_IOMEM) += memremap.o</span>
<span class="quote">&gt;&gt;  obj-$(CONFIG_RSEQ) += rseq.o</span>
<span class="quote">&gt;&gt; +obj-$(CONFIG_CPU_OPV) += cpu_opv.o</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  $(obj)/configs.o: $(obj)/config_data.h</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/kernel/cpu_opv.c b/kernel/cpu_opv.c</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..a81837a14b17</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/kernel/cpu_opv.c</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,968 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * CPU preempt-off operation vector system call</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * It allows user-space to perform a sequence of operations on per-cpu</span>
<span class="quote">&gt;&gt; + * data with preemption disabled. Useful as single-stepping fall-back</span>
<span class="quote">&gt;&gt; + * for restartable sequences, and for performing more complex operations</span>
<span class="quote">&gt;&gt; + * on per-cpu data that would not be otherwise possible to do with</span>
<span class="quote">&gt;&gt; + * restartable sequences.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt;&gt; + * it under the terms of the GNU General Public License as published by</span>
<span class="quote">&gt;&gt; + * the Free Software Foundation; either version 2 of the License, or</span>
<span class="quote">&gt;&gt; + * (at your option) any later version.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt;&gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt;&gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt;&gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2017, EfficiOS Inc.,</span>
<span class="quote">&gt;&gt; + * Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/syscalls.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/cpu_opv.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/types.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/mutex.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/pagemap.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/ptrace.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/byteorder.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &quot;sched/sched.h&quot;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define TMP_BUFLEN                     64</span>
<span class="quote">&gt;&gt; +#define NR_PINNED_PAGES_ON_STACK       8</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +union op_fn_data {</span>
<span class="quote">&gt;&gt; +       uint8_t _u8;</span>
<span class="quote">&gt;&gt; +       uint16_t _u16;</span>
<span class="quote">&gt;&gt; +       uint32_t _u32;</span>
<span class="quote">&gt;&gt; +       uint64_t _u64;</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt;&gt; +       uint32_t _u64_split[2];</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +struct cpu_opv_pinned_pages {</span>
<span class="quote">&gt;&gt; +       struct page **pages;</span>
<span class="quote">&gt;&gt; +       size_t nr;</span>
<span class="quote">&gt;&gt; +       bool is_kmalloc;</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +typedef int (*op_fn_t)(union op_fn_data *data, uint64_t v, uint32_t len);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static DEFINE_MUTEX(cpu_opv_offline_lock);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * The cpu_opv system call executes a vector of operations on behalf of</span>
<span class="quote">&gt;&gt; + * user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="quote">&gt;&gt; + * from readv() and writev() system calls which take a &quot;struct iovec&quot;</span>
<span class="quote">&gt;&gt; + * array as argument.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt;&gt; + * left shift, and right shift. The system call receives a CPU number</span>
<span class="quote">&gt;&gt; + * from user-space as argument, which is the CPU on which those</span>
<span class="quote">&gt;&gt; + * operations need to be performed. All preparation steps such as</span>
<span class="quote">&gt;&gt; + * loading pointers, and applying offsets to arrays, need to be</span>
<span class="quote">&gt;&gt; + * performed by user-space before invoking the system call. The</span>
<span class="quote">&gt;&gt; + * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="quote">&gt;&gt; + * preparation step did not change between preparation of system call</span>
<span class="quote">&gt;&gt; + * inputs and operation execution within the preempt-off critical</span>
<span class="quote">&gt;&gt; + * section.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt;&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt;&gt; + * to first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt;&gt; + * faulting-in the pages. Then, preemption is disabled, and the</span>
<span class="quote">&gt;&gt; + * operations are performed atomically with respect to other thread</span>
<span class="quote">&gt;&gt; + * execution on that CPU, without generating any page fault.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt;&gt; + * enforced, and a overall maximum length sum, so user-space cannot</span>
<span class="quote">&gt;&gt; + * generate a too long preempt-off critical section. Each operation is</span>
<span class="quote">&gt;&gt; + * also limited a length of PAGE_SIZE bytes, meaning that an operation</span>
<span class="quote">&gt;&gt; + * can touch a maximum of 4 pages (memcpy: 2 pages for source, 2 pages</span>
<span class="quote">&gt;&gt; + * for destination if addresses are not aligned on page boundaries).</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * If the thread is not running on the requested CPU, a new</span>
<span class="quote">&gt;&gt; + * push_task_to_cpu() is invoked to migrate the task to the requested</span>
<span class="quote">&gt;&gt; + * CPU.  If the requested CPU is not part of the cpus allowed mask of</span>
<span class="quote">&gt;&gt; + * the thread, the system call fails with EINVAL. After the migration</span>
<span class="quote">&gt;&gt; + * has been performed, preemption is disabled, and the current CPU</span>
<span class="quote">&gt;&gt; + * number is checked again and compared to the requested CPU number. If</span>
<span class="quote">&gt;&gt; + * it still differs, it means the scheduler migrated us away from that</span>
<span class="quote">&gt;&gt; + * CPU. Return EAGAIN to user-space in that case, and let user-space</span>
<span class="quote">&gt;&gt; + * retry (either requesting the same CPU number, or a different one,</span>
<span class="quote">&gt;&gt; + * depending on the user-space algorithm constraints).</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Check operation types and length parameters.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static int cpu_opv_check(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int i;</span>
<span class="quote">&gt;&gt; +       uint32_t sum = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt;&gt; +               struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt;&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               default:</span>
<span class="quote">&gt;&gt; +                       sum += op-&gt;len;</span>
<span class="quote">&gt;&gt; +               }</span>
<span class="quote">&gt;&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt;&gt; +               case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_MEMCPY_OP:</span>
<span class="quote">&gt;&gt; +                       if (op-&gt;len &gt; CPU_OP_DATA_LEN_MAX)</span>
<span class="quote">&gt;&gt; +                               return -EINVAL;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_ADD_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_OR_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_AND_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_XOR_OP:</span>
<span class="quote">&gt;&gt; +                       switch (op-&gt;len) {</span>
<span class="quote">&gt;&gt; +                       case 1:</span>
<span class="quote">&gt;&gt; +                       case 2:</span>
<span class="quote">&gt;&gt; +                       case 4:</span>
<span class="quote">&gt;&gt; +                       case 8:</span>
<span class="quote">&gt;&gt; +                               break;</span>
<span class="quote">&gt;&gt; +                       default:</span>
<span class="quote">&gt;&gt; +                               return -EINVAL;</span>
<span class="quote">&gt;&gt; +                       }</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_LSHIFT_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_RSHIFT_OP:</span>
<span class="quote">&gt;&gt; +                       switch (op-&gt;len) {</span>
<span class="quote">&gt;&gt; +                       case 1:</span>
<span class="quote">&gt;&gt; +                               if (op-&gt;u.shift_op.bits &gt; 7)</span>
<span class="quote">&gt;&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt;&gt; +                               break;</span>
<span class="quote">&gt;&gt; +                       case 2:</span>
<span class="quote">&gt;&gt; +                               if (op-&gt;u.shift_op.bits &gt; 15)</span>
<span class="quote">&gt;&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt;&gt; +                               break;</span>
<span class="quote">&gt;&gt; +                       case 4:</span>
<span class="quote">&gt;&gt; +                               if (op-&gt;u.shift_op.bits &gt; 31)</span>
<span class="quote">&gt;&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt;&gt; +                               break;</span>
<span class="quote">&gt;&gt; +                       case 8:</span>
<span class="quote">&gt;&gt; +                               if (op-&gt;u.shift_op.bits &gt; 63)</span>
<span class="quote">&gt;&gt; +                                       return -EINVAL;</span>
<span class="quote">&gt;&gt; +                               break;</span>
<span class="quote">&gt;&gt; +                       default:</span>
<span class="quote">&gt;&gt; +                               return -EINVAL;</span>
<span class="quote">&gt;&gt; +                       }</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               default:</span>
<span class="quote">&gt;&gt; +                       return -EINVAL;</span>
<span class="quote">&gt;&gt; +               }</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       if (sum &gt; CPU_OP_VEC_DATA_LEN_MAX)</span>
<span class="quote">&gt;&gt; +               return -EINVAL;</span>
<span class="quote">&gt;&gt; +       return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="quote">&gt;&gt; +               unsigned long len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       return ((addr + len - 1) &gt;&gt; PAGE_SHIFT) - (addr &gt;&gt; PAGE_SHIFT) + 1;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int cpu_op_check_page(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       struct address_space *mapping;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (is_zone_device_page(page))</span>
<span class="quote">&gt;&gt; +               return -EFAULT;</span>
<span class="quote">&gt;&gt; +       page = compound_head(page);</span>
<span class="quote">&gt;&gt; +       mapping = READ_ONCE(page-&gt;mapping);</span>
<span class="quote">&gt;&gt; +       if (!mapping) {</span>
<span class="quote">&gt;&gt; +               int shmem_swizzled;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               /*</span>
<span class="quote">&gt;&gt; +                * Check again with page lock held to guard against</span>
<span class="quote">&gt;&gt; +                * memory pressure making shmem_writepage move the page</span>
<span class="quote">&gt;&gt; +                * from filecache to swapcache.</span>
<span class="quote">&gt;&gt; +                */</span>
<span class="quote">&gt;&gt; +               lock_page(page);</span>
<span class="quote">&gt;&gt; +               shmem_swizzled = PageSwapCache(page) || page-&gt;mapping;</span>
<span class="quote">&gt;&gt; +               unlock_page(page);</span>
<span class="quote">&gt;&gt; +               if (shmem_swizzled)</span>
<span class="quote">&gt;&gt; +                       return -EAGAIN;</span>
<span class="quote">&gt;&gt; +               return -EFAULT;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Refusing device pages, the zero page, pages in the gate area, and</span>
<span class="quote">&gt;&gt; + * special mappings. Inspired from futex.c checks.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static int cpu_op_check_pages(struct page **pages,</span>
<span class="quote">&gt;&gt; +               unsigned long nr_pages)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       unsigned long i;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       for (i = 0; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt;&gt; +               int ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               ret = cpu_op_check_page(pages[i]);</span>
<span class="quote">&gt;&gt; +               if (ret)</span>
<span class="quote">&gt;&gt; +                       return ret;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt;&gt; +               struct cpu_opv_pinned_pages *pin_pages, int write)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       struct page *pages[2];</span>
<span class="quote">&gt;&gt; +       int ret, nr_pages;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (!len)</span>
<span class="quote">&gt;&gt; +               return 0;</span>
<span class="quote">&gt;&gt; +       nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="quote">&gt;&gt; +       BUG_ON(nr_pages &gt; 2);</span>
<span class="quote">&gt;&gt; +       if (!pin_pages-&gt;is_kmalloc &amp;&amp; pin_pages-&gt;nr + nr_pages</span>
<span class="quote">&gt;&gt; +                       &gt; NR_PINNED_PAGES_ON_STACK) {</span>
<span class="quote">&gt;&gt; +               struct page **pinned_pages =</span>
<span class="quote">&gt;&gt; +                       kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="quote">&gt;&gt; +                               * sizeof(struct page *), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +               if (!pinned_pages)</span>
<span class="quote">&gt;&gt; +                       return -ENOMEM;</span>
<span class="quote">&gt;&gt; +               memcpy(pinned_pages, pin_pages-&gt;pages,</span>
<span class="quote">&gt;&gt; +                       pin_pages-&gt;nr * sizeof(struct page *));</span>
<span class="quote">&gt;&gt; +               pin_pages-&gt;pages = pinned_pages;</span>
<span class="quote">&gt;&gt; +               pin_pages-&gt;is_kmalloc = true;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +again:</span>
<span class="quote">&gt;&gt; +       ret = get_user_pages_fast(addr, nr_pages, write, pages);</span>
<span class="quote">&gt;&gt; +       if (ret &lt; nr_pages) {</span>
<span class="quote">&gt;&gt; +               if (ret &gt; 0)</span>
<span class="quote">&gt;&gt; +                       put_page(pages[0]);</span>
<span class="quote">&gt;&gt; +               return -EFAULT;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * Refuse device pages, the zero page, pages in the gate area,</span>
<span class="quote">&gt;&gt; +        * and special mappings.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       ret = cpu_op_check_pages(pages, nr_pages);</span>
<span class="quote">&gt;&gt; +       if (ret == -EAGAIN) {</span>
<span class="quote">&gt;&gt; +               put_page(pages[0]);</span>
<span class="quote">&gt;&gt; +               if (nr_pages &gt; 1)</span>
<span class="quote">&gt;&gt; +                       put_page(pages[1]);</span>
<span class="quote">&gt;&gt; +               goto again;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       if (ret)</span>
<span class="quote">&gt;&gt; +               goto error;</span>
<span class="quote">&gt;&gt; +       pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[0];</span>
<span class="quote">&gt;&gt; +       if (nr_pages &gt; 1)</span>
<span class="quote">&gt;&gt; +               pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[1];</span>
<span class="quote">&gt;&gt; +       return 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +error:</span>
<span class="quote">&gt;&gt; +       put_page(pages[0]);</span>
<span class="quote">&gt;&gt; +       if (nr_pages &gt; 1)</span>
<span class="quote">&gt;&gt; +               put_page(pages[1]);</span>
<span class="quote">&gt;&gt; +       return -EFAULT;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="quote">&gt;&gt; +               struct cpu_opv_pinned_pages *pin_pages)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret, i;</span>
<span class="quote">&gt;&gt; +       bool expect_fault = false;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       /* Check access, pin pages. */</span>
<span class="quote">&gt;&gt; +       for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt;&gt; +               struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt;&gt; +               case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt;&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +                       expect_fault = op-&gt;u.compare_op.expect_fault_a;</span>
<span class="quote">&gt;&gt; +                       if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +                                       (unsigned long)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len, pin_pages, 0);</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +                       expect_fault = op-&gt;u.compare_op.expect_fault_b;</span>
<span class="quote">&gt;&gt; +                       if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +                                       (unsigned long)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len, pin_pages, 0);</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_MEMCPY_OP:</span>
<span class="quote">&gt;&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +                       expect_fault = op-&gt;u.memcpy_op.expect_fault_dst;</span>
<span class="quote">&gt;&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +                                       (unsigned long)op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +                       expect_fault = op-&gt;u.memcpy_op.expect_fault_src;</span>
<span class="quote">&gt;&gt; +                       if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +                                       (unsigned long)op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len, pin_pages, 0);</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_ADD_OP:</span>
<span class="quote">&gt;&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +                       expect_fault = op-&gt;u.arithmetic_op.expect_fault_p;</span>
<span class="quote">&gt;&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +                                       (unsigned long)op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_OR_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_AND_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_XOR_OP:</span>
<span class="quote">&gt;&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +                       expect_fault = op-&gt;u.bitwise_op.expect_fault_p;</span>
<span class="quote">&gt;&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +                                       (unsigned long)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_LSHIFT_OP:</span>
<span class="quote">&gt;&gt; +               case CPU_RSHIFT_OP:</span>
<span class="quote">&gt;&gt; +                       ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +                       expect_fault = op-&gt;u.shift_op.expect_fault_p;</span>
<span class="quote">&gt;&gt; +                       if (!access_ok(VERIFY_WRITE,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len))</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +                                       (unsigned long)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len, pin_pages, 1);</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               goto error;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               default:</span>
<span class="quote">&gt;&gt; +                       return -EINVAL;</span>
<span class="quote">&gt;&gt; +               }</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +error:</span>
<span class="quote">&gt;&gt; +       for (i = 0; i &lt; pin_pages-&gt;nr; i++)</span>
<span class="quote">&gt;&gt; +               put_page(pin_pages-&gt;pages[i]);</span>
<span class="quote">&gt;&gt; +       pin_pages-&gt;nr = 0;</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * If faulting access is expected, return EAGAIN to user-space.</span>
<span class="quote">&gt;&gt; +        * It allows user-space to distinguish between a fault caused by</span>
<span class="quote">&gt;&gt; +        * an access which is expect to fault (e.g. due to concurrent</span>
<span class="quote">&gt;&gt; +        * unmapping of underlying memory) from an unexpected fault from</span>
<span class="quote">&gt;&gt; +        * which a retry would not recover.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       if (ret == -EFAULT &amp;&amp; expect_fault)</span>
<span class="quote">&gt;&gt; +               return -EAGAIN;</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt;&gt; +static int do_cpu_op_compare_iter(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       char bufa[TMP_BUFLEN], bufb[TMP_BUFLEN];</span>
<span class="quote">&gt;&gt; +       uint32_t compared = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       while (compared != len) {</span>
<span class="quote">&gt;&gt; +               unsigned long to_compare;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               to_compare = min_t(uint32_t, TMP_BUFLEN, len - compared);</span>
<span class="quote">&gt;&gt; +               if (__copy_from_user_inatomic(bufa, a + compared, to_compare))</span>
<span class="quote">&gt;&gt; +                       return -EFAULT;</span>
<span class="quote">&gt;&gt; +               if (__copy_from_user_inatomic(bufb, b + compared, to_compare))</span>
<span class="quote">&gt;&gt; +                       return -EFAULT;</span>
<span class="quote">&gt;&gt; +               if (memcmp(bufa, bufb, to_compare))</span>
<span class="quote">&gt;&gt; +                       return 1;       /* different */</span>
<span class="quote">&gt;&gt; +               compared += to_compare;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return 0;       /* same */</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt;&gt; +static int do_cpu_op_compare(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +       union {</span>
<span class="quote">&gt;&gt; +               uint8_t _u8;</span>
<span class="quote">&gt;&gt; +               uint16_t _u16;</span>
<span class="quote">&gt;&gt; +               uint32_t _u32;</span>
<span class="quote">&gt;&gt; +               uint64_t _u64;</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt;&gt; +               uint32_t _u64_split[2];</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +       } tmp[2];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       pagefault_disable();</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[0]._u8, (uint8_t __user *)a))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[1]._u8, (uint8_t __user *)b))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               ret = !!(tmp[0]._u8 != tmp[1]._u8);</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[0]._u16, (uint16_t __user *)a))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[1]._u16, (uint16_t __user *)b))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               ret = !!(tmp[0]._u16 != tmp[1]._u16);</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[0]._u32, (uint32_t __user *)a))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[1]._u32, (uint32_t __user *)b))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               ret = !!(tmp[0]._u32 != tmp[1]._u32);</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[0]._u64, (uint64_t __user *)a))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[1]._u64, (uint64_t __user *)b))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[0]._u64_split[0], (uint32_t __user *)a))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[0]._u64_split[1], (uint32_t __user *)a + 1))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[1]._u64_split[0], (uint32_t __user *)b))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp[1]._u64_split[1], (uint32_t __user *)b + 1))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +               ret = !!(tmp[0]._u64 != tmp[1]._u64);</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               pagefault_enable();</span>
<span class="quote">&gt;&gt; +               return do_cpu_op_compare_iter(a, b, len);</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +       pagefault_enable();</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Return 0 on success, &lt; 0 on error. */</span>
<span class="quote">&gt;&gt; +static int do_cpu_op_memcpy_iter(void __user *dst, void __user *src,</span>
<span class="quote">&gt;&gt; +               uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       char buf[TMP_BUFLEN];</span>
<span class="quote">&gt;&gt; +       uint32_t copied = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       while (copied != len) {</span>
<span class="quote">&gt;&gt; +               unsigned long to_copy;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               to_copy = min_t(uint32_t, TMP_BUFLEN, len - copied);</span>
<span class="quote">&gt;&gt; +               if (__copy_from_user_inatomic(buf, src + copied, to_copy))</span>
<span class="quote">&gt;&gt; +                       return -EFAULT;</span>
<span class="quote">&gt;&gt; +               if (__copy_to_user_inatomic(dst + copied, buf, to_copy))</span>
<span class="quote">&gt;&gt; +                       return -EFAULT;</span>
<span class="quote">&gt;&gt; +               copied += to_copy;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Return 0 on success, &lt; 0 on error. */</span>
<span class="quote">&gt;&gt; +static int do_cpu_op_memcpy(void __user *dst, void __user *src, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +       union {</span>
<span class="quote">&gt;&gt; +               uint8_t _u8;</span>
<span class="quote">&gt;&gt; +               uint16_t _u16;</span>
<span class="quote">&gt;&gt; +               uint32_t _u32;</span>
<span class="quote">&gt;&gt; +               uint64_t _u64;</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt;&gt; +               uint32_t _u64_split[2];</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +       } tmp;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       pagefault_disable();</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u8, (uint8_t __user *)src))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u8, (uint8_t __user *)dst))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u16, (uint16_t __user *)src))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u16, (uint16_t __user *)dst))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u32, (uint32_t __user *)src))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u32, (uint32_t __user *)dst))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u64, (uint64_t __user *)src))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u64, (uint64_t __user *)dst))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u64_split[0], (uint32_t __user *)src))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u64_split[1], (uint32_t __user *)src + 1))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u64_split[0], (uint32_t __user *)dst))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u64_split[1], (uint32_t __user *)dst + 1))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               pagefault_enable();</span>
<span class="quote">&gt;&gt; +               return do_cpu_op_memcpy_iter(dst, src, len);</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       ret = 0;</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +       pagefault_enable();</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int op_add_fn(union op_fn_data *data, uint64_t count, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u8 += (uint8_t)count;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u16 += (uint16_t)count;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u32 += (uint32_t)count;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u64 += (uint64_t)count;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int op_or_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u8 |= (uint8_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u16 |= (uint16_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u32 |= (uint32_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u64 |= (uint64_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int op_and_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u8 &amp;= (uint8_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u16 &amp;= (uint16_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u32 &amp;= (uint32_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u64 &amp;= (uint64_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int op_xor_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u8 ^= (uint8_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u16 ^= (uint16_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u32 ^= (uint32_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u64 ^= (uint64_t)mask;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int op_lshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u8 &lt;&lt;= (uint8_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u16 &lt;&lt;= (uint16_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u32 &lt;&lt;= (uint32_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u64 &lt;&lt;= (uint64_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int op_rshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u8 &gt;&gt;= (uint8_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u16 &gt;&gt;= (uint16_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u32 &gt;&gt;= (uint32_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +               data-&gt;_u64 &gt;&gt;= (uint64_t)bits;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Return 0 on success, &lt; 0 on error. */</span>
<span class="quote">&gt;&gt; +static int do_cpu_op_fn(op_fn_t op_fn, void __user *p, uint64_t v,</span>
<span class="quote">&gt;&gt; +               uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +       union op_fn_data tmp;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       pagefault_disable();</span>
<span class="quote">&gt;&gt; +       switch (len) {</span>
<span class="quote">&gt;&gt; +       case 1:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 2:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 4:</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case 8:</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +               if (op_fn(&amp;tmp, v, len))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +               if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="quote">&gt;&gt; +                       goto end;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               goto end;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       ret = 0;</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +       pagefault_enable();</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int __do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int i, ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt;&gt; +               struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               /* Guarantee a compiler barrier between each operation. */</span>
<span class="quote">&gt;&gt; +               barrier();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +               switch (op-&gt;op) {</span>
<span class="quote">&gt;&gt; +               case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_compare(</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret &lt; 0)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       /*</span>
<span class="quote">&gt;&gt; +                        * Stop execution, return op index + 1 if comparison</span>
<span class="quote">&gt;&gt; +                        * differs.</span>
<span class="quote">&gt;&gt; +                        */</span>
<span class="quote">&gt;&gt; +                       if (ret &gt; 0)</span>
<span class="quote">&gt;&gt; +                               return i + 1;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_compare(</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret &lt; 0)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       /*</span>
<span class="quote">&gt;&gt; +                        * Stop execution, return op index + 1 if comparison</span>
<span class="quote">&gt;&gt; +                        * is identical.</span>
<span class="quote">&gt;&gt; +                        */</span>
<span class="quote">&gt;&gt; +                       if (ret == 0)</span>
<span class="quote">&gt;&gt; +                               return i + 1;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_MEMCPY_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_memcpy(</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_ADD_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_fn(op_add_fn,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;u.arithmetic_op.count, op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_OR_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_fn(op_or_fn,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_AND_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_fn(op_and_fn,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_XOR_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_fn(op_xor_fn,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_LSHIFT_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_fn(op_lshift_fn,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_RSHIFT_OP:</span>
<span class="quote">&gt;&gt; +                       ret = do_cpu_op_fn(op_rshift_fn,</span>
<span class="quote">&gt;&gt; +                                       (void __user *)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt;&gt; +                                       op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="quote">&gt;&gt; +                       /* Stop execution on error. */</span>
<span class="quote">&gt;&gt; +                       if (ret)</span>
<span class="quote">&gt;&gt; +                               return ret;</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               case CPU_MB_OP:</span>
<span class="quote">&gt;&gt; +                       smp_mb();</span>
<span class="quote">&gt;&gt; +                       break;</span>
<span class="quote">&gt;&gt; +               default:</span>
<span class="quote">&gt;&gt; +                       return -EINVAL;</span>
<span class="quote">&gt;&gt; +               }</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt, int cpu)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       int ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (cpu != raw_smp_processor_id()) {</span>
<span class="quote">&gt;&gt; +               ret = push_task_to_cpu(current, cpu);</span>
<span class="quote">&gt;&gt; +               if (ret)</span>
<span class="quote">&gt;&gt; +                       goto check_online;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       preempt_disable();</span>
<span class="quote">&gt;&gt; +       if (cpu != smp_processor_id()) {</span>
<span class="quote">&gt;&gt; +               ret = -EAGAIN;</span>
<span class="quote">&gt;&gt; +               goto end;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +       preempt_enable();</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +check_online:</span>
<span class="quote">&gt;&gt; +       if (!cpu_possible(cpu))</span>
<span class="quote">&gt;&gt; +               return -EINVAL;</span>
<span class="quote">&gt;&gt; +       get_online_cpus();</span>
<span class="quote">&gt;&gt; +       if (cpu_online(cpu)) {</span>
<span class="quote">&gt;&gt; +               ret = -EAGAIN;</span>
<span class="quote">&gt;&gt; +               goto put_online_cpus;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * CPU is offline. Perform operation from the current CPU with</span>
<span class="quote">&gt;&gt; +        * cpu_online read lock held, preventing that CPU from coming online,</span>
<span class="quote">&gt;&gt; +        * and with mutex held, providing mutual exclusion against other</span>
<span class="quote">&gt;&gt; +        * CPUs also finding out about an offline CPU.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       mutex_lock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt;&gt; +       ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt;&gt; +       mutex_unlock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt;&gt; +put_online_cpus:</span>
<span class="quote">&gt;&gt; +       put_online_cpus();</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * cpu_opv - execute operation vector on a given CPU with preempt off.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Userspace should pass current CPU number as parameter. May fail with</span>
<span class="quote">&gt;&gt; + * -EAGAIN if currently executing on the wrong CPU.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +SYSCALL_DEFINE4(cpu_opv, struct cpu_op __user *, ucpuopv, int, cpuopcnt,</span>
<span class="quote">&gt;&gt; +               int, cpu, int, flags)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       struct cpu_op cpuopv[CPU_OP_VEC_LEN_MAX];</span>
<span class="quote">&gt;&gt; +       struct page *pinned_pages_on_stack[NR_PINNED_PAGES_ON_STACK];</span>
<span class="quote">&gt;&gt; +       struct cpu_opv_pinned_pages pin_pages = {</span>
<span class="quote">&gt;&gt; +               .pages = pinned_pages_on_stack,</span>
<span class="quote">&gt;&gt; +               .nr = 0,</span>
<span class="quote">&gt;&gt; +               .is_kmalloc = false,</span>
<span class="quote">&gt;&gt; +       };</span>
<span class="quote">&gt;&gt; +       int ret, i;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (unlikely(flags))</span>
<span class="quote">&gt;&gt; +               return -EINVAL;</span>
<span class="quote">&gt;&gt; +       if (unlikely(cpu &lt; 0))</span>
<span class="quote">&gt;&gt; +               return -EINVAL;</span>
<span class="quote">&gt;&gt; +       if (cpuopcnt &lt; 0 || cpuopcnt &gt; CPU_OP_VEC_LEN_MAX)</span>
<span class="quote">&gt;&gt; +               return -EINVAL;</span>
<span class="quote">&gt;&gt; +       if (copy_from_user(cpuopv, ucpuopv, cpuopcnt * sizeof(struct cpu_op)))</span>
<span class="quote">&gt;&gt; +               return -EFAULT;</span>
<span class="quote">&gt;&gt; +       ret = cpu_opv_check(cpuopv, cpuopcnt);</span>
<span class="quote">&gt;&gt; +       if (ret)</span>
<span class="quote">&gt;&gt; +               return ret;</span>
<span class="quote">&gt;&gt; +       ret = cpu_opv_pin_pages(cpuopv, cpuopcnt, &amp;pin_pages);</span>
<span class="quote">&gt;&gt; +       if (ret)</span>
<span class="quote">&gt;&gt; +               goto end;</span>
<span class="quote">&gt;&gt; +       ret = do_cpu_opv(cpuopv, cpuopcnt, cpu);</span>
<span class="quote">&gt;&gt; +       for (i = 0; i &lt; pin_pages.nr; i++)</span>
<span class="quote">&gt;&gt; +               put_page(pin_pages.pages[i]);</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +       if (pin_pages.is_kmalloc)</span>
<span class="quote">&gt;&gt; +               kfree(pin_pages.pages);</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="quote">&gt;&gt; index 6bba05f47e51..e547f93a46c2 100644</span>
<span class="quote">&gt;&gt; --- a/kernel/sched/core.c</span>
<span class="quote">&gt;&gt; +++ b/kernel/sched/core.c</span>
<span class="quote">&gt;&gt; @@ -1052,6 +1052,43 @@ void do_set_cpus_allowed(struct task_struct *p, const</span>
<span class="quote">&gt;&gt; struct cpumask *new_mask)</span>
<span class="quote">&gt;&gt;                 set_curr_task(rq, p);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       struct rq_flags rf;</span>
<span class="quote">&gt;&gt; +       struct rq *rq;</span>
<span class="quote">&gt;&gt; +       int ret = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       rq = task_rq_lock(p, &amp;rf);</span>
<span class="quote">&gt;&gt; +       update_rq_clock(rq);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (!cpumask_test_cpu(dest_cpu, &amp;p-&gt;cpus_allowed)) {</span>
<span class="quote">&gt;&gt; +               ret = -EINVAL;</span>
<span class="quote">&gt;&gt; +               goto out;</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (task_cpu(p) == dest_cpu)</span>
<span class="quote">&gt;&gt; +               goto out;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (task_running(rq, p) || p-&gt;state == TASK_WAKING) {</span>
<span class="quote">&gt;&gt; +               struct migration_arg arg = { p, dest_cpu };</span>
<span class="quote">&gt;&gt; +               /* Need help from migration thread: drop lock and wait. */</span>
<span class="quote">&gt;&gt; +               task_rq_unlock(rq, p, &amp;rf);</span>
<span class="quote">&gt;&gt; +               stop_one_cpu(cpu_of(rq), migration_cpu_stop, &amp;arg);</span>
<span class="quote">&gt;&gt; +               tlb_migrate_finish(p-&gt;mm);</span>
<span class="quote">&gt;&gt; +               return 0;</span>
<span class="quote">&gt;&gt; +       } else if (task_on_rq_queued(p)) {</span>
<span class="quote">&gt;&gt; +               /*</span>
<span class="quote">&gt;&gt; +                * OK, since we&#39;re going to drop the lock immediately</span>
<span class="quote">&gt;&gt; +                * afterwards anyway.</span>
<span class="quote">&gt;&gt; +                */</span>
<span class="quote">&gt;&gt; +               rq = move_queued_task(rq, &amp;rf, p, dest_cpu);</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +out:</span>
<span class="quote">&gt;&gt; +       task_rq_unlock(rq, p, &amp;rf);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * Change a given task&#39;s CPU affinity. Migrate the thread to a</span>
<span class="quote">&gt;&gt;   * proper CPU and schedule it away if the CPU it&#39;s executing on</span>
<span class="quote">&gt;&gt; diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="quote">&gt;&gt; index 3b448ba82225..cab256c1720a 100644</span>
<span class="quote">&gt;&gt; --- a/kernel/sched/sched.h</span>
<span class="quote">&gt;&gt; +++ b/kernel/sched/sched.h</span>
<span class="quote">&gt;&gt; @@ -1209,6 +1209,8 @@ static inline void __set_task_cpu(struct task_struct *p,</span>
<span class="quote">&gt;&gt; unsigned int cpu)</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * Tunables that become constants when CONFIG_SCHED_DEBUG is off:</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt; diff --git a/kernel/sys_ni.c b/kernel/sys_ni.c</span>
<span class="quote">&gt;&gt; index bfa1ee1bf669..59e622296dc3 100644</span>
<span class="quote">&gt;&gt; --- a/kernel/sys_ni.c</span>
<span class="quote">&gt;&gt; +++ b/kernel/sys_ni.c</span>
<span class="quote">&gt;&gt; @@ -262,3 +262,4 @@ cond_syscall(sys_pkey_free);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  /* restartable sequence */</span>
<span class="quote">&gt;&gt;  cond_syscall(sys_rseq);</span>
<span class="quote">&gt;&gt; +cond_syscall(sys_cpu_opv);</span>
<span class="quote">&gt;&gt; --</span>
<span class="quote">&gt;&gt; 2.11.0</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; Michael Kerrisk</span>
<span class="quote">&gt; Linux man-pages maintainer; http://www.kernel.org/doc/man-pages/</span>
<span class="quote">&gt; Linux/UNIX System Programming Training: http://man7.org/training/</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 16, 2017, 11:26 p.m.</div>
<pre class="content">
On Tue, 14 Nov 2017, Mathieu Desnoyers wrote:
<span class="quote">&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt; +# include &lt;linux/types.h&gt;</span>
<span class="quote">&gt; +#else	/* #ifdef __KERNEL__ */</span>

  		   Sigh.
<span class="quote">
&gt; +# include &lt;stdint.h&gt;</span>
<span class="quote">&gt; +#endif	/* #else #ifdef __KERNEL__ */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/byteorder.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef __LP64__</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64(field)			uint64_t field</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	field = (intptr_t)v</span>
<span class="quote">&gt; +#elif defined(__BYTE_ORDER) ? \</span>
<span class="quote">&gt; +	__BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64(field)	uint32_t field ## _padding, field</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="quote">&gt; +	field ## _padding = 0, field = (intptr_t)v</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64(field)	uint32_t field, field ## _padding</span>
<span class="quote">&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="quote">&gt; +	field = (intptr_t)v, field ## _padding = 0</span>
<span class="quote">&gt; +#endif</span>

So in the rseq patch you have:

+#ifdef __LP64__
+# define RSEQ_FIELD_u32_u64(field)                     uint64_t field
+# define RSEQ_FIELD_u32_u64_INIT_ONSTACK(field, v)     field = (intptr_t)v
+#elif defined(__BYTE_ORDER) ? \
+       __BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)
+# define RSEQ_FIELD_u32_u64(field)     uint32_t field ## _padding, field
+# define RSEQ_FIELD_u32_u64_INIT_ONSTACK(field, v)     \
+       field ## _padding = 0, field = (intptr_t)v
+#else
+# define RSEQ_FIELD_u32_u64(field)     uint32_t field, field ## _padding
+# define RSEQ_FIELD_u32_u64_INIT_ONSTACK(field, v)     \
+       field = (intptr_t)v, field ## _padding = 0
+#endif

IOW the same macro maze. Please use a separate header file which provides
these macros once and share them between the two facilities.
<span class="quote">
&gt; +#define CPU_OP_VEC_LEN_MAX		16</span>
<span class="quote">&gt; +#define CPU_OP_ARG_LEN_MAX		24</span>
<span class="quote">&gt; +/* Max. data len per operation. */</span>
<span class="quote">&gt; +#define CPU_OP_DATA_LEN_MAX		PAGE_SIZE</span>

That&#39;s something between 4K and 256K depending on the architecture. 

You really want to allow up to 256K data copy with preemption disabled?
Shudder.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * Max. data len for overall vector. We to restrict the amount of</span>

We to ?
<span class="quote">
&gt; + * user-space data touched by the kernel in non-preemptible context so</span>
<span class="quote">&gt; + * we do not introduce long scheduler latencies.</span>
<span class="quote">&gt; + * This allows one copy of up to 4096 bytes, and 15 operations touching</span>
<span class="quote">&gt; + * 8 bytes each.</span>
<span class="quote">&gt; + * This limit is applied to the sum of length specified for all</span>
<span class="quote">&gt; + * operations in a vector.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define CPU_OP_VEC_DATA_LEN_MAX		(4096 + 15*8)</span>

Magic numbers. Please use proper defines for heavens sake.
<span class="quote">
&gt; +#define CPU_OP_MAX_PAGES		4	/* Max. pages per op. */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +enum cpu_op_type {</span>
<span class="quote">&gt; +	CPU_COMPARE_EQ_OP,	/* compare */</span>
<span class="quote">&gt; +	CPU_COMPARE_NE_OP,	/* compare */</span>
<span class="quote">&gt; +	CPU_MEMCPY_OP,		/* memcpy */</span>
<span class="quote">&gt; +	CPU_ADD_OP,		/* arithmetic */</span>
<span class="quote">&gt; +	CPU_OR_OP,		/* bitwise */</span>
<span class="quote">&gt; +	CPU_AND_OP,		/* bitwise */</span>
<span class="quote">&gt; +	CPU_XOR_OP,		/* bitwise */</span>
<span class="quote">&gt; +	CPU_LSHIFT_OP,		/* shift */</span>
<span class="quote">&gt; +	CPU_RSHIFT_OP,		/* shift */</span>
<span class="quote">&gt; +	CPU_MB_OP,		/* memory barrier */</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Vector of operations to perform. Limited to 16. */</span>
<span class="quote">&gt; +struct cpu_op {</span>
<span class="quote">&gt; +	int32_t op;	/* enum cpu_op_type. */</span>
<span class="quote">&gt; +	uint32_t len;	/* data length, in bytes. */</span>

Please get rid of these tail comments
<span class="quote">
&gt; +	union {</span>
<span class="quote">&gt; +#define TMP_BUFLEN			64</span>
<span class="quote">&gt; +#define NR_PINNED_PAGES_ON_STACK	8</span>

8 pinned pages on stack? Which stack?
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * The cpu_opv system call executes a vector of operations on behalf of</span>
<span class="quote">&gt; + * user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="quote">&gt; + * from readv() and writev() system calls which take a &quot;struct iovec&quot;</span>

s/from/by/
<span class="quote">
&gt; + * array as argument.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt; + * left shift, and right shift. The system call receives a CPU number</span>
<span class="quote">&gt; + * from user-space as argument, which is the CPU on which those</span>
<span class="quote">&gt; + * operations need to be performed. All preparation steps such as</span>
<span class="quote">&gt; + * loading pointers, and applying offsets to arrays, need to be</span>
<span class="quote">&gt; + * performed by user-space before invoking the system call. The</span>

loading pointers and applying offsets? That makes no sense.
<span class="quote">
&gt; + * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="quote">&gt; + * preparation step did not change between preparation of system call</span>
<span class="quote">&gt; + * inputs and operation execution within the preempt-off critical</span>
<span class="quote">&gt; + * section.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt; + * to first pin all pages touched by each operation. This takes care of</span>

That doesnt explain it either.
<span class="quote">
&gt; + * faulting-in the pages.  Then, preemption is disabled, and the</span>
<span class="quote">&gt; + * operations are performed atomically with respect to other thread</span>
<span class="quote">&gt; + * execution on that CPU, without generating any page fault.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt; + * enforced, and a overall maximum length sum, so user-space cannot</span>
<span class="quote">&gt; + * generate a too long preempt-off critical section. Each operation is</span>
<span class="quote">&gt; + * also limited a length of PAGE_SIZE bytes, meaning that an operation</span>
<span class="quote">&gt; + * can touch a maximum of 4 pages (memcpy: 2 pages for source, 2 pages</span>
<span class="quote">&gt; + * for destination if addresses are not aligned on page boundaries).</span>

What&#39;s the critical section duration for operations which go to the limits
of this on a average x86 64 machine?
<span class="quote">
&gt; + * If the thread is not running on the requested CPU, a new</span>
<span class="quote">&gt; + * push_task_to_cpu() is invoked to migrate the task to the requested</span>

new push_task_to_cpu()? Once that patch is merged push_task_to_cpu() is
hardly new.

Please refrain from putting function level details into comments which
describe the concept. The function name might change in 3 month from now
and the comment will stay stale, Its sufficient to say:

 * If the thread is not running on the requested CPU it is migrated
 * to it.

That explains the concept. It&#39;s completely irrelevant which mechanism is
used to achieve that.
<span class="quote">
&gt; + * CPU.  If the requested CPU is not part of the cpus allowed mask of</span>
<span class="quote">&gt; + * the thread, the system call fails with EINVAL. After the migration</span>
<span class="quote">&gt; + * has been performed, preemption is disabled, and the current CPU</span>
<span class="quote">&gt; + * number is checked again and compared to the requested CPU number. If</span>
<span class="quote">&gt; + * it still differs, it means the scheduler migrated us away from that</span>
<span class="quote">&gt; + * CPU. Return EAGAIN to user-space in that case, and let user-space</span>
<span class="quote">&gt; + * retry (either requesting the same CPU number, or a different one,</span>
<span class="quote">&gt; + * depending on the user-space algorithm constraints).</span>

This mixture of imperative and impersonated mood is really hard to read.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * Check operation types and length parameters.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static int cpu_opv_check(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +	uint32_t sum = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt; +		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		switch (op-&gt;op) {</span>
<span class="quote">&gt; +		case CPU_MB_OP:</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		default:</span>
<span class="quote">&gt; +			sum += op-&gt;len;</span>
<span class="quote">&gt; +		}</span>

Please separate the switch cases with an empty line.
<span class="quote">
&gt; +static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="quote">&gt; +		unsigned long len)</span>

Please align the arguments

static unsigned long cpu_op_range_nr_pages(unsigned long addr,
					   unsigned long len)

is way simpler to parse. All over the place please.
<span class="quote">
&gt; +{</span>
<span class="quote">&gt; +	return ((addr + len - 1) &gt;&gt; PAGE_SHIFT) - (addr &gt;&gt; PAGE_SHIFT) + 1;</span>

I&#39;m surprised that there is no existing magic for this.
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpu_op_check_page(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct address_space *mapping;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (is_zone_device_page(page))</span>
<span class="quote">&gt; +		return -EFAULT;</span>
<span class="quote">&gt; +	page = compound_head(page);</span>
<span class="quote">&gt; +	mapping = READ_ONCE(page-&gt;mapping);</span>
<span class="quote">&gt; +	if (!mapping) {</span>
<span class="quote">&gt; +		int shmem_swizzled;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Check again with page lock held to guard against</span>
<span class="quote">&gt; +		 * memory pressure making shmem_writepage move the page</span>
<span class="quote">&gt; +		 * from filecache to swapcache.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		lock_page(page);</span>
<span class="quote">&gt; +		shmem_swizzled = PageSwapCache(page) || page-&gt;mapping;</span>
<span class="quote">&gt; +		unlock_page(page);</span>
<span class="quote">&gt; +		if (shmem_swizzled)</span>
<span class="quote">&gt; +			return -EAGAIN;</span>
<span class="quote">&gt; +		return -EFAULT;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Refusing device pages, the zero page, pages in the gate area, and</span>
<span class="quote">&gt; + * special mappings. Inspired from futex.c checks.</span>

That comment should be on the function above, because this loop does not
much checking. Aside of that a more elaborate explanation how those checks
are done and how that works would be appreciated. 
<span class="quote">
&gt; + */</span>
<span class="quote">&gt; +static int cpu_op_check_pages(struct page **pages,</span>
<span class="quote">&gt; +		unsigned long nr_pages)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt; +		int ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		ret = cpu_op_check_page(pages[i]);</span>
<span class="quote">&gt; +		if (ret)</span>
<span class="quote">&gt; +			return ret;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt; +		struct cpu_opv_pinned_pages *pin_pages, int write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page *pages[2];</span>
<span class="quote">&gt; +	int ret, nr_pages;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!len)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +	nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="quote">&gt; +	BUG_ON(nr_pages &gt; 2);</span>

If that happens then you can emit a warning and return a proper error
code. BUG() is the last resort if there is no way to recover. This really
does not qualify.
<span class="quote">
&gt; +	if (!pin_pages-&gt;is_kmalloc &amp;&amp; pin_pages-&gt;nr + nr_pages</span>
<span class="quote">&gt; +			&gt; NR_PINNED_PAGES_ON_STACK) {</span>

Now I see what this is used for. That&#39;s a complete misnomer.

And this check is of course completely self explaining..... NOT!
<span class="quote">
&gt; +		struct page **pinned_pages =</span>
<span class="quote">&gt; +			kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="quote">&gt; +				* sizeof(struct page *), GFP_KERNEL);</span>
<span class="quote">&gt; +		if (!pinned_pages)</span>
<span class="quote">&gt; +			return -ENOMEM;</span>
<span class="quote">&gt; +		memcpy(pinned_pages, pin_pages-&gt;pages,</span>
<span class="quote">&gt; +			pin_pages-&gt;nr * sizeof(struct page *));</span>
<span class="quote">&gt; +		pin_pages-&gt;pages = pinned_pages;</span>
<span class="quote">&gt; +		pin_pages-&gt;is_kmalloc = true;</span>

I have no idea why this needs to be done here and cannot be done in a
preparation step. That&#39;s horrible. You allocate conditionally at some
random place and then free at the end of the syscall.

What&#39;s wrong with:

       prepare_stuff()
       pin_pages()
       do_ops()
       cleanup_stuff()

Hmm?
<span class="quote">
&gt; +	}</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +	ret = get_user_pages_fast(addr, nr_pages, write, pages);</span>
<span class="quote">&gt; +	if (ret &lt; nr_pages) {</span>
<span class="quote">&gt; +		if (ret &gt; 0)</span>
<span class="quote">&gt; +			put_page(pages[0]);</span>
<span class="quote">&gt; +		return -EFAULT;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Refuse device pages, the zero page, pages in the gate area,</span>
<span class="quote">&gt; +	 * and special mappings.</span>

So the same comment again. Really helpful.
<span class="quote">
&gt; +	 */</span>
<span class="quote">&gt; +	ret = cpu_op_check_pages(pages, nr_pages);</span>
<span class="quote">&gt; +	if (ret == -EAGAIN) {</span>
<span class="quote">&gt; +		put_page(pages[0]);</span>
<span class="quote">&gt; +		if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +			put_page(pages[1]);</span>
<span class="quote">&gt; +		goto again;</span>
<span class="quote">&gt; +	}</span>

So why can&#39;t you propagate EAGAIN to the caller and use the error cleanup
label? Or put the sequence of get_user_pages_fast() and check_pages() into
one function and confine the mess there instead of having the same cleanup
sequence 3 times in this function.
<span class="quote">
&gt; +	if (ret)</span>
<span class="quote">&gt; +		goto error;</span>
<span class="quote">&gt; +	pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[0];</span>
<span class="quote">&gt; +	if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +		pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[1];</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +error:</span>
<span class="quote">&gt; +	put_page(pages[0]);</span>
<span class="quote">&gt; +	if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +		put_page(pages[1]);</span>
<span class="quote">&gt; +	return -EFAULT;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="quote">&gt; +		struct cpu_opv_pinned_pages *pin_pages)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int ret, i;</span>
<span class="quote">&gt; +	bool expect_fault = false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Check access, pin pages. */</span>
<span class="quote">&gt; +	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt; +		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		switch (op-&gt;op) {</span>
<span class="quote">&gt; +		case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt; +		case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.compare_op.expect_fault_a;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt; +					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +					op-&gt;len, pin_pages, 0);</span>

Bah, this sucks. Moving the switch() into a separate function spares you
one indentation level and all these horrible to read line breaks.

And again I really have to ask why all of this stuff needs to be type
casted for every invocation. If you use the proper type for the argument
and then do the cast at the function entry then you can spare all that hard
to read crap.
<span class="quote">
&gt; +error:</span>
<span class="quote">&gt; +	for (i = 0; i &lt; pin_pages-&gt;nr; i++)</span>
<span class="quote">&gt; +		put_page(pin_pages-&gt;pages[i]);</span>
<span class="quote">&gt; +	pin_pages-&gt;nr = 0;</span>

Sigh. Why can&#39;t you do that at the call site where you have exactly the
same thing?
<span class="quote">
&gt; +	/*</span>
<span class="quote">&gt; +	 * If faulting access is expected, return EAGAIN to user-space.</span>
<span class="quote">&gt; +	 * It allows user-space to distinguish between a fault caused by</span>
<span class="quote">&gt; +	 * an access which is expect to fault (e.g. due to concurrent</span>
<span class="quote">&gt; +	 * unmapping of underlying memory) from an unexpected fault from</span>
<span class="quote">&gt; +	 * which a retry would not recover.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (ret == -EFAULT &amp;&amp; expect_fault)</span>
<span class="quote">&gt; +		return -EAGAIN;</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt; +static int do_cpu_op_compare_iter(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	char bufa[TMP_BUFLEN], bufb[TMP_BUFLEN];</span>
<span class="quote">&gt; +	uint32_t compared = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	while (compared != len) {</span>
<span class="quote">&gt; +		unsigned long to_compare;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		to_compare = min_t(uint32_t, TMP_BUFLEN, len - compared);</span>
<span class="quote">&gt; +		if (__copy_from_user_inatomic(bufa, a + compared, to_compare))</span>
<span class="quote">&gt; +			return -EFAULT;</span>
<span class="quote">&gt; +		if (__copy_from_user_inatomic(bufb, b + compared, to_compare))</span>
<span class="quote">&gt; +			return -EFAULT;</span>
<span class="quote">&gt; +		if (memcmp(bufa, bufb, to_compare))</span>
<span class="quote">&gt; +			return 1;	/* different */</span>

These tail comments are really crap. It&#39;s entirely obvious that if memcmp
!= 0 the result is different. So what is the exact value aside of making it
hard to read?
<span class="quote">
&gt; +		compared += to_compare;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return 0;	/* same */</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt; +static int do_cpu_op_compare(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int ret = -EFAULT;</span>
<span class="quote">&gt; +	union {</span>
<span class="quote">&gt; +		uint8_t _u8;</span>
<span class="quote">&gt; +		uint16_t _u16;</span>
<span class="quote">&gt; +		uint32_t _u32;</span>
<span class="quote">&gt; +		uint64_t _u64;</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt; +		uint32_t _u64_split[2];</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +	} tmp[2];</span>

I&#39;ve seen the same union before
<span class="quote">
&gt; +union op_fn_data {</span>

......
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	pagefault_disable();</span>
<span class="quote">&gt; +	switch (len) {</span>
<span class="quote">&gt; +	case 1:</span>
<span class="quote">&gt; +		if (__get_user(tmp[0]._u8, (uint8_t __user *)a))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		if (__get_user(tmp[1]._u8, (uint8_t __user *)b))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		ret = !!(tmp[0]._u8 != tmp[1]._u8);</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case 2:</span>
<span class="quote">&gt; +		if (__get_user(tmp[0]._u16, (uint16_t __user *)a))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		if (__get_user(tmp[1]._u16, (uint16_t __user *)b))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		ret = !!(tmp[0]._u16 != tmp[1]._u16);</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case 4:</span>
<span class="quote">&gt; +		if (__get_user(tmp[0]._u32, (uint32_t __user *)a))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		if (__get_user(tmp[1]._u32, (uint32_t __user *)b))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		ret = !!(tmp[0]._u32 != tmp[1]._u32);</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case 8:</span>
<span class="quote">&gt; +#if (BITS_PER_LONG &gt;= 64)</span>

We alredy prepare for 128 bit?
<span class="quote">
&gt; +		if (__get_user(tmp[0]._u64, (uint64_t __user *)a))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		if (__get_user(tmp[1]._u64, (uint64_t __user *)b))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +		if (__get_user(tmp[0]._u64_split[0], (uint32_t __user *)a))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		if (__get_user(tmp[0]._u64_split[1], (uint32_t __user *)a + 1))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		if (__get_user(tmp[1]._u64_split[0], (uint32_t __user *)b))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		if (__get_user(tmp[1]._u64_split[1], (uint32_t __user *)b + 1))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +		ret = !!(tmp[0]._u64 != tmp[1]._u64);</span>

This really sucks.

        union foo va, vb;

	pagefault_disable();
	switch (len) {
	case 1:
	case 2:
	case 4:
	case 8:
		va._u64 = _vb._u64 = 0;
		if (op_get_user(&amp;va, a, len))
			goto out;
		if (op_get_user(&amp;vb, b, len))
			goto out;
		ret = !!(va._u64 != vb._u64);
		break;
	default:
		...

and have

int op_get_user(union foo *val, void *p, int len)
{
	switch (len) {
	case 1:
	     ......

And do the magic once in that function then you spare that copy and pasted
code above. It can be reused in the other ops as well and reduces the amount
of copy and paste code significantly.
<span class="quote">
&gt; +		break;</span>
<span class="quote">&gt; +	default:</span>
<span class="quote">&gt; +		pagefault_enable();</span>
<span class="quote">&gt; +		return do_cpu_op_compare_iter(a, b, len);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +	pagefault_enable();</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">
&gt; +static int __do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i, ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt; +		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Guarantee a compiler barrier between each operation. */</span>
<span class="quote">&gt; +		barrier();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		switch (op-&gt;op) {</span>
<span class="quote">&gt; +		case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt; +			ret = do_cpu_op_compare(</span>
<span class="quote">&gt; +					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt; +					op-&gt;len);</span>

I think you know by now how to spare an indentation level and type casts.
<span class="quote">
&gt; +static int do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt, int cpu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (cpu != raw_smp_processor_id()) {</span>
<span class="quote">&gt; +		ret = push_task_to_cpu(current, cpu);</span>
<span class="quote">&gt; +		if (ret)</span>
<span class="quote">&gt; +			goto check_online;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	preempt_disable();</span>
<span class="quote">&gt; +	if (cpu != smp_processor_id()) {</span>
<span class="quote">&gt; +		ret = -EAGAIN;</span>

This is weird. When the above raw_smp_processor_id() check fails you push,
but here you return. Not really consistent behaviour.
<span class="quote">
&gt; +		goto end;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +	preempt_enable();</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +check_online:</span>
<span class="quote">&gt; +	if (!cpu_possible(cpu))</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	get_online_cpus();</span>
<span class="quote">&gt; +	if (cpu_online(cpu)) {</span>
<span class="quote">&gt; +		ret = -EAGAIN;</span>
<span class="quote">&gt; +		goto put_online_cpus;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * CPU is offline. Perform operation from the current CPU with</span>
<span class="quote">&gt; +	 * cpu_online read lock held, preventing that CPU from coming online,</span>
<span class="quote">&gt; +	 * and with mutex held, providing mutual exclusion against other</span>
<span class="quote">&gt; +	 * CPUs also finding out about an offline CPU.</span>
<span class="quote">&gt; +	 */</span>

That&#39;s not mentioned in the comment at the top IIRC. 
<span class="quote">
&gt; +	mutex_lock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt; +	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt; +	mutex_unlock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt; +put_online_cpus:</span>
<span class="quote">&gt; +	put_online_cpus();</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * cpu_opv - execute operation vector on a given CPU with preempt off.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Userspace should pass current CPU number as parameter. May fail with</span>
<span class="quote">&gt; + * -EAGAIN if currently executing on the wrong CPU.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +SYSCALL_DEFINE4(cpu_opv, struct cpu_op __user *, ucpuopv, int, cpuopcnt,</span>
<span class="quote">&gt; +		int, cpu, int, flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct cpu_op cpuopv[CPU_OP_VEC_LEN_MAX];</span>
<span class="quote">&gt; +	struct page *pinned_pages_on_stack[NR_PINNED_PAGES_ON_STACK];</span>

Oh well.... Naming sucks. 
<span class="quote">
&gt; +	struct cpu_opv_pinned_pages pin_pages = {</span>
<span class="quote">&gt; +		.pages = pinned_pages_on_stack,</span>
<span class="quote">&gt; +		.nr = 0,</span>
<span class="quote">&gt; +		.is_kmalloc = false,</span>
<span class="quote">&gt; +	};</span>
<span class="quote">&gt; +	int ret, i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(flags))</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (unlikely(cpu &lt; 0))</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (cpuopcnt &lt; 0 || cpuopcnt &gt; CPU_OP_VEC_LEN_MAX)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (copy_from_user(cpuopv, ucpuopv, cpuopcnt * sizeof(struct cpu_op)))</span>
<span class="quote">&gt; +		return -EFAULT;</span>
<span class="quote">&gt; +	ret = cpu_opv_check(cpuopv, cpuopcnt);</span>

AFAICT you can calculate the number of pages already in the check and then
do that allocation before pinning the pages.
<span class="quote">
&gt; +	if (ret)</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt; +	ret = cpu_opv_pin_pages(cpuopv, cpuopcnt, &amp;pin_pages);</span>
<span class="quote">&gt; +	if (ret)</span>
<span class="quote">&gt; +		goto end;</span>
<span class="quote">&gt; +	ret = do_cpu_opv(cpuopv, cpuopcnt, cpu);</span>
<span class="quote">&gt; +	for (i = 0; i &lt; pin_pages.nr; i++)</span>
<span class="quote">&gt; +		put_page(pin_pages.pages[i]);</span>
<span class="quote">&gt; +end:</span>
<span class="quote">&gt; +	if (pin_pages.is_kmalloc)</span>
<span class="quote">&gt; +		kfree(pin_pages.pages);</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">

&gt; diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="quote">&gt; index 6bba05f47e51..e547f93a46c2 100644</span>
<span class="quote">&gt; --- a/kernel/sched/core.c</span>
<span class="quote">&gt; +++ b/kernel/sched/core.c</span>
<span class="quote">&gt; @@ -1052,6 +1052,43 @@ void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)</span>
<span class="quote">&gt;  		set_curr_task(rq, p);</span>
<span class="quote">&gt;  }</span>

This is NOT part of this functionality. It&#39;s a prerequisite and wants to be
in a separate patch. And I&#39;m dead tired by now so I leave that thing for
tomorrow or for Peter.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104">Andi Kleen</a> - Nov. 17, 2017, 12:14 a.m.</div>
<pre class="content">
My preference would be just to drop this new super ugly system call. 

It&#39;s also not just the ugliness, but the very large attack surface
that worries me here.

As far as I know it is only needed to support single stepping, correct?

We already have other code that cannot be single stepped, most
prominently the ring 3 vdso time functions that rely on seq locks.

The code that actually cannot be single stepped is very small
and only a few lines.

As far as I know this wasn&#39;t ever a significant problem for anybody, and
there is always a simple workaround (set a temporary break point
after it and continue) 

Same should apply to the new rseq regions. They should be all
tiny, and we should just accept that they cannot be single-stepped,
but only skipped.

Then this whole mess would disappear.

-Andi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 17, 2017, 10:09 a.m.</div>
<pre class="content">
On Thu, 16 Nov 2017, Andi Kleen wrote:
<span class="quote">&gt; My preference would be just to drop this new super ugly system call. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s also not just the ugliness, but the very large attack surface</span>
<span class="quote">&gt; that worries me here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As far as I know it is only needed to support single stepping, correct?</span>

I can&#39;t figure that out because the changelog describes only WHAT the patch
does and not WHY. Useful, isn&#39;t it?
<span class="quote">
&gt; Then this whole mess would disappear.</span>

Agreed. That would be much appreciated.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 17, 2017, 5:14 p.m.</div>
<pre class="content">
----- On Nov 17, 2017, at 5:09 AM, Thomas Gleixner tglx@linutronix.de wrote:
<span class="quote">
&gt; On Thu, 16 Nov 2017, Andi Kleen wrote:</span>
<span class="quote">&gt;&gt; My preference would be just to drop this new super ugly system call.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; It&#39;s also not just the ugliness, but the very large attack surface</span>
<span class="quote">&gt;&gt; that worries me here.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; As far as I know it is only needed to support single stepping, correct?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I can&#39;t figure that out because the changelog describes only WHAT the patch</span>
<span class="quote">&gt; does and not WHY. Useful, isn&#39;t it?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Then this whole mess would disappear.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Agreed. That would be much appreciated.</span>

Let&#39;s have a look at why cpu_opv is needed. I&#39;ll make sure to enhance the
changelog and documentation to include that information.

1) Handling single-stepping from tools

Tools like debuggers, and simulators like record-replay (&quot;rr&quot;) use
single-stepping to run through existing programs. If core libraries start
to use restartable sequences for e.g. memory allocation, this means
pre-existing programs cannot be single-stepped, simply because the
underlying glibc or jemalloc has changed.

The rseq user-space does expose a __rseq_table section for the sake of
debuggers, so they can skip over the rseq critical sections if they want.
However, this requires upgrading tools, and still breaks single-stepping
in case where glibc or jemalloc is updated, but not the tooling.

Having a performance-related library improvement break tooling is likely to
cause a big push-back against wide adoption of rseq. *I* would not even
be using rseq in liburcu and lttng-ust until gdb gets updated in every
distributions that my users depend on. This will likely happen... never.


2) Forward-progress guarantee

Having a piece of user-space code that stops progressing due to
external conditions is pretty bad. We are used to think of fast-path and
slow-path (e.g. for locking), where the contended vs uncontended cases
have different performance characteristics, but each need to provide some
level of progress guarantees.

I&#39;m very concerned about proposing just &quot;rseq&quot; without the associated
slow-path (cpu_opv) that guarantees progress. It&#39;s just asking for trouble
when real-life will happen: page faults, uprobes, and other unforeseen
conditions that would seldom cause a rseq fast-path to never progress.


3) Handling page faults

If we get creative enough, it&#39;s pretty easy to come up with corner-case
scenarios where rseq does not progress without the help from cpu_opv. For
instance, a system with swap enabled which is under high memory pressure
could trigger page faults at pretty much every rseq attempt. I recognize
that this scenario is extremely unlikely, but I&#39;m not comfortable making
rseq the weak link of the chain here.


4) Comparison with LL/SC

The layman versed in the load-link/store-conditional instructions in
RISC architectures will notice the similarity between rseq and LL/SC
critical sections. The comparison can even be pushed further: since
debuggers can handle those LL/SC critical sections, they should be
able to handle rseq c.s. in the same way.

First, the way gdb recognises LL/SC c.s. patterns is very fragile:
it&#39;s limited to specific common patterns, and will miss the pattern
in all other cases. But fear not, having the rseq c.s. expose a
__rseq_table to debuggers removes that guessing part.

The main difference between LL/SC and rseq is that debuggers had
to support single-stepping through LL/SC critical sections from the
get go in order to support a given architecture. For rseq, we&#39;re
adding critical sections into pre-existing applications/libraries,
so the user expectation is that tools don&#39;t break due to a library
optimization.


5) Perform maintenance operations on per-cpu data

rseq c.s. are quite limited feature-wise: they need to end with a
*single* commit instruction that updates a memory location. On the
other hand, the cpu_opv system call can combine a sequence of operations
that need to be executed with preemption disabled. While slower than
rseq, this allows for more complex maintenance operations to be
performed on per-cpu data concurrently with rseq fast-paths, in cases
where it&#39;s not possible to map those sequences of ops to a rseq.


6) Use cpu_opv as generic implementation for architectures not
   implementing rseq assembly code

rseq critical sections require architecture-specific user-space code
to be crafted in order to port an algorithm to a given architecture.
In addition, it requires that the kernel architecture implementation
adds hooks into signal delivery and resume to user-space.

In order to facilitate integration of rseq into user-space, cpu_opv
can provide a (relatively slower) architecture-agnostic implementation
of rseq. This means that user-space code can be ported to all
architectures through use of cpu_opv initially, and have the fast-path
use rseq whenever the asm code is implemented.


7) Allow libraries with multi-part algorithms to work on same per-cpu
   data without affecting the allowed cpu mask

I stumbled on an interesting use-case within the lttng-ust tracer
per-cpu buffers: the algorithm needs to update a &quot;reserve&quot; counter,
serialize data into the buffer, and then update a &quot;commit&quot; counter
_on the same per-cpu buffer_. My goal is to use rseq for both reserve
and commit.

Clearly, if rseq reserve fails, the algorithm can retry on a different
per-cpu buffer. However, it&#39;s not that easy for the commit. It needs to
be performed on the same per-cpu buffer as the reserve.

The cpu_opv system call solves that problem by receiving the cpu number
on which the operation needs to be performed as argument. It can push
the task to the right CPU if needed, and perform the operations there
with preemption disabled.

Changing the allowed cpu mask for the current thread is not an acceptable
alternative for a tracing library, because the application being traced
does not expect that mask to be changed by libraries.


So, TLDR: cpu_opv is needed for many use-cases other that single-stepping,
and facilitates adoption of rseq into pre-existing applications.


Thanks,

Mathieu
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tglx</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104">Andi Kleen</a> - Nov. 17, 2017, 6:18 p.m.</div>
<pre class="content">
Thanks for the detailed write up. That should have been in the
changelog...

Some comments below. Overall I think the case for the syscall is still
very weak.
<span class="quote">
&gt; Let&#39;s have a look at why cpu_opv is needed. I&#39;ll make sure to enhance the</span>
<span class="quote">&gt; changelog and documentation to include that information.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1) Handling single-stepping from tools</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Tools like debuggers, and simulators like record-replay (&quot;rr&quot;) use</span>
<span class="quote">&gt; single-stepping to run through existing programs. If core libraries start</span>

No rr doesn&#39;t use single stepping. It uses branch stepping based on the
PMU, and those should only happen on external events or syscalls which would
abort the rseq anyways.

Eventually it will suceeed because not every retry there will be a new
signal. If you put a syscall into your rseq you will just get
what you deserve.

If it was single stepping it couldn&#39;t execute the vDSO (and it would
be incredible slow)

Yes debuggers have to skip instead of step, but that&#39;s easily done
(and needed today already for every gettimeofday, which tends to be
the most common syscall...) 
<span class="quote">
&gt; to use restartable sequences for e.g. memory allocation, this means</span>
<span class="quote">&gt; pre-existing programs cannot be single-stepped, simply because the</span>
<span class="quote">&gt; underlying glibc or jemalloc has changed.</span>

But how likely is it that the fall back path even works?

It would never be exercised in normal operation, so it would be a prime
target for bit rot, or not ever being tested and be broken in the first place.
<span class="quote">
&gt; Having a performance-related library improvement break tooling is likely to</span>
<span class="quote">&gt; cause a big push-back against wide adoption of rseq. *I* would not even</span>
<span class="quote">&gt; be using rseq in liburcu and lttng-ust until gdb gets updated in every</span>
<span class="quote">&gt; distributions that my users depend on. This will likely happen... never.</span>

I suspect your scheme already has a &lt;50% likelihood of working due to
the above that it&#39;s equivalent.
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 2) Forward-progress guarantee</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having a piece of user-space code that stops progressing due to</span>
<span class="quote">&gt; external conditions is pretty bad. We are used to think of fast-path and</span>
<span class="quote">&gt; slow-path (e.g. for locking), where the contended vs uncontended cases</span>
<span class="quote">&gt; have different performance characteristics, but each need to provide some</span>
<span class="quote">&gt; level of progress guarantees.</span>

We already have that in the vDSO. Has never been a problem.
<span class="quote">
&gt; 3) Handling page faults</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we get creative enough, it&#39;s pretty easy to come up with corner-case</span>
<span class="quote">&gt; scenarios where rseq does not progress without the help from cpu_opv. For</span>
<span class="quote">&gt; instance, a system with swap enabled which is under high memory pressure</span>
<span class="quote">&gt; could trigger page faults at pretty much every rseq attempt. I recognize</span>
<span class="quote">&gt; that this scenario is extremely unlikely, but I&#39;m not comfortable making</span>
<span class="quote">&gt; rseq the weak link of the chain here.</span>

Seems very unlikely. But if this happens the program is dead anyways,
so doesn&#39;t make much difference.
<span class="quote">

&gt; The main difference between LL/SC and rseq is that debuggers had</span>
<span class="quote">&gt; to support single-stepping through LL/SC critical sections from the</span>
<span class="quote">&gt; get go in order to support a given architecture. For rseq, we&#39;re</span>
<span class="quote">&gt; adding critical sections into pre-existing applications/libraries,</span>
<span class="quote">&gt; so the user expectation is that tools don&#39;t break due to a library</span>
<span class="quote">&gt; optimization.</span>

I would argue that debugging some other path that is normally
executed is wrong by definition. How would you find the bug if it is 
in the original path only, but not the fallback.

The whole point of debugging is to punch through abstractions,
but you&#39;re adding another layer of obfuscation here. And worse
you have no guarantee that the new layer is actually functionally
equivalent.

Having less magic and just assume the user can do the right thing
seems like a far more practical scheme.
<span class="quote">
&gt; In order to facilitate integration of rseq into user-space, cpu_opv</span>
<span class="quote">&gt; can provide a (relatively slower) architecture-agnostic implementation</span>
<span class="quote">&gt; of rseq. This means that user-space code can be ported to all</span>
<span class="quote">&gt; architectures through use of cpu_opv initially, and have the fast-path</span>
<span class="quote">&gt; use rseq whenever the asm code is implemented.</span>

While that&#39;s in theory correct, in practice it will be so slow
that it is useless. Nobody will want a system call in their malloc
fast path.


-Andi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 17, 2017, 6:59 p.m.</div>
<pre class="content">
On Fri, 17 Nov 2017, Andi Kleen wrote:
<span class="quote">&gt; &gt; 1) Handling single-stepping from tools</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Tools like debuggers, and simulators like record-replay (&quot;rr&quot;) use</span>
<span class="quote">&gt; &gt; single-stepping to run through existing programs. If core libraries start</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No rr doesn&#39;t use single stepping. It uses branch stepping based on the</span>
<span class="quote">&gt; PMU, and those should only happen on external events or syscalls which would</span>
<span class="quote">&gt; abort the rseq anyways.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Eventually it will suceeed because not every retry there will be a new</span>
<span class="quote">&gt; signal. If you put a syscall into your rseq you will just get</span>
<span class="quote">&gt; what you deserve.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If it was single stepping it couldn&#39;t execute the vDSO (and it would</span>
<span class="quote">&gt; be incredible slow)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes debuggers have to skip instead of step, but that&#39;s easily done</span>
<span class="quote">&gt; (and needed today already for every gettimeofday, which tends to be</span>
<span class="quote">&gt; the most common syscall...)</span>

The same problem exists with TSX. Hitting a break point inside a
transaction section triggers abort with the cause bit &#39;breakpoint&#39; set.

rseq can be looked at as a variant of software transactional memory with a
limited feature set, but the underlying problems are exactly the
same. Breakpoints are not working either.
<span class="quote">
&gt; &gt; to use restartable sequences for e.g. memory allocation, this means</span>
<span class="quote">&gt; &gt; pre-existing programs cannot be single-stepped, simply because the</span>
<span class="quote">&gt; &gt; underlying glibc or jemalloc has changed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But how likely is it that the fall back path even works?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would never be exercised in normal operation, so it would be a prime</span>
<span class="quote">&gt; target for bit rot, or not ever being tested and be broken in the first place.</span>

What&#39;s worse is that the fall back path cannot be debugged at all. It&#39;s a
magic byte code which is interpreted inside the kernel.


I really do not understand why this does not utilize existing design
patterns well known from transactional memory.

The most straight forward is to have a mechanism which forces everything
into the slow path in case of debugging, lack of progress, etc. The slow
path uses traditional locking to resolve the situation. That&#39;s well known
to work and if done correctly then the only difference between slow path
and fast path is locking/transaction control, i.e. it&#39;s a single
implementation of the actual memory operations which can be single stepped,
debug traced and whatever.

It solves _ALL_ of the problems you describe including support for systems
which do not support rseq at all.

This syscall is horribly overengineered and creates more problems than it
solves.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104">Andi Kleen</a> - Nov. 17, 2017, 7:15 p.m.</div>
<pre class="content">
<span class="quote">&gt; The most straight forward is to have a mechanism which forces everything</span>
<span class="quote">&gt; into the slow path in case of debugging, lack of progress, etc. The slow</span>

That&#39;s the abort address, right?

For the generic case the fall back path would require disabling preemption
unfortunately, for which we don&#39;t have a mechanism in user space.

I think that is what Mathieu tried to implement here with this call.

There may be some special cases where it&#39;s possible without preemption
control, e.g. a malloc could just not use the per cpu cache. But I doubt
that is possible in all cases that Mathieu envisions. 

But again would be a different code path, and I question the need for it when
we can just let the operator of the debugger deal with it.

-Andi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 17, 2017, 8:07 p.m.</div>
<pre class="content">
On Fri, 17 Nov 2017, Andi Kleen wrote:
<span class="quote">&gt; &gt; The most straight forward is to have a mechanism which forces everything</span>
<span class="quote">&gt; &gt; into the slow path in case of debugging, lack of progress, etc. The slow</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s the abort address, right?</span>

Yes.
<span class="quote">
&gt; For the generic case the fall back path would require disabling preemption</span>
<span class="quote">&gt; unfortunately, for which we don&#39;t have a mechanism in user space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think that is what Mathieu tried to implement here with this call.</span>

Yes. preempt disabled execution of byte code to make sure that the
transaction succeeds.

But, why is disabling preemption mandatory? If stuff fails due to hitting a
breakpoint or because it retried a gazillion times without progress, then
the abort code can detect that and act accordingly. Pseudo code:

abort:
	if (!slowpath_required() &amp;&amp;
	    !breakpoint_caused_abort() &amp;&amp;
	    !stall_detected()) {
		do_the_normal_abort_postprocessing();
		goto retry;
	}

	lock(slowpath_lock[cpu]);

	if (!slowpath_required()) {
	   	unlock(slowpath_lock[cpu]);
		goto retry;
	}

	if (rseq_supported)
		set_slow_path();

	/* Same code as inside the actual rseq */
	do_transaction();

	if (rseq_supported)
		unset_slow_path();

	unlock(slowpath_lock[cpu]);

The only interesting question is how to make sure that all threads on that
CPU see the slowpath required before they execute the commit so they are
forced into the slow path. The simplest thing would be atomics, but that&#39;s
what rseq wants to avoid.

I think that this can be solved cleanly with the help of the membarrier
syscall or some variant of that without all that &#39;yet another byte code
interpreter&#39; mess.

The other question is whether do_transaction() is required to run on that
specific CPU. I don&#39;t think so because that magic interpreter operates even
when the required target cpu is offline and with locking in place there is
no reason why running on the target CPU would be required.

Sure, that&#39;s going to affect performance, but only for two cases:

  1) Debugging. That&#39;s completely uninteresting

  2) No progress at all. Performance is down the drain anyway, so it does
     not matter at all whether you spend a few more cycles or not to
     resolve that.

I might be missing something as usual :)

Thanks

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 17, 2017, 8:22 p.m.</div>
<pre class="content">
On Fri, 17 Nov 2017, Mathieu Desnoyers wrote:
<span class="quote">&gt; ----- On Nov 17, 2017, at 5:09 AM, Thomas Gleixner tglx@linutronix.de wrote:</span>
<span class="quote">&gt; 7) Allow libraries with multi-part algorithms to work on same per-cpu</span>
<span class="quote">&gt;    data without affecting the allowed cpu mask</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I stumbled on an interesting use-case within the lttng-ust tracer</span>
<span class="quote">&gt; per-cpu buffers: the algorithm needs to update a &quot;reserve&quot; counter,</span>
<span class="quote">&gt; serialize data into the buffer, and then update a &quot;commit&quot; counter</span>
<span class="quote">&gt; _on the same per-cpu buffer_. My goal is to use rseq for both reserve</span>
<span class="quote">&gt; and commit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Clearly, if rseq reserve fails, the algorithm can retry on a different</span>
<span class="quote">&gt; per-cpu buffer. However, it&#39;s not that easy for the commit. It needs to</span>
<span class="quote">&gt; be performed on the same per-cpu buffer as the reserve.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The cpu_opv system call solves that problem by receiving the cpu number</span>
<span class="quote">&gt; on which the operation needs to be performed as argument. It can push</span>
<span class="quote">&gt; the task to the right CPU if needed, and perform the operations there</span>
<span class="quote">&gt; with preemption disabled.</span>

If your transaction cannot be done in one go, then abusing that byte code
interpreter for concluding it is just hillarious. That whole exercise is a
gazillion times slower than the atomic operations which are neccesary to do
it without all that.

I&#39;m even more convinced now that this is overengineered beyond repair.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Nov. 18, 2017, 9:09 p.m.</div>
<pre class="content">
On Fri, Nov 17, 2017 at 12:07 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt; On Fri, 17 Nov 2017, Andi Kleen wrote:</span>
<span class="quote">&gt;&gt; &gt; The most straight forward is to have a mechanism which forces everything</span>
<span class="quote">&gt;&gt; &gt; into the slow path in case of debugging, lack of progress, etc. The slow</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That&#39;s the abort address, right?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; For the generic case the fall back path would require disabling preemption</span>
<span class="quote">&gt;&gt; unfortunately, for which we don&#39;t have a mechanism in user space.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think that is what Mathieu tried to implement here with this call.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes. preempt disabled execution of byte code to make sure that the</span>
<span class="quote">&gt; transaction succeeds.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But, why is disabling preemption mandatory? If stuff fails due to hitting a</span>
<span class="quote">&gt; breakpoint or because it retried a gazillion times without progress, then</span>
<span class="quote">&gt; the abort code can detect that and act accordingly. Pseudo code:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; abort:</span>
<span class="quote">&gt;         if (!slowpath_required() &amp;&amp;</span>
<span class="quote">&gt;             !breakpoint_caused_abort() &amp;&amp;</span>
<span class="quote">&gt;             !stall_detected()) {</span>
<span class="quote">&gt;                 do_the_normal_abort_postprocessing();</span>
<span class="quote">&gt;                 goto retry;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         lock(slowpath_lock[cpu]);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (!slowpath_required()) {</span>
<span class="quote">&gt;                 unlock(slowpath_lock[cpu]);</span>
<span class="quote">&gt;                 goto retry;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (rseq_supported)</span>
<span class="quote">&gt;                 set_slow_path();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* Same code as inside the actual rseq */</span>
<span class="quote">&gt;         do_transaction();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (rseq_supported)</span>
<span class="quote">&gt;                 unset_slow_path();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         unlock(slowpath_lock[cpu]);</span>

My objection to this approach is that people will get it wrong and not
notice until it&#39;s too late.  TSX has two things going for it:

1. It&#39;s part of the ISA, so debuggers have very well-defined semantics
to deal with and debuggers will know about it.  rseq is a made-up
Linux thing and debuggers may not know what to do with it.

2. TSX is slow and crappy, so it may not be that widely used.  glibc,
OTOH, will probably start using rseq on all machines if the patches
are merged.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 20, 2017, 4:13 p.m.</div>
<pre class="content">
----- On Nov 16, 2017, at 6:26 PM, Thomas Gleixner tglx@linutronix.de wrote:
<span class="quote">
&gt; On Tue, 14 Nov 2017, Mathieu Desnoyers wrote:</span>
<span class="quote">&gt;&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt;&gt; +# include &lt;linux/types.h&gt;</span>
<span class="quote">&gt;&gt; +#else	/* #ifdef __KERNEL__ */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		   Sigh.</span>

fixed.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +# include &lt;stdint.h&gt;</span>
<span class="quote">&gt;&gt; +#endif	/* #else #ifdef __KERNEL__ */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm/byteorder.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef __LP64__</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64(field)			uint64_t field</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	field = (intptr_t)v</span>
<span class="quote">&gt;&gt; +#elif defined(__BYTE_ORDER) ? \</span>
<span class="quote">&gt;&gt; +	__BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64(field)	uint32_t field ## _padding, field</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="quote">&gt;&gt; +	field ## _padding = 0, field = (intptr_t)v</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64(field)	uint32_t field, field ## _padding</span>
<span class="quote">&gt;&gt; +# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="quote">&gt;&gt; +	field = (intptr_t)v, field ## _padding = 0</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So in the rseq patch you have:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +#ifdef __LP64__</span>
<span class="quote">&gt; +# define RSEQ_FIELD_u32_u64(field)                     uint64_t field</span>
<span class="quote">&gt; +# define RSEQ_FIELD_u32_u64_INIT_ONSTACK(field, v)     field = (intptr_t)v</span>
<span class="quote">&gt; +#elif defined(__BYTE_ORDER) ? \</span>
<span class="quote">&gt; +       __BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="quote">&gt; +# define RSEQ_FIELD_u32_u64(field)     uint32_t field ## _padding, field</span>
<span class="quote">&gt; +# define RSEQ_FIELD_u32_u64_INIT_ONSTACK(field, v)     \</span>
<span class="quote">&gt; +       field ## _padding = 0, field = (intptr_t)v</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +# define RSEQ_FIELD_u32_u64(field)     uint32_t field, field ## _padding</span>
<span class="quote">&gt; +# define RSEQ_FIELD_u32_u64_INIT_ONSTACK(field, v)     \</span>
<span class="quote">&gt; +       field = (intptr_t)v, field ## _padding = 0</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IOW the same macro maze. Please use a separate header file which provides</span>
<span class="quote">&gt; these macros once and share them between the two facilities.</span>

ok. I&#39;ll introduce uapi/linux/types_32_64.h, and prefix defines with
&quot;LINUX_&quot;:

LINUX_FIELD_u32_u64()

unless other names are preferred. It will be in a separate patch.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +#define CPU_OP_VEC_LEN_MAX		16</span>
<span class="quote">&gt;&gt; +#define CPU_OP_ARG_LEN_MAX		24</span>
<span class="quote">&gt;&gt; +/* Max. data len per operation. */</span>
<span class="quote">&gt;&gt; +#define CPU_OP_DATA_LEN_MAX		PAGE_SIZE</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s something between 4K and 256K depending on the architecture.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You really want to allow up to 256K data copy with preemption disabled?</span>
<span class="quote">&gt; Shudder.</span>

This is the max per operation. Following peterz&#39; comments at KS, I added a
limit of 4096 + 15 * 8 on the sum of len for all operations in a vector. This
is defined below as CPU_OP_VEC_DATA_LEN_MAX.

So each of the 16 op cannot have a len larger than PAGE_SIZE, so we
pin at most 4 pages per op (e.g. memcpy 2 pages for src, 2 pages for dst),
*and* the sum of all ops len needs to be &lt;= 4216. So the max limit you are
interested in here is the 4216 bytes limit.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Max. data len for overall vector. We to restrict the amount of</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We to ?</span>

fixed
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; + * user-space data touched by the kernel in non-preemptible context so</span>
<span class="quote">&gt;&gt; + * we do not introduce long scheduler latencies.</span>
<span class="quote">&gt;&gt; + * This allows one copy of up to 4096 bytes, and 15 operations touching</span>
<span class="quote">&gt;&gt; + * 8 bytes each.</span>
<span class="quote">&gt;&gt; + * This limit is applied to the sum of length specified for all</span>
<span class="quote">&gt;&gt; + * operations in a vector.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define CPU_OP_VEC_DATA_LEN_MAX		(4096 + 15*8)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Magic numbers. Please use proper defines for heavens sake.</span>

ok, it becomes:

#define CPU_OP_MEMCPY_EXPECT_LEN        4096
#define CPU_OP_EXPECT_LEN               8
#define CPU_OP_VEC_DATA_LEN_MAX         \
        (CPU_OP_MEMCPY_EXPECT_LEN +     \
         (CPU_OP_VEC_LEN_MAX - 1) * CPU_OP_EXPECT_LEN)
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +#define CPU_OP_MAX_PAGES		4	/* Max. pages per op. */</span>

Actually, I&#39;ll move the CPU_OP_MAX_PAGES define to cpu_opv.c. It&#39;s not
needed in the uapi header.
<span class="quote">

&gt;&gt; +</span>
<span class="quote">&gt;&gt; +enum cpu_op_type {</span>
<span class="quote">&gt;&gt; +	CPU_COMPARE_EQ_OP,	/* compare */</span>
<span class="quote">&gt;&gt; +	CPU_COMPARE_NE_OP,	/* compare */</span>
<span class="quote">&gt;&gt; +	CPU_MEMCPY_OP,		/* memcpy */</span>
<span class="quote">&gt;&gt; +	CPU_ADD_OP,		/* arithmetic */</span>
<span class="quote">&gt;&gt; +	CPU_OR_OP,		/* bitwise */</span>
<span class="quote">&gt;&gt; +	CPU_AND_OP,		/* bitwise */</span>
<span class="quote">&gt;&gt; +	CPU_XOR_OP,		/* bitwise */</span>
<span class="quote">&gt;&gt; +	CPU_LSHIFT_OP,		/* shift */</span>
<span class="quote">&gt;&gt; +	CPU_RSHIFT_OP,		/* shift */</span>
<span class="quote">&gt;&gt; +	CPU_MB_OP,		/* memory barrier */</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Vector of operations to perform. Limited to 16. */</span>
<span class="quote">&gt;&gt; +struct cpu_op {</span>
<span class="quote">&gt;&gt; +	int32_t op;	/* enum cpu_op_type. */</span>
<span class="quote">&gt;&gt; +	uint32_t len;	/* data length, in bytes. */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please get rid of these tail comments</span>

ok
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	union {</span>
<span class="quote">&gt;&gt; +#define TMP_BUFLEN			64</span>
<span class="quote">&gt;&gt; +#define NR_PINNED_PAGES_ON_STACK	8</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 8 pinned pages on stack? Which stack?</span>

The common cases need to touch few pages, and we can keep the
pointers in an array on the kernel stack within the cpu_opv system
call.

Updating to:

/*
 * Typical invocation of cpu_opv need few pages. Keep struct page
 * pointers in an array on the stack of the cpu_opv system call up to
 * this limit, beyond which the array is dynamically allocated.
 */
#define NR_PIN_PAGES_ON_STACK        8
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * The cpu_opv system call executes a vector of operations on behalf of</span>
<span class="quote">&gt;&gt; + * user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="quote">&gt;&gt; + * from readv() and writev() system calls which take a &quot;struct iovec&quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; s/from/by/</span>

ok
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; + * array as argument.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt;&gt; + * left shift, and right shift. The system call receives a CPU number</span>
<span class="quote">&gt;&gt; + * from user-space as argument, which is the CPU on which those</span>
<span class="quote">&gt;&gt; + * operations need to be performed. All preparation steps such as</span>
<span class="quote">&gt;&gt; + * loading pointers, and applying offsets to arrays, need to be</span>
<span class="quote">&gt;&gt; + * performed by user-space before invoking the system call. The</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; loading pointers and applying offsets? That makes no sense.</span>

Updating to:

 * All preparation steps such as
 * loading base pointers, and adding offsets derived from the current
 * CPU number, need to be performed by user-space before invoking the
 * system call.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; + * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="quote">&gt;&gt; + * preparation step did not change between preparation of system call</span>
<span class="quote">&gt;&gt; + * inputs and operation execution within the preempt-off critical</span>
<span class="quote">&gt;&gt; + * section.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt;&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt;&gt; + * to first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That doesnt explain it either.</span>

What kind of explication are you looking for here ? Perhaps being too close
to the implementation prevents me from understanding what is unclear from
your perspective.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; + * faulting-in the pages.  Then, preemption is disabled, and the</span>
<span class="quote">&gt;&gt; + * operations are performed atomically with respect to other thread</span>
<span class="quote">&gt;&gt; + * execution on that CPU, without generating any page fault.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt;&gt; + * enforced, and a overall maximum length sum, so user-space cannot</span>
<span class="quote">&gt;&gt; + * generate a too long preempt-off critical section. Each operation is</span>
<span class="quote">&gt;&gt; + * also limited a length of PAGE_SIZE bytes, meaning that an operation</span>
<span class="quote">&gt;&gt; + * can touch a maximum of 4 pages (memcpy: 2 pages for source, 2 pages</span>
<span class="quote">&gt;&gt; + * for destination if addresses are not aligned on page boundaries).</span>
<span class="quote">&gt; </span>

Sorry, that paragraph was unclear. Updated:

 * An overall maximum of 4216 bytes in enforced on the sum of operation
 * length within an operation vector, so user-space cannot generate a
 * too long preempt-off critical section (cache cold critical section
 * duration measured as 4.7s on x86-64). Each operation is also limited
 * a length of PAGE_SIZE bytes, meaning that an operation can touch a
 * maximum of 4 pages (memcpy: 2 pages for source, 2 pages for
 * destination if addresses are not aligned on page boundaries).
<span class="quote">
&gt; What&#39;s the critical section duration for operations which go to the limits</span>
<span class="quote">&gt; of this on a average x86 64 machine?</span>

When cache-cold, I measure 4.7 s per critical section doing a
4k memcpy and 15 * 8 bytes memcpy on a E5-2630 v3 @2.4GHz. Is it an
acceptable preempt-off latency for RT ?
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; + * If the thread is not running on the requested CPU, a new</span>
<span class="quote">&gt;&gt; + * push_task_to_cpu() is invoked to migrate the task to the requested</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; new push_task_to_cpu()? Once that patch is merged push_task_to_cpu() is</span>
<span class="quote">&gt; hardly new.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please refrain from putting function level details into comments which</span>
<span class="quote">&gt; describe the concept. The function name might change in 3 month from now</span>
<span class="quote">&gt; and the comment will stay stale, Its sufficient to say:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; * If the thread is not running on the requested CPU it is migrated</span>
<span class="quote">&gt; * to it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That explains the concept. It&#39;s completely irrelevant which mechanism is</span>
<span class="quote">&gt; used to achieve that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; + * CPU.  If the requested CPU is not part of the cpus allowed mask of</span>
<span class="quote">&gt;&gt; + * the thread, the system call fails with EINVAL. After the migration</span>
<span class="quote">&gt;&gt; + * has been performed, preemption is disabled, and the current CPU</span>
<span class="quote">&gt;&gt; + * number is checked again and compared to the requested CPU number. If</span>
<span class="quote">&gt;&gt; + * it still differs, it means the scheduler migrated us away from that</span>
<span class="quote">&gt;&gt; + * CPU. Return EAGAIN to user-space in that case, and let user-space</span>
<span class="quote">&gt;&gt; + * retry (either requesting the same CPU number, or a different one,</span>
<span class="quote">&gt;&gt; + * depending on the user-space algorithm constraints).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This mixture of imperative and impersonated mood is really hard to read.</span>

This whole paragraph is replaced by:

 * If the thread is not running on the requested CPU, it is migrated to
 * it. If the scheduler then migrates the task away from the requested CPU
 * before the critical section executes, return EAGAIN to user-space,
 * and let user-space retry (either requesting the same CPU number, or a
 * different one, depending on the user-space algorithm constraints).
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Check operation types and length parameters.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static int cpu_opv_check(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int i;</span>
<span class="quote">&gt;&gt; +	uint32_t sum = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt;&gt; +		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		switch (op-&gt;op) {</span>
<span class="quote">&gt;&gt; +		case CPU_MB_OP:</span>
<span class="quote">&gt;&gt; +			break;</span>
<span class="quote">&gt;&gt; +		default:</span>
<span class="quote">&gt;&gt; +			sum += op-&gt;len;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please separate the switch cases with an empty line.</span>

ok
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="quote">&gt;&gt; +		unsigned long len)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please align the arguments</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="quote">&gt;					   unsigned long len)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; is way simpler to parse. All over the place please.</span>

ok
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return ((addr + len - 1) &gt;&gt; PAGE_SHIFT) - (addr &gt;&gt; PAGE_SHIFT) + 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m surprised that there is no existing magic for this.</span>

populate_vma_page_range() does:

unsigned long nr_pages = (end - start) / PAGE_SIZE;

where &quot;start&quot; and &quot;end&quot; need to fall onto a page boundary. It does not
seem to be appropriate for cases where addr is not page aligned, and where
the length is smaller than a page.

I have not seen helpers for this neither.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int cpu_op_check_page(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct address_space *mapping;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (is_zone_device_page(page))</span>
<span class="quote">&gt;&gt; +		return -EFAULT;</span>
<span class="quote">&gt;&gt; +	page = compound_head(page);</span>
<span class="quote">&gt;&gt; +	mapping = READ_ONCE(page-&gt;mapping);</span>
<span class="quote">&gt;&gt; +	if (!mapping) {</span>
<span class="quote">&gt;&gt; +		int shmem_swizzled;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * Check again with page lock held to guard against</span>
<span class="quote">&gt;&gt; +		 * memory pressure making shmem_writepage move the page</span>
<span class="quote">&gt;&gt; +		 * from filecache to swapcache.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		lock_page(page);</span>
<span class="quote">&gt;&gt; +		shmem_swizzled = PageSwapCache(page) || page-&gt;mapping;</span>
<span class="quote">&gt;&gt; +		unlock_page(page);</span>
<span class="quote">&gt;&gt; +		if (shmem_swizzled)</span>
<span class="quote">&gt;&gt; +			return -EAGAIN;</span>
<span class="quote">&gt;&gt; +		return -EFAULT;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Refusing device pages, the zero page, pages in the gate area, and</span>
<span class="quote">&gt;&gt; + * special mappings. Inspired from futex.c checks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That comment should be on the function above, because this loop does not</span>
<span class="quote">&gt; much checking. Aside of that a more elaborate explanation how those checks</span>
<span class="quote">&gt; are done and how that works would be appreciated.</span>

OK. I also noticed through testing that I missed faulting in pages, similarly
to sys_futex(). I&#39;ll add it, and I&#39;m also adding a test in the selftests
for this case.

I&#39;ll import comments from futex.c.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static int cpu_op_check_pages(struct page **pages,</span>
<span class="quote">&gt;&gt; +		unsigned long nr_pages)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	unsigned long i;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt;&gt; +		int ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		ret = cpu_op_check_page(pages[i]);</span>
<span class="quote">&gt;&gt; +		if (ret)</span>
<span class="quote">&gt;&gt; +			return ret;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt;&gt; +		struct cpu_opv_pinned_pages *pin_pages, int write)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct page *pages[2];</span>
<span class="quote">&gt;&gt; +	int ret, nr_pages;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (!len)</span>
<span class="quote">&gt;&gt; +		return 0;</span>
<span class="quote">&gt;&gt; +	nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="quote">&gt;&gt; +	BUG_ON(nr_pages &gt; 2);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If that happens then you can emit a warning and return a proper error</span>
<span class="quote">&gt; code. BUG() is the last resort if there is no way to recover. This really</span>
<span class="quote">&gt; does not qualify.</span>

ok. will do:

        nr_pages = cpu_op_range_nr_pages(addr, len);
        if (nr_pages &gt; 2) {
                WARN_ON(1);
                return -EINVAL;
        }
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	if (!pin_pages-&gt;is_kmalloc &amp;&amp; pin_pages-&gt;nr + nr_pages</span>
<span class="quote">&gt;&gt; +			&gt; NR_PINNED_PAGES_ON_STACK) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now I see what this is used for. That&#39;s a complete misnomer.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And this check is of course completely self explaining..... NOT!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +		struct page **pinned_pages =</span>
<span class="quote">&gt;&gt; +			kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="quote">&gt;&gt; +				* sizeof(struct page *), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +		if (!pinned_pages)</span>
<span class="quote">&gt;&gt; +			return -ENOMEM;</span>
<span class="quote">&gt;&gt; +		memcpy(pinned_pages, pin_pages-&gt;pages,</span>
<span class="quote">&gt;&gt; +			pin_pages-&gt;nr * sizeof(struct page *));</span>
<span class="quote">&gt;&gt; +		pin_pages-&gt;pages = pinned_pages;</span>
<span class="quote">&gt;&gt; +		pin_pages-&gt;is_kmalloc = true;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have no idea why this needs to be done here and cannot be done in a</span>
<span class="quote">&gt; preparation step. That&#39;s horrible. You allocate conditionally at some</span>
<span class="quote">&gt; random place and then free at the end of the syscall.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What&#39;s wrong with:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;       prepare_stuff()</span>
<span class="quote">&gt;       pin_pages()</span>
<span class="quote">&gt;       do_ops()</span>
<span class="quote">&gt;       cleanup_stuff()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm?</span>

Will do.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +again:</span>
<span class="quote">&gt;&gt; +	ret = get_user_pages_fast(addr, nr_pages, write, pages);</span>
<span class="quote">&gt;&gt; +	if (ret &lt; nr_pages) {</span>
<span class="quote">&gt;&gt; +		if (ret &gt; 0)</span>
<span class="quote">&gt;&gt; +			put_page(pages[0]);</span>
<span class="quote">&gt;&gt; +		return -EFAULT;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Refuse device pages, the zero page, pages in the gate area,</span>
<span class="quote">&gt;&gt; +	 * and special mappings.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the same comment again. Really helpful.</span>

I&#39;ll remove this duplicated comment.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	ret = cpu_op_check_pages(pages, nr_pages);</span>
<span class="quote">&gt;&gt; +	if (ret == -EAGAIN) {</span>
<span class="quote">&gt;&gt; +		put_page(pages[0]);</span>
<span class="quote">&gt;&gt; +		if (nr_pages &gt; 1)</span>
<span class="quote">&gt;&gt; +			put_page(pages[1]);</span>
<span class="quote">&gt;&gt; +		goto again;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So why can&#39;t you propagate EAGAIN to the caller and use the error cleanup</span>
<span class="quote">&gt; label?</span>

Because it needs to retry immediately in case the page has been faulted in,
or is being swapped in.
<span class="quote">
&gt; Or put the sequence of get_user_pages_fast() and check_pages() into</span>
<span class="quote">&gt; one function and confine the mess there instead of having the same cleanup</span>
<span class="quote">&gt; sequence 3 times in this function.</span>

I&#39;ll merge all this into a single error path.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	if (ret)</span>
<span class="quote">&gt;&gt; +		goto error;</span>
<span class="quote">&gt;&gt; +	pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[0];</span>
<span class="quote">&gt;&gt; +	if (nr_pages &gt; 1)</span>
<span class="quote">&gt;&gt; +		pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[1];</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +error:</span>
<span class="quote">&gt;&gt; +	put_page(pages[0]);</span>
<span class="quote">&gt;&gt; +	if (nr_pages &gt; 1)</span>
<span class="quote">&gt;&gt; +		put_page(pages[1]);</span>
<span class="quote">&gt;&gt; +	return -EFAULT;</span>
<span class="quote">&gt;&gt; +}</span>

Updated function:

static int cpu_op_pin_pages(unsigned long addr, unsigned long len,
                            struct cpu_opv_pin_pages *pin_pages,
                            int write)
{
        struct page *pages[2];
        int ret, nr_pages, nr_put_pages, n;

        nr_pages = cpu_op_count_pages(addr, len);
        if (!nr_pages)
                return 0;
again:
        ret = get_user_pages_fast(addr, nr_pages, write, pages);
        if (ret &lt; nr_pages) {
                if (ret &gt;= 0) {
                        nr_put_pages = ret;
                        ret = -EFAULT;
                } else {
                        nr_put_pages = 0;
                }
                goto error;
        }
        ret = cpu_op_check_pages(pages, nr_pages, addr);
        if (ret) {
                nr_put_pages = nr_pages;
                goto error;
        }
        for (n = 0; n &lt; nr_pages; n++)
                pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[n];
        return 0;

error:
        for (n = 0; n &lt; nr_put_pages; n++)
                put_page(pages[n]);
        /*
         * Retry if a page has been faulted in, or is being swapped in.
         */
        if (ret == -EAGAIN)
                goto again;
        return ret;
}
<span class="quote">

&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="quote">&gt;&gt; +		struct cpu_opv_pinned_pages *pin_pages)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int ret, i;</span>
<span class="quote">&gt;&gt; +	bool expect_fault = false;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Check access, pin pages. */</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt;&gt; +		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		switch (op-&gt;op) {</span>
<span class="quote">&gt;&gt; +		case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt;&gt; +		case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt;&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +			expect_fault = op-&gt;u.compare_op.expect_fault_a;</span>
<span class="quote">&gt;&gt; +			if (!access_ok(VERIFY_READ,</span>
<span class="quote">&gt;&gt; +					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt;&gt; +					op-&gt;len))</span>
<span class="quote">&gt;&gt; +				goto error;</span>
<span class="quote">&gt;&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt;&gt; +					(unsigned long)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt;&gt; +					op-&gt;len, pin_pages, 0);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Bah, this sucks. Moving the switch() into a separate function spares you</span>
<span class="quote">&gt; one indentation level and all these horrible to read line breaks.</span>

done
<span class="quote">
&gt; </span>
<span class="quote">&gt; And again I really have to ask why all of this stuff needs to be type</span>
<span class="quote">&gt; casted for every invocation. If you use the proper type for the argument</span>
<span class="quote">&gt; and then do the cast at the function entry then you can spare all that hard</span>
<span class="quote">&gt; to read crap.</span>

fixed for cpu_op_pin_pages. I don&#39;t control the type expected by access_ok()
though.
<span class="quote">

&gt; </span>
<span class="quote">&gt;&gt; +error:</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; pin_pages-&gt;nr; i++)</span>
<span class="quote">&gt;&gt; +		put_page(pin_pages-&gt;pages[i]);</span>
<span class="quote">&gt;&gt; +	pin_pages-&gt;nr = 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sigh. Why can&#39;t you do that at the call site where you have exactly the</span>
<span class="quote">&gt; same thing?</span>

Good point. fixed.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * If faulting access is expected, return EAGAIN to user-space.</span>
<span class="quote">&gt;&gt; +	 * It allows user-space to distinguish between a fault caused by</span>
<span class="quote">&gt;&gt; +	 * an access which is expect to fault (e.g. due to concurrent</span>
<span class="quote">&gt;&gt; +	 * unmapping of underlying memory) from an unexpected fault from</span>
<span class="quote">&gt;&gt; +	 * which a retry would not recover.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (ret == -EFAULT &amp;&amp; expect_fault)</span>
<span class="quote">&gt;&gt; +		return -EAGAIN;</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt;&gt; +static int do_cpu_op_compare_iter(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	char bufa[TMP_BUFLEN], bufb[TMP_BUFLEN];</span>
<span class="quote">&gt;&gt; +	uint32_t compared = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	while (compared != len) {</span>
<span class="quote">&gt;&gt; +		unsigned long to_compare;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		to_compare = min_t(uint32_t, TMP_BUFLEN, len - compared);</span>
<span class="quote">&gt;&gt; +		if (__copy_from_user_inatomic(bufa, a + compared, to_compare))</span>
<span class="quote">&gt;&gt; +			return -EFAULT;</span>
<span class="quote">&gt;&gt; +		if (__copy_from_user_inatomic(bufb, b + compared, to_compare))</span>
<span class="quote">&gt;&gt; +			return -EFAULT;</span>
<span class="quote">&gt;&gt; +		if (memcmp(bufa, bufb, to_compare))</span>
<span class="quote">&gt;&gt; +			return 1;	/* different */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; These tail comments are really crap. It&#39;s entirely obvious that if memcmp</span>
<span class="quote">&gt; != 0 the result is different. So what is the exact value aside of making it</span>
<span class="quote">&gt; hard to read?</span>

Removed.
<span class="quote">

&gt; </span>
<span class="quote">&gt;&gt; +		compared += to_compare;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	return 0;	/* same */</span>

Ditto.
<span class="quote">

&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="quote">&gt;&gt; +static int do_cpu_op_compare(void __user *a, void __user *b, uint32_t len)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int ret = -EFAULT;</span>
<span class="quote">&gt;&gt; +	union {</span>
<span class="quote">&gt;&gt; +		uint8_t _u8;</span>
<span class="quote">&gt;&gt; +		uint16_t _u16;</span>
<span class="quote">&gt;&gt; +		uint32_t _u32;</span>
<span class="quote">&gt;&gt; +		uint64_t _u64;</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &lt; 64)</span>
<span class="quote">&gt;&gt; +		uint32_t _u64_split[2];</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +	} tmp[2];</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve seen the same union before</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +union op_fn_data {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ......</span>

Ah, yes. It&#39;s already declared! I should indeed use it :)
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pagefault_disable();</span>
<span class="quote">&gt;&gt; +	switch (len) {</span>
<span class="quote">&gt;&gt; +	case 1:</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[0]._u8, (uint8_t __user *)a))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[1]._u8, (uint8_t __user *)b))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		ret = !!(tmp[0]._u8 != tmp[1]._u8);</span>
<span class="quote">&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt; +	case 2:</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[0]._u16, (uint16_t __user *)a))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[1]._u16, (uint16_t __user *)b))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		ret = !!(tmp[0]._u16 != tmp[1]._u16);</span>
<span class="quote">&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt; +	case 4:</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[0]._u32, (uint32_t __user *)a))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[1]._u32, (uint32_t __user *)b))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		ret = !!(tmp[0]._u32 != tmp[1]._u32);</span>
<span class="quote">&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt; +	case 8:</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG &gt;= 64)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We alredy prepare for 128 bit?</span>

== it is then ;)
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[0]._u64, (uint64_t __user *)a))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[1]._u64, (uint64_t __user *)b))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[0]._u64_split[0], (uint32_t __user *)a))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[0]._u64_split[1], (uint32_t __user *)a + 1))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[1]._u64_split[0], (uint32_t __user *)b))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp[1]._u64_split[1], (uint32_t __user *)b + 1))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +		ret = !!(tmp[0]._u64 != tmp[1]._u64);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This really sucks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;        union foo va, vb;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;	pagefault_disable();</span>
<span class="quote">&gt;	switch (len) {</span>
<span class="quote">&gt;	case 1:</span>
<span class="quote">&gt;	case 2:</span>
<span class="quote">&gt;	case 4:</span>
<span class="quote">&gt;	case 8:</span>
<span class="quote">&gt;		va._u64 = _vb._u64 = 0;</span>
<span class="quote">&gt;		if (op_get_user(&amp;va, a, len))</span>
<span class="quote">&gt;			goto out;</span>
<span class="quote">&gt;		if (op_get_user(&amp;vb, b, len))</span>
<span class="quote">&gt;			goto out;</span>
<span class="quote">&gt;		ret = !!(va._u64 != vb._u64);</span>
<span class="quote">&gt;		break;</span>
<span class="quote">&gt;	default:</span>
<span class="quote">&gt;		...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; and have</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; int op_get_user(union foo *val, void *p, int len)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;	switch (len) {</span>
<span class="quote">&gt;	case 1:</span>
<span class="quote">&gt;	     ......</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And do the magic once in that function then you spare that copy and pasted</span>
<span class="quote">&gt; code above. It can be reused in the other ops as well and reduces the amount</span>
<span class="quote">&gt; of copy and paste code significantly.</span>

Good point! done.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt; +	default:</span>
<span class="quote">&gt;&gt; +		pagefault_enable();</span>
<span class="quote">&gt;&gt; +		return do_cpu_op_compare_iter(a, b, len);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +	pagefault_enable();</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +static int __do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int i, ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt;&gt; +		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		/* Guarantee a compiler barrier between each operation. */</span>
<span class="quote">&gt;&gt; +		barrier();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		switch (op-&gt;op) {</span>
<span class="quote">&gt;&gt; +		case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt;&gt; +			ret = do_cpu_op_compare(</span>
<span class="quote">&gt;&gt; +					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt;&gt; +					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt;&gt; +					op-&gt;len);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think you know by now how to spare an indentation level and type casts.</span>

done
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +static int do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt, int cpu)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (cpu != raw_smp_processor_id()) {</span>
<span class="quote">&gt;&gt; +		ret = push_task_to_cpu(current, cpu);</span>
<span class="quote">&gt;&gt; +		if (ret)</span>
<span class="quote">&gt;&gt; +			goto check_online;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	preempt_disable();</span>
<span class="quote">&gt;&gt; +	if (cpu != smp_processor_id()) {</span>
<span class="quote">&gt;&gt; +		ret = -EAGAIN;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is weird. When the above raw_smp_processor_id() check fails you push,</span>
<span class="quote">&gt; but here you return. Not really consistent behaviour.</span>

Good point. We could re-try internally rather than let user-space
deal with an EAGAIN. It will make the error checking easier in
user-space.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +		goto end;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +	preempt_enable();</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +check_online:</span>
<span class="quote">&gt;&gt; +	if (!cpu_possible(cpu))</span>
<span class="quote">&gt;&gt; +		return -EINVAL;</span>
<span class="quote">&gt;&gt; +	get_online_cpus();</span>
<span class="quote">&gt;&gt; +	if (cpu_online(cpu)) {</span>
<span class="quote">&gt;&gt; +		ret = -EAGAIN;</span>
<span class="quote">&gt;&gt; +		goto put_online_cpus;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * CPU is offline. Perform operation from the current CPU with</span>
<span class="quote">&gt;&gt; +	 * cpu_online read lock held, preventing that CPU from coming online,</span>
<span class="quote">&gt;&gt; +	 * and with mutex held, providing mutual exclusion against other</span>
<span class="quote">&gt;&gt; +	 * CPUs also finding out about an offline CPU.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s not mentioned in the comment at the top IIRC.</span>

Updated.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	mutex_lock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt;&gt; +	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="quote">&gt;&gt; +	mutex_unlock(&amp;cpu_opv_offline_lock);</span>
<span class="quote">&gt;&gt; +put_online_cpus:</span>
<span class="quote">&gt;&gt; +	put_online_cpus();</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * cpu_opv - execute operation vector on a given CPU with preempt off.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Userspace should pass current CPU number as parameter. May fail with</span>
<span class="quote">&gt;&gt; + * -EAGAIN if currently executing on the wrong CPU.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +SYSCALL_DEFINE4(cpu_opv, struct cpu_op __user *, ucpuopv, int, cpuopcnt,</span>
<span class="quote">&gt;&gt; +		int, cpu, int, flags)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct cpu_op cpuopv[CPU_OP_VEC_LEN_MAX];</span>
<span class="quote">&gt;&gt; +	struct page *pinned_pages_on_stack[NR_PINNED_PAGES_ON_STACK];</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Oh well.... Naming sucks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +	struct cpu_opv_pinned_pages pin_pages = {</span>
<span class="quote">&gt;&gt; +		.pages = pinned_pages_on_stack,</span>
<span class="quote">&gt;&gt; +		.nr = 0,</span>
<span class="quote">&gt;&gt; +		.is_kmalloc = false,</span>
<span class="quote">&gt;&gt; +	};</span>
<span class="quote">&gt;&gt; +	int ret, i;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (unlikely(flags))</span>
<span class="quote">&gt;&gt; +		return -EINVAL;</span>
<span class="quote">&gt;&gt; +	if (unlikely(cpu &lt; 0))</span>
<span class="quote">&gt;&gt; +		return -EINVAL;</span>
<span class="quote">&gt;&gt; +	if (cpuopcnt &lt; 0 || cpuopcnt &gt; CPU_OP_VEC_LEN_MAX)</span>
<span class="quote">&gt;&gt; +		return -EINVAL;</span>
<span class="quote">&gt;&gt; +	if (copy_from_user(cpuopv, ucpuopv, cpuopcnt * sizeof(struct cpu_op)))</span>
<span class="quote">&gt;&gt; +		return -EFAULT;</span>
<span class="quote">&gt;&gt; +	ret = cpu_opv_check(cpuopv, cpuopcnt);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; AFAICT you can calculate the number of pages already in the check and then</span>
<span class="quote">&gt; do that allocation before pinning the pages.</span>

will do.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +	if (ret)</span>
<span class="quote">&gt;&gt; +		return ret;</span>
<span class="quote">&gt;&gt; +	ret = cpu_opv_pin_pages(cpuopv, cpuopcnt, &amp;pin_pages);</span>
<span class="quote">&gt;&gt; +	if (ret)</span>
<span class="quote">&gt;&gt; +		goto end;</span>
<span class="quote">&gt;&gt; +	ret = do_cpu_opv(cpuopv, cpuopcnt, cpu);</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; pin_pages.nr; i++)</span>
<span class="quote">&gt;&gt; +		put_page(pin_pages.pages[i]);</span>
<span class="quote">&gt;&gt; +end:</span>
<span class="quote">&gt;&gt; +	if (pin_pages.is_kmalloc)</span>
<span class="quote">&gt;&gt; +		kfree(pin_pages.pages);</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="quote">&gt;&gt; index 6bba05f47e51..e547f93a46c2 100644</span>
<span class="quote">&gt;&gt; --- a/kernel/sched/core.c</span>
<span class="quote">&gt;&gt; +++ b/kernel/sched/core.c</span>
<span class="quote">&gt;&gt; @@ -1052,6 +1052,43 @@ void do_set_cpus_allowed(struct task_struct *p, const</span>
<span class="quote">&gt;&gt; struct cpumask *new_mask)</span>
<span class="quote">&gt;&gt;  		set_curr_task(rq, p);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is NOT part of this functionality. It&#39;s a prerequisite and wants to be</span>
<span class="quote">&gt; in a separate patch. And I&#39;m dead tired by now so I leave that thing for</span>
<span class="quote">&gt; tomorrow or for Peter.</span>

I&#39;ll split that part into a separate patch.

Thanks!

Mathieu
<span class="quote">

&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tglx</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 20, 2017, 5:13 p.m.</div>
<pre class="content">
----- On Nov 17, 2017, at 3:22 PM, Thomas Gleixner tglx@linutronix.de wrote:
<span class="quote">
&gt; On Fri, 17 Nov 2017, Mathieu Desnoyers wrote:</span>
<span class="quote">&gt;&gt; ----- On Nov 17, 2017, at 5:09 AM, Thomas Gleixner tglx@linutronix.de wrote:</span>
<span class="quote">&gt;&gt; 7) Allow libraries with multi-part algorithms to work on same per-cpu</span>
<span class="quote">&gt;&gt;    data without affecting the allowed cpu mask</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I stumbled on an interesting use-case within the lttng-ust tracer</span>
<span class="quote">&gt;&gt; per-cpu buffers: the algorithm needs to update a &quot;reserve&quot; counter,</span>
<span class="quote">&gt;&gt; serialize data into the buffer, and then update a &quot;commit&quot; counter</span>
<span class="quote">&gt;&gt; _on the same per-cpu buffer_. My goal is to use rseq for both reserve</span>
<span class="quote">&gt;&gt; and commit.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Clearly, if rseq reserve fails, the algorithm can retry on a different</span>
<span class="quote">&gt;&gt; per-cpu buffer. However, it&#39;s not that easy for the commit. It needs to</span>
<span class="quote">&gt;&gt; be performed on the same per-cpu buffer as the reserve.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The cpu_opv system call solves that problem by receiving the cpu number</span>
<span class="quote">&gt;&gt; on which the operation needs to be performed as argument. It can push</span>
<span class="quote">&gt;&gt; the task to the right CPU if needed, and perform the operations there</span>
<span class="quote">&gt;&gt; with preemption disabled.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If your transaction cannot be done in one go, then abusing that byte code</span>
<span class="quote">&gt; interpreter for concluding it is just hillarious. That whole exercise is a</span>
<span class="quote">&gt; gazillion times slower than the atomic operations which are neccesary to do</span>
<span class="quote">&gt; it without all that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m even more convinced now that this is overengineered beyond repair.</span>

The fast-path (typical case) will execute on the right CPU, and rseq will
do both the reserve and the commit, and that is faster than atomic ops.

However, we need to handle migration between reserve and commit.
Unfortunately, concurrent rseq and atomic ops don&#39;t mix well on the
same per-cpu data, so we cannot fall-back on atomic ops, except in
very specific cases where we can use a split-counter strategy.

So cpu_opv handles migration for this use-case by ensuring
the slow-path is performed with preemption-off after migrating
the current task to the right CPU.

Thanks,

Mathieu
<span class="quote">

&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tglx</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 20, 2017, 5:48 p.m.</div>
<pre class="content">
On Mon, 20 Nov 2017, Mathieu Desnoyers wrote:
<span class="quote">&gt; ----- On Nov 16, 2017, at 6:26 PM, Thomas Gleixner tglx@linutronix.de wrote:</span>
<span class="quote">&gt; &gt;&gt; +#define NR_PINNED_PAGES_ON_STACK	8</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 8 pinned pages on stack? Which stack?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The common cases need to touch few pages, and we can keep the</span>
<span class="quote">&gt; pointers in an array on the kernel stack within the cpu_opv system</span>
<span class="quote">&gt; call.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Updating to:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; /*</span>
<span class="quote">&gt;  * Typical invocation of cpu_opv need few pages. Keep struct page</span>
<span class="quote">&gt;  * pointers in an array on the stack of the cpu_opv system call up to</span>
<span class="quote">&gt;  * this limit, beyond which the array is dynamically allocated.</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt; #define NR_PIN_PAGES_ON_STACK        8</span>

That name still sucks. NR_PAGE_PTRS_ON_STACK would be immediately obvious.
<span class="quote">
&gt; &gt;&gt; + * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt; &gt;&gt; + * left shift, and right shift. The system call receives a CPU number</span>
<span class="quote">&gt; &gt;&gt; + * from user-space as argument, which is the CPU on which those</span>
<span class="quote">&gt; &gt;&gt; + * operations need to be performed. All preparation steps such as</span>
<span class="quote">&gt; &gt;&gt; + * loading pointers, and applying offsets to arrays, need to be</span>
<span class="quote">&gt; &gt;&gt; + * performed by user-space before invoking the system call. The</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; loading pointers and applying offsets? That makes no sense.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Updating to:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  * All preparation steps such as</span>
<span class="quote">&gt;  * loading base pointers, and adding offsets derived from the current</span>
<span class="quote">&gt;  * CPU number, need to be performed by user-space before invoking the</span>
<span class="quote">&gt;  * system call.</span>

This still does not explain anything, really.

Which base pointer is loaded?  I nowhere see a reference to a base
pointer.

And what are the offsets about?

derived from current cpu number? What is current CPU number? The one on
which the task executes now or the one which it should execute on?

I assume what you want to say is:

  All pointers in the ops must have been set up to point to the per CPU
  memory of the CPU on which the operations should be executed.

At least that&#39;s what I oracle in to that.
<span class="quote">
&gt; &gt;&gt; + * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="quote">&gt; &gt;&gt; + * preparation step did not change between preparation of system call</span>
<span class="quote">&gt; &gt;&gt; + * inputs and operation execution within the preempt-off critical</span>
<span class="quote">&gt; &gt;&gt; + * section.</span>
<span class="quote">&gt; &gt;&gt; + *</span>
<span class="quote">&gt; &gt;&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt; &gt;&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt; &gt;&gt; + * to first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That doesnt explain it either.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What kind of explication are you looking for here ? Perhaps being too close</span>
<span class="quote">&gt; to the implementation prevents me from understanding what is unclear from</span>
<span class="quote">&gt; your perspective.</span>

What the heck are pointer offsets?

The ops have one or two pointer(s) to a lump of memory. So if a pointer
points to the wrong lump of memory then you&#39;re screwed, but that&#39;s true for
all pointers handed to the kernel.
<span class="quote">
&gt; Sorry, that paragraph was unclear. Updated:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  * An overall maximum of 4216 bytes in enforced on the sum of operation</span>
<span class="quote">&gt;  * length within an operation vector, so user-space cannot generate a</span>
<span class="quote">&gt;  * too long preempt-off critical section (cache cold critical section</span>
<span class="quote">&gt;  * duration measured as 4.7s on x86-64). Each operation is also limited</span>
<span class="quote">&gt;  * a length of PAGE_SIZE bytes,</span>

Again PAGE_SIZE is the wrong unit here. PAGE_SIZE can vary. What you want
is a hard limit of 4K. And because there is no alignment requiremnt the
rest of the sentence is stating the obvious.
<span class="quote">
&gt;  * meaning that an operation can touch a</span>
<span class="quote">&gt;  * maximum of 4 pages (memcpy: 2 pages for source, 2 pages for</span>
<span class="quote">&gt;  * destination if addresses are not aligned on page boundaries).</span>

I still have to understand why the 4K copy is necessary in the first place.
<span class="quote">
&gt; &gt; What&#39;s the critical section duration for operations which go to the limits</span>
<span class="quote">&gt; &gt; of this on a average x86 64 machine?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When cache-cold, I measure 4.7 s per critical section doing a</span>
<span class="quote">&gt; 4k memcpy and 15 * 8 bytes memcpy on a E5-2630 v3 @2.4GHz. Is it an</span>
<span class="quote">&gt; acceptable preempt-off latency for RT ?</span>

Depends on the use case as always ....

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 20, 2017, 6:03 p.m.</div>
<pre class="content">
On Mon, 20 Nov 2017, Thomas Gleixner wrote:
<span class="quote">&gt; On Mon, 20 Nov 2017, Mathieu Desnoyers wrote:</span>
<span class="quote">&gt; &gt; &gt;&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt; &gt; &gt;&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt; &gt; &gt;&gt; + * to first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; That doesnt explain it either.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; What kind of explication are you looking for here ? Perhaps being too close</span>
<span class="quote">&gt; &gt; to the implementation prevents me from understanding what is unclear from</span>
<span class="quote">&gt; &gt; your perspective.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What the heck are pointer offsets?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The ops have one or two pointer(s) to a lump of memory. So if a pointer</span>
<span class="quote">&gt; points to the wrong lump of memory then you&#39;re screwed, but that&#39;s true for</span>
<span class="quote">&gt; all pointers handed to the kernel.</span>

So I think you mix here the &#39;user space programmer guide - how to set up
the magic ops - into the kernel side documentation. The kernel side does
not care about the pointers, as long as they are valid to access.

Again. Try to explain things at the conceptual level.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 20, 2017, 6:39 p.m.</div>
<pre class="content">
----- On Nov 20, 2017, at 12:48 PM, Thomas Gleixner tglx@linutronix.de wrote:
<span class="quote">
&gt; On Mon, 20 Nov 2017, Mathieu Desnoyers wrote:</span>
<span class="quote">&gt;&gt; ----- On Nov 16, 2017, at 6:26 PM, Thomas Gleixner tglx@linutronix.de wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt; +#define NR_PINNED_PAGES_ON_STACK	8</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; 8 pinned pages on stack? Which stack?</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The common cases need to touch few pages, and we can keep the</span>
<span class="quote">&gt;&gt; pointers in an array on the kernel stack within the cpu_opv system</span>
<span class="quote">&gt;&gt; call.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Updating to:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; /*</span>
<span class="quote">&gt;&gt;  * Typical invocation of cpu_opv need few pages. Keep struct page</span>
<span class="quote">&gt;&gt;  * pointers in an array on the stack of the cpu_opv system call up to</span>
<span class="quote">&gt;&gt;  * this limit, beyond which the array is dynamically allocated.</span>
<span class="quote">&gt;&gt;  */</span>
<span class="quote">&gt;&gt; #define NR_PIN_PAGES_ON_STACK        8</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That name still sucks. NR_PAGE_PTRS_ON_STACK would be immediately obvious.</span>

fixed.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; + * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt;&gt; &gt;&gt; + * left shift, and right shift. The system call receives a CPU number</span>
<span class="quote">&gt;&gt; &gt;&gt; + * from user-space as argument, which is the CPU on which those</span>
<span class="quote">&gt;&gt; &gt;&gt; + * operations need to be performed. All preparation steps such as</span>
<span class="quote">&gt;&gt; &gt;&gt; + * loading pointers, and applying offsets to arrays, need to be</span>
<span class="quote">&gt;&gt; &gt;&gt; + * performed by user-space before invoking the system call. The</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; loading pointers and applying offsets? That makes no sense.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Updating to:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;  * All preparation steps such as</span>
<span class="quote">&gt;&gt;  * loading base pointers, and adding offsets derived from the current</span>
<span class="quote">&gt;&gt;  * CPU number, need to be performed by user-space before invoking the</span>
<span class="quote">&gt;&gt;  * system call.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This still does not explain anything, really.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which base pointer is loaded?  I nowhere see a reference to a base</span>
<span class="quote">&gt; pointer.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And what are the offsets about?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; derived from current cpu number? What is current CPU number? The one on</span>
<span class="quote">&gt; which the task executes now or the one which it should execute on?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I assume what you want to say is:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  All pointers in the ops must have been set up to point to the per CPU</span>
<span class="quote">&gt;  memory of the CPU on which the operations should be executed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; At least that&#39;s what I oracle in to that.</span>

Exactly that. Will update to use this description instead.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; + * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="quote">&gt;&gt; &gt;&gt; + * preparation step did not change between preparation of system call</span>
<span class="quote">&gt;&gt; &gt;&gt; + * inputs and operation execution within the preempt-off critical</span>
<span class="quote">&gt;&gt; &gt;&gt; + * section.</span>
<span class="quote">&gt;&gt; &gt;&gt; + *</span>
<span class="quote">&gt;&gt; &gt;&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt;&gt; &gt;&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt;&gt; &gt;&gt; + * to first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; That doesnt explain it either.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; What kind of explication are you looking for here ? Perhaps being too close</span>
<span class="quote">&gt;&gt; to the implementation prevents me from understanding what is unclear from</span>
<span class="quote">&gt;&gt; your perspective.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What the heck are pointer offsets?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The ops have one or two pointer(s) to a lump of memory. So if a pointer</span>
<span class="quote">&gt; points to the wrong lump of memory then you&#39;re screwed, but that&#39;s true for</span>
<span class="quote">&gt; all pointers handed to the kernel.</span>

I think the sentence you suggested above is clear enough. I&#39;ll simply use
it.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; Sorry, that paragraph was unclear. Updated:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;  * An overall maximum of 4216 bytes in enforced on the sum of operation</span>
<span class="quote">&gt;&gt;  * length within an operation vector, so user-space cannot generate a</span>
<span class="quote">&gt;&gt;  * too long preempt-off critical section (cache cold critical section</span>
<span class="quote">&gt;&gt;  * duration measured as 4.7s on x86-64). Each operation is also limited</span>
<span class="quote">&gt;&gt;  * a length of PAGE_SIZE bytes,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again PAGE_SIZE is the wrong unit here. PAGE_SIZE can vary. What you want</span>
<span class="quote">&gt; is a hard limit of 4K. And because there is no alignment requiremnt the</span>
<span class="quote">&gt; rest of the sentence is stating the obvious.</span>

I can make that a 4K limit if you prefer. This presumes that no architecture
has pages smaller than 4K, which is true on Linux.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt;  * meaning that an operation can touch a</span>
<span class="quote">&gt;&gt;  * maximum of 4 pages (memcpy: 2 pages for source, 2 pages for</span>
<span class="quote">&gt;&gt;  * destination if addresses are not aligned on page boundaries).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I still have to understand why the 4K copy is necessary in the first place.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; &gt; What&#39;s the critical section duration for operations which go to the limits</span>
<span class="quote">&gt;&gt; &gt; of this on a average x86 64 machine?</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; When cache-cold, I measure 4.7 s per critical section doing a</span>
<span class="quote">&gt;&gt; 4k memcpy and 15 * 8 bytes memcpy on a E5-2630 v3 @2.4GHz. Is it an</span>
<span class="quote">&gt;&gt; acceptable preempt-off latency for RT ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Depends on the use case as always ....</span>

The use-case for 4k memcpy operation is a per-cpu ring buffer where
the rseq fast-path does the following:

- ring buffer push: in the rseq asm instruction sequence, a memcpy of a
  given structure (limited to 4k in size) into a ring buffer,
  followed by the final commit instruction which increments the current
  position offset by the number of bytes pushed.

- ring buffer pop: in the rseq asm instruction sequence, a memcpy of
  a given structure (up to 4k) from the ring buffer, at &quot;position&quot; offset.
  The final commit instruction decrements the current position offset by
  the number of bytes pop&#39;d.

Having cpu_opv do a 4k memcpy allow it to handle scenarios where
rseq fails to progress.

Thanks,

Mathieu
<span class="quote">


&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tglx</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 20, 2017, 6:42 p.m.</div>
<pre class="content">
----- On Nov 20, 2017, at 1:03 PM, Thomas Gleixner tglx@linutronix.de wrote:
<span class="quote">
&gt; On Mon, 20 Nov 2017, Thomas Gleixner wrote:</span>
<span class="quote">&gt;&gt; On Mon, 20 Nov 2017, Mathieu Desnoyers wrote:</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; + * The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; + * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; + * to first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt;&gt; &gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; &gt; That doesnt explain it either.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; What kind of explication are you looking for here ? Perhaps being too close</span>
<span class="quote">&gt;&gt; &gt; to the implementation prevents me from understanding what is unclear from</span>
<span class="quote">&gt;&gt; &gt; your perspective.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; What the heck are pointer offsets?</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The ops have one or two pointer(s) to a lump of memory. So if a pointer</span>
<span class="quote">&gt;&gt; points to the wrong lump of memory then you&#39;re screwed, but that&#39;s true for</span>
<span class="quote">&gt;&gt; all pointers handed to the kernel.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I think you mix here the &#39;user space programmer guide - how to set up</span>
<span class="quote">&gt; the magic ops - into the kernel side documentation. The kernel side does</span>
<span class="quote">&gt; not care about the pointers, as long as they are valid to access.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again. Try to explain things at the conceptual level.</span>

Yes, I took the sentence you suggested for that reason: it removes
usage details that are meant for user-space implementer, which do not
belong in those comments.

Thanks,

Mathieu
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tglx</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104">Andi Kleen</a> - Nov. 20, 2017, 6:49 p.m.</div>
<pre class="content">
<span class="quote">&gt; Having cpu_opv do a 4k memcpy allow it to handle scenarios where</span>
<span class="quote">&gt; rseq fails to progress.</span>

If anybody ever gets that right. It will be really hard to just
test such a path.

It also seems fairly theoretical to me. Do you even have a 
test case where the normal path stops making forward progress?

-Andi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 20, 2017, 7:44 p.m.</div>
<pre class="content">
On Mon, 20 Nov 2017, Mathieu Desnoyers wrote:
<span class="quote">&gt; ----- On Nov 20, 2017, at 12:48 PM, Thomas Gleixner tglx@linutronix.de wrote:</span>
<span class="quote">&gt; The use-case for 4k memcpy operation is a per-cpu ring buffer where</span>
<span class="quote">&gt; the rseq fast-path does the following:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - ring buffer push: in the rseq asm instruction sequence, a memcpy of a</span>
<span class="quote">&gt;   given structure (limited to 4k in size) into a ring buffer,</span>
<span class="quote">&gt;   followed by the final commit instruction which increments the current</span>
<span class="quote">&gt;   position offset by the number of bytes pushed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - ring buffer pop: in the rseq asm instruction sequence, a memcpy of</span>
<span class="quote">&gt;   a given structure (up to 4k) from the ring buffer, at &quot;position&quot; offset.</span>
<span class="quote">&gt;   The final commit instruction decrements the current position offset by</span>
<span class="quote">&gt;   the number of bytes pop&#39;d.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having cpu_opv do a 4k memcpy allow it to handle scenarios where</span>
<span class="quote">&gt; rseq fails to progress.</span>

I&#39;m still confused. Before you talked about the sequence:

    1) Reserve

    2) Commit

and both use rseq, but obviously you cannot do two &quot;atomic&quot; operation in
one section.

So now you talk about push. Is that what you described earlier as commit?

Assumed that it is, I still have no idea why the memcpy needs to happen in
that preempt disabled region.

If you have space reserved, then the copy does not have any dependencies
neither on the cpu it runs on nor on anything else. So the only
interresting operation is the final commit instruction which tells the
consumer that its ready.

So what is the part I am missing here aside of having difficulties to map
the constantly changing names of this stuff?

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 20, 2017, 10:46 p.m.</div>
<pre class="content">
----- On Nov 20, 2017, at 1:49 PM, Andi Kleen andi@firstfloor.org wrote:
<span class="quote">
&gt;&gt; Having cpu_opv do a 4k memcpy allow it to handle scenarios where</span>
<span class="quote">&gt;&gt; rseq fails to progress.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If anybody ever gets that right. It will be really hard to just</span>
<span class="quote">&gt; test such a path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It also seems fairly theoretical to me. Do you even have a</span>
<span class="quote">&gt; test case where the normal path stops making forward progress?</span>

We expect the following loop to progress, typically after a single
iteration:

	do {
		cpu = rseq_cpu_start();
		ret = rseq_addv(&amp;v, 1, cpu);
		attempts++;
	} while (ret);

Now runnig this in gdb, break on &quot;main&quot;, run, and single-step
execution with &quot;next&quot;, the program is stuck in an infinite loop.

What solution do you have in mind to handle this kind of
scenario without breaking pre-existing debuggers ?

Looking at vDSO examples of vgetcpu and vclock_gettime under
gdb 7.7.1 (debian) with glibc 2.19:

sched_getcpu behavior under single-stepping per source line
with &quot;step&quot; seems to only see the ../sysdeps/unix/sysv/linux/x86_64/sched_getcpu.S
source lines, which makes it skip single-stepping of the vDSO.

sched_getcpu under &quot;stepi&quot;: it does go through the vDSO instruction
addresses. It does progress, given that there is no loop there.

clock_gettime under &quot;step&quot;: it only sees source lines of
../sysdeps/unix/clock_gettime.c.

clock_gettime under &quot;stepi&quot;: it&#39;s stuck in an infinite loop.

So instruction-level stepping from gdb turns clock_gettime vDSO
into a never-ending loop, which is already bad. But with rseq,
the situation is even worse, because it turns source line level
single-stepping into infinite loops.

My understanding from https://sourceware.org/bugzilla/show_bug.cgi?id=14466
is that GDB currently simply removes the vDSO from its list of library
mappings, which is probably why it skips over vDSO for the source
lines single-stepping case. We cannot do that with rseq, because we
_want_ the rseq critical section to be inlined into the application
or library. A function call costs more than most rseq critical sections.

I plan to have the rseq user-space code provide a &quot;__rseq_table&quot; section
so debuggers can eventually figure out that they need to skip over the
rseq critical sections. However, it won&#39;t help the fact that pre-existing
debugger single-stepping will start turning perfectly working programs
into never-ending loops simply by having glibc use rseq for memory
allocation.

Using the cpu_opv system call on rseq failure solves this problem
entirely.

I would even go further and recommend to take a similar approach when
lack of progress is detected in a vDSO, and invoke the equivalent
system call. The current implementation of the clock_gettime()
vDSO turns instruction-level single-stepping into never
ending loops, which is far from being elegant.

Thanks,

Mathieu
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 21, 2017, 11:25 a.m.</div>
<pre class="content">
----- On Nov 20, 2017, at 2:44 PM, Thomas Gleixner tglx@linutronix.de wrote:
<span class="quote">
&gt; On Mon, 20 Nov 2017, Mathieu Desnoyers wrote:</span>
<span class="quote">&gt;&gt; ----- On Nov 20, 2017, at 12:48 PM, Thomas Gleixner tglx@linutronix.de wrote:</span>
<span class="quote">&gt;&gt; The use-case for 4k memcpy operation is a per-cpu ring buffer where</span>
<span class="quote">&gt;&gt; the rseq fast-path does the following:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; - ring buffer push: in the rseq asm instruction sequence, a memcpy of a</span>
<span class="quote">&gt;&gt;   given structure (limited to 4k in size) into a ring buffer,</span>
<span class="quote">&gt;&gt;   followed by the final commit instruction which increments the current</span>
<span class="quote">&gt;&gt;   position offset by the number of bytes pushed.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; - ring buffer pop: in the rseq asm instruction sequence, a memcpy of</span>
<span class="quote">&gt;&gt;   a given structure (up to 4k) from the ring buffer, at &quot;position&quot; offset.</span>
<span class="quote">&gt;&gt;   The final commit instruction decrements the current position offset by</span>
<span class="quote">&gt;&gt;   the number of bytes pop&#39;d.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Having cpu_opv do a 4k memcpy allow it to handle scenarios where</span>
<span class="quote">&gt;&gt; rseq fails to progress.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m still confused. Before you talked about the sequence:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    1) Reserve</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    2) Commit</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; and both use rseq, but obviously you cannot do two &quot;atomic&quot; operation in</span>
<span class="quote">&gt; one section.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So now you talk about push. Is that what you described earlier as commit?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Assumed that it is, I still have no idea why the memcpy needs to happen in</span>
<span class="quote">&gt; that preempt disabled region.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you have space reserved, then the copy does not have any dependencies</span>
<span class="quote">&gt; neither on the cpu it runs on nor on anything else. So the only</span>
<span class="quote">&gt; interresting operation is the final commit instruction which tells the</span>
<span class="quote">&gt; consumer that its ready.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So what is the part I am missing here aside of having difficulties to map</span>
<span class="quote">&gt; the constantly changing names of this stuff?</span>

Let&#39;s clear up some confusion: those are two different use-cases. The
ring buffer with reserve+commit is a FIFO ring buffer, and the ring buffer
with memcpy+position update is a LIFO queue.

Let me explain the various use-cases here, so we know what we&#39;re talking
about.

rseq and cpu_opv use-cases

1) per-cpu spinlock

A per-cpu spinlock can be implemented as a rseq consisting of a
comparison operation (== 0) on a word, and a word store (1), followed
by an acquire barrier after control dependency. The unlock path can be
performed with a simple store-release of 0 to the word, which does
not require rseq.

The cpu_opv fallback requires a single-word comparison (== 0) and a
single-word store (1).


2) per-cpu statistics counters

A per-cpu statistics counters can be implemented as a rseq consisting
of a final &quot;add&quot; instruction on a word as commit.

The cpu_opv fallback can be implemented as a &quot;ADD&quot; operation.

Besides statistics tracking, these counters can be used to implement
user-space RCU per-cpu grace period tracking for both single and
multi-process user-space RCU.


3) per-cpu LIFO linked-list (unlimited size stack)

A per-cpu LIFO linked-list has a &quot;push&quot; and &quot;pop&quot; operation,
which respectively adds an item to the list, and removes an
item from the list.

The &quot;push&quot; operation can be implemented as a rseq consisting of
a word comparison instruction against head followed by a word store
(commit) to head. Its cpu_opv fallback can be implemented as a
word-compare followed by word-store as well.

The &quot;pop&quot; operation can be implemented as a rseq consisting of
loading head, comparing it against NULL, loading the next pointer
at the right offset within the head item, and the next pointer as
a new head, returning the old head on success.

The cpu_opv fallback for &quot;pop&quot; differs from its rseq algorithm:
considering that cpu_opv requires to know all pointers at system
call entry so it can pin all pages, so cpu_opv cannot simply load
head and then load the head-&gt;next address within the preempt-off
critical section. User-space needs to pass the head and head-&gt;next
addresses to the kernel, and the kernel needs to check that the
head address is unchanged since it has been loaded by user-space.
However, when accessing head-&gt;next in a ABA situation, it&#39;s
possible that head is unchanged, but loading head-&gt;next can
result in a page fault due to a concurrently freed head object.
This is why the &quot;expect_fault&quot; operation field is introduced: if a
fault is triggered by this access, &quot;-EAGAIN&quot; will be returned by
cpu_opv rather than -EFAULT, thus indicating the the operation
vector should be attempted again. The &quot;pop&quot; operation can thus be
implemented as a word comparison of head against the head loaded
by user-space, followed by a load of the head-&gt;next pointer (which
may fault), and a store of that pointer as a new head.


4) per-cpu LIFO ring buffer with pointers to objects (fixed-sized stack)

This structure is useful for passing around allocated objects
by passing pointers through per-cpu fixed-sized stack.

The &quot;push&quot; side can be implemented with a check of the current
offset against the maximum buffer length, followed by a rseq
consisting of a comparison of the previously loaded offset
against the current offset, a word &quot;try store&quot; operation into the
next ring buffer array index (it&#39;s OK to abort after a try-store,
since it&#39;s not the commit, and its side-effect can be overwritten),
then followed by a word-store to increment the current offset (commit).

The &quot;push&quot; cpu_opv fallback can be done with the comparison, and
two consecutive word stores, all within the preempt-off section.

The &quot;pop&quot; side can be implemented with a check that offset is not
0 (whether the buffer is empty), a load of the &quot;head&quot; pointer before the
offset array index, followed by a rseq consisting of a word
comparison checking that the offset is unchanged since previously
loaded, another check ensuring that the &quot;head&quot; pointer is unchanged,
followed by a store decrementing the current offset.

The cpu_opv &quot;pop&quot; can be implemented with the same algorithm
as the rseq fast-path (compare, compare, store).


5) per-cpu LIFO ring buffer with pointers to objects (fixed-sized stack)
   supporting &quot;peek&quot; from remote CPU

In order to implement work queues with work-stealing between CPUs, it is
useful to ensure the offset &quot;commit&quot; in scenario 4) &quot;push&quot; have a
store-release semantic, thus allowing remote CPU to load the offset
with acquire semantic, and load the top pointer, in order to check if
work-stealing should be performed. The task (work queue item) existence
should be protected by other means, e.g. RCU.

If the peek operation notices that work-stealing should indeed be
performed, a thread can use cpu_opv to move the task between per-cpu
workqueues, by first invoking cpu_opv passing the remote work queue
cpu number as argument to pop the task, and then again as &quot;push&quot; with
the target work queue CPU number.


6) per-cpu LIFO ring buffer with data copy (fixed-sized stack)
   (with and without acquire-release)

This structure is useful for passing around data without requiring
memory allocation by copying the data content into per-cpu fixed-sized
stack.

The &quot;push&quot; operation is performed with an offset comparison against
the buffer size (figuring out if the buffer is full), followed by
a rseq consisting of a comparison of the offset, a try-memcpy attempting
to copy the data content into the buffer (which can be aborted and
overwritten), and a final store incrementing the offset.

The cpu_opv fallback needs to same operations, except that the memcpy
is guaranteed to complete, given that it is performed with preemption
disabled. This requires a memcpy operation supporting length up to 4kB.

The &quot;pop&quot; operation is similar to the &quot;push, except that the offset
is first compared to 0 to ensure the buffer is not empty. The
copy source is the ring buffer, and the destination is an output
buffer.


7) per-cpu FIFO ring buffer (fixed-sized queue)

This structure is useful wherever a FIFO behavior (queue) is needed.
One major use-case is tracer ring buffer.

An implementation of this ring buffer has a &quot;reserve&quot;, followed by
serialization of multiple bytes into the buffer, ended by a &quot;commit&quot;.
The &quot;reserve&quot; can be implemented as a rseq consisting of a word
comparison followed by a word store. The reserve operation moves the
producer &quot;head&quot;. The multi-byte serialization can be performed
non-atomically. Finally, the &quot;commit&quot; update can be performed with
a rseq &quot;add&quot; commit instruction with store-release semantic. The
ring buffer consumer reads the commit value with load-acquire
semantic to know whenever it is safe to read from the ring buffer.

This use-case requires that both &quot;reserve&quot; and &quot;commit&quot; operations
be performed on the same per-cpu ring buffer, even if a migration
happens between those operations. In the typical case, both operations
will happens on the same CPU and use rseq. In the unlikely event of a
migration, the cpu_opv system call will ensure the commit can be
performed on the right CPU by migrating the task to that CPU.

On the consumer side, an alternative to using store-release and
load-acquire on the commit counter would be to use cpu_opv to
ensure the commit counter load is performed on the right CPU. This
effectively allows moving a consumer thread between CPUs to execute
close to the ring buffer cache lines it will read.

Thanks,

Mathieu
<span class="quote">

&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tglx</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index c9f95f8b07ed..45a1bbdaa287 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -3675,6 +3675,13 @@</span> <span class="p_context"> B:	https://bugzilla.kernel.org</span>
 F:	drivers/cpuidle/*
 F:	include/linux/cpuidle.h
 
<span class="p_add">+CPU NON-PREEMPTIBLE OPERATION VECTOR SUPPORT</span>
<span class="p_add">+M:	Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+L:	linux-kernel@vger.kernel.org</span>
<span class="p_add">+S:	Supported</span>
<span class="p_add">+F:	kernel/cpu_opv.c</span>
<span class="p_add">+F:	include/uapi/linux/cpu_opv.h</span>
<span class="p_add">+</span>
 CRAMFS FILESYSTEM
 W:	http://sourceforge.net/projects/cramfs/
 S:	Orphan / Obsolete
<span class="p_header">diff --git a/include/uapi/linux/cpu_opv.h b/include/uapi/linux/cpu_opv.h</span>
new file mode 100644
<span class="p_header">index 000000000000..17f7d46e053b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/uapi/linux/cpu_opv.h</span>
<span class="p_chunk">@@ -0,0 +1,117 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _UAPI_LINUX_CPU_OPV_H</span>
<span class="p_add">+#define _UAPI_LINUX_CPU_OPV_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * linux/cpu_opv.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * CPU preempt-off operation vector system call API</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2017 Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="p_add">+ * of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="p_add">+ * in the Software without restriction, including without limitation the rights</span>
<span class="p_add">+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="p_add">+ * copies of the Software, and to permit persons to whom the Software is</span>
<span class="p_add">+ * furnished to do so, subject to the following conditions:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The above copyright notice and this permission notice shall be included in</span>
<span class="p_add">+ * all copies or substantial portions of the Software.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="p_add">+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="p_add">+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="p_add">+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="p_add">+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="p_add">+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="p_add">+ * SOFTWARE.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+# include &lt;linux/types.h&gt;</span>
<span class="p_add">+#else	/* #ifdef __KERNEL__ */</span>
<span class="p_add">+# include &lt;stdint.h&gt;</span>
<span class="p_add">+#endif	/* #else #ifdef __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __LP64__</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)			uint64_t field</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	field = (intptr_t)v</span>
<span class="p_add">+#elif defined(__BYTE_ORDER) ? \</span>
<span class="p_add">+	__BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)	uint32_t field ## _padding, field</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="p_add">+	field ## _padding = 0, field = (intptr_t)v</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)	uint32_t field, field ## _padding</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="p_add">+	field = (intptr_t)v, field ## _padding = 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_OP_VEC_LEN_MAX		16</span>
<span class="p_add">+#define CPU_OP_ARG_LEN_MAX		24</span>
<span class="p_add">+/* Max. data len per operation. */</span>
<span class="p_add">+#define CPU_OP_DATA_LEN_MAX		PAGE_SIZE</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Max. data len for overall vector. We to restrict the amount of</span>
<span class="p_add">+ * user-space data touched by the kernel in non-preemptible context so</span>
<span class="p_add">+ * we do not introduce long scheduler latencies.</span>
<span class="p_add">+ * This allows one copy of up to 4096 bytes, and 15 operations touching</span>
<span class="p_add">+ * 8 bytes each.</span>
<span class="p_add">+ * This limit is applied to the sum of length specified for all</span>
<span class="p_add">+ * operations in a vector.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define CPU_OP_VEC_DATA_LEN_MAX		(4096 + 15*8)</span>
<span class="p_add">+#define CPU_OP_MAX_PAGES		4	/* Max. pages per op. */</span>
<span class="p_add">+</span>
<span class="p_add">+enum cpu_op_type {</span>
<span class="p_add">+	CPU_COMPARE_EQ_OP,	/* compare */</span>
<span class="p_add">+	CPU_COMPARE_NE_OP,	/* compare */</span>
<span class="p_add">+	CPU_MEMCPY_OP,		/* memcpy */</span>
<span class="p_add">+	CPU_ADD_OP,		/* arithmetic */</span>
<span class="p_add">+	CPU_OR_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_AND_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_XOR_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_LSHIFT_OP,		/* shift */</span>
<span class="p_add">+	CPU_RSHIFT_OP,		/* shift */</span>
<span class="p_add">+	CPU_MB_OP,		/* memory barrier */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Vector of operations to perform. Limited to 16. */</span>
<span class="p_add">+struct cpu_op {</span>
<span class="p_add">+	int32_t op;	/* enum cpu_op_type. */</span>
<span class="p_add">+	uint32_t len;	/* data length, in bytes. */</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(a);</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(b);</span>
<span class="p_add">+			uint8_t expect_fault_a;</span>
<span class="p_add">+			uint8_t expect_fault_b;</span>
<span class="p_add">+		} compare_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(dst);</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(src);</span>
<span class="p_add">+			uint8_t expect_fault_dst;</span>
<span class="p_add">+			uint8_t expect_fault_src;</span>
<span class="p_add">+		} memcpy_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			int64_t count;</span>
<span class="p_add">+			uint8_t expect_fault_p;</span>
<span class="p_add">+		} arithmetic_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			uint64_t mask;</span>
<span class="p_add">+			uint8_t expect_fault_p;</span>
<span class="p_add">+		} bitwise_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			uint32_t bits;</span>
<span class="p_add">+			uint8_t expect_fault_p;</span>
<span class="p_add">+		} shift_op;</span>
<span class="p_add">+		char __padding[CPU_OP_ARG_LEN_MAX];</span>
<span class="p_add">+	} u;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_LINUX_CPU_OPV_H */</span>
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index cbedfb91b40a..e4fbb5dd6a24 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -1404,6 +1404,7 @@</span> <span class="p_context"> config RSEQ</span>
 	bool &quot;Enable rseq() system call&quot; if EXPERT
 	default y
 	depends on HAVE_RSEQ
<span class="p_add">+	select CPU_OPV</span>
 	select MEMBARRIER
 	help
 	  Enable the restartable sequences system call. It provides a
<span class="p_chunk">@@ -1414,6 +1415,19 @@</span> <span class="p_context"> config RSEQ</span>
 
 	  If unsure, say Y.
 
<span class="p_add">+config CPU_OPV</span>
<span class="p_add">+	bool &quot;Enable cpu_opv() system call&quot; if EXPERT</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Enable the CPU preempt-off operation vector system call.</span>
<span class="p_add">+	  It allows user-space to perform a sequence of operations on</span>
<span class="p_add">+	  per-cpu data with preemption disabled. Useful as</span>
<span class="p_add">+	  single-stepping fall-back for restartable sequences, and for</span>
<span class="p_add">+	  performing more complex operations on per-cpu data that would</span>
<span class="p_add">+	  not be otherwise possible to do with restartable sequences.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say Y.</span>
<span class="p_add">+</span>
 config EMBEDDED
 	bool &quot;Embedded system&quot;
 	option allnoconfig_y
<span class="p_header">diff --git a/kernel/Makefile b/kernel/Makefile</span>
<span class="p_header">index 3574669dafd9..cac8855196ff 100644</span>
<span class="p_header">--- a/kernel/Makefile</span>
<span class="p_header">+++ b/kernel/Makefile</span>
<span class="p_chunk">@@ -113,6 +113,7 @@</span> <span class="p_context"> obj-$(CONFIG_TORTURE_TEST) += torture.o</span>
 
 obj-$(CONFIG_HAS_IOMEM) += memremap.o
 obj-$(CONFIG_RSEQ) += rseq.o
<span class="p_add">+obj-$(CONFIG_CPU_OPV) += cpu_opv.o</span>
 
 $(obj)/configs.o: $(obj)/config_data.h
 
<span class="p_header">diff --git a/kernel/cpu_opv.c b/kernel/cpu_opv.c</span>
new file mode 100644
<span class="p_header">index 000000000000..a81837a14b17</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/kernel/cpu_opv.c</span>
<span class="p_chunk">@@ -0,0 +1,968 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * CPU preempt-off operation vector system call</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It allows user-space to perform a sequence of operations on per-cpu</span>
<span class="p_add">+ * data with preemption disabled. Useful as single-stepping fall-back</span>
<span class="p_add">+ * for restartable sequences, and for performing more complex operations</span>
<span class="p_add">+ * on per-cpu data that would not be otherwise possible to do with</span>
<span class="p_add">+ * restartable sequences.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2017, EfficiOS Inc.,</span>
<span class="p_add">+ * Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/syscalls.h&gt;</span>
<span class="p_add">+#include &lt;linux/cpu_opv.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mutex.h&gt;</span>
<span class="p_add">+#include &lt;linux/pagemap.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;sched/sched.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#define TMP_BUFLEN			64</span>
<span class="p_add">+#define NR_PINNED_PAGES_ON_STACK	8</span>
<span class="p_add">+</span>
<span class="p_add">+union op_fn_data {</span>
<span class="p_add">+	uint8_t _u8;</span>
<span class="p_add">+	uint16_t _u16;</span>
<span class="p_add">+	uint32_t _u32;</span>
<span class="p_add">+	uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+	uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct cpu_opv_pinned_pages {</span>
<span class="p_add">+	struct page **pages;</span>
<span class="p_add">+	size_t nr;</span>
<span class="p_add">+	bool is_kmalloc;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+typedef int (*op_fn_t)(union op_fn_data *data, uint64_t v, uint32_t len);</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_MUTEX(cpu_opv_offline_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The cpu_opv system call executes a vector of operations on behalf of</span>
<span class="p_add">+ * user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="p_add">+ * from readv() and writev() system calls which take a &quot;struct iovec&quot;</span>
<span class="p_add">+ * array as argument.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="p_add">+ * left shift, and right shift. The system call receives a CPU number</span>
<span class="p_add">+ * from user-space as argument, which is the CPU on which those</span>
<span class="p_add">+ * operations need to be performed. All preparation steps such as</span>
<span class="p_add">+ * loading pointers, and applying offsets to arrays, need to be</span>
<span class="p_add">+ * performed by user-space before invoking the system call. The</span>
<span class="p_add">+ * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="p_add">+ * preparation step did not change between preparation of system call</span>
<span class="p_add">+ * inputs and operation execution within the preempt-off critical</span>
<span class="p_add">+ * section.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The reason why we require all pointer offsets to be calculated by</span>
<span class="p_add">+ * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="p_add">+ * to first pin all pages touched by each operation. This takes care of</span>
<span class="p_add">+ * faulting-in the pages. Then, preemption is disabled, and the</span>
<span class="p_add">+ * operations are performed atomically with respect to other thread</span>
<span class="p_add">+ * execution on that CPU, without generating any page fault.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="p_add">+ * enforced, and a overall maximum length sum, so user-space cannot</span>
<span class="p_add">+ * generate a too long preempt-off critical section. Each operation is</span>
<span class="p_add">+ * also limited a length of PAGE_SIZE bytes, meaning that an operation</span>
<span class="p_add">+ * can touch a maximum of 4 pages (memcpy: 2 pages for source, 2 pages</span>
<span class="p_add">+ * for destination if addresses are not aligned on page boundaries).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If the thread is not running on the requested CPU, a new</span>
<span class="p_add">+ * push_task_to_cpu() is invoked to migrate the task to the requested</span>
<span class="p_add">+ * CPU.  If the requested CPU is not part of the cpus allowed mask of</span>
<span class="p_add">+ * the thread, the system call fails with EINVAL. After the migration</span>
<span class="p_add">+ * has been performed, preemption is disabled, and the current CPU</span>
<span class="p_add">+ * number is checked again and compared to the requested CPU number. If</span>
<span class="p_add">+ * it still differs, it means the scheduler migrated us away from that</span>
<span class="p_add">+ * CPU. Return EAGAIN to user-space in that case, and let user-space</span>
<span class="p_add">+ * retry (either requesting the same CPU number, or a different one,</span>
<span class="p_add">+ * depending on the user-space algorithm constraints).</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Check operation types and length parameters.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpu_opv_check(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	uint32_t sum = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			sum += op-&gt;len;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			if (op-&gt;len &gt; CPU_OP_DATA_LEN_MAX)</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			switch (op-&gt;len) {</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+			case 4:</span>
<span class="p_add">+			case 8:</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			default:</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			switch (op-&gt;len) {</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 7)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 15)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 4:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 31)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 8:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 63)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			default:</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (sum &gt; CPU_OP_VEC_DATA_LEN_MAX)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="p_add">+		unsigned long len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((addr + len - 1) &gt;&gt; PAGE_SHIFT) - (addr &gt;&gt; PAGE_SHIFT) + 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_op_check_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_zone_device_page(page))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	page = compound_head(page);</span>
<span class="p_add">+	mapping = READ_ONCE(page-&gt;mapping);</span>
<span class="p_add">+	if (!mapping) {</span>
<span class="p_add">+		int shmem_swizzled;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Check again with page lock held to guard against</span>
<span class="p_add">+		 * memory pressure making shmem_writepage move the page</span>
<span class="p_add">+		 * from filecache to swapcache.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		lock_page(page);</span>
<span class="p_add">+		shmem_swizzled = PageSwapCache(page) || page-&gt;mapping;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		if (shmem_swizzled)</span>
<span class="p_add">+			return -EAGAIN;</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Refusing device pages, the zero page, pages in the gate area, and</span>
<span class="p_add">+ * special mappings. Inspired from futex.c checks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpu_op_check_pages(struct page **pages,</span>
<span class="p_add">+		unsigned long nr_pages)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nr_pages; i++) {</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = cpu_op_check_page(pages[i]);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		struct cpu_opv_pinned_pages *pin_pages, int write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *pages[2];</span>
<span class="p_add">+	int ret, nr_pages;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!len)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="p_add">+	BUG_ON(nr_pages &gt; 2);</span>
<span class="p_add">+	if (!pin_pages-&gt;is_kmalloc &amp;&amp; pin_pages-&gt;nr + nr_pages</span>
<span class="p_add">+			&gt; NR_PINNED_PAGES_ON_STACK) {</span>
<span class="p_add">+		struct page **pinned_pages =</span>
<span class="p_add">+			kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="p_add">+				* sizeof(struct page *), GFP_KERNEL);</span>
<span class="p_add">+		if (!pinned_pages)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		memcpy(pinned_pages, pin_pages-&gt;pages,</span>
<span class="p_add">+			pin_pages-&gt;nr * sizeof(struct page *));</span>
<span class="p_add">+		pin_pages-&gt;pages = pinned_pages;</span>
<span class="p_add">+		pin_pages-&gt;is_kmalloc = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+again:</span>
<span class="p_add">+	ret = get_user_pages_fast(addr, nr_pages, write, pages);</span>
<span class="p_add">+	if (ret &lt; nr_pages) {</span>
<span class="p_add">+		if (ret &gt; 0)</span>
<span class="p_add">+			put_page(pages[0]);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Refuse device pages, the zero page, pages in the gate area,</span>
<span class="p_add">+	 * and special mappings.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ret = cpu_op_check_pages(pages, nr_pages);</span>
<span class="p_add">+	if (ret == -EAGAIN) {</span>
<span class="p_add">+		put_page(pages[0]);</span>
<span class="p_add">+		if (nr_pages &gt; 1)</span>
<span class="p_add">+			put_page(pages[1]);</span>
<span class="p_add">+		goto again;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto error;</span>
<span class="p_add">+	pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[0];</span>
<span class="p_add">+	if (nr_pages &gt; 1)</span>
<span class="p_add">+		pin_pages-&gt;pages[pin_pages-&gt;nr++] = pages[1];</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+error:</span>
<span class="p_add">+	put_page(pages[0]);</span>
<span class="p_add">+	if (nr_pages &gt; 1)</span>
<span class="p_add">+		put_page(pages[1]);</span>
<span class="p_add">+	return -EFAULT;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="p_add">+		struct cpu_opv_pinned_pages *pin_pages)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret, i;</span>
<span class="p_add">+	bool expect_fault = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check access, pin pages. */</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.compare_op.expect_fault_a;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					op-&gt;len, pin_pages, 0);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.compare_op.expect_fault_b;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len, pin_pages, 0);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.memcpy_op.expect_fault_dst;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE,</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					op-&gt;len, pin_pages, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.memcpy_op.expect_fault_src;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ,</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len, pin_pages, 0);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.arithmetic_op.expect_fault_p;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE,</span>
<span class="p_add">+					(void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;len, pin_pages, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.bitwise_op.expect_fault_p;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE,</span>
<span class="p_add">+					(void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;len, pin_pages, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.shift_op.expect_fault_p;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE,</span>
<span class="p_add">+					(void __user *)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;len, pin_pages, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+error:</span>
<span class="p_add">+	for (i = 0; i &lt; pin_pages-&gt;nr; i++)</span>
<span class="p_add">+		put_page(pin_pages-&gt;pages[i]);</span>
<span class="p_add">+	pin_pages-&gt;nr = 0;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If faulting access is expected, return EAGAIN to user-space.</span>
<span class="p_add">+	 * It allows user-space to distinguish between a fault caused by</span>
<span class="p_add">+	 * an access which is expect to fault (e.g. due to concurrent</span>
<span class="p_add">+	 * unmapping of underlying memory) from an unexpected fault from</span>
<span class="p_add">+	 * which a retry would not recover.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (ret == -EFAULT &amp;&amp; expect_fault)</span>
<span class="p_add">+		return -EAGAIN;</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_compare_iter(void __user *a, void __user *b, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char bufa[TMP_BUFLEN], bufb[TMP_BUFLEN];</span>
<span class="p_add">+	uint32_t compared = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (compared != len) {</span>
<span class="p_add">+		unsigned long to_compare;</span>
<span class="p_add">+</span>
<span class="p_add">+		to_compare = min_t(uint32_t, TMP_BUFLEN, len - compared);</span>
<span class="p_add">+		if (__copy_from_user_inatomic(bufa, a + compared, to_compare))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (__copy_from_user_inatomic(bufb, b + compared, to_compare))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (memcmp(bufa, bufb, to_compare))</span>
<span class="p_add">+			return 1;	/* different */</span>
<span class="p_add">+		compared += to_compare;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;	/* same */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_compare(void __user *a, void __user *b, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp[2];</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u8, (uint8_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u8, (uint8_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u8 != tmp[1]._u8);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u16, (uint16_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u16, (uint16_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u16 != tmp[1]._u16);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u32, (uint32_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u32, (uint32_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u32 != tmp[1]._u32);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64, (uint64_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64, (uint64_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64_split[0], (uint32_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64_split[1], (uint32_t __user *)a + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64_split[0], (uint32_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64_split[1], (uint32_t __user *)b + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		ret = !!(tmp[0]._u64 != tmp[1]._u64);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		pagefault_enable();</span>
<span class="p_add">+		return do_cpu_op_compare_iter(a, b, len);</span>
<span class="p_add">+	}</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_memcpy_iter(void __user *dst, void __user *src,</span>
<span class="p_add">+		uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char buf[TMP_BUFLEN];</span>
<span class="p_add">+	uint32_t copied = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (copied != len) {</span>
<span class="p_add">+		unsigned long to_copy;</span>
<span class="p_add">+</span>
<span class="p_add">+		to_copy = min_t(uint32_t, TMP_BUFLEN, len - copied);</span>
<span class="p_add">+		if (__copy_from_user_inatomic(buf, src + copied, to_copy))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (__copy_to_user_inatomic(dst + copied, buf, to_copy))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		copied += to_copy;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_memcpy(void __user *dst, void __user *src, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)src + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)dst + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		pagefault_enable();</span>
<span class="p_add">+		return do_cpu_op_memcpy_iter(dst, src, len);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_add_fn(union op_fn_data *data, uint64_t count, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 += (uint8_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 += (uint16_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 += (uint32_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 += (uint64_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_or_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 |= (uint8_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 |= (uint16_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 |= (uint32_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 |= (uint64_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_and_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 &amp;= (uint8_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 &amp;= (uint16_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 &amp;= (uint32_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 &amp;= (uint64_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_xor_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 ^= (uint8_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 ^= (uint16_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 ^= (uint32_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 ^= (uint64_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_lshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 &lt;&lt;= (uint8_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 &lt;&lt;= (uint16_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 &lt;&lt;= (uint32_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 &lt;&lt;= (uint64_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_rshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 &gt;&gt;= (uint8_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 &gt;&gt;= (uint16_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 &gt;&gt;= (uint32_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 &gt;&gt;= (uint64_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_fn(op_fn_t op_fn, void __user *p, uint64_t v,</span>
<span class="p_add">+		uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union op_fn_data tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Guarantee a compiler barrier between each operation. */</span>
<span class="p_add">+		barrier();</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+			ret = do_cpu_op_compare(</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret &lt; 0)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Stop execution, return op index + 1 if comparison</span>
<span class="p_add">+			 * differs.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (ret &gt; 0)</span>
<span class="p_add">+				return i + 1;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+			ret = do_cpu_op_compare(</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret &lt; 0)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Stop execution, return op index + 1 if comparison</span>
<span class="p_add">+			 * is identical.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (ret == 0)</span>
<span class="p_add">+				return i + 1;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			ret = do_cpu_op_memcpy(</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_add_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;u.arithmetic_op.count, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_or_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_and_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_xor_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_lshift_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_rshift_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			smp_mb();</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt, int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu != raw_smp_processor_id()) {</span>
<span class="p_add">+		ret = push_task_to_cpu(current, cpu);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto check_online;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	if (cpu != smp_processor_id()) {</span>
<span class="p_add">+		ret = -EAGAIN;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="p_add">+end:</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+check_online:</span>
<span class="p_add">+	if (!cpu_possible(cpu))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	get_online_cpus();</span>
<span class="p_add">+	if (cpu_online(cpu)) {</span>
<span class="p_add">+		ret = -EAGAIN;</span>
<span class="p_add">+		goto put_online_cpus;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * CPU is offline. Perform operation from the current CPU with</span>
<span class="p_add">+	 * cpu_online read lock held, preventing that CPU from coming online,</span>
<span class="p_add">+	 * and with mutex held, providing mutual exclusion against other</span>
<span class="p_add">+	 * CPUs also finding out about an offline CPU.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mutex_lock(&amp;cpu_opv_offline_lock);</span>
<span class="p_add">+	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="p_add">+	mutex_unlock(&amp;cpu_opv_offline_lock);</span>
<span class="p_add">+put_online_cpus:</span>
<span class="p_add">+	put_online_cpus();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * cpu_opv - execute operation vector on a given CPU with preempt off.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Userspace should pass current CPU number as parameter. May fail with</span>
<span class="p_add">+ * -EAGAIN if currently executing on the wrong CPU.</span>
<span class="p_add">+ */</span>
<span class="p_add">+SYSCALL_DEFINE4(cpu_opv, struct cpu_op __user *, ucpuopv, int, cpuopcnt,</span>
<span class="p_add">+		int, cpu, int, flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpu_op cpuopv[CPU_OP_VEC_LEN_MAX];</span>
<span class="p_add">+	struct page *pinned_pages_on_stack[NR_PINNED_PAGES_ON_STACK];</span>
<span class="p_add">+	struct cpu_opv_pinned_pages pin_pages = {</span>
<span class="p_add">+		.pages = pinned_pages_on_stack,</span>
<span class="p_add">+		.nr = 0,</span>
<span class="p_add">+		.is_kmalloc = false,</span>
<span class="p_add">+	};</span>
<span class="p_add">+	int ret, i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(flags))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (unlikely(cpu &lt; 0))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (cpuopcnt &lt; 0 || cpuopcnt &gt; CPU_OP_VEC_LEN_MAX)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (copy_from_user(cpuopv, ucpuopv, cpuopcnt * sizeof(struct cpu_op)))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	ret = cpu_opv_check(cpuopv, cpuopcnt);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	ret = cpu_opv_pin_pages(cpuopv, cpuopcnt, &amp;pin_pages);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	ret = do_cpu_opv(cpuopv, cpuopcnt, cpu);</span>
<span class="p_add">+	for (i = 0; i &lt; pin_pages.nr; i++)</span>
<span class="p_add">+		put_page(pin_pages.pages[i]);</span>
<span class="p_add">+end:</span>
<span class="p_add">+	if (pin_pages.is_kmalloc)</span>
<span class="p_add">+		kfree(pin_pages.pages);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 6bba05f47e51..e547f93a46c2 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -1052,6 +1052,43 @@</span> <span class="p_context"> void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)</span>
 		set_curr_task(rq, p);
 }
 
<span class="p_add">+int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
<span class="p_add">+	update_rq_clock(rq);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpumask_test_cpu(dest_cpu, &amp;p-&gt;cpus_allowed)) {</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task_cpu(p) == dest_cpu)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task_running(rq, p) || p-&gt;state == TASK_WAKING) {</span>
<span class="p_add">+		struct migration_arg arg = { p, dest_cpu };</span>
<span class="p_add">+		/* Need help from migration thread: drop lock and wait. */</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
<span class="p_add">+		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &amp;arg);</span>
<span class="p_add">+		tlb_migrate_finish(p-&gt;mm);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	} else if (task_on_rq_queued(p)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * OK, since we&#39;re going to drop the lock immediately</span>
<span class="p_add">+		 * afterwards anyway.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		rq = move_queued_task(rq, &amp;rf, p, dest_cpu);</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Change a given task&#39;s CPU affinity. Migrate the thread to a
  * proper CPU and schedule it away if the CPU it&#39;s executing on
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index 3b448ba82225..cab256c1720a 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -1209,6 +1209,8 @@</span> <span class="p_context"> static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)</span>
 #endif
 }
 
<span class="p_add">+int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu);</span>
<span class="p_add">+</span>
 /*
  * Tunables that become constants when CONFIG_SCHED_DEBUG is off:
  */
<span class="p_header">diff --git a/kernel/sys_ni.c b/kernel/sys_ni.c</span>
<span class="p_header">index bfa1ee1bf669..59e622296dc3 100644</span>
<span class="p_header">--- a/kernel/sys_ni.c</span>
<span class="p_header">+++ b/kernel/sys_ni.c</span>
<span class="p_chunk">@@ -262,3 +262,4 @@</span> <span class="p_context"> cond_syscall(sys_pkey_free);</span>
 
 /* restartable sequence */
 cond_syscall(sys_rseq);
<span class="p_add">+cond_syscall(sys_cpu_opv);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



