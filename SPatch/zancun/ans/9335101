
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,1/3] powerpc: port 64 bits pgtable_cache to 32 bits - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,1/3] powerpc: port 64 bits pgtable_cache to 32 bits</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 16, 2016, 7:40 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;82db857d48d3e2017267e67404d4b6748b66729c.1474009019.git.christophe.leroy@c-s.fr&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9335101/mbox/"
   >mbox</a>
|
   <a href="/patch/9335101/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9335101/">/patch/9335101/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	35AD360839 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Sep 2016 07:41:07 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 27BB329E66
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Sep 2016 07:41:07 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 1C19C29EE3; Fri, 16 Sep 2016 07:41:07 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4278429E66
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Sep 2016 07:41:02 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1758202AbcIPHkx (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 16 Sep 2016 03:40:53 -0400
Received: from pegase1.c-s.fr ([93.17.236.30]:64145 &quot;EHLO pegase1.c-s.fr&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1755216AbcIPHkH (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 16 Sep 2016 03:40:07 -0400
Received: from localhost (unknown [192.168.12.234])
	by localhost (Postfix) with ESMTP id 3sb6d06ZYrz9ttFT;
	Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
X-Virus-Scanned: Debian amavisd-new at c-s.fr
Received: from pegase1.c-s.fr ([192.168.12.234])
	by localhost (pegase1.c-s.fr [192.168.12.234]) (amavisd-new,
	port 10024)
	with ESMTP id z2Xjy30tTAI7; Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
Received: from messagerie.si.c-s.fr (messagerie.si.c-s.fr [192.168.25.192])
	by pegase1.c-s.fr (Postfix) with ESMTP id 3sb6d05Dyxz9ttFQ;
	Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
Received: from localhost (localhost [127.0.0.1])
	by messagerie.si.c-s.fr (Postfix) with ESMTP id DB74F8B96F;
	Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
X-Virus-Scanned: amavisd-new at c-s.fr
Received: from messagerie.si.c-s.fr ([127.0.0.1])
	by localhost (messagerie.si.c-s.fr [127.0.0.1]) (amavisd-new,
	port 10023)
	with ESMTP id VOVkH2J5RRwI; Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
Received: from PO10863.localdomain (po10863.idsi0.si.c-s.fr [172.25.231.27])
	by messagerie.si.c-s.fr (Postfix) with ESMTP id AAD268B96D;
	Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
Received: by localhost.localdomain (Postfix, from userid 0)
	id 635E71A2455; Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
Message-Id: &lt;82db857d48d3e2017267e67404d4b6748b66729c.1474009019.git.christophe.leroy@c-s.fr&gt;
In-Reply-To: &lt;cover.1474009019.git.christophe.leroy@c-s.fr&gt;
References: &lt;cover.1474009019.git.christophe.leroy@c-s.fr&gt;
From: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;
Subject: [PATCH v2 1/3] powerpc: port 64 bits pgtable_cache to 32 bits
To: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;,
	Paul Mackerras &lt;paulus@samba.org&gt;, Michael Ellerman &lt;mpe@ellerman.id.au&gt;,
	Scott Wood &lt;oss@buserror.net&gt;
Cc: linux-kernel@vger.kernel.org, linuxppc-dev@lists.ozlabs.org
Date: Fri, 16 Sep 2016 09:40:04 +0200 (CEST)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a> - Sept. 16, 2016, 7:40 a.m.</div>
<pre class="content">
Today powerpc64 uses a set of pgtable_caches while powerpc32 uses
standard pages when using 4k pages and a single pgtable_cache
if using other size pages.

In preparation of implementing huge pages on the 8xx, this patch
replaces the specific powerpc32 handling by the 64 bits approach.

This is done by:
* moving 64 bits pgtable_cache_add() and pgtable_cache_init()
in a new file called init-common.c
* modifying pgtable_cache_init() to also handle the case
without PMD
* removing the 32 bits version of pgtable_cache_add() and
pgtable_cache_init()
* copying related header contents from 64 bits into both the
book3s/32 and nohash/32 header files

On the 8xx, the following cache sizes will be used:
* 4k pages mode:
- PGT_CACHE(10) for PGD
- PGT_CACHE(3) for 512k hugepage tables
* 16k pages mode:
- PGT_CACHE(6) for PGD
- PGT_CACHE(7) for 512k hugepage tables
- PGT_CACHE(3) for 8M hugepage tables
<span class="signed-off-by">
Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
---
v2: in v1, hugepte_cache was wrongly replaced by PGT_CACHE(1).
This modification has been removed from v2.

 arch/powerpc/include/asm/book3s/32/pgalloc.h |  44 ++++++--
 arch/powerpc/include/asm/book3s/32/pgtable.h |  43 ++++----
 arch/powerpc/include/asm/book3s/64/pgtable.h |   3 -
 arch/powerpc/include/asm/nohash/32/pgalloc.h |  44 ++++++--
 arch/powerpc/include/asm/nohash/32/pgtable.h |  45 ++++----
 arch/powerpc/include/asm/nohash/64/pgtable.h |   2 -
 arch/powerpc/include/asm/pgtable.h           |   2 +
 arch/powerpc/mm/Makefile                     |   3 +-
 arch/powerpc/mm/init-common.c                | 147 +++++++++++++++++++++++++++
 arch/powerpc/mm/init_64.c                    |  77 --------------
 arch/powerpc/mm/pgtable_32.c                 |  37 -------
 11 files changed, 273 insertions(+), 174 deletions(-)
 create mode 100644 arch/powerpc/mm/init-common.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - Sept. 19, 2016, 5:22 a.m.</div>
<pre class="content">
Christophe Leroy &lt;christophe.leroy@c-s.fr&gt; writes:
<span class="quote">
&gt; Today powerpc64 uses a set of pgtable_caches while powerpc32 uses</span>
<span class="quote">&gt; standard pages when using 4k pages and a single pgtable_cache</span>
<span class="quote">&gt; if using other size pages.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In preparation of implementing huge pages on the 8xx, this patch</span>
<span class="quote">&gt; replaces the specific powerpc32 handling by the 64 bits approach.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is done by:</span>
<span class="quote">&gt; * moving 64 bits pgtable_cache_add() and pgtable_cache_init()</span>
<span class="quote">&gt; in a new file called init-common.c</span>
<span class="quote">&gt; * modifying pgtable_cache_init() to also handle the case</span>
<span class="quote">&gt; without PMD</span>
<span class="quote">&gt; * removing the 32 bits version of pgtable_cache_add() and</span>
<span class="quote">&gt; pgtable_cache_init()</span>
<span class="quote">&gt; * copying related header contents from 64 bits into both the</span>
<span class="quote">&gt; book3s/32 and nohash/32 header files</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On the 8xx, the following cache sizes will be used:</span>
<span class="quote">&gt; * 4k pages mode:</span>
<span class="quote">&gt; - PGT_CACHE(10) for PGD</span>
<span class="quote">&gt; - PGT_CACHE(3) for 512k hugepage tables</span>
<span class="quote">&gt; * 16k pages mode:</span>
<span class="quote">&gt; - PGT_CACHE(6) for PGD</span>
<span class="quote">&gt; - PGT_CACHE(7) for 512k hugepage tables</span>
<span class="quote">&gt; - PGT_CACHE(3) for 8M hugepage tables</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; v2: in v1, hugepte_cache was wrongly replaced by PGT_CACHE(1).</span>
<span class="quote">&gt; This modification has been removed from v2.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  arch/powerpc/include/asm/book3s/32/pgalloc.h |  44 ++++++--</span>
<span class="quote">&gt;  arch/powerpc/include/asm/book3s/32/pgtable.h |  43 ++++----</span>
<span class="quote">&gt;  arch/powerpc/include/asm/book3s/64/pgtable.h |   3 -</span>
<span class="quote">&gt;  arch/powerpc/include/asm/nohash/32/pgalloc.h |  44 ++++++--</span>
<span class="quote">&gt;  arch/powerpc/include/asm/nohash/32/pgtable.h |  45 ++++----</span>
<span class="quote">&gt;  arch/powerpc/include/asm/nohash/64/pgtable.h |   2 -</span>
<span class="quote">&gt;  arch/powerpc/include/asm/pgtable.h           |   2 +</span>
<span class="quote">&gt;  arch/powerpc/mm/Makefile                     |   3 +-</span>
<span class="quote">&gt;  arch/powerpc/mm/init-common.c                | 147 +++++++++++++++++++++++++++</span>
<span class="quote">&gt;  arch/powerpc/mm/init_64.c                    |  77 --------------</span>
<span class="quote">&gt;  arch/powerpc/mm/pgtable_32.c                 |  37 -------</span>
<span class="quote">&gt;  11 files changed, 273 insertions(+), 174 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 arch/powerpc/mm/init-common.c</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/book3s/32/pgalloc.h b/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="quote">&gt; index 8e21bb4..d310546 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="quote">&gt; @@ -2,14 +2,42 @@</span>
<span class="quote">&gt;  #define _ASM_POWERPC_BOOK3S_32_PGALLOC_H</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;linux/threads.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* For 32-bit, all levels of page tables are just drawn from get_free_page() */</span>
<span class="quote">&gt; -#define MAX_PGTABLE_INDEX_SIZE	0</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Functions that deal with pagetables that could be at any level of</span>
<span class="quote">&gt; + * the table need to be passed an &quot;index_size&quot; so they know how to</span>
<span class="quote">&gt; + * handle allocation.  For PTE pages (which are linked to a struct</span>
<span class="quote">&gt; + * page for now, and drawn from the main get_free_pages() pool), the</span>
<span class="quote">&gt; + * allocation size will be (2^index_size * sizeof(pointer)) and</span>
<span class="quote">&gt; + * allocations are drawn from the kmem_cache in PGT_CACHE(index_size).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The maximum index size needs to be big enough to allow any</span>
<span class="quote">&gt; + * pagetable sizes we need, but small enough to fit in the low bits of</span>
<span class="quote">&gt; + * any page table pointer.  In other words all pagetables, even tiny</span>
<span class="quote">&gt; + * ones, must be aligned to allow at least enough low 0 bits to</span>
<span class="quote">&gt; + * contain this value.  This value is also used as a mask, so it must</span>
<span class="quote">&gt; + * be one less than a power of two.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define MAX_PGTABLE_INDEX_SIZE	0xf</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern void __bad_pte(pmd_t *pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -extern pgd_t *pgd_alloc(struct mm_struct *mm);</span>
<span class="quote">&gt; -extern void pgd_free(struct mm_struct *mm, pgd_t *pgd);</span>
<span class="quote">&gt; +extern struct kmem_cache *pgtable_cache[];</span>
<span class="quote">&gt; +#define PGT_CACHE(shift) ({				\</span>
<span class="quote">&gt; +			BUG_ON(!(shift));		\</span>
<span class="quote">&gt; +			pgtable_cache[(shift) - 1];	\</span>
<span class="quote">&gt; +		})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return kmem_cache_alloc(PGT_CACHE(PGD_INDEX_SIZE), GFP_KERNEL);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	kmem_cache_free(PGT_CACHE(PGD_INDEX_SIZE), pgd);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * We don&#39;t have any real pmd&#39;s, and this code never triggers because</span>
<span class="quote">&gt; @@ -68,8 +96,12 @@ static inline void pte_free(struct mm_struct *mm, pgtable_t ptepage)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void pgtable_free(void *table, unsigned index_size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	BUG_ON(index_size); /* 32-bit doesn&#39;t use this */</span>
<span class="quote">&gt; -	free_page((unsigned long)table);</span>
<span class="quote">&gt; +	if (!index_size) {</span>
<span class="quote">&gt; +		free_page((unsigned long)table);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		BUG_ON(index_size &gt; MAX_PGTABLE_INDEX_SIZE);</span>
<span class="quote">&gt; +		kmem_cache_free(PGT_CACHE(index_size), table);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define check_pgt_cache()	do { } while (0)</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/book3s/32/pgtable.h b/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="quote">&gt; index 6b8b2d5..f887499 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="quote">&gt; @@ -8,6 +8,26 @@</span>
<span class="quote">&gt;  /* And here we include common definitions */</span>
<span class="quote">&gt;  #include &lt;asm/pte-common.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define PTE_INDEX_SIZE	PTE_SHIFT</span>
<span class="quote">&gt; +#define PMD_INDEX_SIZE	0</span>
<span class="quote">&gt; +#define PUD_INDEX_SIZE	0</span>
<span class="quote">&gt; +#define PGD_INDEX_SIZE	(32 - PGDIR_SHIFT)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define PMD_CACHE_INDEX	PMD_INDEX_SIZE</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef __ASSEMBLY__</span>
<span class="quote">&gt; +#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PMD_TABLE_SIZE	(sizeof(pmd_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PUD_TABLE_SIZE	(sizeof(pud_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt; +#endif	/* __ASSEMBLY__ */</span>

Are these table size correct ? IIUC, We will have only PGD and PTE
tables right ?
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +#define PTRS_PER_PTE	(1 &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PTRS_PER_PGD	(1 &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* With 4k base page size, hugepage PTEs go at the PMD level */</span>
<span class="quote">&gt; +#define MIN_HUGEPTE_SHIFT	PMD_SHIFT</span>
<span class="quote">&gt; +</span>

What does that comment mean ? I guess that came from copy-paste from
other headers. I am not sure what it means there either other than the
64k hash config, where we place hugepage PTE at the PMD level. (ie, no hugepd).
<span class="quote">

&gt;  /*</span>
<span class="quote">&gt;   * The normal case is that PTEs are 32-bits and we have a 1-page</span>
<span class="quote">&gt;   * 1024-entry pgdir pointing to 1-page 1024-entry PTE pages.  -- paulus</span>
<span class="quote">&gt; @@ -19,14 +39,10 @@</span>
<span class="quote">&gt;   * -Matt</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  /* PGDIR_SHIFT determines what a top-level page table entry can map */</span>
<span class="quote">&gt; -#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_SHIFT)</span>
<span class="quote">&gt; +#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_INDEX_SIZE)</span>
<span class="quote">&gt;  #define PGDIR_SIZE	(1UL &lt;&lt; PGDIR_SHIFT)</span>
<span class="quote">&gt;  #define PGDIR_MASK	(~(PGDIR_SIZE-1))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define PTRS_PER_PTE	(1 &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt; -#define PTRS_PER_PMD	1</span>
<span class="quote">&gt; -#define PTRS_PER_PGD	(1 &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  #define USER_PTRS_PER_PGD	(TASK_SIZE / PGDIR_SIZE)</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * This is the bottom of the PKMAP area with HIGHMEM or an arbitrary</span>
<span class="quote">&gt; @@ -82,12 +98,8 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern unsigned long ioremap_bot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * entries per page directory level: our page-table tree is two-level, so</span>
<span class="quote">&gt; - * we don&#39;t really have any PMD directory.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt; -#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt; +/* Bits to mask out from a PGD to get to the PUD page */</span>
<span class="quote">&gt; +#define PGD_MASKED_BITS		0</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define pte_ERROR(e) \</span>
<span class="quote">&gt;  	pr_err(&quot;%s:%d: bad pte %llx.\n&quot;, __FILE__, __LINE__, \</span>
<span class="quote">&gt; @@ -283,15 +295,6 @@ static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
<span class="quote">&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val(pte) &gt;&gt; 3 })</span>
<span class="quote">&gt;  #define __swp_entry_to_pte(x)		((pte_t) { (x).val &lt;&lt; 3 })</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * No page table caches to initialise</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -#define pgtable_cache_init()	do { } while (0)</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  extern int get_pteptr(struct mm_struct *mm, unsigned long addr, pte_t **ptep,</span>
<span class="quote">&gt;  		      pmd_t **pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/book3s/64/pgtable.h b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="quote">&gt; index 9fd77f8..0a46a5f 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="quote">&gt; @@ -789,9 +789,6 @@ extern struct page *pgd_page(pgd_t pgd);</span>
<span class="quote">&gt;  #define pgd_ERROR(e) \</span>
<span class="quote">&gt;  	pr_err(&quot;%s:%d: bad pgd %08lx.\n&quot;, __FILE__, __LINE__, pgd_val(e))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="quote">&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static inline int map_kernel_page(unsigned long ea, unsigned long pa,</span>
<span class="quote">&gt;  				  unsigned long flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/nohash/32/pgalloc.h b/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="quote">&gt; index 76d6b9e..6331392 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="quote">&gt; @@ -2,14 +2,42 @@</span>
<span class="quote">&gt;  #define _ASM_POWERPC_PGALLOC_32_H</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;linux/threads.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* For 32-bit, all levels of page tables are just drawn from get_free_page() */</span>
<span class="quote">&gt; -#define MAX_PGTABLE_INDEX_SIZE	0</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Functions that deal with pagetables that could be at any level of</span>
<span class="quote">&gt; + * the table need to be passed an &quot;index_size&quot; so they know how to</span>
<span class="quote">&gt; + * handle allocation.  For PTE pages (which are linked to a struct</span>
<span class="quote">&gt; + * page for now, and drawn from the main get_free_pages() pool), the</span>
<span class="quote">&gt; + * allocation size will be (2^index_size * sizeof(pointer)) and</span>
<span class="quote">&gt; + * allocations are drawn from the kmem_cache in PGT_CACHE(index_size).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The maximum index size needs to be big enough to allow any</span>
<span class="quote">&gt; + * pagetable sizes we need, but small enough to fit in the low bits of</span>
<span class="quote">&gt; + * any page table pointer.  In other words all pagetables, even tiny</span>
<span class="quote">&gt; + * ones, must be aligned to allow at least enough low 0 bits to</span>
<span class="quote">&gt; + * contain this value.  This value is also used as a mask, so it must</span>
<span class="quote">&gt; + * be one less than a power of two.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define MAX_PGTABLE_INDEX_SIZE	0xf</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern void __bad_pte(pmd_t *pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -extern pgd_t *pgd_alloc(struct mm_struct *mm);</span>
<span class="quote">&gt; -extern void pgd_free(struct mm_struct *mm, pgd_t *pgd);</span>
<span class="quote">&gt; +extern struct kmem_cache *pgtable_cache[];</span>
<span class="quote">&gt; +#define PGT_CACHE(shift) ({				\</span>
<span class="quote">&gt; +			BUG_ON(!(shift));		\</span>
<span class="quote">&gt; +			pgtable_cache[(shift) - 1];	\</span>
<span class="quote">&gt; +		})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return kmem_cache_alloc(PGT_CACHE(PGD_INDEX_SIZE), GFP_KERNEL);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	kmem_cache_free(PGT_CACHE(PGD_INDEX_SIZE), pgd);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * We don&#39;t have any real pmd&#39;s, and this code never triggers because</span>
<span class="quote">&gt; @@ -68,8 +96,12 @@ static inline void pte_free(struct mm_struct *mm, pgtable_t ptepage)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void pgtable_free(void *table, unsigned index_size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	BUG_ON(index_size); /* 32-bit doesn&#39;t use this */</span>
<span class="quote">&gt; -	free_page((unsigned long)table);</span>
<span class="quote">&gt; +	if (!index_size) {</span>
<span class="quote">&gt; +		free_page((unsigned long)table);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		BUG_ON(index_size &gt; MAX_PGTABLE_INDEX_SIZE);</span>
<span class="quote">&gt; +		kmem_cache_free(PGT_CACHE(index_size), table);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define check_pgt_cache()	do { } while (0)</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/nohash/32/pgtable.h b/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="quote">&gt; index c219ef7..8cbe222 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="quote">&gt; @@ -16,6 +16,26 @@ extern int icache_44x_need_flush;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif /* __ASSEMBLY__ */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define PTE_INDEX_SIZE	PTE_SHIFT</span>
<span class="quote">&gt; +#define PMD_INDEX_SIZE	0</span>
<span class="quote">&gt; +#define PUD_INDEX_SIZE	0</span>
<span class="quote">&gt; +#define PGD_INDEX_SIZE	(32 - PGDIR_SHIFT)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define PMD_CACHE_INDEX	PMD_INDEX_SIZE</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef __ASSEMBLY__</span>
<span class="quote">&gt; +#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PMD_TABLE_SIZE	(sizeof(pmd_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PUD_TABLE_SIZE	(sizeof(pud_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt; +#endif	/* __ASSEMBLY__ */</span>
<span class="quote">&gt; +</span>

Same, please comment on why those TABLE sizes ?
<span class="quote">

&gt; +#define PTRS_PER_PTE	(1 &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt; +#define PTRS_PER_PGD	(1 &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* With 4k base page size, hugepage PTEs go at the PMD level */</span>
<span class="quote">&gt; +#define MIN_HUGEPTE_SHIFT	PMD_SHIFT</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * The normal case is that PTEs are 32-bits and we have a 1-page</span>
<span class="quote">&gt;   * 1024-entry pgdir pointing to 1-page 1024-entry PTE pages.  -- paulus</span>
<span class="quote">&gt; @@ -27,22 +47,12 @@ extern int icache_44x_need_flush;</span>
<span class="quote">&gt;   * -Matt</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  /* PGDIR_SHIFT determines what a top-level page table entry can map */</span>
<span class="quote">&gt; -#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_SHIFT)</span>
<span class="quote">&gt; +#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_INDEX_SIZE)</span>
<span class="quote">&gt;  #define PGDIR_SIZE	(1UL &lt;&lt; PGDIR_SHIFT)</span>
<span class="quote">&gt;  #define PGDIR_MASK	(~(PGDIR_SIZE-1))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * entries per page directory level: our page-table tree is two-level, so</span>
<span class="quote">&gt; - * we don&#39;t really have any PMD directory.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -#ifndef __ASSEMBLY__</span>
<span class="quote">&gt; -#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt; -#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt; -#endif	/* __ASSEMBLY__ */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#define PTRS_PER_PTE	(1 &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt; -#define PTRS_PER_PMD	1</span>
<span class="quote">&gt; -#define PTRS_PER_PGD	(1 &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt; +/* Bits to mask out from a PGD to get to the PUD page */</span>
<span class="quote">&gt; +#define PGD_MASKED_BITS		0</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define USER_PTRS_PER_PGD	(TASK_SIZE / PGDIR_SIZE)</span>
<span class="quote">&gt;  #define FIRST_USER_ADDRESS	0UL</span>
<span class="quote">&gt; @@ -328,15 +338,6 @@ static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
<span class="quote">&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val(pte) &gt;&gt; 3 })</span>
<span class="quote">&gt;  #define __swp_entry_to_pte(x)		((pte_t) { (x).val &lt;&lt; 3 })</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * No page table caches to initialise</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -#define pgtable_cache_init()	do { } while (0)</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  extern int get_pteptr(struct mm_struct *mm, unsigned long addr, pte_t **ptep,</span>
<span class="quote">&gt;  		      pmd_t **pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/nohash/64/pgtable.h b/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="quote">&gt; index 653a183..619018a 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="quote">&gt; @@ -358,8 +358,6 @@ static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
<span class="quote">&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt;  #define __swp_entry_to_pte(x)		__pte((x).val)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="quote">&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt;  extern int map_kernel_page(unsigned long ea, unsigned long pa,</span>
<span class="quote">&gt;  			   unsigned long flags);</span>
<span class="quote">&gt;  extern int __meminit vmemmap_create_mapping(unsigned long start,</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/pgtable.h b/arch/powerpc/include/asm/pgtable.h</span>
<span class="quote">&gt; index 9bd87f2..dd01212 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/pgtable.h</span>
<span class="quote">&gt; @@ -78,6 +78,8 @@ static inline pte_t *find_linux_pte_or_hugepte(pgd_t *pgdir, unsigned long ea,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  unsigned long vmalloc_to_phys(void *vmalloc_addr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="quote">&gt; +void pgtable_cache_init(void);</span>
<span class="quote">&gt;  #endif /* __ASSEMBLY__ */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif /* _ASM_POWERPC_PGTABLE_H */</span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/Makefile b/arch/powerpc/mm/Makefile</span>
<span class="quote">&gt; index 1a4e570..e8a86d2 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/mm/Makefile</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/Makefile</span>
<span class="quote">&gt; @@ -7,7 +7,8 @@ subdir-ccflags-$(CONFIG_PPC_WERROR) := -Werror</span>
<span class="quote">&gt;  ccflags-$(CONFIG_PPC64)	:= $(NO_MINIMAL_TOC)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  obj-y				:= fault.o mem.o pgtable.o mmap.o \</span>
<span class="quote">&gt; -				   init_$(BITS).o pgtable_$(BITS).o</span>
<span class="quote">&gt; +				   init_$(BITS).o pgtable_$(BITS).o \</span>
<span class="quote">&gt; +				   init-common.o</span>
<span class="quote">&gt;  obj-$(CONFIG_PPC_MMU_NOHASH)	+= mmu_context_nohash.o tlb_nohash.o \</span>
<span class="quote">&gt;  				   tlb_nohash_low.o</span>
<span class="quote">&gt;  obj-$(CONFIG_PPC_BOOK3E)	+= tlb_low_$(BITS)e.o</span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/init-common.c b/arch/powerpc/mm/init-common.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..ab2b947</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/init-common.c</span>
<span class="quote">&gt; @@ -0,0 +1,147 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + *  PowerPC version</span>
<span class="quote">&gt; + *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)</span>
<span class="quote">&gt; + *  and Cort Dougan (PReP) (cort@cs.nmt.edu)</span>
<span class="quote">&gt; + *    Copyright (C) 1996 Paul Mackerras</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *  Derived from &quot;arch/i386/mm/init.c&quot;</span>
<span class="quote">&gt; + *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *  Dave Engebretsen &lt;engebret@us.ibm.com&gt;</span>
<span class="quote">&gt; + *      Rework for PPC64 port.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *  This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt; + *  modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt; + *  as published by the Free Software Foundation; either version</span>
<span class="quote">&gt; + *  2 of the License, or (at your option) any later version.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#undef DEBUG</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/signal.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/errno.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/string.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/types.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/mman.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/stddef.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/vmalloc.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/init.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/delay.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/idr.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/nodemask.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/poison.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/memblock.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hugetlb.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/page.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/prom.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/rtas.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/io.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mmu_context.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mmu.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/uaccess.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/smp.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/machdep.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/tlb.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/eeh.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/processor.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mmzone.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/cputable.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/sections.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/iommu.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/vdso.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &quot;mmu_decl.h&quot;</span>


Do you need all these headers to get it compiled ?
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +static void pgd_ctor(void *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	memset(addr, 0, PGD_TABLE_SIZE);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void pud_ctor(void *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	memset(addr, 0, PUD_TABLE_SIZE);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void pmd_ctor(void *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	memset(addr, 0, PMD_TABLE_SIZE);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct kmem_cache *pgtable_cache[MAX_PGTABLE_INDEX_SIZE];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Create a kmem_cache() for pagetables.  This is not used for PTE</span>
<span class="quote">&gt; + * pages - they&#39;re linked to struct page, come from the normal free</span>
<span class="quote">&gt; + * pages pool and have a different entry size (see real_pte_t) to</span>
<span class="quote">&gt; + * everything else.  Caches created by this function are used for all</span>
<span class="quote">&gt; + * the higher level pagetables, and for hugepage pagetables.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +void pgtable_cache_add(unsigned shift, void (*ctor)(void *))</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	char *name;</span>
<span class="quote">&gt; +	unsigned long table_size = sizeof(void *) &lt;&lt; shift;</span>
<span class="quote">&gt; +	unsigned long align = table_size;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* When batching pgtable pointers for RCU freeing, we store</span>
<span class="quote">&gt; +	 * the index size in the low bits.  Table alignment must be</span>
<span class="quote">&gt; +	 * big enough to fit it.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Likewise, hugeapge pagetable pointers contain a (different)</span>
<span class="quote">&gt; +	 * shift value in the low bits.  All tables must be aligned so</span>
<span class="quote">&gt; +	 * as to leave enough 0 bits in the address to contain it. */</span>
<span class="quote">&gt; +	unsigned long minalign = max(MAX_PGTABLE_INDEX_SIZE + 1,</span>
<span class="quote">&gt; +				     HUGEPD_SHIFT_MASK + 1);</span>
<span class="quote">&gt; +	struct kmem_cache *new;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* It would be nice if this was a BUILD_BUG_ON(), but at the</span>
<span class="quote">&gt; +	 * moment, gcc doesn&#39;t seem to recognize is_power_of_2 as a</span>
<span class="quote">&gt; +	 * constant expression, so so much for that. */</span>
<span class="quote">&gt; +	BUG_ON(!is_power_of_2(minalign));</span>
<span class="quote">&gt; +	BUG_ON((shift &lt; 1) || (shift &gt; MAX_PGTABLE_INDEX_SIZE));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (PGT_CACHE(shift))</span>
<span class="quote">&gt; +		return; /* Already have a cache of this size */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	align = max_t(unsigned long, align, minalign);</span>
<span class="quote">&gt; +	name = kasprintf(GFP_KERNEL, &quot;pgtable-2^%d&quot;, shift);</span>
<span class="quote">&gt; +	new = kmem_cache_create(name, table_size, align, 0, ctor);</span>
<span class="quote">&gt; +	kfree(name);</span>
<span class="quote">&gt; +	pgtable_cache[shift - 1] = new;</span>
<span class="quote">&gt; +	pr_debug(&quot;Allocated pgtable cache for order %d\n&quot;, shift);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void pgtable_cache_init(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pgtable_cache_add(PGD_INDEX_SIZE, pgd_ctor);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (PMD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PMD_INDEX_SIZE))</span>
<span class="quote">&gt; +		pgtable_cache_add(PMD_CACHE_INDEX, pmd_ctor);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * In all current configs, when the PUD index exists it&#39;s the</span>
<span class="quote">&gt; +	 * same size as either the pgd or pmd index except with THP enabled</span>
<span class="quote">&gt; +	 * on book3s 64</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt; +		pgtable_cache_add(PUD_INDEX_SIZE, pud_ctor);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!PGT_CACHE(PGD_INDEX_SIZE))</span>
<span class="quote">&gt; +		panic(&quot;Couldn&#39;t allocate pgd cache&quot;);</span>
<span class="quote">&gt; +	if (PMD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PMD_INDEX_SIZE))</span>
<span class="quote">&gt; +		panic(&quot;Couldn&#39;t allocate pmd pgtable caches&quot;);</span>
<span class="quote">&gt; +	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt; +		panic(&quot;Couldn&#39;t allocate pud pgtable caches&quot;);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/init_64.c b/arch/powerpc/mm/init_64.c</span>
<span class="quote">&gt; index 16ada1e..a000c35 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/mm/init_64.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/init_64.c</span>
<span class="quote">&gt; @@ -80,83 +80,6 @@ EXPORT_SYMBOL_GPL(memstart_addr);</span>
<span class="quote">&gt;  phys_addr_t kernstart_addr;</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(kernstart_addr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void pgd_ctor(void *addr)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	memset(addr, 0, PGD_TABLE_SIZE);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static void pud_ctor(void *addr)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	memset(addr, 0, PUD_TABLE_SIZE);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static void pmd_ctor(void *addr)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	memset(addr, 0, PMD_TABLE_SIZE);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -struct kmem_cache *pgtable_cache[MAX_PGTABLE_INDEX_SIZE];</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * Create a kmem_cache() for pagetables.  This is not used for PTE</span>
<span class="quote">&gt; - * pages - they&#39;re linked to struct page, come from the normal free</span>
<span class="quote">&gt; - * pages pool and have a different entry size (see real_pte_t) to</span>
<span class="quote">&gt; - * everything else.  Caches created by this function are used for all</span>
<span class="quote">&gt; - * the higher level pagetables, and for hugepage pagetables.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -void pgtable_cache_add(unsigned shift, void (*ctor)(void *))</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	char *name;</span>
<span class="quote">&gt; -	unsigned long table_size = sizeof(void *) &lt;&lt; shift;</span>
<span class="quote">&gt; -	unsigned long align = table_size;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* When batching pgtable pointers for RCU freeing, we store</span>
<span class="quote">&gt; -	 * the index size in the low bits.  Table alignment must be</span>
<span class="quote">&gt; -	 * big enough to fit it.</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * Likewise, hugeapge pagetable pointers contain a (different)</span>
<span class="quote">&gt; -	 * shift value in the low bits.  All tables must be aligned so</span>
<span class="quote">&gt; -	 * as to leave enough 0 bits in the address to contain it. */</span>
<span class="quote">&gt; -	unsigned long minalign = max(MAX_PGTABLE_INDEX_SIZE + 1,</span>
<span class="quote">&gt; -				     HUGEPD_SHIFT_MASK + 1);</span>
<span class="quote">&gt; -	struct kmem_cache *new;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* It would be nice if this was a BUILD_BUG_ON(), but at the</span>
<span class="quote">&gt; -	 * moment, gcc doesn&#39;t seem to recognize is_power_of_2 as a</span>
<span class="quote">&gt; -	 * constant expression, so so much for that. */</span>
<span class="quote">&gt; -	BUG_ON(!is_power_of_2(minalign));</span>
<span class="quote">&gt; -	BUG_ON((shift &lt; 1) || (shift &gt; MAX_PGTABLE_INDEX_SIZE));</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (PGT_CACHE(shift))</span>
<span class="quote">&gt; -		return; /* Already have a cache of this size */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	align = max_t(unsigned long, align, minalign);</span>
<span class="quote">&gt; -	name = kasprintf(GFP_KERNEL, &quot;pgtable-2^%d&quot;, shift);</span>
<span class="quote">&gt; -	new = kmem_cache_create(name, table_size, align, 0, ctor);</span>
<span class="quote">&gt; -	kfree(name);</span>
<span class="quote">&gt; -	pgtable_cache[shift - 1] = new;</span>
<span class="quote">&gt; -	pr_debug(&quot;Allocated pgtable cache for order %d\n&quot;, shift);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -void pgtable_cache_init(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	pgtable_cache_add(PGD_INDEX_SIZE, pgd_ctor);</span>
<span class="quote">&gt; -	pgtable_cache_add(PMD_CACHE_INDEX, pmd_ctor);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * In all current configs, when the PUD index exists it&#39;s the</span>
<span class="quote">&gt; -	 * same size as either the pgd or pmd index except with THP enabled</span>
<span class="quote">&gt; -	 * on book3s 64</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt; -		pgtable_cache_add(PUD_INDEX_SIZE, pud_ctor);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (!PGT_CACHE(PGD_INDEX_SIZE) || !PGT_CACHE(PMD_CACHE_INDEX))</span>
<span class="quote">&gt; -		panic(&quot;Couldn&#39;t allocate pgtable caches&quot;);</span>
<span class="quote">&gt; -	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt; -		panic(&quot;Couldn&#39;t allocate pud pgtable caches&quot;);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  #ifdef CONFIG_SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Given an address within the vmemmap, determine the pfn of the page that</span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/pgtable_32.c b/arch/powerpc/mm/pgtable_32.c</span>
<span class="quote">&gt; index 0ae0572..a65c0b4 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/mm/pgtable_32.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/pgtable_32.c</span>
<span class="quote">&gt; @@ -42,43 +42,6 @@ EXPORT_SYMBOL(ioremap_bot);	/* aka VMALLOC_END */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern char etext[], _stext[], _sinittext[], _einittext[];</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define PGDIR_ORDER	(32 + PGD_T_LOG2 - PGDIR_SHIFT)</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt; -static struct kmem_cache *pgtable_cache;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -void pgtable_cache_init(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	pgtable_cache = kmem_cache_create(&quot;PGDIR cache&quot;, 1 &lt;&lt; PGDIR_ORDER,</span>
<span class="quote">&gt; -					  1 &lt;&lt; PGDIR_ORDER, 0, NULL);</span>
<span class="quote">&gt; -	if (pgtable_cache == NULL)</span>
<span class="quote">&gt; -		panic(&quot;Couldn&#39;t allocate pgtable caches&quot;);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	pgd_t *ret;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* pgdir take page or two with 4K pages and a page fraction otherwise */</span>
<span class="quote">&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt; -	ret = kmem_cache_alloc(pgtable_cache, GFP_KERNEL | __GFP_ZERO);</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	ret = (pgd_t *)__get_free_pages(GFP_KERNEL|__GFP_ZERO,</span>
<span class="quote">&gt; -			PGDIR_ORDER - PAGE_SHIFT);</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -	return ret;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt; -	kmem_cache_free(pgtable_cache, (void *)pgd);</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	free_pages((unsigned long)pgd, PGDIR_ORDER - PAGE_SHIFT);</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  __ref pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pte_t *pte;</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.1.0</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a> - Sept. 19, 2016, 6:46 p.m.</div>
<pre class="content">
Le 19/09/2016  07:22, Aneesh Kumar K.V a crit :
<span class="quote">&gt; Christophe Leroy &lt;christophe.leroy@c-s.fr&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Today powerpc64 uses a set of pgtable_caches while powerpc32 uses</span>
<span class="quote">&gt;&gt; standard pages when using 4k pages and a single pgtable_cache</span>
<span class="quote">&gt;&gt; if using other size pages.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In preparation of implementing huge pages on the 8xx, this patch</span>
<span class="quote">&gt;&gt; replaces the specific powerpc32 handling by the 64 bits approach.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is done by:</span>
<span class="quote">&gt;&gt; * moving 64 bits pgtable_cache_add() and pgtable_cache_init()</span>
<span class="quote">&gt;&gt; in a new file called init-common.c</span>
<span class="quote">&gt;&gt; * modifying pgtable_cache_init() to also handle the case</span>
<span class="quote">&gt;&gt; without PMD</span>
<span class="quote">&gt;&gt; * removing the 32 bits version of pgtable_cache_add() and</span>
<span class="quote">&gt;&gt; pgtable_cache_init()</span>
<span class="quote">&gt;&gt; * copying related header contents from 64 bits into both the</span>
<span class="quote">&gt;&gt; book3s/32 and nohash/32 header files</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On the 8xx, the following cache sizes will be used:</span>
<span class="quote">&gt;&gt; * 4k pages mode:</span>
<span class="quote">&gt;&gt; - PGT_CACHE(10) for PGD</span>
<span class="quote">&gt;&gt; - PGT_CACHE(3) for 512k hugepage tables</span>
<span class="quote">&gt;&gt; * 16k pages mode:</span>
<span class="quote">&gt;&gt; - PGT_CACHE(6) for PGD</span>
<span class="quote">&gt;&gt; - PGT_CACHE(7) for 512k hugepage tables</span>
<span class="quote">&gt;&gt; - PGT_CACHE(3) for 8M hugepage tables</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt; v2: in v1, hugepte_cache was wrongly replaced by PGT_CACHE(1).</span>
<span class="quote">&gt;&gt; This modification has been removed from v2.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/book3s/32/pgalloc.h |  44 ++++++--</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/book3s/32/pgtable.h |  43 ++++----</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/book3s/64/pgtable.h |   3 -</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/nohash/32/pgalloc.h |  44 ++++++--</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/nohash/32/pgtable.h |  45 ++++----</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/nohash/64/pgtable.h |   2 -</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/pgtable.h           |   2 +</span>
<span class="quote">&gt;&gt;  arch/powerpc/mm/Makefile                     |   3 +-</span>
<span class="quote">&gt;&gt;  arch/powerpc/mm/init-common.c                | 147 +++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;  arch/powerpc/mm/init_64.c                    |  77 --------------</span>
<span class="quote">&gt;&gt;  arch/powerpc/mm/pgtable_32.c                 |  37 -------</span>
<span class="quote">&gt;&gt;  11 files changed, 273 insertions(+), 174 deletions(-)</span>
<span class="quote">&gt;&gt;  create mode 100644 arch/powerpc/mm/init-common.c</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/book3s/32/pgalloc.h b/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="quote">&gt;&gt; index 8e21bb4..d310546 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="quote">&gt;&gt; @@ -2,14 +2,42 @@</span>
<span class="quote">&gt;&gt;  #define _ASM_POWERPC_BOOK3S_32_PGALLOC_H</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/threads.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -/* For 32-bit, all levels of page tables are just drawn from get_free_page() */</span>
<span class="quote">&gt;&gt; -#define MAX_PGTABLE_INDEX_SIZE	0</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Functions that deal with pagetables that could be at any level of</span>
<span class="quote">&gt;&gt; + * the table need to be passed an &quot;index_size&quot; so they know how to</span>
<span class="quote">&gt;&gt; + * handle allocation.  For PTE pages (which are linked to a struct</span>
<span class="quote">&gt;&gt; + * page for now, and drawn from the main get_free_pages() pool), the</span>
<span class="quote">&gt;&gt; + * allocation size will be (2^index_size * sizeof(pointer)) and</span>
<span class="quote">&gt;&gt; + * allocations are drawn from the kmem_cache in PGT_CACHE(index_size).</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * The maximum index size needs to be big enough to allow any</span>
<span class="quote">&gt;&gt; + * pagetable sizes we need, but small enough to fit in the low bits of</span>
<span class="quote">&gt;&gt; + * any page table pointer.  In other words all pagetables, even tiny</span>
<span class="quote">&gt;&gt; + * ones, must be aligned to allow at least enough low 0 bits to</span>
<span class="quote">&gt;&gt; + * contain this value.  This value is also used as a mask, so it must</span>
<span class="quote">&gt;&gt; + * be one less than a power of two.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define MAX_PGTABLE_INDEX_SIZE	0xf</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  extern void __bad_pte(pmd_t *pmd);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -extern pgd_t *pgd_alloc(struct mm_struct *mm);</span>
<span class="quote">&gt;&gt; -extern void pgd_free(struct mm_struct *mm, pgd_t *pgd);</span>
<span class="quote">&gt;&gt; +extern struct kmem_cache *pgtable_cache[];</span>
<span class="quote">&gt;&gt; +#define PGT_CACHE(shift) ({				\</span>
<span class="quote">&gt;&gt; +			BUG_ON(!(shift));		\</span>
<span class="quote">&gt;&gt; +			pgtable_cache[(shift) - 1];	\</span>
<span class="quote">&gt;&gt; +		})</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return kmem_cache_alloc(PGT_CACHE(PGD_INDEX_SIZE), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	kmem_cache_free(PGT_CACHE(PGD_INDEX_SIZE), pgd);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * We don&#39;t have any real pmd&#39;s, and this code never triggers because</span>
<span class="quote">&gt;&gt; @@ -68,8 +96,12 @@ static inline void pte_free(struct mm_struct *mm, pgtable_t ptepage)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  static inline void pgtable_free(void *table, unsigned index_size)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	BUG_ON(index_size); /* 32-bit doesn&#39;t use this */</span>
<span class="quote">&gt;&gt; -	free_page((unsigned long)table);</span>
<span class="quote">&gt;&gt; +	if (!index_size) {</span>
<span class="quote">&gt;&gt; +		free_page((unsigned long)table);</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		BUG_ON(index_size &gt; MAX_PGTABLE_INDEX_SIZE);</span>
<span class="quote">&gt;&gt; +		kmem_cache_free(PGT_CACHE(index_size), table);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define check_pgt_cache()	do { } while (0)</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/book3s/32/pgtable.h b/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="quote">&gt;&gt; index 6b8b2d5..f887499 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -8,6 +8,26 @@</span>
<span class="quote">&gt;&gt;  /* And here we include common definitions */</span>
<span class="quote">&gt;&gt;  #include &lt;asm/pte-common.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +#define PTE_INDEX_SIZE	PTE_SHIFT</span>
<span class="quote">&gt;&gt; +#define PMD_INDEX_SIZE	0</span>
<span class="quote">&gt;&gt; +#define PUD_INDEX_SIZE	0</span>
<span class="quote">&gt;&gt; +#define PGD_INDEX_SIZE	(32 - PGDIR_SHIFT)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define PMD_CACHE_INDEX	PMD_INDEX_SIZE</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef __ASSEMBLY__</span>
<span class="quote">&gt;&gt; +#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PMD_TABLE_SIZE	(sizeof(pmd_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PUD_TABLE_SIZE	(sizeof(pud_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#endif	/* __ASSEMBLY__ */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Are these table size correct ? IIUC, We will have only PGD and PTE</span>
<span class="quote">&gt; tables right ?</span>

Oops, copy/paste error.
We won&#39;t have those tables, they won&#39;t be created as PMD_INDEX_SIZE and 
PUD_INDEX_SIZE are 0. But they need to be defined to avoid compilation 
errors of pmd_ctor() and pud_ctor()
Then what should I do ? Define them as 0, or just keep the standard 
definition which follows ?
#define PMD_TABLE_SIZE	(sizeof(pmd_t) &lt;&lt; PMD_INDEX_SIZE)
#define PUD_TABLE_SIZE	(sizeof(pud_t) &lt;&lt; PUD_INDEX_SIZE)
<span class="quote">
&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define PTRS_PER_PTE	(1 &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PTRS_PER_PGD	(1 &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* With 4k base page size, hugepage PTEs go at the PMD level */</span>
<span class="quote">&gt;&gt; +#define MIN_HUGEPTE_SHIFT	PMD_SHIFT</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What does that comment mean ? I guess that came from copy-paste from</span>
<span class="quote">&gt; other headers. I am not sure what it means there either other than the</span>
<span class="quote">&gt; 64k hash config, where we place hugepage PTE at the PMD level. (ie, no hugepd).</span>

Well, I&#39;ll remove the comment from here.
<span class="quote">
&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * The normal case is that PTEs are 32-bits and we have a 1-page</span>
<span class="quote">&gt;&gt;   * 1024-entry pgdir pointing to 1-page 1024-entry PTE pages.  -- paulus</span>
<span class="quote">&gt;&gt; @@ -19,14 +39,10 @@</span>
<span class="quote">&gt;&gt;   * -Matt</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt;  /* PGDIR_SHIFT determines what a top-level page table entry can map */</span>
<span class="quote">&gt;&gt; -#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_SHIFT)</span>
<span class="quote">&gt;&gt; +#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt;  #define PGDIR_SIZE	(1UL &lt;&lt; PGDIR_SHIFT)</span>
<span class="quote">&gt;&gt;  #define PGDIR_MASK	(~(PGDIR_SIZE-1))</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -#define PTRS_PER_PTE	(1 &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt;&gt; -#define PTRS_PER_PMD	1</span>
<span class="quote">&gt;&gt; -#define PTRS_PER_PGD	(1 &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  #define USER_PTRS_PER_PGD	(TASK_SIZE / PGDIR_SIZE)</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * This is the bottom of the PKMAP area with HIGHMEM or an arbitrary</span>
<span class="quote">&gt;&gt; @@ -82,12 +98,8 @@</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  extern unsigned long ioremap_bot;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * entries per page directory level: our page-table tree is two-level, so</span>
<span class="quote">&gt;&gt; - * we don&#39;t really have any PMD directory.</span>
<span class="quote">&gt;&gt; - */</span>
<span class="quote">&gt;&gt; -#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt;&gt; -#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt;&gt; +/* Bits to mask out from a PGD to get to the PUD page */</span>
<span class="quote">&gt;&gt; +#define PGD_MASKED_BITS		0</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define pte_ERROR(e) \</span>
<span class="quote">&gt;&gt;  	pr_err(&quot;%s:%d: bad pte %llx.\n&quot;, __FILE__, __LINE__, \</span>
<span class="quote">&gt;&gt; @@ -283,15 +295,6 @@ static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val(pte) &gt;&gt; 3 })</span>
<span class="quote">&gt;&gt;  #define __swp_entry_to_pte(x)		((pte_t) { (x).val &lt;&lt; 3 })</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt;&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt;&gt; -#else</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * No page table caches to initialise</span>
<span class="quote">&gt;&gt; - */</span>
<span class="quote">&gt;&gt; -#define pgtable_cache_init()	do { } while (0)</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  extern int get_pteptr(struct mm_struct *mm, unsigned long addr, pte_t **ptep,</span>
<span class="quote">&gt;&gt;  		      pmd_t **pmdp);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/book3s/64/pgtable.h b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="quote">&gt;&gt; index 9fd77f8..0a46a5f 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -789,9 +789,6 @@ extern struct page *pgd_page(pgd_t pgd);</span>
<span class="quote">&gt;&gt;  #define pgd_ERROR(e) \</span>
<span class="quote">&gt;&gt;  	pr_err(&quot;%s:%d: bad pgd %08lx.\n&quot;, __FILE__, __LINE__, pgd_val(e))</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="quote">&gt;&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  static inline int map_kernel_page(unsigned long ea, unsigned long pa,</span>
<span class="quote">&gt;&gt;  				  unsigned long flags)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/nohash/32/pgalloc.h b/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="quote">&gt;&gt; index 76d6b9e..6331392 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="quote">&gt;&gt; @@ -2,14 +2,42 @@</span>
<span class="quote">&gt;&gt;  #define _ASM_POWERPC_PGALLOC_32_H</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/threads.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -/* For 32-bit, all levels of page tables are just drawn from get_free_page() */</span>
<span class="quote">&gt;&gt; -#define MAX_PGTABLE_INDEX_SIZE	0</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Functions that deal with pagetables that could be at any level of</span>
<span class="quote">&gt;&gt; + * the table need to be passed an &quot;index_size&quot; so they know how to</span>
<span class="quote">&gt;&gt; + * handle allocation.  For PTE pages (which are linked to a struct</span>
<span class="quote">&gt;&gt; + * page for now, and drawn from the main get_free_pages() pool), the</span>
<span class="quote">&gt;&gt; + * allocation size will be (2^index_size * sizeof(pointer)) and</span>
<span class="quote">&gt;&gt; + * allocations are drawn from the kmem_cache in PGT_CACHE(index_size).</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * The maximum index size needs to be big enough to allow any</span>
<span class="quote">&gt;&gt; + * pagetable sizes we need, but small enough to fit in the low bits of</span>
<span class="quote">&gt;&gt; + * any page table pointer.  In other words all pagetables, even tiny</span>
<span class="quote">&gt;&gt; + * ones, must be aligned to allow at least enough low 0 bits to</span>
<span class="quote">&gt;&gt; + * contain this value.  This value is also used as a mask, so it must</span>
<span class="quote">&gt;&gt; + * be one less than a power of two.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define MAX_PGTABLE_INDEX_SIZE	0xf</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  extern void __bad_pte(pmd_t *pmd);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -extern pgd_t *pgd_alloc(struct mm_struct *mm);</span>
<span class="quote">&gt;&gt; -extern void pgd_free(struct mm_struct *mm, pgd_t *pgd);</span>
<span class="quote">&gt;&gt; +extern struct kmem_cache *pgtable_cache[];</span>
<span class="quote">&gt;&gt; +#define PGT_CACHE(shift) ({				\</span>
<span class="quote">&gt;&gt; +			BUG_ON(!(shift));		\</span>
<span class="quote">&gt;&gt; +			pgtable_cache[(shift) - 1];	\</span>
<span class="quote">&gt;&gt; +		})</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return kmem_cache_alloc(PGT_CACHE(PGD_INDEX_SIZE), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	kmem_cache_free(PGT_CACHE(PGD_INDEX_SIZE), pgd);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * We don&#39;t have any real pmd&#39;s, and this code never triggers because</span>
<span class="quote">&gt;&gt; @@ -68,8 +96,12 @@ static inline void pte_free(struct mm_struct *mm, pgtable_t ptepage)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  static inline void pgtable_free(void *table, unsigned index_size)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	BUG_ON(index_size); /* 32-bit doesn&#39;t use this */</span>
<span class="quote">&gt;&gt; -	free_page((unsigned long)table);</span>
<span class="quote">&gt;&gt; +	if (!index_size) {</span>
<span class="quote">&gt;&gt; +		free_page((unsigned long)table);</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		BUG_ON(index_size &gt; MAX_PGTABLE_INDEX_SIZE);</span>
<span class="quote">&gt;&gt; +		kmem_cache_free(PGT_CACHE(index_size), table);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define check_pgt_cache()	do { } while (0)</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/nohash/32/pgtable.h b/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="quote">&gt;&gt; index c219ef7..8cbe222 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -16,6 +16,26 @@ extern int icache_44x_need_flush;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #endif /* __ASSEMBLY__ */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +#define PTE_INDEX_SIZE	PTE_SHIFT</span>
<span class="quote">&gt;&gt; +#define PMD_INDEX_SIZE	0</span>
<span class="quote">&gt;&gt; +#define PUD_INDEX_SIZE	0</span>
<span class="quote">&gt;&gt; +#define PGD_INDEX_SIZE	(32 - PGDIR_SHIFT)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define PMD_CACHE_INDEX	PMD_INDEX_SIZE</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef __ASSEMBLY__</span>
<span class="quote">&gt;&gt; +#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PMD_TABLE_SIZE	(sizeof(pmd_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PUD_TABLE_SIZE	(sizeof(pud_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#endif	/* __ASSEMBLY__ */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Same, please comment on why those TABLE sizes ?</span>
<span class="quote">&gt;</span>

Yes same error, will fix it the same way.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +#define PTRS_PER_PTE	(1 &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +#define PTRS_PER_PGD	(1 &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* With 4k base page size, hugepage PTEs go at the PMD level */</span>
<span class="quote">&gt;&gt; +#define MIN_HUGEPTE_SHIFT	PMD_SHIFT</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * The normal case is that PTEs are 32-bits and we have a 1-page</span>
<span class="quote">&gt;&gt;   * 1024-entry pgdir pointing to 1-page 1024-entry PTE pages.  -- paulus</span>
<span class="quote">&gt;&gt; @@ -27,22 +47,12 @@ extern int icache_44x_need_flush;</span>
<span class="quote">&gt;&gt;   * -Matt</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt;  /* PGDIR_SHIFT determines what a top-level page table entry can map */</span>
<span class="quote">&gt;&gt; -#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_SHIFT)</span>
<span class="quote">&gt;&gt; +#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_INDEX_SIZE)</span>
<span class="quote">&gt;&gt;  #define PGDIR_SIZE	(1UL &lt;&lt; PGDIR_SHIFT)</span>
<span class="quote">&gt;&gt;  #define PGDIR_MASK	(~(PGDIR_SIZE-1))</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * entries per page directory level: our page-table tree is two-level, so</span>
<span class="quote">&gt;&gt; - * we don&#39;t really have any PMD directory.</span>
<span class="quote">&gt;&gt; - */</span>
<span class="quote">&gt;&gt; -#ifndef __ASSEMBLY__</span>
<span class="quote">&gt;&gt; -#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt;&gt; -#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt;&gt; -#endif	/* __ASSEMBLY__ */</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -#define PTRS_PER_PTE	(1 &lt;&lt; PTE_SHIFT)</span>
<span class="quote">&gt;&gt; -#define PTRS_PER_PMD	1</span>
<span class="quote">&gt;&gt; -#define PTRS_PER_PGD	(1 &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="quote">&gt;&gt; +/* Bits to mask out from a PGD to get to the PUD page */</span>
<span class="quote">&gt;&gt; +#define PGD_MASKED_BITS		0</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define USER_PTRS_PER_PGD	(TASK_SIZE / PGDIR_SIZE)</span>
<span class="quote">&gt;&gt;  #define FIRST_USER_ADDRESS	0UL</span>
<span class="quote">&gt;&gt; @@ -328,15 +338,6 @@ static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val(pte) &gt;&gt; 3 })</span>
<span class="quote">&gt;&gt;  #define __swp_entry_to_pte(x)		((pte_t) { (x).val &lt;&lt; 3 })</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt;&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt;&gt; -#else</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * No page table caches to initialise</span>
<span class="quote">&gt;&gt; - */</span>
<span class="quote">&gt;&gt; -#define pgtable_cache_init()	do { } while (0)</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  extern int get_pteptr(struct mm_struct *mm, unsigned long addr, pte_t **ptep,</span>
<span class="quote">&gt;&gt;  		      pmd_t **pmdp);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/nohash/64/pgtable.h b/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="quote">&gt;&gt; index 653a183..619018a 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -358,8 +358,6 @@ static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt;&gt;  #define __swp_entry_to_pte(x)		__pte((x).val)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="quote">&gt;&gt; -void pgtable_cache_init(void);</span>
<span class="quote">&gt;&gt;  extern int map_kernel_page(unsigned long ea, unsigned long pa,</span>
<span class="quote">&gt;&gt;  			   unsigned long flags);</span>
<span class="quote">&gt;&gt;  extern int __meminit vmemmap_create_mapping(unsigned long start,</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/pgtable.h b/arch/powerpc/include/asm/pgtable.h</span>
<span class="quote">&gt;&gt; index 9bd87f2..dd01212 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -78,6 +78,8 @@ static inline pte_t *find_linux_pte_or_hugepte(pgd_t *pgdir, unsigned long ea,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  unsigned long vmalloc_to_phys(void *vmalloc_addr);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="quote">&gt;&gt; +void pgtable_cache_init(void);</span>
<span class="quote">&gt;&gt;  #endif /* __ASSEMBLY__ */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #endif /* _ASM_POWERPC_PGTABLE_H */</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/mm/Makefile b/arch/powerpc/mm/Makefile</span>
<span class="quote">&gt;&gt; index 1a4e570..e8a86d2 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/mm/Makefile</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/mm/Makefile</span>
<span class="quote">&gt;&gt; @@ -7,7 +7,8 @@ subdir-ccflags-$(CONFIG_PPC_WERROR) := -Werror</span>
<span class="quote">&gt;&gt;  ccflags-$(CONFIG_PPC64)	:= $(NO_MINIMAL_TOC)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  obj-y				:= fault.o mem.o pgtable.o mmap.o \</span>
<span class="quote">&gt;&gt; -				   init_$(BITS).o pgtable_$(BITS).o</span>
<span class="quote">&gt;&gt; +				   init_$(BITS).o pgtable_$(BITS).o \</span>
<span class="quote">&gt;&gt; +				   init-common.o</span>
<span class="quote">&gt;&gt;  obj-$(CONFIG_PPC_MMU_NOHASH)	+= mmu_context_nohash.o tlb_nohash.o \</span>
<span class="quote">&gt;&gt;  				   tlb_nohash_low.o</span>
<span class="quote">&gt;&gt;  obj-$(CONFIG_PPC_BOOK3E)	+= tlb_low_$(BITS)e.o</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/mm/init-common.c b/arch/powerpc/mm/init-common.c</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 0000000..ab2b947</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/mm/init-common.c</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,147 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + *  PowerPC version</span>
<span class="quote">&gt;&gt; + *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)</span>
<span class="quote">&gt;&gt; + *  and Cort Dougan (PReP) (cort@cs.nmt.edu)</span>
<span class="quote">&gt;&gt; + *    Copyright (C) 1996 Paul Mackerras</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *  Derived from &quot;arch/i386/mm/init.c&quot;</span>
<span class="quote">&gt;&gt; + *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *  Dave Engebretsen &lt;engebret@us.ibm.com&gt;</span>
<span class="quote">&gt;&gt; + *      Rework for PPC64 port.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *  This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt;&gt; + *  modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt;&gt; + *  as published by the Free Software Foundation; either version</span>
<span class="quote">&gt;&gt; + *  2 of the License, or (at your option) any later version.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#undef DEBUG</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;linux/signal.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/errno.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/string.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/types.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/mman.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/stddef.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/vmalloc.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/init.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/delay.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/idr.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/nodemask.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/poison.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/memblock.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/hugetlb.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/page.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/prom.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/rtas.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/io.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/mmu_context.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/mmu.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/uaccess.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/smp.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/machdep.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/tlb.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/eeh.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/processor.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/mmzone.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/cputable.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/sections.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/iommu.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/vdso.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &quot;mmu_decl.h&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Do you need all these headers to get it compiled ?</span>
<span class="quote">&gt;</span>

Probably not, I just copied the ones from init_64.c
Will clean it up.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void pgd_ctor(void *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	memset(addr, 0, PGD_TABLE_SIZE);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void pud_ctor(void *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	memset(addr, 0, PUD_TABLE_SIZE);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void pmd_ctor(void *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	memset(addr, 0, PMD_TABLE_SIZE);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +struct kmem_cache *pgtable_cache[MAX_PGTABLE_INDEX_SIZE];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Create a kmem_cache() for pagetables.  This is not used for PTE</span>
<span class="quote">&gt;&gt; + * pages - they&#39;re linked to struct page, come from the normal free</span>
<span class="quote">&gt;&gt; + * pages pool and have a different entry size (see real_pte_t) to</span>
<span class="quote">&gt;&gt; + * everything else.  Caches created by this function are used for all</span>
<span class="quote">&gt;&gt; + * the higher level pagetables, and for hugepage pagetables.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +void pgtable_cache_add(unsigned shift, void (*ctor)(void *))</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	char *name;</span>
<span class="quote">&gt;&gt; +	unsigned long table_size = sizeof(void *) &lt;&lt; shift;</span>
<span class="quote">&gt;&gt; +	unsigned long align = table_size;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* When batching pgtable pointers for RCU freeing, we store</span>
<span class="quote">&gt;&gt; +	 * the index size in the low bits.  Table alignment must be</span>
<span class="quote">&gt;&gt; +	 * big enough to fit it.</span>
<span class="quote">&gt;&gt; +	 *</span>
<span class="quote">&gt;&gt; +	 * Likewise, hugeapge pagetable pointers contain a (different)</span>
<span class="quote">&gt;&gt; +	 * shift value in the low bits.  All tables must be aligned so</span>
<span class="quote">&gt;&gt; +	 * as to leave enough 0 bits in the address to contain it. */</span>
<span class="quote">&gt;&gt; +	unsigned long minalign = max(MAX_PGTABLE_INDEX_SIZE + 1,</span>
<span class="quote">&gt;&gt; +				     HUGEPD_SHIFT_MASK + 1);</span>
<span class="quote">&gt;&gt; +	struct kmem_cache *new;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* It would be nice if this was a BUILD_BUG_ON(), but at the</span>
<span class="quote">&gt;&gt; +	 * moment, gcc doesn&#39;t seem to recognize is_power_of_2 as a</span>
<span class="quote">&gt;&gt; +	 * constant expression, so so much for that. */</span>
<span class="quote">&gt;&gt; +	BUG_ON(!is_power_of_2(minalign));</span>
<span class="quote">&gt;&gt; +	BUG_ON((shift &lt; 1) || (shift &gt; MAX_PGTABLE_INDEX_SIZE));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (PGT_CACHE(shift))</span>
<span class="quote">&gt;&gt; +		return; /* Already have a cache of this size */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	align = max_t(unsigned long, align, minalign);</span>
<span class="quote">&gt;&gt; +	name = kasprintf(GFP_KERNEL, &quot;pgtable-2^%d&quot;, shift);</span>
<span class="quote">&gt;&gt; +	new = kmem_cache_create(name, table_size, align, 0, ctor);</span>
<span class="quote">&gt;&gt; +	kfree(name);</span>
<span class="quote">&gt;&gt; +	pgtable_cache[shift - 1] = new;</span>
<span class="quote">&gt;&gt; +	pr_debug(&quot;Allocated pgtable cache for order %d\n&quot;, shift);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void pgtable_cache_init(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	pgtable_cache_add(PGD_INDEX_SIZE, pgd_ctor);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (PMD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PMD_INDEX_SIZE))</span>
<span class="quote">&gt;&gt; +		pgtable_cache_add(PMD_CACHE_INDEX, pmd_ctor);</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * In all current configs, when the PUD index exists it&#39;s the</span>
<span class="quote">&gt;&gt; +	 * same size as either the pgd or pmd index except with THP enabled</span>
<span class="quote">&gt;&gt; +	 * on book3s 64</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt;&gt; +		pgtable_cache_add(PUD_INDEX_SIZE, pud_ctor);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (!PGT_CACHE(PGD_INDEX_SIZE))</span>
<span class="quote">&gt;&gt; +		panic(&quot;Couldn&#39;t allocate pgd cache&quot;);</span>
<span class="quote">&gt;&gt; +	if (PMD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PMD_INDEX_SIZE))</span>
<span class="quote">&gt;&gt; +		panic(&quot;Couldn&#39;t allocate pmd pgtable caches&quot;);</span>
<span class="quote">&gt;&gt; +	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt;&gt; +		panic(&quot;Couldn&#39;t allocate pud pgtable caches&quot;);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/mm/init_64.c b/arch/powerpc/mm/init_64.c</span>
<span class="quote">&gt;&gt; index 16ada1e..a000c35 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/mm/init_64.c</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/mm/init_64.c</span>
<span class="quote">&gt;&gt; @@ -80,83 +80,6 @@ EXPORT_SYMBOL_GPL(memstart_addr);</span>
<span class="quote">&gt;&gt;  phys_addr_t kernstart_addr;</span>
<span class="quote">&gt;&gt;  EXPORT_SYMBOL_GPL(kernstart_addr);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -static void pgd_ctor(void *addr)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	memset(addr, 0, PGD_TABLE_SIZE);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -static void pud_ctor(void *addr)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	memset(addr, 0, PUD_TABLE_SIZE);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -static void pmd_ctor(void *addr)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	memset(addr, 0, PMD_TABLE_SIZE);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -struct kmem_cache *pgtable_cache[MAX_PGTABLE_INDEX_SIZE];</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * Create a kmem_cache() for pagetables.  This is not used for PTE</span>
<span class="quote">&gt;&gt; - * pages - they&#39;re linked to struct page, come from the normal free</span>
<span class="quote">&gt;&gt; - * pages pool and have a different entry size (see real_pte_t) to</span>
<span class="quote">&gt;&gt; - * everything else.  Caches created by this function are used for all</span>
<span class="quote">&gt;&gt; - * the higher level pagetables, and for hugepage pagetables.</span>
<span class="quote">&gt;&gt; - */</span>
<span class="quote">&gt;&gt; -void pgtable_cache_add(unsigned shift, void (*ctor)(void *))</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	char *name;</span>
<span class="quote">&gt;&gt; -	unsigned long table_size = sizeof(void *) &lt;&lt; shift;</span>
<span class="quote">&gt;&gt; -	unsigned long align = table_size;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	/* When batching pgtable pointers for RCU freeing, we store</span>
<span class="quote">&gt;&gt; -	 * the index size in the low bits.  Table alignment must be</span>
<span class="quote">&gt;&gt; -	 * big enough to fit it.</span>
<span class="quote">&gt;&gt; -	 *</span>
<span class="quote">&gt;&gt; -	 * Likewise, hugeapge pagetable pointers contain a (different)</span>
<span class="quote">&gt;&gt; -	 * shift value in the low bits.  All tables must be aligned so</span>
<span class="quote">&gt;&gt; -	 * as to leave enough 0 bits in the address to contain it. */</span>
<span class="quote">&gt;&gt; -	unsigned long minalign = max(MAX_PGTABLE_INDEX_SIZE + 1,</span>
<span class="quote">&gt;&gt; -				     HUGEPD_SHIFT_MASK + 1);</span>
<span class="quote">&gt;&gt; -	struct kmem_cache *new;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	/* It would be nice if this was a BUILD_BUG_ON(), but at the</span>
<span class="quote">&gt;&gt; -	 * moment, gcc doesn&#39;t seem to recognize is_power_of_2 as a</span>
<span class="quote">&gt;&gt; -	 * constant expression, so so much for that. */</span>
<span class="quote">&gt;&gt; -	BUG_ON(!is_power_of_2(minalign));</span>
<span class="quote">&gt;&gt; -	BUG_ON((shift &lt; 1) || (shift &gt; MAX_PGTABLE_INDEX_SIZE));</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	if (PGT_CACHE(shift))</span>
<span class="quote">&gt;&gt; -		return; /* Already have a cache of this size */</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	align = max_t(unsigned long, align, minalign);</span>
<span class="quote">&gt;&gt; -	name = kasprintf(GFP_KERNEL, &quot;pgtable-2^%d&quot;, shift);</span>
<span class="quote">&gt;&gt; -	new = kmem_cache_create(name, table_size, align, 0, ctor);</span>
<span class="quote">&gt;&gt; -	kfree(name);</span>
<span class="quote">&gt;&gt; -	pgtable_cache[shift - 1] = new;</span>
<span class="quote">&gt;&gt; -	pr_debug(&quot;Allocated pgtable cache for order %d\n&quot;, shift);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -void pgtable_cache_init(void)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	pgtable_cache_add(PGD_INDEX_SIZE, pgd_ctor);</span>
<span class="quote">&gt;&gt; -	pgtable_cache_add(PMD_CACHE_INDEX, pmd_ctor);</span>
<span class="quote">&gt;&gt; -	/*</span>
<span class="quote">&gt;&gt; -	 * In all current configs, when the PUD index exists it&#39;s the</span>
<span class="quote">&gt;&gt; -	 * same size as either the pgd or pmd index except with THP enabled</span>
<span class="quote">&gt;&gt; -	 * on book3s 64</span>
<span class="quote">&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt; -	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt;&gt; -		pgtable_cache_add(PUD_INDEX_SIZE, pud_ctor);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	if (!PGT_CACHE(PGD_INDEX_SIZE) || !PGT_CACHE(PMD_CACHE_INDEX))</span>
<span class="quote">&gt;&gt; -		panic(&quot;Couldn&#39;t allocate pgtable caches&quot;);</span>
<span class="quote">&gt;&gt; -	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="quote">&gt;&gt; -		panic(&quot;Couldn&#39;t allocate pud pgtable caches&quot;);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  #ifdef CONFIG_SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * Given an address within the vmemmap, determine the pfn of the page that</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/mm/pgtable_32.c b/arch/powerpc/mm/pgtable_32.c</span>
<span class="quote">&gt;&gt; index 0ae0572..a65c0b4 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/mm/pgtable_32.c</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/mm/pgtable_32.c</span>
<span class="quote">&gt;&gt; @@ -42,43 +42,6 @@ EXPORT_SYMBOL(ioremap_bot);	/* aka VMALLOC_END */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  extern char etext[], _stext[], _sinittext[], _einittext[];</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -#define PGDIR_ORDER	(32 + PGD_T_LOG2 - PGDIR_SHIFT)</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt;&gt; -static struct kmem_cache *pgtable_cache;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -void pgtable_cache_init(void)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	pgtable_cache = kmem_cache_create(&quot;PGDIR cache&quot;, 1 &lt;&lt; PGDIR_ORDER,</span>
<span class="quote">&gt;&gt; -					  1 &lt;&lt; PGDIR_ORDER, 0, NULL);</span>
<span class="quote">&gt;&gt; -	if (pgtable_cache == NULL)</span>
<span class="quote">&gt;&gt; -		panic(&quot;Couldn&#39;t allocate pgtable caches&quot;);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	pgd_t *ret;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	/* pgdir take page or two with 4K pages and a page fraction otherwise */</span>
<span class="quote">&gt;&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt;&gt; -	ret = kmem_cache_alloc(pgtable_cache, GFP_KERNEL | __GFP_ZERO);</span>
<span class="quote">&gt;&gt; -#else</span>
<span class="quote">&gt;&gt; -	ret = (pgd_t *)__get_free_pages(GFP_KERNEL|__GFP_ZERO,</span>
<span class="quote">&gt;&gt; -			PGDIR_ORDER - PAGE_SHIFT);</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; -	return ret;</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="quote">&gt;&gt; -	kmem_cache_free(pgtable_cache, (void *)pgd);</span>
<span class="quote">&gt;&gt; -#else</span>
<span class="quote">&gt;&gt; -	free_pages((unsigned long)pgd, PGDIR_ORDER - PAGE_SHIFT);</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  __ref pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	pte_t *pte;</span>
<span class="quote">&gt;&gt; --</span>
<span class="quote">&gt;&gt; 2.1.0</span>

---
L&#39;absence de virus dans ce courrier lectronique a t vrifie par le logiciel antivirus Avast.
https://www.avast.com/antivirus
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/32/pgalloc.h b/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="p_header">index 8e21bb4..d310546 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/32/pgalloc.h</span>
<span class="p_chunk">@@ -2,14 +2,42 @@</span> <span class="p_context"></span>
 #define _ASM_POWERPC_BOOK3S_32_PGALLOC_H
 
 #include &lt;linux/threads.h&gt;
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
 
<span class="p_del">-/* For 32-bit, all levels of page tables are just drawn from get_free_page() */</span>
<span class="p_del">-#define MAX_PGTABLE_INDEX_SIZE	0</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Functions that deal with pagetables that could be at any level of</span>
<span class="p_add">+ * the table need to be passed an &quot;index_size&quot; so they know how to</span>
<span class="p_add">+ * handle allocation.  For PTE pages (which are linked to a struct</span>
<span class="p_add">+ * page for now, and drawn from the main get_free_pages() pool), the</span>
<span class="p_add">+ * allocation size will be (2^index_size * sizeof(pointer)) and</span>
<span class="p_add">+ * allocations are drawn from the kmem_cache in PGT_CACHE(index_size).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The maximum index size needs to be big enough to allow any</span>
<span class="p_add">+ * pagetable sizes we need, but small enough to fit in the low bits of</span>
<span class="p_add">+ * any page table pointer.  In other words all pagetables, even tiny</span>
<span class="p_add">+ * ones, must be aligned to allow at least enough low 0 bits to</span>
<span class="p_add">+ * contain this value.  This value is also used as a mask, so it must</span>
<span class="p_add">+ * be one less than a power of two.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define MAX_PGTABLE_INDEX_SIZE	0xf</span>
 
 extern void __bad_pte(pmd_t *pmd);
 
<span class="p_del">-extern pgd_t *pgd_alloc(struct mm_struct *mm);</span>
<span class="p_del">-extern void pgd_free(struct mm_struct *mm, pgd_t *pgd);</span>
<span class="p_add">+extern struct kmem_cache *pgtable_cache[];</span>
<span class="p_add">+#define PGT_CACHE(shift) ({				\</span>
<span class="p_add">+			BUG_ON(!(shift));		\</span>
<span class="p_add">+			pgtable_cache[(shift) - 1];	\</span>
<span class="p_add">+		})</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kmem_cache_alloc(PGT_CACHE(PGD_INDEX_SIZE), GFP_KERNEL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	kmem_cache_free(PGT_CACHE(PGD_INDEX_SIZE), pgd);</span>
<span class="p_add">+}</span>
 
 /*
  * We don&#39;t have any real pmd&#39;s, and this code never triggers because
<span class="p_chunk">@@ -68,8 +96,12 @@</span> <span class="p_context"> static inline void pte_free(struct mm_struct *mm, pgtable_t ptepage)</span>
 
 static inline void pgtable_free(void *table, unsigned index_size)
 {
<span class="p_del">-	BUG_ON(index_size); /* 32-bit doesn&#39;t use this */</span>
<span class="p_del">-	free_page((unsigned long)table);</span>
<span class="p_add">+	if (!index_size) {</span>
<span class="p_add">+		free_page((unsigned long)table);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		BUG_ON(index_size &gt; MAX_PGTABLE_INDEX_SIZE);</span>
<span class="p_add">+		kmem_cache_free(PGT_CACHE(index_size), table);</span>
<span class="p_add">+	}</span>
 }
 
 #define check_pgt_cache()	do { } while (0)
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/32/pgtable.h b/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="p_header">index 6b8b2d5..f887499 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/32/pgtable.h</span>
<span class="p_chunk">@@ -8,6 +8,26 @@</span> <span class="p_context"></span>
 /* And here we include common definitions */
 #include &lt;asm/pte-common.h&gt;
 
<span class="p_add">+#define PTE_INDEX_SIZE	PTE_SHIFT</span>
<span class="p_add">+#define PMD_INDEX_SIZE	0</span>
<span class="p_add">+#define PUD_INDEX_SIZE	0</span>
<span class="p_add">+#define PGD_INDEX_SIZE	(32 - PGDIR_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_CACHE_INDEX	PMD_INDEX_SIZE</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PMD_TABLE_SIZE	(sizeof(pmd_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PUD_TABLE_SIZE	(sizeof(pud_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="p_add">+#endif	/* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTRS_PER_PTE	(1 &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PTRS_PER_PGD	(1 &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/* With 4k base page size, hugepage PTEs go at the PMD level */</span>
<span class="p_add">+#define MIN_HUGEPTE_SHIFT	PMD_SHIFT</span>
<span class="p_add">+</span>
 /*
  * The normal case is that PTEs are 32-bits and we have a 1-page
  * 1024-entry pgdir pointing to 1-page 1024-entry PTE pages.  -- paulus
<span class="p_chunk">@@ -19,14 +39,10 @@</span> <span class="p_context"></span>
  * -Matt
  */
 /* PGDIR_SHIFT determines what a top-level page table entry can map */
<span class="p_del">-#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_SHIFT)</span>
<span class="p_add">+#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_INDEX_SIZE)</span>
 #define PGDIR_SIZE	(1UL &lt;&lt; PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE-1))
 
<span class="p_del">-#define PTRS_PER_PTE	(1 &lt;&lt; PTE_SHIFT)</span>
<span class="p_del">-#define PTRS_PER_PMD	1</span>
<span class="p_del">-#define PTRS_PER_PGD	(1 &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="p_del">-</span>
 #define USER_PTRS_PER_PGD	(TASK_SIZE / PGDIR_SIZE)
 /*
  * This is the bottom of the PKMAP area with HIGHMEM or an arbitrary
<span class="p_chunk">@@ -82,12 +98,8 @@</span> <span class="p_context"></span>
 
 extern unsigned long ioremap_bot;
 
<span class="p_del">-/*</span>
<span class="p_del">- * entries per page directory level: our page-table tree is two-level, so</span>
<span class="p_del">- * we don&#39;t really have any PMD directory.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_SHIFT)</span>
<span class="p_del">-#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="p_add">+/* Bits to mask out from a PGD to get to the PUD page */</span>
<span class="p_add">+#define PGD_MASKED_BITS		0</span>
 
 #define pte_ERROR(e) \
 	pr_err(&quot;%s:%d: bad pte %llx.\n&quot;, __FILE__, __LINE__, \
<span class="p_chunk">@@ -283,15 +295,6 @@</span> <span class="p_context"> static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val(pte) &gt;&gt; 3 })
 #define __swp_entry_to_pte(x)		((pte_t) { (x).val &lt;&lt; 3 })
 
<span class="p_del">-#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="p_del">-void pgtable_cache_init(void);</span>
<span class="p_del">-#else</span>
<span class="p_del">-/*</span>
<span class="p_del">- * No page table caches to initialise</span>
<span class="p_del">- */</span>
<span class="p_del">-#define pgtable_cache_init()	do { } while (0)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 extern int get_pteptr(struct mm_struct *mm, unsigned long addr, pte_t **ptep,
 		      pmd_t **pmdp);
 
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/pgtable.h b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_header">index 9fd77f8..0a46a5f 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_chunk">@@ -789,9 +789,6 @@</span> <span class="p_context"> extern struct page *pgd_page(pgd_t pgd);</span>
 #define pgd_ERROR(e) \
 	pr_err(&quot;%s:%d: bad pgd %08lx.\n&quot;, __FILE__, __LINE__, pgd_val(e))
 
<span class="p_del">-void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="p_del">-void pgtable_cache_init(void);</span>
<span class="p_del">-</span>
 static inline int map_kernel_page(unsigned long ea, unsigned long pa,
 				  unsigned long flags)
 {
<span class="p_header">diff --git a/arch/powerpc/include/asm/nohash/32/pgalloc.h b/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="p_header">index 76d6b9e..6331392 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/nohash/32/pgalloc.h</span>
<span class="p_chunk">@@ -2,14 +2,42 @@</span> <span class="p_context"></span>
 #define _ASM_POWERPC_PGALLOC_32_H
 
 #include &lt;linux/threads.h&gt;
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
 
<span class="p_del">-/* For 32-bit, all levels of page tables are just drawn from get_free_page() */</span>
<span class="p_del">-#define MAX_PGTABLE_INDEX_SIZE	0</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Functions that deal with pagetables that could be at any level of</span>
<span class="p_add">+ * the table need to be passed an &quot;index_size&quot; so they know how to</span>
<span class="p_add">+ * handle allocation.  For PTE pages (which are linked to a struct</span>
<span class="p_add">+ * page for now, and drawn from the main get_free_pages() pool), the</span>
<span class="p_add">+ * allocation size will be (2^index_size * sizeof(pointer)) and</span>
<span class="p_add">+ * allocations are drawn from the kmem_cache in PGT_CACHE(index_size).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The maximum index size needs to be big enough to allow any</span>
<span class="p_add">+ * pagetable sizes we need, but small enough to fit in the low bits of</span>
<span class="p_add">+ * any page table pointer.  In other words all pagetables, even tiny</span>
<span class="p_add">+ * ones, must be aligned to allow at least enough low 0 bits to</span>
<span class="p_add">+ * contain this value.  This value is also used as a mask, so it must</span>
<span class="p_add">+ * be one less than a power of two.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define MAX_PGTABLE_INDEX_SIZE	0xf</span>
 
 extern void __bad_pte(pmd_t *pmd);
 
<span class="p_del">-extern pgd_t *pgd_alloc(struct mm_struct *mm);</span>
<span class="p_del">-extern void pgd_free(struct mm_struct *mm, pgd_t *pgd);</span>
<span class="p_add">+extern struct kmem_cache *pgtable_cache[];</span>
<span class="p_add">+#define PGT_CACHE(shift) ({				\</span>
<span class="p_add">+			BUG_ON(!(shift));		\</span>
<span class="p_add">+			pgtable_cache[(shift) - 1];	\</span>
<span class="p_add">+		})</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kmem_cache_alloc(PGT_CACHE(PGD_INDEX_SIZE), GFP_KERNEL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	kmem_cache_free(PGT_CACHE(PGD_INDEX_SIZE), pgd);</span>
<span class="p_add">+}</span>
 
 /*
  * We don&#39;t have any real pmd&#39;s, and this code never triggers because
<span class="p_chunk">@@ -68,8 +96,12 @@</span> <span class="p_context"> static inline void pte_free(struct mm_struct *mm, pgtable_t ptepage)</span>
 
 static inline void pgtable_free(void *table, unsigned index_size)
 {
<span class="p_del">-	BUG_ON(index_size); /* 32-bit doesn&#39;t use this */</span>
<span class="p_del">-	free_page((unsigned long)table);</span>
<span class="p_add">+	if (!index_size) {</span>
<span class="p_add">+		free_page((unsigned long)table);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		BUG_ON(index_size &gt; MAX_PGTABLE_INDEX_SIZE);</span>
<span class="p_add">+		kmem_cache_free(PGT_CACHE(index_size), table);</span>
<span class="p_add">+	}</span>
 }
 
 #define check_pgt_cache()	do { } while (0)
<span class="p_header">diff --git a/arch/powerpc/include/asm/nohash/32/pgtable.h b/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="p_header">index c219ef7..8cbe222 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/nohash/32/pgtable.h</span>
<span class="p_chunk">@@ -16,6 +16,26 @@</span> <span class="p_context"> extern int icache_44x_need_flush;</span>
 
 #endif /* __ASSEMBLY__ */
 
<span class="p_add">+#define PTE_INDEX_SIZE	PTE_SHIFT</span>
<span class="p_add">+#define PMD_INDEX_SIZE	0</span>
<span class="p_add">+#define PUD_INDEX_SIZE	0</span>
<span class="p_add">+#define PGD_INDEX_SIZE	(32 - PGDIR_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_CACHE_INDEX	PMD_INDEX_SIZE</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PMD_TABLE_SIZE	(sizeof(pmd_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PUD_TABLE_SIZE	(sizeof(pud_t) &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="p_add">+#endif	/* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTRS_PER_PTE	(1 &lt;&lt; PTE_INDEX_SIZE)</span>
<span class="p_add">+#define PTRS_PER_PGD	(1 &lt;&lt; PGD_INDEX_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/* With 4k base page size, hugepage PTEs go at the PMD level */</span>
<span class="p_add">+#define MIN_HUGEPTE_SHIFT	PMD_SHIFT</span>
<span class="p_add">+</span>
 /*
  * The normal case is that PTEs are 32-bits and we have a 1-page
  * 1024-entry pgdir pointing to 1-page 1024-entry PTE pages.  -- paulus
<span class="p_chunk">@@ -27,22 +47,12 @@</span> <span class="p_context"> extern int icache_44x_need_flush;</span>
  * -Matt
  */
 /* PGDIR_SHIFT determines what a top-level page table entry can map */
<span class="p_del">-#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_SHIFT)</span>
<span class="p_add">+#define PGDIR_SHIFT	(PAGE_SHIFT + PTE_INDEX_SIZE)</span>
 #define PGDIR_SIZE	(1UL &lt;&lt; PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE-1))
 
<span class="p_del">-/*</span>
<span class="p_del">- * entries per page directory level: our page-table tree is two-level, so</span>
<span class="p_del">- * we don&#39;t really have any PMD directory.</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifndef __ASSEMBLY__</span>
<span class="p_del">-#define PTE_TABLE_SIZE	(sizeof(pte_t) &lt;&lt; PTE_SHIFT)</span>
<span class="p_del">-#define PGD_TABLE_SIZE	(sizeof(pgd_t) &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="p_del">-#endif	/* __ASSEMBLY__ */</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTRS_PER_PTE	(1 &lt;&lt; PTE_SHIFT)</span>
<span class="p_del">-#define PTRS_PER_PMD	1</span>
<span class="p_del">-#define PTRS_PER_PGD	(1 &lt;&lt; (32 - PGDIR_SHIFT))</span>
<span class="p_add">+/* Bits to mask out from a PGD to get to the PUD page */</span>
<span class="p_add">+#define PGD_MASKED_BITS		0</span>
 
 #define USER_PTRS_PER_PGD	(TASK_SIZE / PGDIR_SIZE)
 #define FIRST_USER_ADDRESS	0UL
<span class="p_chunk">@@ -328,15 +338,6 @@</span> <span class="p_context"> static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val(pte) &gt;&gt; 3 })
 #define __swp_entry_to_pte(x)		((pte_t) { (x).val &lt;&lt; 3 })
 
<span class="p_del">-#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="p_del">-void pgtable_cache_init(void);</span>
<span class="p_del">-#else</span>
<span class="p_del">-/*</span>
<span class="p_del">- * No page table caches to initialise</span>
<span class="p_del">- */</span>
<span class="p_del">-#define pgtable_cache_init()	do { } while (0)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 extern int get_pteptr(struct mm_struct *mm, unsigned long addr, pte_t **ptep,
 		      pmd_t **pmdp);
 
<span class="p_header">diff --git a/arch/powerpc/include/asm/nohash/64/pgtable.h b/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="p_header">index 653a183..619018a 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/nohash/64/pgtable.h</span>
<span class="p_chunk">@@ -358,8 +358,6 @@</span> <span class="p_context"> static inline void __ptep_set_access_flags(struct mm_struct *mm,</span>
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })
 #define __swp_entry_to_pte(x)		__pte((x).val)
 
<span class="p_del">-void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="p_del">-void pgtable_cache_init(void);</span>
 extern int map_kernel_page(unsigned long ea, unsigned long pa,
 			   unsigned long flags);
 extern int __meminit vmemmap_create_mapping(unsigned long start,
<span class="p_header">diff --git a/arch/powerpc/include/asm/pgtable.h b/arch/powerpc/include/asm/pgtable.h</span>
<span class="p_header">index 9bd87f2..dd01212 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -78,6 +78,8 @@</span> <span class="p_context"> static inline pte_t *find_linux_pte_or_hugepte(pgd_t *pgdir, unsigned long ea,</span>
 
 unsigned long vmalloc_to_phys(void *vmalloc_addr);
 
<span class="p_add">+void pgtable_cache_add(unsigned shift, void (*ctor)(void *));</span>
<span class="p_add">+void pgtable_cache_init(void);</span>
 #endif /* __ASSEMBLY__ */
 
 #endif /* _ASM_POWERPC_PGTABLE_H */
<span class="p_header">diff --git a/arch/powerpc/mm/Makefile b/arch/powerpc/mm/Makefile</span>
<span class="p_header">index 1a4e570..e8a86d2 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/Makefile</span>
<span class="p_header">+++ b/arch/powerpc/mm/Makefile</span>
<span class="p_chunk">@@ -7,7 +7,8 @@</span> <span class="p_context"> subdir-ccflags-$(CONFIG_PPC_WERROR) := -Werror</span>
 ccflags-$(CONFIG_PPC64)	:= $(NO_MINIMAL_TOC)
 
 obj-y				:= fault.o mem.o pgtable.o mmap.o \
<span class="p_del">-				   init_$(BITS).o pgtable_$(BITS).o</span>
<span class="p_add">+				   init_$(BITS).o pgtable_$(BITS).o \</span>
<span class="p_add">+				   init-common.o</span>
 obj-$(CONFIG_PPC_MMU_NOHASH)	+= mmu_context_nohash.o tlb_nohash.o \
 				   tlb_nohash_low.o
 obj-$(CONFIG_PPC_BOOK3E)	+= tlb_low_$(BITS)e.o
<span class="p_header">diff --git a/arch/powerpc/mm/init-common.c b/arch/powerpc/mm/init-common.c</span>
new file mode 100644
<span class="p_header">index 0000000..ab2b947</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/powerpc/mm/init-common.c</span>
<span class="p_chunk">@@ -0,0 +1,147 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ *  PowerPC version</span>
<span class="p_add">+ *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)</span>
<span class="p_add">+ *  and Cort Dougan (PReP) (cort@cs.nmt.edu)</span>
<span class="p_add">+ *    Copyright (C) 1996 Paul Mackerras</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Derived from &quot;arch/i386/mm/init.c&quot;</span>
<span class="p_add">+ *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Dave Engebretsen &lt;engebret@us.ibm.com&gt;</span>
<span class="p_add">+ *      Rework for PPC64 port.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *  modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *  as published by the Free Software Foundation; either version</span>
<span class="p_add">+ *  2 of the License, or (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#undef DEBUG</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/signal.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mman.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/stddef.h&gt;</span>
<span class="p_add">+#include &lt;linux/vmalloc.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/delay.h&gt;</span>
<span class="p_add">+#include &lt;linux/highmem.h&gt;</span>
<span class="p_add">+#include &lt;linux/idr.h&gt;</span>
<span class="p_add">+#include &lt;linux/nodemask.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;linux/poison.h&gt;</span>
<span class="p_add">+#include &lt;linux/memblock.h&gt;</span>
<span class="p_add">+#include &lt;linux/hugetlb.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+#include &lt;asm/prom.h&gt;</span>
<span class="p_add">+#include &lt;asm/rtas.h&gt;</span>
<span class="p_add">+#include &lt;asm/io.h&gt;</span>
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/mmu.h&gt;</span>
<span class="p_add">+#include &lt;asm/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;asm/smp.h&gt;</span>
<span class="p_add">+#include &lt;asm/machdep.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
<span class="p_add">+#include &lt;asm/eeh.h&gt;</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+#include &lt;asm/mmzone.h&gt;</span>
<span class="p_add">+#include &lt;asm/cputable.h&gt;</span>
<span class="p_add">+#include &lt;asm/sections.h&gt;</span>
<span class="p_add">+#include &lt;asm/iommu.h&gt;</span>
<span class="p_add">+#include &lt;asm/vdso.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;mmu_decl.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+static void pgd_ctor(void *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	memset(addr, 0, PGD_TABLE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void pud_ctor(void *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	memset(addr, 0, PUD_TABLE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void pmd_ctor(void *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	memset(addr, 0, PMD_TABLE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct kmem_cache *pgtable_cache[MAX_PGTABLE_INDEX_SIZE];</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Create a kmem_cache() for pagetables.  This is not used for PTE</span>
<span class="p_add">+ * pages - they&#39;re linked to struct page, come from the normal free</span>
<span class="p_add">+ * pages pool and have a different entry size (see real_pte_t) to</span>
<span class="p_add">+ * everything else.  Caches created by this function are used for all</span>
<span class="p_add">+ * the higher level pagetables, and for hugepage pagetables.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void pgtable_cache_add(unsigned shift, void (*ctor)(void *))</span>
<span class="p_add">+{</span>
<span class="p_add">+	char *name;</span>
<span class="p_add">+	unsigned long table_size = sizeof(void *) &lt;&lt; shift;</span>
<span class="p_add">+	unsigned long align = table_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* When batching pgtable pointers for RCU freeing, we store</span>
<span class="p_add">+	 * the index size in the low bits.  Table alignment must be</span>
<span class="p_add">+	 * big enough to fit it.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Likewise, hugeapge pagetable pointers contain a (different)</span>
<span class="p_add">+	 * shift value in the low bits.  All tables must be aligned so</span>
<span class="p_add">+	 * as to leave enough 0 bits in the address to contain it. */</span>
<span class="p_add">+	unsigned long minalign = max(MAX_PGTABLE_INDEX_SIZE + 1,</span>
<span class="p_add">+				     HUGEPD_SHIFT_MASK + 1);</span>
<span class="p_add">+	struct kmem_cache *new;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* It would be nice if this was a BUILD_BUG_ON(), but at the</span>
<span class="p_add">+	 * moment, gcc doesn&#39;t seem to recognize is_power_of_2 as a</span>
<span class="p_add">+	 * constant expression, so so much for that. */</span>
<span class="p_add">+	BUG_ON(!is_power_of_2(minalign));</span>
<span class="p_add">+	BUG_ON((shift &lt; 1) || (shift &gt; MAX_PGTABLE_INDEX_SIZE));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (PGT_CACHE(shift))</span>
<span class="p_add">+		return; /* Already have a cache of this size */</span>
<span class="p_add">+</span>
<span class="p_add">+	align = max_t(unsigned long, align, minalign);</span>
<span class="p_add">+	name = kasprintf(GFP_KERNEL, &quot;pgtable-2^%d&quot;, shift);</span>
<span class="p_add">+	new = kmem_cache_create(name, table_size, align, 0, ctor);</span>
<span class="p_add">+	kfree(name);</span>
<span class="p_add">+	pgtable_cache[shift - 1] = new;</span>
<span class="p_add">+	pr_debug(&quot;Allocated pgtable cache for order %d\n&quot;, shift);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+void pgtable_cache_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgtable_cache_add(PGD_INDEX_SIZE, pgd_ctor);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (PMD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PMD_INDEX_SIZE))</span>
<span class="p_add">+		pgtable_cache_add(PMD_CACHE_INDEX, pmd_ctor);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * In all current configs, when the PUD index exists it&#39;s the</span>
<span class="p_add">+	 * same size as either the pgd or pmd index except with THP enabled</span>
<span class="p_add">+	 * on book3s 64</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="p_add">+		pgtable_cache_add(PUD_INDEX_SIZE, pud_ctor);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!PGT_CACHE(PGD_INDEX_SIZE))</span>
<span class="p_add">+		panic(&quot;Couldn&#39;t allocate pgd cache&quot;);</span>
<span class="p_add">+	if (PMD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PMD_INDEX_SIZE))</span>
<span class="p_add">+		panic(&quot;Couldn&#39;t allocate pmd pgtable caches&quot;);</span>
<span class="p_add">+	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="p_add">+		panic(&quot;Couldn&#39;t allocate pud pgtable caches&quot;);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/powerpc/mm/init_64.c b/arch/powerpc/mm/init_64.c</span>
<span class="p_header">index 16ada1e..a000c35 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/init_64.c</span>
<span class="p_chunk">@@ -80,83 +80,6 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(memstart_addr);</span>
 phys_addr_t kernstart_addr;
 EXPORT_SYMBOL_GPL(kernstart_addr);
 
<span class="p_del">-static void pgd_ctor(void *addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	memset(addr, 0, PGD_TABLE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void pud_ctor(void *addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	memset(addr, 0, PUD_TABLE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void pmd_ctor(void *addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	memset(addr, 0, PMD_TABLE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-struct kmem_cache *pgtable_cache[MAX_PGTABLE_INDEX_SIZE];</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Create a kmem_cache() for pagetables.  This is not used for PTE</span>
<span class="p_del">- * pages - they&#39;re linked to struct page, come from the normal free</span>
<span class="p_del">- * pages pool and have a different entry size (see real_pte_t) to</span>
<span class="p_del">- * everything else.  Caches created by this function are used for all</span>
<span class="p_del">- * the higher level pagetables, and for hugepage pagetables.</span>
<span class="p_del">- */</span>
<span class="p_del">-void pgtable_cache_add(unsigned shift, void (*ctor)(void *))</span>
<span class="p_del">-{</span>
<span class="p_del">-	char *name;</span>
<span class="p_del">-	unsigned long table_size = sizeof(void *) &lt;&lt; shift;</span>
<span class="p_del">-	unsigned long align = table_size;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* When batching pgtable pointers for RCU freeing, we store</span>
<span class="p_del">-	 * the index size in the low bits.  Table alignment must be</span>
<span class="p_del">-	 * big enough to fit it.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Likewise, hugeapge pagetable pointers contain a (different)</span>
<span class="p_del">-	 * shift value in the low bits.  All tables must be aligned so</span>
<span class="p_del">-	 * as to leave enough 0 bits in the address to contain it. */</span>
<span class="p_del">-	unsigned long minalign = max(MAX_PGTABLE_INDEX_SIZE + 1,</span>
<span class="p_del">-				     HUGEPD_SHIFT_MASK + 1);</span>
<span class="p_del">-	struct kmem_cache *new;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* It would be nice if this was a BUILD_BUG_ON(), but at the</span>
<span class="p_del">-	 * moment, gcc doesn&#39;t seem to recognize is_power_of_2 as a</span>
<span class="p_del">-	 * constant expression, so so much for that. */</span>
<span class="p_del">-	BUG_ON(!is_power_of_2(minalign));</span>
<span class="p_del">-	BUG_ON((shift &lt; 1) || (shift &gt; MAX_PGTABLE_INDEX_SIZE));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (PGT_CACHE(shift))</span>
<span class="p_del">-		return; /* Already have a cache of this size */</span>
<span class="p_del">-</span>
<span class="p_del">-	align = max_t(unsigned long, align, minalign);</span>
<span class="p_del">-	name = kasprintf(GFP_KERNEL, &quot;pgtable-2^%d&quot;, shift);</span>
<span class="p_del">-	new = kmem_cache_create(name, table_size, align, 0, ctor);</span>
<span class="p_del">-	kfree(name);</span>
<span class="p_del">-	pgtable_cache[shift - 1] = new;</span>
<span class="p_del">-	pr_debug(&quot;Allocated pgtable cache for order %d\n&quot;, shift);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-void pgtable_cache_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgtable_cache_add(PGD_INDEX_SIZE, pgd_ctor);</span>
<span class="p_del">-	pgtable_cache_add(PMD_CACHE_INDEX, pmd_ctor);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * In all current configs, when the PUD index exists it&#39;s the</span>
<span class="p_del">-	 * same size as either the pgd or pmd index except with THP enabled</span>
<span class="p_del">-	 * on book3s 64</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="p_del">-		pgtable_cache_add(PUD_INDEX_SIZE, pud_ctor);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!PGT_CACHE(PGD_INDEX_SIZE) || !PGT_CACHE(PMD_CACHE_INDEX))</span>
<span class="p_del">-		panic(&quot;Couldn&#39;t allocate pgtable caches&quot;);</span>
<span class="p_del">-	if (PUD_INDEX_SIZE &amp;&amp; !PGT_CACHE(PUD_INDEX_SIZE))</span>
<span class="p_del">-		panic(&quot;Couldn&#39;t allocate pud pgtable caches&quot;);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 /*
  * Given an address within the vmemmap, determine the pfn of the page that
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable_32.c b/arch/powerpc/mm/pgtable_32.c</span>
<span class="p_header">index 0ae0572..a65c0b4 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable_32.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable_32.c</span>
<span class="p_chunk">@@ -42,43 +42,6 @@</span> <span class="p_context"> EXPORT_SYMBOL(ioremap_bot);	/* aka VMALLOC_END */</span>
 
 extern char etext[], _stext[], _sinittext[], _einittext[];
 
<span class="p_del">-#define PGDIR_ORDER	(32 + PGD_T_LOG2 - PGDIR_SHIFT)</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="p_del">-static struct kmem_cache *pgtable_cache;</span>
<span class="p_del">-</span>
<span class="p_del">-void pgtable_cache_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgtable_cache = kmem_cache_create(&quot;PGDIR cache&quot;, 1 &lt;&lt; PGDIR_ORDER,</span>
<span class="p_del">-					  1 &lt;&lt; PGDIR_ORDER, 0, NULL);</span>
<span class="p_del">-	if (pgtable_cache == NULL)</span>
<span class="p_del">-		panic(&quot;Couldn&#39;t allocate pgtable caches&quot;);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgd_t *ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* pgdir take page or two with 4K pages and a page fraction otherwise */</span>
<span class="p_del">-#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="p_del">-	ret = kmem_cache_alloc(pgtable_cache, GFP_KERNEL | __GFP_ZERO);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	ret = (pgd_t *)__get_free_pages(GFP_KERNEL|__GFP_ZERO,</span>
<span class="p_del">-			PGDIR_ORDER - PAGE_SHIFT);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="p_del">-{</span>
<span class="p_del">-#ifndef CONFIG_PPC_4K_PAGES</span>
<span class="p_del">-	kmem_cache_free(pgtable_cache, (void *)pgd);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	free_pages((unsigned long)pgd, PGDIR_ORDER - PAGE_SHIFT);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 __ref pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
 {
 	pte_t *pte;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



