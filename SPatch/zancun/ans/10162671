
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,pull] x86/pti updates for 4.15 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,pull] x86/pti updates for 4.15</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 14, 2018, 3:27 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.DEB.2.20.1801141550270.2371@nanos&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10162671/mbox/"
   >mbox</a>
|
   <a href="/patch/10162671/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10162671/">/patch/10162671/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DDAA460390 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 14 Jan 2018 15:27:57 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B9FE328ABB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 14 Jan 2018 15:27:57 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id ADDAF28ABD; Sun, 14 Jan 2018 15:27:57 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4D41028AC0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 14 Jan 2018 15:27:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751907AbeANP1s (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 14 Jan 2018 10:27:48 -0500
Received: from Galois.linutronix.de ([146.0.238.70]:37890 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751215AbeANP1m (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 14 Jan 2018 10:27:42 -0500
Received: from p4fea5f09.dip0.t-ipconnect.de ([79.234.95.9] helo=nanos)
	by Galois.linutronix.de with esmtpsa
	(TLS1.2:DHE_RSA_AES_256_CBC_SHA256:256) (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1eak9r-0007Wy-9a; Sun, 14 Jan 2018 16:25:16 +0100
Date: Sun, 14 Jan 2018 16:27:35 +0100 (CET)
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
cc: LKML &lt;linux-kernel@vger.kernel.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	Greg KH &lt;gregkh@linuxfoundation.org&gt;
Subject: [GIT pull] x86/pti updates for 4.15
Message-ID: &lt;alpine.DEB.2.20.1801141550270.2371@nanos&gt;
User-Agent: Alpine 2.20 (DEB 67 2015-01-07)
MIME-Version: 1.0
Content-Type: multipart/mixed; BOUNDARY=&quot;8323329-7676866-1515943659=:2371&quot;
X-Linutronix-Spam-Score: -1.0
X-Linutronix-Spam-Level: -
X-Linutronix-Spam-Status: No , -1.0 points, 5.0 required, ALL_TRUSTED=-1,
	SHORTCIRCUIT=-0.0001
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Jan. 14, 2018, 3:27 p.m.</div>
<pre class="content">
Linus,

please pull the latest x86-pti-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus

This update contains:

  - A PTI bugfix to avoid setting reserved CR3 bits when PCID is
    disabled. This seems to cause issues on a virtual machine at least and
    is incorrect according to the AMD manual.

  - A PTI bugfix which disables the perf BTS facility if PTI is
    enabled. The BTS AUX buffer is not globally visible and causes the CPU
    to fault when the mapping disappears on switching CR3 to user space. A
    full fix which restores BTS on PTI is non trivial and will be worked
    on.

  - PTI bugfixes for EFI and trusted boot which make sure that the user
    space visible page table entries have the NX bit cleared

  - Removal of dead code in the PTI pagetable setup functions

  - Add PTI documentation

  - Add a selftest for vsyscall to verify that the kernel actually
    implements what it advertises.

  - A sysfs interface to expose vulnerability and mitigation information so
    there is a coherent way for users to retrieve the status.

  - The initial spectre_v2 mitigations, aka. retpoline.

    - The necessary ASM thunk and compiler support
       
    - The ASM variants of retpoline and the conversion of affected ASM
      code

    - Make LFENCE serializing on AMD so it can be used as speculation trap

    - The RSB fill after vmexit

  - Initial objtool support for retpoline

As I said in the status mail this is the most of the set of patches which
should go into 4.15 except two straight forward patches still on hold:

  - The retpoline add on of LFENCE which waits for ACKs
  - The RSB fill after context switch 

Both should be ready to go early next week and with that we&#39;ll have covered
the major holes of spectre_v2 and go back to normality.

Thanks,

	Thomas

------------------&gt;
Andi Kleen (1):
      x86/retpoline/irq32: Convert assembler indirect jumps

Andy Lutomirski (1):
      selftests/x86: Add test_vsyscall

Borislav Petkov (1):
      x86/alternatives: Fix optimize_nops() checking

Dave Hansen (3):
      x86/Documentation: Add PTI description
      x86/tboot: Unbreak tboot with PTI enabled
      x86/pti: Make unpoison of pgd for trusted boot work for real

David Woodhouse (11):
      x86/cpufeatures: Add X86_BUG_SPECTRE_V[12]
      sysfs/cpu: Fix typos in vulnerability documentation
      x86/retpoline: Add initial retpoline support
      x86/spectre: Add boot time option to select Spectre v2 mitigation
      x86/retpoline/crypto: Convert crypto assembler indirect jumps
      x86/retpoline/entry: Convert entry assembler indirect jumps
      x86/retpoline/ftrace: Convert ftrace assembler indirect jumps
      x86/retpoline/hyperv: Convert assembler indirect jumps
      x86/retpoline/xen: Convert Xen hypercall indirect jumps
      x86/retpoline/checksum32: Convert assembler indirect jumps
      x86/retpoline: Fill return stack buffer on vmexit

Jike Song (1):
      x86/mm/pti: Remove dead logic in pti_user_pagetable_walk*()

Jiri Kosina (1):
      x86/pti: Unbreak EFI old_memmap

Josh Poimboeuf (2):
      objtool: Detect jumps to retpoline thunks
      objtool: Allow alternatives to be ignored

Peter Zijlstra (1):
      x86,perf: Disable intel_bts when PTI

Thomas Gleixner (3):
      sysfs/cpu: Add vulnerability folder
      x86/cpu: Implement CPU vulnerabilites sysfs functions
      x86/pti: Fix !PCID and sanitize defines

Tom Lendacky (2):
      x86/cpu/AMD: Make LFENCE a serializing instruction
      x86/cpu/AMD: Use LFENCE_RDTSC in preference to MFENCE_RDTSC

W. Trevor King (1):
      security/Kconfig: Correct the Documentation reference for PTI


 Documentation/ABI/testing/sysfs-devices-system-cpu |  16 +
 Documentation/admin-guide/kernel-parameters.txt    |  49 +-
 Documentation/x86/pti.txt                          | 186 ++++++++
 arch/x86/Kconfig                                   |  14 +
 arch/x86/Makefile                                  |  10 +
 arch/x86/crypto/aesni-intel_asm.S                  |   5 +-
 arch/x86/crypto/camellia-aesni-avx-asm_64.S        |   3 +-
 arch/x86/crypto/camellia-aesni-avx2-asm_64.S       |   3 +-
 arch/x86/crypto/crc32c-pcl-intel-asm_64.S          |   3 +-
 arch/x86/entry/calling.h                           |  36 +-
 arch/x86/entry/entry_32.S                          |   5 +-
 arch/x86/entry/entry_64.S                          |  12 +-
 arch/x86/events/intel/bts.c                        |  18 +
 arch/x86/include/asm/asm-prototypes.h              |  25 ++
 arch/x86/include/asm/cpufeatures.h                 |   4 +
 arch/x86/include/asm/mshyperv.h                    |  18 +-
 arch/x86/include/asm/msr-index.h                   |   3 +
 arch/x86/include/asm/nospec-branch.h               | 214 +++++++++
 arch/x86/include/asm/processor-flags.h             |   2 +-
 arch/x86/include/asm/tlbflush.h                    |   6 +-
 arch/x86/include/asm/xen/hypercall.h               |   5 +-
 arch/x86/kernel/alternative.c                      |   7 +-
 arch/x86/kernel/cpu/amd.c                          |  28 +-
 arch/x86/kernel/cpu/bugs.c                         | 185 ++++++++
 arch/x86/kernel/cpu/common.c                       |   3 +
 arch/x86/kernel/ftrace_32.S                        |   6 +-
 arch/x86/kernel/ftrace_64.S                        |   8 +-
 arch/x86/kernel/irq_32.c                           |   9 +-
 arch/x86/kernel/tboot.c                            |  11 +
 arch/x86/kvm/svm.c                                 |   4 +
 arch/x86/kvm/vmx.c                                 |   4 +
 arch/x86/lib/Makefile                              |   1 +
 arch/x86/lib/checksum_32.S                         |   7 +-
 arch/x86/lib/retpoline.S                           |  48 ++
 arch/x86/mm/pti.c                                  |  32 +-
 arch/x86/platform/efi/efi_64.c                     |   2 +
 drivers/base/Kconfig                               |   3 +
 drivers/base/cpu.c                                 |  48 ++
 include/linux/cpu.h                                |   7 +
 security/Kconfig                                   |   2 +-
 tools/objtool/check.c                              |  69 ++-
 tools/objtool/check.h                              |   2 +-
 tools/testing/selftests/x86/Makefile               |   2 +-
 tools/testing/selftests/x86/test_vsyscall.c        | 500 +++++++++++++++++++++
 44 files changed, 1525 insertions(+), 100 deletions(-)
 create mode 100644 Documentation/x86/pti.txt
 create mode 100644 arch/x86/include/asm/nospec-branch.h
 create mode 100644 arch/x86/lib/retpoline.S
 create mode 100644 tools/testing/selftests/x86/test_vsyscall.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Jan. 14, 2018, 8:59 p.m.</div>
<pre class="content">
On Sun, Jan 14, 2018 at 7:27 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;    git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus</span>

So I do think this:

        $(warning CONFIG_RETPOLINE=y, but not supported by the
compiler. Toolchain update recommended.)

needs to be removed.

Yes, yes, I understand why it&#39;s warning. It&#39;s still both annoying and wrong.

It&#39;s wrong because it will just make people turn off RETPOLINE, and
the asm updates - and return stack clearing - that are independent of
the compiler are likely the most important parts because they are
likely the ones easiest to target.

And it&#39;s annoying because most people won&#39;t be able to do anything
about it. The number of people building their own compiler? Very
small. So if their distro hasn&#39;t got a compiler yet (and pretty much
nobody does), the warning is just annoying crap.

It is already properly reported as part of the sysfs interface. The
compile-time warning only encourages bad things.

             Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Jan. 14, 2018, 9:01 p.m.</div>
<pre class="content">
On Sun, 14 Jan 2018, Linus Torvalds wrote:
<span class="quote">&gt; On Sun, Jan 14, 2018 at 7:27 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;    git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I do think this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         $(warning CONFIG_RETPOLINE=y, but not supported by the</span>
<span class="quote">&gt; compiler. Toolchain update recommended.)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; needs to be removed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, yes, I understand why it&#39;s warning. It&#39;s still both annoying and wrong.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s wrong because it will just make people turn off RETPOLINE, and</span>
<span class="quote">&gt; the asm updates - and return stack clearing - that are independent of</span>
<span class="quote">&gt; the compiler are likely the most important parts because they are</span>
<span class="quote">&gt; likely the ones easiest to target.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And it&#39;s annoying because most people won&#39;t be able to do anything</span>
<span class="quote">&gt; about it. The number of people building their own compiler? Very</span>
<span class="quote">&gt; small. So if their distro hasn&#39;t got a compiler yet (and pretty much</span>
<span class="quote">&gt; nobody does), the warning is just annoying crap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is already properly reported as part of the sysfs interface. The</span>
<span class="quote">&gt; compile-time warning only encourages bad things.</span>

Good point. I&#39;ll queue a patch to that effect or do you just want to do
that yourself?

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Jan. 14, 2018, 9:12 p.m.</div>
<pre class="content">
On Sun, Jan 14, 2018 at 1:01 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; Good point. I&#39;ll queue a patch to that effect or do you just want to do</span>
<span class="quote">&gt; that yourself?</span>

I don&#39;t think it&#39;s critical, and I don&#39;t care for rc8, so it&#39;s not
timing-sensitive.

Maybe David should do it as part of the &quot;the compiler people changed
the name of the thunks&quot; series anyway, since now I think that
RETPOLINE_CFLAGS test might want to be extended too (to see which
_version_ of the thunks we need to export).

Or maybe we&#39;d just export the damn thunks under both names, just to
allow people to use both versions of the compilers.

Christ, what a mess.

My &quot;let&#39;s not warn&quot; is not important, and the thunk naming thing
should be resolved first. Gaah.

So I think letting David worry about it sounds about right.

                 Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=308">David Woodhouse</a> - Jan. 14, 2018, 9:15 p.m.</div>
<pre class="content">
On Sun, 2018-01-14 at 13:12 -0800, Linus Torvalds wrote:
<span class="quote">&gt; On Sun, Jan 14, 2018 at 1:01 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt; wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Good point. I&#39;ll queue a patch to that effect or do you just want</span>
<span class="quote">&gt; &gt; to do</span>
<span class="quote">&gt; &gt; that yourself?</span>
<span class="quote">&gt; I don&#39;t think it&#39;s critical, and I don&#39;t care for rc8, so it&#39;s not</span>
<span class="quote">&gt; timing-sensitive.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Maybe David should do it as part of the &quot;the compiler people changed</span>
<span class="quote">&gt; the name of the thunks&quot; series anyway, since now I think that</span>
<span class="quote">&gt; RETPOLINE_CFLAGS test might want to be extended too (to see which</span>
<span class="quote">&gt; _version_ of the thunks we need to export).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or maybe we&#39;d just export the damn thunks under both names, just to</span>
<span class="quote">&gt; allow people to use both versions of the compilers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Christ, what a mess.</span>

No, screw that. We&#39;re updating, and there&#39;s only one version of
anything. Let&#39;s not mess around with compatibility for thunk names that
never made the light of day in either a GCC or a Linux (even -rc)
release.
<span class="quote">
&gt; My &quot;let&#39;s not warn&quot; is not important, and the thunk naming thing</span>
<span class="quote">&gt; should be resolved first. Gaah.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I think letting David worry about it sounds about right.</span>

Thanks. :)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/ABI/testing/sysfs-devices-system-cpu b/Documentation/ABI/testing/sysfs-devices-system-cpu</span>
<span class="p_header">index f3d5817c4ef0..258902db14bf 100644</span>
<span class="p_header">--- a/Documentation/ABI/testing/sysfs-devices-system-cpu</span>
<span class="p_header">+++ b/Documentation/ABI/testing/sysfs-devices-system-cpu</span>
<span class="p_chunk">@@ -373,3 +373,19 @@</span> <span class="p_context"> Contact:	Linux kernel mailing list &lt;linux-kernel@vger.kernel.org&gt;</span>
 Description:	information about CPUs heterogeneity.
 
 		cpu_capacity: capacity of cpu#.
<span class="p_add">+</span>
<span class="p_add">+What:		/sys/devices/system/cpu/vulnerabilities</span>
<span class="p_add">+		/sys/devices/system/cpu/vulnerabilities/meltdown</span>
<span class="p_add">+		/sys/devices/system/cpu/vulnerabilities/spectre_v1</span>
<span class="p_add">+		/sys/devices/system/cpu/vulnerabilities/spectre_v2</span>
<span class="p_add">+Date:		January 2018</span>
<span class="p_add">+Contact:	Linux kernel mailing list &lt;linux-kernel@vger.kernel.org&gt;</span>
<span class="p_add">+Description:	Information about CPU vulnerabilities</span>
<span class="p_add">+</span>
<span class="p_add">+		The files are named after the code names of CPU</span>
<span class="p_add">+		vulnerabilities. The output of those files reflects the</span>
<span class="p_add">+		state of the CPUs in the system. Possible output values:</span>
<span class="p_add">+</span>
<span class="p_add">+		&quot;Not affected&quot;	  CPU is not affected by the vulnerability</span>
<span class="p_add">+		&quot;Vulnerable&quot;	  CPU is affected and no mitigation in effect</span>
<span class="p_add">+		&quot;Mitigation: $M&quot;  CPU is affected and mitigation $M is in effect</span>
<span class="p_header">diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">index 520fdec15bbb..8122b5f98ea1 100644</span>
<span class="p_header">--- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2599,6 +2599,11 @@</span> <span class="p_context"></span>
 	nosmt		[KNL,S390] Disable symmetric multithreading (SMT).
 			Equivalent to smt=1.
 
<span class="p_add">+	nospectre_v2	[X86] Disable all mitigations for the Spectre variant 2</span>
<span class="p_add">+			(indirect branch prediction) vulnerability. System may</span>
<span class="p_add">+			allow data leaks with this option, which is equivalent</span>
<span class="p_add">+			to spectre_v2=off.</span>
<span class="p_add">+</span>
 	noxsave		[BUGS=X86] Disables x86 extended register state save
 			and restore using xsave. The kernel will fallback to
 			enabling legacy floating-point and sse state.
<span class="p_chunk">@@ -2685,8 +2690,6 @@</span> <span class="p_context"></span>
 			steal time is computed, but won&#39;t influence scheduler
 			behaviour
 
<span class="p_del">-	nopti		[X86-64] Disable kernel page table isolation</span>
<span class="p_del">-</span>
 	nolapic		[X86-32,APIC] Do not enable or use the local APIC.
 
 	nolapic_timer	[X86-32,APIC] Do not use the local APIC timer.
<span class="p_chunk">@@ -3255,11 +3258,20 @@</span> <span class="p_context"></span>
 	pt.		[PARIDE]
 			See Documentation/blockdev/paride.txt.
 
<span class="p_del">-	pti=		[X86_64]</span>
<span class="p_del">-			Control user/kernel address space isolation:</span>
<span class="p_del">-			on - enable</span>
<span class="p_del">-			off - disable</span>
<span class="p_del">-			auto - default setting</span>
<span class="p_add">+	pti=		[X86_64] Control Page Table Isolation of user and</span>
<span class="p_add">+			kernel address spaces.  Disabling this feature</span>
<span class="p_add">+			removes hardening, but improves performance of</span>
<span class="p_add">+			system calls and interrupts.</span>
<span class="p_add">+</span>
<span class="p_add">+			on   - unconditionally enable</span>
<span class="p_add">+			off  - unconditionally disable</span>
<span class="p_add">+			auto - kernel detects whether your CPU model is</span>
<span class="p_add">+			       vulnerable to issues that PTI mitigates</span>
<span class="p_add">+</span>
<span class="p_add">+			Not specifying this option is equivalent to pti=auto.</span>
<span class="p_add">+</span>
<span class="p_add">+	nopti		[X86_64]</span>
<span class="p_add">+			Equivalent to pti=off</span>
 
 	pty.legacy_count=
 			[KNL] Number of legacy pty&#39;s. Overwrites compiled-in
<span class="p_chunk">@@ -3901,6 +3913,29 @@</span> <span class="p_context"></span>
 	sonypi.*=	[HW] Sony Programmable I/O Control Device driver
 			See Documentation/laptops/sonypi.txt
 
<span class="p_add">+	spectre_v2=	[X86] Control mitigation of Spectre variant 2</span>
<span class="p_add">+			(indirect branch speculation) vulnerability.</span>
<span class="p_add">+</span>
<span class="p_add">+			on   - unconditionally enable</span>
<span class="p_add">+			off  - unconditionally disable</span>
<span class="p_add">+			auto - kernel detects whether your CPU model is</span>
<span class="p_add">+			       vulnerable</span>
<span class="p_add">+</span>
<span class="p_add">+			Selecting &#39;on&#39; will, and &#39;auto&#39; may, choose a</span>
<span class="p_add">+			mitigation method at run time according to the</span>
<span class="p_add">+			CPU, the available microcode, the setting of the</span>
<span class="p_add">+			CONFIG_RETPOLINE configuration option, and the</span>
<span class="p_add">+			compiler with which the kernel was built.</span>
<span class="p_add">+</span>
<span class="p_add">+			Specific mitigations can also be selected manually:</span>
<span class="p_add">+</span>
<span class="p_add">+			retpoline	  - replace indirect branches</span>
<span class="p_add">+			retpoline,generic - google&#39;s original retpoline</span>
<span class="p_add">+			retpoline,amd     - AMD-specific minimal thunk</span>
<span class="p_add">+</span>
<span class="p_add">+			Not specifying this option is equivalent to</span>
<span class="p_add">+			spectre_v2=auto.</span>
<span class="p_add">+</span>
 	spia_io_base=	[HW,MTD]
 	spia_fio_base=
 	spia_pedr=
<span class="p_header">diff --git a/Documentation/x86/pti.txt b/Documentation/x86/pti.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..d11eff61fc9a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/x86/pti.txt</span>
<span class="p_chunk">@@ -0,0 +1,186 @@</span> <span class="p_context"></span>
<span class="p_add">+Overview</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+Page Table Isolation (pti, previously known as KAISER[1]) is a</span>
<span class="p_add">+countermeasure against attacks on the shared user/kernel address</span>
<span class="p_add">+space such as the &quot;Meltdown&quot; approach[2].</span>
<span class="p_add">+</span>
<span class="p_add">+To mitigate this class of attacks, we create an independent set of</span>
<span class="p_add">+page tables for use only when running userspace applications.  When</span>
<span class="p_add">+the kernel is entered via syscalls, interrupts or exceptions, the</span>
<span class="p_add">+page tables are switched to the full &quot;kernel&quot; copy.  When the system</span>
<span class="p_add">+switches back to user mode, the user copy is used again.</span>
<span class="p_add">+</span>
<span class="p_add">+The userspace page tables contain only a minimal amount of kernel</span>
<span class="p_add">+data: only what is needed to enter/exit the kernel such as the</span>
<span class="p_add">+entry/exit functions themselves and the interrupt descriptor table</span>
<span class="p_add">+(IDT).  There are a few strictly unnecessary things that get mapped</span>
<span class="p_add">+such as the first C function when entering an interrupt (see</span>
<span class="p_add">+comments in pti.c).</span>
<span class="p_add">+</span>
<span class="p_add">+This approach helps to ensure that side-channel attacks leveraging</span>
<span class="p_add">+the paging structures do not function when PTI is enabled.  It can be</span>
<span class="p_add">+enabled by setting CONFIG_PAGE_TABLE_ISOLATION=y at compile time.</span>
<span class="p_add">+Once enabled at compile-time, it can be disabled at boot with the</span>
<span class="p_add">+&#39;nopti&#39; or &#39;pti=&#39; kernel parameters (see kernel-parameters.txt).</span>
<span class="p_add">+</span>
<span class="p_add">+Page Table Management</span>
<span class="p_add">+=====================</span>
<span class="p_add">+</span>
<span class="p_add">+When PTI is enabled, the kernel manages two sets of page tables.</span>
<span class="p_add">+The first set is very similar to the single set which is present in</span>
<span class="p_add">+kernels without PTI.  This includes a complete mapping of userspace</span>
<span class="p_add">+that the kernel can use for things like copy_to_user().</span>
<span class="p_add">+</span>
<span class="p_add">+Although _complete_, the user portion of the kernel page tables is</span>
<span class="p_add">+crippled by setting the NX bit in the top level.  This ensures</span>
<span class="p_add">+that any missed kernel-&gt;user CR3 switch will immediately crash</span>
<span class="p_add">+userspace upon executing its first instruction.</span>
<span class="p_add">+</span>
<span class="p_add">+The userspace page tables map only the kernel data needed to enter</span>
<span class="p_add">+and exit the kernel.  This data is entirely contained in the &#39;struct</span>
<span class="p_add">+cpu_entry_area&#39; structure which is placed in the fixmap which gives</span>
<span class="p_add">+each CPU&#39;s copy of the area a compile-time-fixed virtual address.</span>
<span class="p_add">+</span>
<span class="p_add">+For new userspace mappings, the kernel makes the entries in its</span>
<span class="p_add">+page tables like normal.  The only difference is when the kernel</span>
<span class="p_add">+makes entries in the top (PGD) level.  In addition to setting the</span>
<span class="p_add">+entry in the main kernel PGD, a copy of the entry is made in the</span>
<span class="p_add">+userspace page tables&#39; PGD.</span>
<span class="p_add">+</span>
<span class="p_add">+This sharing at the PGD level also inherently shares all the lower</span>
<span class="p_add">+layers of the page tables.  This leaves a single, shared set of</span>
<span class="p_add">+userspace page tables to manage.  One PTE to lock, one set of</span>
<span class="p_add">+accessed bits, dirty bits, etc...</span>
<span class="p_add">+</span>
<span class="p_add">+Overhead</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+Protection against side-channel attacks is important.  But,</span>
<span class="p_add">+this protection comes at a cost:</span>
<span class="p_add">+</span>
<span class="p_add">+1. Increased Memory Use</span>
<span class="p_add">+  a. Each process now needs an order-1 PGD instead of order-0.</span>
<span class="p_add">+     (Consumes an additional 4k per process).</span>
<span class="p_add">+  b. The &#39;cpu_entry_area&#39; structure must be 2MB in size and 2MB</span>
<span class="p_add">+     aligned so that it can be mapped by setting a single PMD</span>
<span class="p_add">+     entry.  This consumes nearly 2MB of RAM once the kernel</span>
<span class="p_add">+     is decompressed, but no space in the kernel image itself.</span>
<span class="p_add">+</span>
<span class="p_add">+2. Runtime Cost</span>
<span class="p_add">+  a. CR3 manipulation to switch between the page table copies</span>
<span class="p_add">+     must be done at interrupt, syscall, and exception entry</span>
<span class="p_add">+     and exit (it can be skipped when the kernel is interrupted,</span>
<span class="p_add">+     though.)  Moves to CR3 are on the order of a hundred</span>
<span class="p_add">+     cycles, and are required at every entry and exit.</span>
<span class="p_add">+  b. A &quot;trampoline&quot; must be used for SYSCALL entry.  This</span>
<span class="p_add">+     trampoline depends on a smaller set of resources than the</span>
<span class="p_add">+     non-PTI SYSCALL entry code, so requires mapping fewer</span>
<span class="p_add">+     things into the userspace page tables.  The downside is</span>
<span class="p_add">+     that stacks must be switched at entry time.</span>
<span class="p_add">+  d. Global pages are disabled for all kernel structures not</span>
<span class="p_add">+     mapped into both kernel and userspace page tables.  This</span>
<span class="p_add">+     feature of the MMU allows different processes to share TLB</span>
<span class="p_add">+     entries mapping the kernel.  Losing the feature means more</span>
<span class="p_add">+     TLB misses after a context switch.  The actual loss of</span>
<span class="p_add">+     performance is very small, however, never exceeding 1%.</span>
<span class="p_add">+  d. Process Context IDentifiers (PCID) is a CPU feature that</span>
<span class="p_add">+     allows us to skip flushing the entire TLB when switching page</span>
<span class="p_add">+     tables by setting a special bit in CR3 when the page tables</span>
<span class="p_add">+     are changed.  This makes switching the page tables (at context</span>
<span class="p_add">+     switch, or kernel entry/exit) cheaper.  But, on systems with</span>
<span class="p_add">+     PCID support, the context switch code must flush both the user</span>
<span class="p_add">+     and kernel entries out of the TLB.  The user PCID TLB flush is</span>
<span class="p_add">+     deferred until the exit to userspace, minimizing the cost.</span>
<span class="p_add">+     See intel.com/sdm for the gory PCID/INVPCID details.</span>
<span class="p_add">+  e. The userspace page tables must be populated for each new</span>
<span class="p_add">+     process.  Even without PTI, the shared kernel mappings</span>
<span class="p_add">+     are created by copying top-level (PGD) entries into each</span>
<span class="p_add">+     new process.  But, with PTI, there are now *two* kernel</span>
<span class="p_add">+     mappings: one in the kernel page tables that maps everything</span>
<span class="p_add">+     and one for the entry/exit structures.  At fork(), we need to</span>
<span class="p_add">+     copy both.</span>
<span class="p_add">+  f. In addition to the fork()-time copying, there must also</span>
<span class="p_add">+     be an update to the userspace PGD any time a set_pgd() is done</span>
<span class="p_add">+     on a PGD used to map userspace.  This ensures that the kernel</span>
<span class="p_add">+     and userspace copies always map the same userspace</span>
<span class="p_add">+     memory.</span>
<span class="p_add">+  g. On systems without PCID support, each CR3 write flushes</span>
<span class="p_add">+     the entire TLB.  That means that each syscall, interrupt</span>
<span class="p_add">+     or exception flushes the TLB.</span>
<span class="p_add">+  h. INVPCID is a TLB-flushing instruction which allows flushing</span>
<span class="p_add">+     of TLB entries for non-current PCIDs.  Some systems support</span>
<span class="p_add">+     PCIDs, but do not support INVPCID.  On these systems, addresses</span>
<span class="p_add">+     can only be flushed from the TLB for the current PCID.  When</span>
<span class="p_add">+     flushing a kernel address, we need to flush all PCIDs, so a</span>
<span class="p_add">+     single kernel address flush will require a TLB-flushing CR3</span>
<span class="p_add">+     write upon the next use of every PCID.</span>
<span class="p_add">+</span>
<span class="p_add">+Possible Future Work</span>
<span class="p_add">+====================</span>
<span class="p_add">+1. We can be more careful about not actually writing to CR3</span>
<span class="p_add">+   unless its value is actually changed.</span>
<span class="p_add">+2. Allow PTI to be enabled/disabled at runtime in addition to the</span>
<span class="p_add">+   boot-time switching.</span>
<span class="p_add">+</span>
<span class="p_add">+Testing</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+To test stability of PTI, the following test procedure is recommended,</span>
<span class="p_add">+ideally doing all of these in parallel:</span>
<span class="p_add">+</span>
<span class="p_add">+1. Set CONFIG_DEBUG_ENTRY=y</span>
<span class="p_add">+2. Run several copies of all of the tools/testing/selftests/x86/ tests</span>
<span class="p_add">+   (excluding MPX and protection_keys) in a loop on multiple CPUs for</span>
<span class="p_add">+   several minutes.  These tests frequently uncover corner cases in the</span>
<span class="p_add">+   kernel entry code.  In general, old kernels might cause these tests</span>
<span class="p_add">+   themselves to crash, but they should never crash the kernel.</span>
<span class="p_add">+3. Run the &#39;perf&#39; tool in a mode (top or record) that generates many</span>
<span class="p_add">+   frequent performance monitoring non-maskable interrupts (see &quot;NMI&quot;</span>
<span class="p_add">+   in /proc/interrupts).  This exercises the NMI entry/exit code which</span>
<span class="p_add">+   is known to trigger bugs in code paths that did not expect to be</span>
<span class="p_add">+   interrupted, including nested NMIs.  Using &quot;-c&quot; boosts the rate of</span>
<span class="p_add">+   NMIs, and using two -c with separate counters encourages nested NMIs</span>
<span class="p_add">+   and less deterministic behavior.</span>
<span class="p_add">+</span>
<span class="p_add">+	while true; do perf record -c 10000 -e instructions,cycles -a sleep 10; done</span>
<span class="p_add">+</span>
<span class="p_add">+4. Launch a KVM virtual machine.</span>
<span class="p_add">+5. Run 32-bit binaries on systems supporting the SYSCALL instruction.</span>
<span class="p_add">+   This has been a lightly-tested code path and needs extra scrutiny.</span>
<span class="p_add">+</span>
<span class="p_add">+Debugging</span>
<span class="p_add">+=========</span>
<span class="p_add">+</span>
<span class="p_add">+Bugs in PTI cause a few different signatures of crashes</span>
<span class="p_add">+that are worth noting here.</span>
<span class="p_add">+</span>
<span class="p_add">+ * Failures of the selftests/x86 code.  Usually a bug in one of the</span>
<span class="p_add">+   more obscure corners of entry_64.S</span>
<span class="p_add">+ * Crashes in early boot, especially around CPU bringup.  Bugs</span>
<span class="p_add">+   in the trampoline code or mappings cause these.</span>
<span class="p_add">+ * Crashes at the first interrupt.  Caused by bugs in entry_64.S,</span>
<span class="p_add">+   like screwing up a page table switch.  Also caused by</span>
<span class="p_add">+   incorrectly mapping the IRQ handler entry code.</span>
<span class="p_add">+ * Crashes at the first NMI.  The NMI code is separate from main</span>
<span class="p_add">+   interrupt handlers and can have bugs that do not affect</span>
<span class="p_add">+   normal interrupts.  Also caused by incorrectly mapping NMI</span>
<span class="p_add">+   code.  NMIs that interrupt the entry code must be very</span>
<span class="p_add">+   careful and can be the cause of crashes that show up when</span>
<span class="p_add">+   running perf.</span>
<span class="p_add">+ * Kernel crashes at the first exit to userspace.  entry_64.S</span>
<span class="p_add">+   bugs, or failing to map some of the exit code.</span>
<span class="p_add">+ * Crashes at first interrupt that interrupts userspace. The paths</span>
<span class="p_add">+   in entry_64.S that return to userspace are sometimes separate</span>
<span class="p_add">+   from the ones that return to the kernel.</span>
<span class="p_add">+ * Double faults: overflowing the kernel stack because of page</span>
<span class="p_add">+   faults upon page faults.  Caused by touching non-pti-mapped</span>
<span class="p_add">+   data in the entry code, or forgetting to switch to kernel</span>
<span class="p_add">+   CR3 before calling into C functions which are not pti-mapped.</span>
<span class="p_add">+ * Userspace segfaults early in boot, sometimes manifesting</span>
<span class="p_add">+   as mount(8) failing to mount the rootfs.  These have</span>
<span class="p_add">+   tended to be TLB invalidation issues.  Usually invalidating</span>
<span class="p_add">+   the wrong PCID, or otherwise missing an invalidation.</span>
<span class="p_add">+</span>
<span class="p_add">+1. https://gruss.cc/files/kaiser.pdf</span>
<span class="p_add">+2. https://meltdownattack.com/meltdown.pdf</span>
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index cd5199de231e..d1819161cc6c 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -89,6 +89,7 @@</span> <span class="p_context"> config X86</span>
 	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 	select GENERIC_CMOS_UPDATE
 	select GENERIC_CPU_AUTOPROBE
<span class="p_add">+	select GENERIC_CPU_VULNERABILITIES</span>
 	select GENERIC_EARLY_IOREMAP
 	select GENERIC_FIND_FIRST_BIT
 	select GENERIC_IOMAP
<span class="p_chunk">@@ -428,6 +429,19 @@</span> <span class="p_context"> config GOLDFISH</span>
        def_bool y
        depends on X86_GOLDFISH
 
<span class="p_add">+config RETPOLINE</span>
<span class="p_add">+	bool &quot;Avoid speculative indirect branches in kernel&quot;</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Compile kernel with the retpoline compiler options to guard against</span>
<span class="p_add">+	  kernel-to-user data leaks by avoiding speculative indirect</span>
<span class="p_add">+	  branches. Requires a compiler with -mindirect-branch=thunk-extern</span>
<span class="p_add">+	  support for full protection. The kernel may run slower.</span>
<span class="p_add">+</span>
<span class="p_add">+	  Without compiler support, at least indirect branches in assembler</span>
<span class="p_add">+	  code are eliminated. Since this includes the syscall entry path,</span>
<span class="p_add">+	  it is not entirely pointless.</span>
<span class="p_add">+</span>
 config INTEL_RDT
 	bool &quot;Intel Resource Director Technology support&quot;
 	default n
<span class="p_header">diff --git a/arch/x86/Makefile b/arch/x86/Makefile</span>
<span class="p_header">index a20eacd9c7e9..974c61864978 100644</span>
<span class="p_header">--- a/arch/x86/Makefile</span>
<span class="p_header">+++ b/arch/x86/Makefile</span>
<span class="p_chunk">@@ -235,6 +235,16 @@</span> <span class="p_context"> KBUILD_CFLAGS += -Wno-sign-compare</span>
 #
 KBUILD_CFLAGS += -fno-asynchronous-unwind-tables
 
<span class="p_add">+# Avoid indirect branches in kernel to deal with Spectre</span>
<span class="p_add">+ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+    RETPOLINE_CFLAGS += $(call cc-option,-mindirect-branch=thunk-extern -mindirect-branch-register)</span>
<span class="p_add">+    ifneq ($(RETPOLINE_CFLAGS),)</span>
<span class="p_add">+        KBUILD_CFLAGS += $(RETPOLINE_CFLAGS) -DRETPOLINE</span>
<span class="p_add">+    else</span>
<span class="p_add">+        $(warning CONFIG_RETPOLINE=y, but not supported by the compiler. Toolchain update recommended.)</span>
<span class="p_add">+    endif</span>
<span class="p_add">+endif</span>
<span class="p_add">+</span>
 archscripts: scripts_basic
 	$(Q)$(MAKE) $(build)=arch/x86/tools relocs
 
<span class="p_header">diff --git a/arch/x86/crypto/aesni-intel_asm.S b/arch/x86/crypto/aesni-intel_asm.S</span>
<span class="p_header">index 16627fec80b2..3d09e3aca18d 100644</span>
<span class="p_header">--- a/arch/x86/crypto/aesni-intel_asm.S</span>
<span class="p_header">+++ b/arch/x86/crypto/aesni-intel_asm.S</span>
<span class="p_chunk">@@ -32,6 +32,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/linkage.h&gt;
 #include &lt;asm/inst.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 /*
  * The following macros are used to move an (un)aligned 16 byte value to/from
<span class="p_chunk">@@ -2884,7 +2885,7 @@</span> <span class="p_context"> ENTRY(aesni_xts_crypt8)</span>
 	pxor INC, STATE4
 	movdqu IV, 0x30(OUTP)
 
<span class="p_del">-	call *%r11</span>
<span class="p_add">+	CALL_NOSPEC %r11</span>
 
 	movdqu 0x00(OUTP), INC
 	pxor INC, STATE1
<span class="p_chunk">@@ -2929,7 +2930,7 @@</span> <span class="p_context"> ENTRY(aesni_xts_crypt8)</span>
 	_aesni_gf128mul_x_ble()
 	movups IV, (IVP)
 
<span class="p_del">-	call *%r11</span>
<span class="p_add">+	CALL_NOSPEC %r11</span>
 
 	movdqu 0x40(OUTP), INC
 	pxor INC, STATE1
<span class="p_header">diff --git a/arch/x86/crypto/camellia-aesni-avx-asm_64.S b/arch/x86/crypto/camellia-aesni-avx-asm_64.S</span>
<span class="p_header">index f7c495e2863c..a14af6eb09cb 100644</span>
<span class="p_header">--- a/arch/x86/crypto/camellia-aesni-avx-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/crypto/camellia-aesni-avx-asm_64.S</span>
<span class="p_chunk">@@ -17,6 +17,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/linkage.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #define CAMELLIA_TABLE_BYTE_LEN 272
 
<span class="p_chunk">@@ -1227,7 +1228,7 @@</span> <span class="p_context"> camellia_xts_crypt_16way:</span>
 	vpxor 14 * 16(%rax), %xmm15, %xmm14;
 	vpxor 15 * 16(%rax), %xmm15, %xmm15;
 
<span class="p_del">-	call *%r9;</span>
<span class="p_add">+	CALL_NOSPEC %r9;</span>
 
 	addq $(16 * 16), %rsp;
 
<span class="p_header">diff --git a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S</span>
<span class="p_header">index eee5b3982cfd..b66bbfa62f50 100644</span>
<span class="p_header">--- a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/linkage.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #define CAMELLIA_TABLE_BYTE_LEN 272
 
<span class="p_chunk">@@ -1343,7 +1344,7 @@</span> <span class="p_context"> camellia_xts_crypt_32way:</span>
 	vpxor 14 * 32(%rax), %ymm15, %ymm14;
 	vpxor 15 * 32(%rax), %ymm15, %ymm15;
 
<span class="p_del">-	call *%r9;</span>
<span class="p_add">+	CALL_NOSPEC %r9;</span>
 
 	addq $(16 * 32), %rsp;
 
<span class="p_header">diff --git a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S</span>
<span class="p_header">index 7a7de27c6f41..d9b734d0c8cc 100644</span>
<span class="p_header">--- a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/inst.h&gt;
 #include &lt;linux/linkage.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 ## ISCSI CRC 32 Implementation with crc32 and pclmulqdq Instruction
 
<span class="p_chunk">@@ -172,7 +173,7 @@</span> <span class="p_context"> continue_block:</span>
 	movzxw  (bufp, %rax, 2), len
 	lea	crc_array(%rip), bufp
 	lea     (bufp, len, 1), bufp
<span class="p_del">-	jmp     *bufp</span>
<span class="p_add">+	JMP_NOSPEC bufp</span>
 
 	################################################################
 	## 2a) PROCESS FULL BLOCKS:
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 45a63e00a6af..3f48f695d5e6 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -198,8 +198,11 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
  * PAGE_TABLE_ISOLATION PGDs are 8k.  Flip bit 12 to switch between the two
  * halves:
  */
<span class="p_del">-#define PTI_SWITCH_PGTABLES_MASK	(1&lt;&lt;PAGE_SHIFT)</span>
<span class="p_del">-#define PTI_SWITCH_MASK		(PTI_SWITCH_PGTABLES_MASK|(1&lt;&lt;X86_CR3_PTI_SWITCH_BIT))</span>
<span class="p_add">+#define PTI_USER_PGTABLE_BIT		PAGE_SHIFT</span>
<span class="p_add">+#define PTI_USER_PGTABLE_MASK		(1 &lt;&lt; PTI_USER_PGTABLE_BIT)</span>
<span class="p_add">+#define PTI_USER_PCID_BIT		X86_CR3_PTI_PCID_USER_BIT</span>
<span class="p_add">+#define PTI_USER_PCID_MASK		(1 &lt;&lt; PTI_USER_PCID_BIT)</span>
<span class="p_add">+#define PTI_USER_PGTABLE_AND_PCID_MASK  (PTI_USER_PCID_MASK | PTI_USER_PGTABLE_MASK)</span>
 
 .macro SET_NOFLUSH_BIT	reg:req
 	bts	$X86_CR3_PCID_NOFLUSH_BIT, \reg
<span class="p_chunk">@@ -208,7 +211,7 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 .macro ADJUST_KERNEL_CR3 reg:req
 	ALTERNATIVE &quot;&quot;, &quot;SET_NOFLUSH_BIT \reg&quot;, X86_FEATURE_PCID
 	/* Clear PCID and &quot;PAGE_TABLE_ISOLATION bit&quot;, point CR3 at kernel pagetables: */
<span class="p_del">-	andq    $(~PTI_SWITCH_MASK), \reg</span>
<span class="p_add">+	andq    $(~PTI_USER_PGTABLE_AND_PCID_MASK), \reg</span>
 .endm
 
 .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req
<span class="p_chunk">@@ -239,15 +242,19 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	/* Flush needed, clear the bit */
 	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask
 	movq	\scratch_reg2, \scratch_reg
<span class="p_del">-	jmp	.Lwrcr3_\@</span>
<span class="p_add">+	jmp	.Lwrcr3_pcid_\@</span>
 
 .Lnoflush_\@:
 	movq	\scratch_reg2, \scratch_reg
 	SET_NOFLUSH_BIT \scratch_reg
 
<span class="p_add">+.Lwrcr3_pcid_\@:</span>
<span class="p_add">+	/* Flip the ASID to the user version */</span>
<span class="p_add">+	orq	$(PTI_USER_PCID_MASK), \scratch_reg</span>
<span class="p_add">+</span>
 .Lwrcr3_\@:
<span class="p_del">-	/* Flip the PGD and ASID to the user version */</span>
<span class="p_del">-	orq     $(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_add">+	/* Flip the PGD to the user version */</span>
<span class="p_add">+	orq     $(PTI_USER_PGTABLE_MASK), \scratch_reg</span>
 	mov	\scratch_reg, %cr3
 .Lend_\@:
 .endm
<span class="p_chunk">@@ -263,17 +270,12 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	movq	%cr3, \scratch_reg
 	movq	\scratch_reg, \save_reg
 	/*
<span class="p_del">-	 * Is the &quot;switch mask&quot; all zero?  That means that both of</span>
<span class="p_del">-	 * these are zero:</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *	1. The user/kernel PCID bit, and</span>
<span class="p_del">-	 *	2. The user/kernel &quot;bit&quot; that points CR3 to the</span>
<span class="p_del">-	 *	   bottom half of the 8k PGD</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * That indicates a kernel CR3 value, not a user CR3.</span>
<span class="p_add">+	 * Test the user pagetable bit. If set, then the user page tables</span>
<span class="p_add">+	 * are active. If clear CR3 already has the kernel page table</span>
<span class="p_add">+	 * active.</span>
 	 */
<span class="p_del">-	testq	$(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_del">-	jz	.Ldone_\@</span>
<span class="p_add">+	bt	$PTI_USER_PGTABLE_BIT, \scratch_reg</span>
<span class="p_add">+	jnc	.Ldone_\@</span>
 
 	ADJUST_KERNEL_CR3 \scratch_reg
 	movq	\scratch_reg, %cr3
<span class="p_chunk">@@ -290,7 +292,7 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	 * KERNEL pages can always resume with NOFLUSH as we do
 	 * explicit flushes.
 	 */
<span class="p_del">-	bt	$X86_CR3_PTI_SWITCH_BIT, \save_reg</span>
<span class="p_add">+	bt	$PTI_USER_PGTABLE_BIT, \save_reg</span>
 	jnc	.Lnoflush_\@
 
 	/*
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index ace8f321a5a1..a1f28a54f23a 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 	.section .entry.text, &quot;ax&quot;
 
<span class="p_chunk">@@ -290,7 +291,7 @@</span> <span class="p_context"> ENTRY(ret_from_fork)</span>
 
 	/* kernel thread */
 1:	movl	%edi, %eax
<span class="p_del">-	call	*%ebx</span>
<span class="p_add">+	CALL_NOSPEC %ebx</span>
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
<span class="p_chunk">@@ -919,7 +920,7 @@</span> <span class="p_context"> common_exception:</span>
 	movl	%ecx, %es
 	TRACE_IRQS_OFF
 	movl	%esp, %eax			# pt_regs pointer
<span class="p_del">-	call	*%edi</span>
<span class="p_add">+	CALL_NOSPEC %edi</span>
 	jmp	ret_from_exception
 END(common_exception)
 
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index ed31d00dc5ee..59874bc1aed2 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable_types.h&gt;
 #include &lt;asm/export.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 #include &lt;linux/err.h&gt;
 
 #include &quot;calling.h&quot;
<span class="p_chunk">@@ -187,7 +188,7 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64_trampoline)</span>
 	 */
 	pushq	%rdi
 	movq	$entry_SYSCALL_64_stage2, %rdi
<span class="p_del">-	jmp	*%rdi</span>
<span class="p_add">+	JMP_NOSPEC %rdi</span>
 END(entry_SYSCALL_64_trampoline)
 
 	.popsection
<span class="p_chunk">@@ -266,7 +267,12 @@</span> <span class="p_context"> entry_SYSCALL_64_fastpath:</span>
 	 * It might end up jumping to the slow path.  If it jumps, RAX
 	 * and all argument registers are clobbered.
 	 */
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	movq	sys_call_table(, %rax, 8), %rax</span>
<span class="p_add">+	call	__x86_indirect_thunk_rax</span>
<span class="p_add">+#else</span>
 	call	*sys_call_table(, %rax, 8)
<span class="p_add">+#endif</span>
 .Lentry_SYSCALL_64_after_fastpath_call:
 
 	movq	%rax, RAX(%rsp)
<span class="p_chunk">@@ -438,7 +444,7 @@</span> <span class="p_context"> ENTRY(stub_ptregs_64)</span>
 	jmp	entry_SYSCALL64_slow_path
 
 1:
<span class="p_del">-	jmp	*%rax				/* Called from C */</span>
<span class="p_add">+	JMP_NOSPEC %rax				/* Called from C */</span>
 END(stub_ptregs_64)
 
 .macro ptregs_stub func
<span class="p_chunk">@@ -517,7 +523,7 @@</span> <span class="p_context"> ENTRY(ret_from_fork)</span>
 1:
 	/* kernel thread */
 	movq	%r12, %rdi
<span class="p_del">-	call	*%rbx</span>
<span class="p_add">+	CALL_NOSPEC %rbx</span>
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
<span class="p_header">diff --git a/arch/x86/events/intel/bts.c b/arch/x86/events/intel/bts.c</span>
<span class="p_header">index 141e07b06216..24ffa1e88cf9 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/bts.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/bts.c</span>
<span class="p_chunk">@@ -582,6 +582,24 @@</span> <span class="p_context"> static __init int bts_init(void)</span>
 	if (!boot_cpu_has(X86_FEATURE_DTES64) || !x86_pmu.bts)
 		return -ENODEV;
 
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * BTS hardware writes through a virtual memory map we must</span>
<span class="p_add">+		 * either use the kernel physical map, or the user mapping of</span>
<span class="p_add">+		 * the AUX buffer.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * However, since this driver supports per-CPU and per-task inherit</span>
<span class="p_add">+		 * we cannot use the user mapping since it will not be availble</span>
<span class="p_add">+		 * if we&#39;re not running the owning process.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * With PTI we can&#39;t use the kernal map either, because its not</span>
<span class="p_add">+		 * there when we run userspace.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * For now, disable this driver when using PTI.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	bts_pmu.capabilities	= PERF_PMU_CAP_AUX_NO_SG | PERF_PMU_CAP_ITRACE |
 				  PERF_PMU_CAP_EXCLUSIVE;
 	bts_pmu.task_ctx_nr	= perf_sw_context;
<span class="p_header">diff --git a/arch/x86/include/asm/asm-prototypes.h b/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_header">index ff700d81e91e..0927cdc4f946 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_chunk">@@ -11,7 +11,32 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/special_insns.h&gt;
 #include &lt;asm/preempt.h&gt;
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
 
 #ifndef CONFIG_X86_CMPXCHG64
 extern void cmpxchg8b_emu(void);
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+#define INDIRECT_THUNK(reg) extern asmlinkage void __x86_indirect_thunk_e ## reg(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define INDIRECT_THUNK(reg) extern asmlinkage void __x86_indirect_thunk_r ## reg(void);</span>
<span class="p_add">+INDIRECT_THUNK(8)</span>
<span class="p_add">+INDIRECT_THUNK(9)</span>
<span class="p_add">+INDIRECT_THUNK(10)</span>
<span class="p_add">+INDIRECT_THUNK(11)</span>
<span class="p_add">+INDIRECT_THUNK(12)</span>
<span class="p_add">+INDIRECT_THUNK(13)</span>
<span class="p_add">+INDIRECT_THUNK(14)</span>
<span class="p_add">+INDIRECT_THUNK(15)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+INDIRECT_THUNK(ax)</span>
<span class="p_add">+INDIRECT_THUNK(bx)</span>
<span class="p_add">+INDIRECT_THUNK(cx)</span>
<span class="p_add">+INDIRECT_THUNK(dx)</span>
<span class="p_add">+INDIRECT_THUNK(si)</span>
<span class="p_add">+INDIRECT_THUNK(di)</span>
<span class="p_add">+INDIRECT_THUNK(bp)</span>
<span class="p_add">+INDIRECT_THUNK(sp)</span>
<span class="p_add">+#endif /* CONFIG_RETPOLINE */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 21ac898df2d8..f275447862f4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -203,6 +203,8 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 #define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
 #define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
<span class="p_add">+#define X86_FEATURE_RETPOLINE		( 7*32+12) /* Generic Retpoline mitigation for Spectre variant 2 */</span>
<span class="p_add">+#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */</span>
 #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 #define X86_FEATURE_INTEL_PT		( 7*32+15) /* Intel Processor Trace */
 #define X86_FEATURE_AVX512_4VNNIW	( 7*32+16) /* AVX-512 Neural Network Instructions */
<span class="p_chunk">@@ -342,5 +344,7 @@</span> <span class="p_context"></span>
 #define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */
 #define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */
 #define X86_BUG_CPU_MELTDOWN		X86_BUG(14) /* CPU is affected by meltdown attack and needs kernel page table isolation */
<span class="p_add">+#define X86_BUG_SPECTRE_V1		X86_BUG(15) /* CPU is affected by Spectre variant 1 attack with conditional branches */</span>
<span class="p_add">+#define X86_BUG_SPECTRE_V2		X86_BUG(16) /* CPU is affected by Spectre variant 2 attack with indirect branches */</span>
 
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/mshyperv.h b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">index 581bb54dd464..5119e4b555cc 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/nmi.h&gt;
 #include &lt;asm/io.h&gt;
 #include &lt;asm/hyperv.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 /*
  * The below CPUID leaves are present if VersionAndFeatures.HypervisorPresent
<span class="p_chunk">@@ -186,10 +187,11 @@</span> <span class="p_context"> static inline u64 hv_do_hypercall(u64 control, void *input, void *output)</span>
 		return U64_MAX;
 
 	__asm__ __volatile__(&quot;mov %4, %%r8\n&quot;
<span class="p_del">-			     &quot;call *%5&quot;</span>
<span class="p_add">+			     CALL_NOSPEC</span>
 			     : &quot;=a&quot; (hv_status), ASM_CALL_CONSTRAINT,
 			       &quot;+c&quot; (control), &quot;+d&quot; (input_address)
<span class="p_del">-			     :  &quot;r&quot; (output_address), &quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+			     :  &quot;r&quot; (output_address),</span>
<span class="p_add">+				THUNK_TARGET(hv_hypercall_pg)</span>
 			     : &quot;cc&quot;, &quot;memory&quot;, &quot;r8&quot;, &quot;r9&quot;, &quot;r10&quot;, &quot;r11&quot;);
 #else
 	u32 input_address_hi = upper_32_bits(input_address);
<span class="p_chunk">@@ -200,13 +202,13 @@</span> <span class="p_context"> static inline u64 hv_do_hypercall(u64 control, void *input, void *output)</span>
 	if (!hv_hypercall_pg)
 		return U64_MAX;
 
<span class="p_del">-	__asm__ __volatile__(&quot;call *%7&quot;</span>
<span class="p_add">+	__asm__ __volatile__(CALL_NOSPEC</span>
 			     : &quot;=A&quot; (hv_status),
 			       &quot;+c&quot; (input_address_lo), ASM_CALL_CONSTRAINT
 			     : &quot;A&quot; (control),
 			       &quot;b&quot; (input_address_hi),
 			       &quot;D&quot;(output_address_hi), &quot;S&quot;(output_address_lo),
<span class="p_del">-			       &quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+			       THUNK_TARGET(hv_hypercall_pg)</span>
 			     : &quot;cc&quot;, &quot;memory&quot;);
 #endif /* !x86_64 */
 	return hv_status;
<span class="p_chunk">@@ -227,10 +229,10 @@</span> <span class="p_context"> static inline u64 hv_do_fast_hypercall8(u16 code, u64 input1)</span>
 
 #ifdef CONFIG_X86_64
 	{
<span class="p_del">-		__asm__ __volatile__(&quot;call *%4&quot;</span>
<span class="p_add">+		__asm__ __volatile__(CALL_NOSPEC</span>
 				     : &quot;=a&quot; (hv_status), ASM_CALL_CONSTRAINT,
 				       &quot;+c&quot; (control), &quot;+d&quot; (input1)
<span class="p_del">-				     : &quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+				     : THUNK_TARGET(hv_hypercall_pg)</span>
 				     : &quot;cc&quot;, &quot;r8&quot;, &quot;r9&quot;, &quot;r10&quot;, &quot;r11&quot;);
 	}
 #else
<span class="p_chunk">@@ -238,13 +240,13 @@</span> <span class="p_context"> static inline u64 hv_do_fast_hypercall8(u16 code, u64 input1)</span>
 		u32 input1_hi = upper_32_bits(input1);
 		u32 input1_lo = lower_32_bits(input1);
 
<span class="p_del">-		__asm__ __volatile__ (&quot;call *%5&quot;</span>
<span class="p_add">+		__asm__ __volatile__ (CALL_NOSPEC</span>
 				      : &quot;=A&quot;(hv_status),
 					&quot;+c&quot;(input1_lo),
 					ASM_CALL_CONSTRAINT
 				      :	&quot;A&quot; (control),
 					&quot;b&quot; (input1_hi),
<span class="p_del">-					&quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+					THUNK_TARGET(hv_hypercall_pg)</span>
 				      : &quot;cc&quot;, &quot;edi&quot;, &quot;esi&quot;);
 	}
 #endif
<span class="p_header">diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h</span>
<span class="p_header">index ab022618a50a..fa11fb1fa570 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/msr-index.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/msr-index.h</span>
<span class="p_chunk">@@ -352,6 +352,9 @@</span> <span class="p_context"></span>
 #define FAM10H_MMIO_CONF_BASE_MASK	0xfffffffULL
 #define FAM10H_MMIO_CONF_BASE_SHIFT	20
 #define MSR_FAM10H_NODE_ID		0xc001100c
<span class="p_add">+#define MSR_F10H_DECFG			0xc0011029</span>
<span class="p_add">+#define MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT	1</span>
<span class="p_add">+#define MSR_F10H_DECFG_LFENCE_SERIALIZE		BIT_ULL(MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT)</span>
 
 /* K8 MSRs */
 #define MSR_K8_TOP_MEM1			0xc001001a
<span class="p_header">diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h</span>
new file mode 100644
<span class="p_header">index 000000000000..402a11c803c3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_chunk">@@ -0,0 +1,214 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __NOSPEC_BRANCH_H__</span>
<span class="p_add">+#define __NOSPEC_BRANCH_H__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/alternative.h&gt;</span>
<span class="p_add">+#include &lt;asm/alternative-asm.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Fill the CPU return stack buffer.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each entry in the RSB, if used for a speculative &#39;ret&#39;, contains an</span>
<span class="p_add">+ * infinite &#39;pause; jmp&#39; loop to capture speculative execution.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is required in various cases for retpoline and IBRS-based</span>
<span class="p_add">+ * mitigations for the Spectre variant 2 vulnerability. Sometimes to</span>
<span class="p_add">+ * eliminate potentially bogus entries from the RSB, and sometimes</span>
<span class="p_add">+ * purely to ensure that it doesn&#39;t get empty, which on some CPUs would</span>
<span class="p_add">+ * allow predictions from other (unwanted!) sources to be used.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We define a CPP macro such that it can be used from both .S files and</span>
<span class="p_add">+ * inline assembly. It&#39;s possible to do a .macro and then include that</span>
<span class="p_add">+ * from C via asm(&quot;.include &lt;asm/nospec-branch.h&gt;&quot;) but let&#39;s not go there.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define RSB_CLEAR_LOOPS		32	/* To forcibly overwrite all entries */</span>
<span class="p_add">+#define RSB_FILL_LOOPS		16	/* To avoid underflow */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Google experimented with loop-unrolling and this turned out to be</span>
<span class="p_add">+ * the optimal version  two calls, each with their own speculation</span>
<span class="p_add">+ * trap should their return address end up getting used, in a loop.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __FILL_RETURN_BUFFER(reg, nr, sp)	\</span>
<span class="p_add">+	mov	$(nr/2), reg;			\</span>
<span class="p_add">+771:						\</span>
<span class="p_add">+	call	772f;				\</span>
<span class="p_add">+773:	/* speculation trap */			\</span>
<span class="p_add">+	pause;					\</span>
<span class="p_add">+	jmp	773b;				\</span>
<span class="p_add">+772:						\</span>
<span class="p_add">+	call	774f;				\</span>
<span class="p_add">+775:	/* speculation trap */			\</span>
<span class="p_add">+	pause;					\</span>
<span class="p_add">+	jmp	775b;				\</span>
<span class="p_add">+774:						\</span>
<span class="p_add">+	dec	reg;				\</span>
<span class="p_add">+	jnz	771b;				\</span>
<span class="p_add">+	add	$(BITS_PER_LONG/8) * nr, sp;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This should be used immediately before a retpoline alternative.  It tells</span>
<span class="p_add">+ * objtool where the retpolines are so that it can make sense of the control</span>
<span class="p_add">+ * flow by just reading the original instruction(s) and ignoring the</span>
<span class="p_add">+ * alternatives.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	.Lannotate_\@:</span>
<span class="p_add">+	.pushsection .discard.nospec</span>
<span class="p_add">+	.long .Lannotate_\@ - .</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These are the bare retpoline primitives for indirect jmp and call.</span>
<span class="p_add">+ * Do not use these directly; they only exist to make the ALTERNATIVE</span>
<span class="p_add">+ * invocation below less ugly.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro RETPOLINE_JMP reg:req</span>
<span class="p_add">+	call	.Ldo_rop_\@</span>
<span class="p_add">+.Lspec_trap_\@:</span>
<span class="p_add">+	pause</span>
<span class="p_add">+	jmp	.Lspec_trap_\@</span>
<span class="p_add">+.Ldo_rop_\@:</span>
<span class="p_add">+	mov	\reg, (%_ASM_SP)</span>
<span class="p_add">+	ret</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is a wrapper around RETPOLINE_JMP so the called function in reg</span>
<span class="p_add">+ * returns to the instruction after the macro.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro RETPOLINE_CALL reg:req</span>
<span class="p_add">+	jmp	.Ldo_call_\@</span>
<span class="p_add">+.Ldo_retpoline_jmp_\@:</span>
<span class="p_add">+	RETPOLINE_JMP \reg</span>
<span class="p_add">+.Ldo_call_\@:</span>
<span class="p_add">+	call	.Ldo_retpoline_jmp_\@</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * JMP_NOSPEC and CALL_NOSPEC macros can be used instead of a simple</span>
<span class="p_add">+ * indirect jmp/call which may be susceptible to the Spectre variant 2</span>
<span class="p_add">+ * attack.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro JMP_NOSPEC reg:req</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	ALTERNATIVE_2 __stringify(jmp *\reg),				\</span>
<span class="p_add">+		__stringify(RETPOLINE_JMP \reg), X86_FEATURE_RETPOLINE,	\</span>
<span class="p_add">+		__stringify(lfence; jmp *\reg), X86_FEATURE_RETPOLINE_AMD</span>
<span class="p_add">+#else</span>
<span class="p_add">+	jmp	*\reg</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro CALL_NOSPEC reg:req</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	ALTERNATIVE_2 __stringify(call *\reg),				\</span>
<span class="p_add">+		__stringify(RETPOLINE_CALL \reg), X86_FEATURE_RETPOLINE,\</span>
<span class="p_add">+		__stringify(lfence; call *\reg), X86_FEATURE_RETPOLINE_AMD</span>
<span class="p_add">+#else</span>
<span class="p_add">+	call	*\reg</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+ /*</span>
<span class="p_add">+  * A simpler FILL_RETURN_BUFFER macro. Don&#39;t make people use the CPP</span>
<span class="p_add">+  * monstrosity above, manually.</span>
<span class="p_add">+  */</span>
<span class="p_add">+.macro FILL_RETURN_BUFFER reg:req nr:req ftr:req</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lskip_rsb_\@&quot;,				\</span>
<span class="p_add">+		__stringify(__FILL_RETURN_BUFFER(\reg,\nr,%_ASM_SP))	\</span>
<span class="p_add">+		\ftr</span>
<span class="p_add">+.Lskip_rsb_\@:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define ANNOTATE_NOSPEC_ALTERNATIVE				\</span>
<span class="p_add">+	&quot;999:\n\t&quot;						\</span>
<span class="p_add">+	&quot;.pushsection .discard.nospec\n\t&quot;			\</span>
<span class="p_add">+	&quot;.long 999b - .\n\t&quot;					\</span>
<span class="p_add">+	&quot;.popsection\n\t&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_X86_64) &amp;&amp; defined(RETPOLINE)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Since the inline asm uses the %V modifier which is only in newer GCC,</span>
<span class="p_add">+ * the 64-bit one is dependent on RETPOLINE not CONFIG_RETPOLINE.</span>
<span class="p_add">+ */</span>
<span class="p_add">+# define CALL_NOSPEC						\</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE				\</span>
<span class="p_add">+	ALTERNATIVE(						\</span>
<span class="p_add">+	&quot;call *%[thunk_target]\n&quot;,				\</span>
<span class="p_add">+	&quot;call __x86_indirect_thunk_%V[thunk_target]\n&quot;,		\</span>
<span class="p_add">+	X86_FEATURE_RETPOLINE)</span>
<span class="p_add">+# define THUNK_TARGET(addr) [thunk_target] &quot;r&quot; (addr)</span>
<span class="p_add">+</span>
<span class="p_add">+#elif defined(CONFIG_X86_32) &amp;&amp; defined(CONFIG_RETPOLINE)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * For i386 we use the original ret-equivalent retpoline, because</span>
<span class="p_add">+ * otherwise we&#39;ll run out of registers. We don&#39;t care about CET</span>
<span class="p_add">+ * here, anyway.</span>
<span class="p_add">+ */</span>
<span class="p_add">+# define CALL_NOSPEC ALTERNATIVE(&quot;call *%[thunk_target]\n&quot;,	\</span>
<span class="p_add">+	&quot;       jmp    904f;\n&quot;					\</span>
<span class="p_add">+	&quot;       .align 16\n&quot;					\</span>
<span class="p_add">+	&quot;901:	call   903f;\n&quot;					\</span>
<span class="p_add">+	&quot;902:	pause;\n&quot;					\</span>
<span class="p_add">+	&quot;       jmp    902b;\n&quot;					\</span>
<span class="p_add">+	&quot;       .align 16\n&quot;					\</span>
<span class="p_add">+	&quot;903:	addl   $4, %%esp;\n&quot;				\</span>
<span class="p_add">+	&quot;       pushl  %[thunk_target];\n&quot;			\</span>
<span class="p_add">+	&quot;       ret;\n&quot;						\</span>
<span class="p_add">+	&quot;       .align 16\n&quot;					\</span>
<span class="p_add">+	&quot;904:	call   901b;\n&quot;,				\</span>
<span class="p_add">+	X86_FEATURE_RETPOLINE)</span>
<span class="p_add">+</span>
<span class="p_add">+# define THUNK_TARGET(addr) [thunk_target] &quot;rm&quot; (addr)</span>
<span class="p_add">+#else /* No retpoline for C / inline asm */</span>
<span class="p_add">+# define CALL_NOSPEC &quot;call *%[thunk_target]\n&quot;</span>
<span class="p_add">+# define THUNK_TARGET(addr) [thunk_target] &quot;rm&quot; (addr)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* The Spectre V2 mitigation variants */</span>
<span class="p_add">+enum spectre_v2_mitigation {</span>
<span class="p_add">+	SPECTRE_V2_NONE,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_MINIMAL,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_MINIMAL_AMD,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_GENERIC,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_AMD,</span>
<span class="p_add">+	SPECTRE_V2_IBRS,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * On VMEXIT we must ensure that no RSB predictions learned in the guest</span>
<span class="p_add">+ * can be followed in the host, by overwriting the RSB completely. Both</span>
<span class="p_add">+ * retpoline and IBRS mitigations for Spectre v2 need this; only on future</span>
<span class="p_add">+ * CPUs with IBRS_ATT *might* it be avoided.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void vmexit_fill_RSB(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	unsigned long loops = RSB_CLEAR_LOOPS / 2;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile (ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+		      ALTERNATIVE(&quot;jmp 910f&quot;,</span>
<span class="p_add">+				  __stringify(__FILL_RETURN_BUFFER(%0, RSB_CLEAR_LOOPS, %1)),</span>
<span class="p_add">+				  X86_FEATURE_RETPOLINE)</span>
<span class="p_add">+		      &quot;910:&quot;</span>
<span class="p_add">+		      : &quot;=&amp;r&quot; (loops), ASM_CALL_CONSTRAINT</span>
<span class="p_add">+		      : &quot;r&quot; (loops) : &quot;memory&quot; );</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+#endif /* __NOSPEC_BRANCH_H__ */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/processor-flags.h b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">index 6a60fea90b9d..625a52a5594f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_chunk">@@ -40,7 +40,7 @@</span> <span class="p_context"></span>
 #define CR3_NOFLUSH	BIT_ULL(63)
 
 #ifdef CONFIG_PAGE_TABLE_ISOLATION
<span class="p_del">-# define X86_CR3_PTI_SWITCH_BIT	11</span>
<span class="p_add">+# define X86_CR3_PTI_PCID_USER_BIT	11</span>
 #endif
 
 #else
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index f9b48ce152eb..3effd3c994af 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -81,13 +81,13 @@</span> <span class="p_context"> static inline u16 kern_pcid(u16 asid)</span>
 	 * Make sure that the dynamic ASID space does not confict with the
 	 * bit we are using to switch between user and kernel ASIDs.
 	 */
<span class="p_del">-	BUILD_BUG_ON(TLB_NR_DYN_ASIDS &gt;= (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+	BUILD_BUG_ON(TLB_NR_DYN_ASIDS &gt;= (1 &lt;&lt; X86_CR3_PTI_PCID_USER_BIT));</span>
 
 	/*
 	 * The ASID being passed in here should have respected the
 	 * MAX_ASID_AVAILABLE and thus never have the switch bit set.
 	 */
<span class="p_del">-	VM_WARN_ON_ONCE(asid &amp; (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+	VM_WARN_ON_ONCE(asid &amp; (1 &lt;&lt; X86_CR3_PTI_PCID_USER_BIT));</span>
 #endif
 	/*
 	 * The dynamically-assigned ASIDs that get passed in are small
<span class="p_chunk">@@ -112,7 +112,7 @@</span> <span class="p_context"> static inline u16 user_pcid(u16 asid)</span>
 {
 	u16 ret = kern_pcid(asid);
 #ifdef CONFIG_PAGE_TABLE_ISOLATION
<span class="p_del">-	ret |= 1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT;</span>
<span class="p_add">+	ret |= 1 &lt;&lt; X86_CR3_PTI_PCID_USER_BIT;</span>
 #endif
 	return ret;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/xen/hypercall.h b/arch/x86/include/asm/xen/hypercall.h</span>
<span class="p_header">index 7cb282e9e587..bfd882617613 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/xen/hypercall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/xen/hypercall.h</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/smap.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &lt;xen/interface/xen.h&gt;
 #include &lt;xen/interface/sched.h&gt;
<span class="p_chunk">@@ -217,9 +218,9 @@</span> <span class="p_context"> privcmd_call(unsigned call,</span>
 	__HYPERCALL_5ARG(a1, a2, a3, a4, a5);
 
 	stac();
<span class="p_del">-	asm volatile(&quot;call *%[call]&quot;</span>
<span class="p_add">+	asm volatile(CALL_NOSPEC</span>
 		     : __HYPERCALL_5PARAM
<span class="p_del">-		     : [call] &quot;a&quot; (&amp;hypercall_page[call])</span>
<span class="p_add">+		     : [thunk_target] &quot;a&quot; (&amp;hypercall_page[call])</span>
 		     : __HYPERCALL_CLOBBER5);
 	clac();
 
<span class="p_header">diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c</span>
<span class="p_header">index 3344d3382e91..e0b97e4d1db5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/alternative.c</span>
<span class="p_header">+++ b/arch/x86/kernel/alternative.c</span>
<span class="p_chunk">@@ -344,9 +344,12 @@</span> <span class="p_context"> recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insnbuf)</span>
 static void __init_or_module noinline optimize_nops(struct alt_instr *a, u8 *instr)
 {
 	unsigned long flags;
<span class="p_add">+	int i;</span>
 
<span class="p_del">-	if (instr[0] != 0x90)</span>
<span class="p_del">-		return;</span>
<span class="p_add">+	for (i = 0; i &lt; a-&gt;padlen; i++) {</span>
<span class="p_add">+		if (instr[i] != 0x90)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
 
 	local_irq_save(flags);
 	add_nops(instr + (a-&gt;instrlen - a-&gt;padlen), a-&gt;padlen);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">index bcb75dc97d44..ea831c858195 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_chunk">@@ -829,8 +829,32 @@</span> <span class="p_context"> static void init_amd(struct cpuinfo_x86 *c)</span>
 		set_cpu_cap(c, X86_FEATURE_K8);
 
 	if (cpu_has(c, X86_FEATURE_XMM2)) {
<span class="p_del">-		/* MFENCE stops RDTSC speculation */</span>
<span class="p_del">-		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);</span>
<span class="p_add">+		unsigned long long val;</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * A serializing LFENCE has less overhead than MFENCE, so</span>
<span class="p_add">+		 * use it for execution serialization.  On families which</span>
<span class="p_add">+		 * don&#39;t have that MSR, LFENCE is already serializing.</span>
<span class="p_add">+		 * msr_set_bit() uses the safe accessors, too, even if the MSR</span>
<span class="p_add">+		 * is not present.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		msr_set_bit(MSR_F10H_DECFG,</span>
<span class="p_add">+			    MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Verify that the MSR write was successful (could be running</span>
<span class="p_add">+		 * under a hypervisor) and only then assume that LFENCE is</span>
<span class="p_add">+		 * serializing.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ret = rdmsrl_safe(MSR_F10H_DECFG, &amp;val);</span>
<span class="p_add">+		if (!ret &amp;&amp; (val &amp; MSR_F10H_DECFG_LFENCE_SERIALIZE)) {</span>
<span class="p_add">+			/* A serializing LFENCE stops RDTSC speculation */</span>
<span class="p_add">+			set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* MFENCE stops RDTSC speculation */</span>
<span class="p_add">+			set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);</span>
<span class="p_add">+		}</span>
 	}
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">index ba0b2424c9b0..e4dc26185aa7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_chunk">@@ -10,6 +10,10 @@</span> <span class="p_context"></span>
  */
 #include &lt;linux/init.h&gt;
 #include &lt;linux/utsname.h&gt;
<span class="p_add">+#include &lt;linux/cpu.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
 #include &lt;asm/bugs.h&gt;
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/processor-flags.h&gt;
<span class="p_chunk">@@ -20,6 +24,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/set_memory.h&gt;
 
<span class="p_add">+static void __init spectre_v2_select_mitigation(void);</span>
<span class="p_add">+</span>
 void __init check_bugs(void)
 {
 	identify_boot_cpu();
<span class="p_chunk">@@ -29,6 +35,9 @@</span> <span class="p_context"> void __init check_bugs(void)</span>
 		print_cpu_info(&amp;boot_cpu_data);
 	}
 
<span class="p_add">+	/* Select the proper spectre mitigation before patching alternatives */</span>
<span class="p_add">+	spectre_v2_select_mitigation();</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_32
 	/*
 	 * Check whether we are able to run this kernel safely on SMP.
<span class="p_chunk">@@ -60,3 +69,179 @@</span> <span class="p_context"> void __init check_bugs(void)</span>
 		set_memory_4k((unsigned long)__va(0), 1);
 #endif
 }
<span class="p_add">+</span>
<span class="p_add">+/* The kernel command line selection */</span>
<span class="p_add">+enum spectre_v2_mitigation_cmd {</span>
<span class="p_add">+	SPECTRE_V2_CMD_NONE,</span>
<span class="p_add">+	SPECTRE_V2_CMD_AUTO,</span>
<span class="p_add">+	SPECTRE_V2_CMD_FORCE,</span>
<span class="p_add">+	SPECTRE_V2_CMD_RETPOLINE,</span>
<span class="p_add">+	SPECTRE_V2_CMD_RETPOLINE_GENERIC,</span>
<span class="p_add">+	SPECTRE_V2_CMD_RETPOLINE_AMD,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const char *spectre_v2_strings[] = {</span>
<span class="p_add">+	[SPECTRE_V2_NONE]			= &quot;Vulnerable&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_MINIMAL]		= &quot;Vulnerable: Minimal generic ASM retpoline&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_MINIMAL_AMD]	= &quot;Vulnerable: Minimal AMD ASM retpoline&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_GENERIC]		= &quot;Mitigation: Full generic retpoline&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_AMD]		= &quot;Mitigation: Full AMD retpoline&quot;,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+#define pr_fmt(fmt)     &quot;Spectre V2 mitigation: &quot; fmt</span>
<span class="p_add">+</span>
<span class="p_add">+static enum spectre_v2_mitigation spectre_v2_enabled = SPECTRE_V2_NONE;</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init spec2_print_if_insecure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init spec2_print_if_secure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool retp_compiler(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __is_defined(RETPOLINE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool match_option(const char *arg, int arglen, const char *opt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int len = strlen(opt);</span>
<span class="p_add">+</span>
<span class="p_add">+	return len == arglen &amp;&amp; !strncmp(arg, opt, len);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char arg[20];</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = cmdline_find_option(boot_command_line, &quot;spectre_v2&quot;, arg,</span>
<span class="p_add">+				  sizeof(arg));</span>
<span class="p_add">+	if (ret &gt; 0)  {</span>
<span class="p_add">+		if (match_option(arg, ret, &quot;off&quot;)) {</span>
<span class="p_add">+			goto disable;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;on&quot;)) {</span>
<span class="p_add">+			spec2_print_if_secure(&quot;force enabled on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_FORCE;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;retpoline&quot;)) {</span>
<span class="p_add">+			spec2_print_if_insecure(&quot;retpoline selected on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_RETPOLINE;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;retpoline,amd&quot;)) {</span>
<span class="p_add">+			if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD) {</span>
<span class="p_add">+				pr_err(&quot;retpoline,amd selected but CPU is not AMD. Switching to AUTO select\n&quot;);</span>
<span class="p_add">+				return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			spec2_print_if_insecure(&quot;AMD retpoline selected on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_RETPOLINE_AMD;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;retpoline,generic&quot;)) {</span>
<span class="p_add">+			spec2_print_if_insecure(&quot;generic retpoline selected on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_RETPOLINE_GENERIC;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;auto&quot;)) {</span>
<span class="p_add">+			return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cmdline_find_option_bool(boot_command_line, &quot;nospectre_v2&quot;))</span>
<span class="p_add">+		return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+disable:</span>
<span class="p_add">+	spec2_print_if_insecure(&quot;disabled on command line.&quot;);</span>
<span class="p_add">+	return SPECTRE_V2_CMD_NONE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init spectre_v2_select_mitigation(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();</span>
<span class="p_add">+	enum spectre_v2_mitigation mode = SPECTRE_V2_NONE;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the CPU is not affected and the command line mode is NONE or AUTO</span>
<span class="p_add">+	 * then nothing to do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2) &amp;&amp;</span>
<span class="p_add">+	    (cmd == SPECTRE_V2_CMD_NONE || cmd == SPECTRE_V2_CMD_AUTO))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (cmd) {</span>
<span class="p_add">+	case SPECTRE_V2_CMD_NONE:</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	case SPECTRE_V2_CMD_FORCE:</span>
<span class="p_add">+		/* FALLTRHU */</span>
<span class="p_add">+	case SPECTRE_V2_CMD_AUTO:</span>
<span class="p_add">+		goto retpoline_auto;</span>
<span class="p_add">+</span>
<span class="p_add">+	case SPECTRE_V2_CMD_RETPOLINE_AMD:</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_RETPOLINE))</span>
<span class="p_add">+			goto retpoline_amd;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case SPECTRE_V2_CMD_RETPOLINE_GENERIC:</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_RETPOLINE))</span>
<span class="p_add">+			goto retpoline_generic;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case SPECTRE_V2_CMD_RETPOLINE:</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_RETPOLINE))</span>
<span class="p_add">+			goto retpoline_auto;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pr_err(&quot;kernel not compiled with retpoline; no mitigation available!&quot;);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+retpoline_auto:</span>
<span class="p_add">+	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {</span>
<span class="p_add">+	retpoline_amd:</span>
<span class="p_add">+		if (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {</span>
<span class="p_add">+			pr_err(&quot;LFENCE not serializing. Switching to generic retpoline\n&quot;);</span>
<span class="p_add">+			goto retpoline_generic;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :</span>
<span class="p_add">+					 SPECTRE_V2_RETPOLINE_MINIMAL_AMD;</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+	retpoline_generic:</span>
<span class="p_add">+		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_GENERIC :</span>
<span class="p_add">+					 SPECTRE_V2_RETPOLINE_MINIMAL;</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spectre_v2_enabled = mode;</span>
<span class="p_add">+	pr_info(&quot;%s\n&quot;, spectre_v2_strings[mode]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SYSFS</span>
<span class="p_add">+ssize_t cpu_show_meltdown(struct device *dev,</span>
<span class="p_add">+			  struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
<span class="p_add">+		return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return sprintf(buf, &quot;Mitigation: PTI\n&quot;);</span>
<span class="p_add">+	return sprintf(buf, &quot;Vulnerable\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t cpu_show_spectre_v1(struct device *dev,</span>
<span class="p_add">+			    struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1))</span>
<span class="p_add">+		return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+	return sprintf(buf, &quot;Vulnerable\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t cpu_show_spectre_v2(struct device *dev,</span>
<span class="p_add">+			    struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))</span>
<span class="p_add">+		return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return sprintf(buf, &quot;%s\n&quot;, spectre_v2_strings[spectre_v2_enabled]);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 2d3bd2215e5b..372ba3fb400f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -902,6 +902,9 @@</span> <span class="p_context"> static void __init early_identify_cpu(struct cpuinfo_x86 *c)</span>
 	if (c-&gt;x86_vendor != X86_VENDOR_AMD)
 		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 
<span class="p_add">+	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);</span>
<span class="p_add">+	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);</span>
<span class="p_add">+</span>
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/kernel/ftrace_32.S b/arch/x86/kernel/ftrace_32.S</span>
<span class="p_header">index b6c6468e10bc..4c8440de3355 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ftrace_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/ftrace_32.S</span>
<span class="p_chunk">@@ -8,6 +8,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/segment.h&gt;
 #include &lt;asm/export.h&gt;
 #include &lt;asm/ftrace.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #ifdef CC_USING_FENTRY
 # define function_hook	__fentry__
<span class="p_chunk">@@ -197,7 +198,8 @@</span> <span class="p_context"> ftrace_stub:</span>
 	movl	0x4(%ebp), %edx
 	subl	$MCOUNT_INSN_SIZE, %eax
 
<span class="p_del">-	call	*ftrace_trace_function</span>
<span class="p_add">+	movl	ftrace_trace_function, %ecx</span>
<span class="p_add">+	CALL_NOSPEC %ecx</span>
 
 	popl	%edx
 	popl	%ecx
<span class="p_chunk">@@ -241,5 +243,5 @@</span> <span class="p_context"> return_to_handler:</span>
 	movl	%eax, %ecx
 	popl	%edx
 	popl	%eax
<span class="p_del">-	jmp	*%ecx</span>
<span class="p_add">+	JMP_NOSPEC %ecx</span>
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/ftrace_64.S b/arch/x86/kernel/ftrace_64.S</span>
<span class="p_header">index c832291d948a..7cb8ba08beb9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ftrace_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/ftrace_64.S</span>
<span class="p_chunk">@@ -7,7 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/ptrace.h&gt;
 #include &lt;asm/ftrace.h&gt;
 #include &lt;asm/export.h&gt;
<span class="p_del">-</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 	.code64
 	.section .entry.text, &quot;ax&quot;
<span class="p_chunk">@@ -286,8 +286,8 @@</span> <span class="p_context"> trace:</span>
 	 * ip and parent ip are used and the list function is called when
 	 * function tracing is enabled.
 	 */
<span class="p_del">-	call   *ftrace_trace_function</span>
<span class="p_del">-</span>
<span class="p_add">+	movq ftrace_trace_function, %r8</span>
<span class="p_add">+	CALL_NOSPEC %r8</span>
 	restore_mcount_regs
 
 	jmp fgraph_trace
<span class="p_chunk">@@ -329,5 +329,5 @@</span> <span class="p_context"> GLOBAL(return_to_handler)</span>
 	movq 8(%rsp), %rdx
 	movq (%rsp), %rax
 	addq $24, %rsp
<span class="p_del">-	jmp *%rdi</span>
<span class="p_add">+	JMP_NOSPEC %rdi</span>
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/irq_32.c b/arch/x86/kernel/irq_32.c</span>
<span class="p_header">index a83b3346a0e1..c1bdbd3d3232 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irq_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irq_32.c</span>
<span class="p_chunk">@@ -20,6 +20,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 
 #include &lt;asm/apic.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #ifdef CONFIG_DEBUG_STACKOVERFLOW
 
<span class="p_chunk">@@ -55,11 +56,11 @@</span> <span class="p_context"> DEFINE_PER_CPU(struct irq_stack *, softirq_stack);</span>
 static void call_on_stack(void *func, void *stack)
 {
 	asm volatile(&quot;xchgl	%%ebx,%%esp	\n&quot;
<span class="p_del">-		     &quot;call	*%%edi		\n&quot;</span>
<span class="p_add">+		     CALL_NOSPEC</span>
 		     &quot;movl	%%ebx,%%esp	\n&quot;
 		     : &quot;=b&quot; (stack)
 		     : &quot;0&quot; (stack),
<span class="p_del">-		       &quot;D&quot;(func)</span>
<span class="p_add">+		       [thunk_target] &quot;D&quot;(func)</span>
 		     : &quot;memory&quot;, &quot;cc&quot;, &quot;edx&quot;, &quot;ecx&quot;, &quot;eax&quot;);
 }
 
<span class="p_chunk">@@ -95,11 +96,11 @@</span> <span class="p_context"> static inline int execute_on_irq_stack(int overflow, struct irq_desc *desc)</span>
 		call_on_stack(print_stack_overflow, isp);
 
 	asm volatile(&quot;xchgl	%%ebx,%%esp	\n&quot;
<span class="p_del">-		     &quot;call	*%%edi		\n&quot;</span>
<span class="p_add">+		     CALL_NOSPEC</span>
 		     &quot;movl	%%ebx,%%esp	\n&quot;
 		     : &quot;=a&quot; (arg1), &quot;=b&quot; (isp)
 		     :  &quot;0&quot; (desc),   &quot;1&quot; (isp),
<span class="p_del">-			&quot;D&quot; (desc-&gt;handle_irq)</span>
<span class="p_add">+			[thunk_target] &quot;D&quot; (desc-&gt;handle_irq)</span>
 		     : &quot;memory&quot;, &quot;cc&quot;, &quot;ecx&quot;);
 	return 1;
 }
<span class="p_header">diff --git a/arch/x86/kernel/tboot.c b/arch/x86/kernel/tboot.c</span>
<span class="p_header">index a4eb27918ceb..a2486f444073 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tboot.c</span>
<span class="p_chunk">@@ -138,6 +138,17 @@</span> <span class="p_context"> static int map_tboot_page(unsigned long vaddr, unsigned long pfn,</span>
 		return -1;
 	set_pte_at(&amp;tboot_mm, vaddr, pte, pfn_pte(pfn, prot));
 	pte_unmap(pte);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * PTI poisons low addresses in the kernel page tables in the</span>
<span class="p_add">+	 * name of making them unusable for userspace.  To execute</span>
<span class="p_add">+	 * code at such a low address, the poison must be cleared.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note: &#39;pgd&#39; actually gets set in p4d_alloc() _or_</span>
<span class="p_add">+	 * pud_alloc() depending on 4/5-level paging.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd-&gt;pgd &amp;= ~_PAGE_NX;</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index 0e68f0b3cbf7..2744b97345b8 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/debugreg.h&gt;
 #include &lt;asm/kvm_para.h&gt;
 #include &lt;asm/irq_remapping.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &lt;asm/virtext.h&gt;
 #include &quot;trace.h&quot;
<span class="p_chunk">@@ -4985,6 +4986,9 @@</span> <span class="p_context"> static void svm_vcpu_run(struct kvm_vcpu *vcpu)</span>
 #endif
 		);
 
<span class="p_add">+	/* Eliminate branch target predictions from guest mode */</span>
<span class="p_add">+	vmexit_fill_RSB();</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_64
 	wrmsrl(MSR_GS_BASE, svm-&gt;host.gs_base);
 #else
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 62ee4362e1c1..d1e25dba3112 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -50,6 +50,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/apic.h&gt;
 #include &lt;asm/irq_remapping.h&gt;
 #include &lt;asm/mmu_context.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &quot;trace.h&quot;
 #include &quot;pmu.h&quot;
<span class="p_chunk">@@ -9403,6 +9404,9 @@</span> <span class="p_context"> static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)</span>
 #endif
 	      );
 
<span class="p_add">+	/* Eliminate branch target predictions from guest mode */</span>
<span class="p_add">+	vmexit_fill_RSB();</span>
<span class="p_add">+</span>
 	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 	if (debugctlmsr)
 		update_debugctlmsr(debugctlmsr);
<span class="p_header">diff --git a/arch/x86/lib/Makefile b/arch/x86/lib/Makefile</span>
<span class="p_header">index 457f681ef379..d435c89875c1 100644</span>
<span class="p_header">--- a/arch/x86/lib/Makefile</span>
<span class="p_header">+++ b/arch/x86/lib/Makefile</span>
<span class="p_chunk">@@ -26,6 +26,7 @@</span> <span class="p_context"> lib-y += memcpy_$(BITS).o</span>
 lib-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem.o
 lib-$(CONFIG_INSTRUCTION_DECODER) += insn.o inat.o
 lib-$(CONFIG_RANDOMIZE_BASE) += kaslr.o
<span class="p_add">+lib-$(CONFIG_RETPOLINE) += retpoline.o</span>
 
 obj-y += msr.o msr-reg.o msr-reg-export.o hweight.o
 
<span class="p_header">diff --git a/arch/x86/lib/checksum_32.S b/arch/x86/lib/checksum_32.S</span>
<span class="p_header">index 4d34bb548b41..46e71a74e612 100644</span>
<span class="p_header">--- a/arch/x86/lib/checksum_32.S</span>
<span class="p_header">+++ b/arch/x86/lib/checksum_32.S</span>
<span class="p_chunk">@@ -29,7 +29,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/errno.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/export.h&gt;
<span class="p_del">-				</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
<span class="p_add">+</span>
 /*
  * computes a partial checksum, e.g. for TCP/UDP fragments
  */
<span class="p_chunk">@@ -156,7 +157,7 @@</span> <span class="p_context"> ENTRY(csum_partial)</span>
 	negl %ebx
 	lea 45f(%ebx,%ebx,2), %ebx
 	testl %esi, %esi
<span class="p_del">-	jmp *%ebx</span>
<span class="p_add">+	JMP_NOSPEC %ebx</span>
 
 	# Handle 2-byte-aligned regions
 20:	addw (%esi), %ax
<span class="p_chunk">@@ -439,7 +440,7 @@</span> <span class="p_context"> ENTRY(csum_partial_copy_generic)</span>
 	andl $-32,%edx
 	lea 3f(%ebx,%ebx), %ebx
 	testl %esi, %esi 
<span class="p_del">-	jmp *%ebx</span>
<span class="p_add">+	JMP_NOSPEC %ebx</span>
 1:	addl $64,%esi
 	addl $64,%edi 
 	SRC(movb -32(%edx),%bl)	; SRC(movb (%edx),%bl)
<span class="p_header">diff --git a/arch/x86/lib/retpoline.S b/arch/x86/lib/retpoline.S</span>
new file mode 100644
<span class="p_header">index 000000000000..cb45c6cb465f</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/lib/retpoline.S</span>
<span class="p_chunk">@@ -0,0 +1,48 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/stringify.h&gt;</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+#include &lt;asm/dwarf2.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+#include &lt;asm/alternative-asm.h&gt;</span>
<span class="p_add">+#include &lt;asm/export.h&gt;</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+.macro THUNK reg</span>
<span class="p_add">+	.section .text.__x86.indirect_thunk.\reg</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(__x86_indirect_thunk_\reg)</span>
<span class="p_add">+	CFI_STARTPROC</span>
<span class="p_add">+	JMP_NOSPEC %\reg</span>
<span class="p_add">+	CFI_ENDPROC</span>
<span class="p_add">+ENDPROC(__x86_indirect_thunk_\reg)</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Despite being an assembler file we can&#39;t just use .irp here</span>
<span class="p_add">+ * because __KSYM_DEPS__ only uses the C preprocessor and would</span>
<span class="p_add">+ * only see one instance of &quot;__x86_indirect_thunk_\reg&quot; rather</span>
<span class="p_add">+ * than one per register with the correct names. So we do it</span>
<span class="p_add">+ * the simple and nasty way...</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define EXPORT_THUNK(reg) EXPORT_SYMBOL(__x86_indirect_thunk_ ## reg)</span>
<span class="p_add">+#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)</span>
<span class="p_add">+</span>
<span class="p_add">+GENERATE_THUNK(_ASM_AX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_BX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_CX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_DX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_SI)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_DI)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_BP)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_SP)</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+GENERATE_THUNK(r8)</span>
<span class="p_add">+GENERATE_THUNK(r9)</span>
<span class="p_add">+GENERATE_THUNK(r10)</span>
<span class="p_add">+GENERATE_THUNK(r11)</span>
<span class="p_add">+GENERATE_THUNK(r12)</span>
<span class="p_add">+GENERATE_THUNK(r13)</span>
<span class="p_add">+GENERATE_THUNK(r14)</span>
<span class="p_add">+GENERATE_THUNK(r15)</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
<span class="p_header">index 43d4a4a29037..ce38f165489b 100644</span>
<span class="p_header">--- a/arch/x86/mm/pti.c</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -149,7 +149,7 @@</span> <span class="p_context"> pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
  *
  * Returns a pointer to a P4D on success, or NULL on failure.
  */
<span class="p_del">-static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
<span class="p_add">+static __init p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
 {
 	pgd_t *pgd = kernel_to_user_pgdp(pgd_offset_k(address));
 	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);
<span class="p_chunk">@@ -164,12 +164,7 @@</span> <span class="p_context"> static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
 		if (!new_p4d_page)
 			return NULL;
 
<span class="p_del">-		if (pgd_none(*pgd)) {</span>
<span class="p_del">-			set_pgd(pgd, __pgd(_KERNPG_TABLE | __pa(new_p4d_page)));</span>
<span class="p_del">-			new_p4d_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_p4d_page)</span>
<span class="p_del">-			free_page(new_p4d_page);</span>
<span class="p_add">+		set_pgd(pgd, __pgd(_KERNPG_TABLE | __pa(new_p4d_page)));</span>
 	}
 	BUILD_BUG_ON(pgd_large(*pgd) != 0);
 
<span class="p_chunk">@@ -182,7 +177,7 @@</span> <span class="p_context"> static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
  *
  * Returns a pointer to a PMD on success, or NULL on failure.
  */
<span class="p_del">-static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
<span class="p_add">+static __init pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
 {
 	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);
 	p4d_t *p4d = pti_user_pagetable_walk_p4d(address);
<span class="p_chunk">@@ -194,12 +189,7 @@</span> <span class="p_context"> static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
 		if (!new_pud_page)
 			return NULL;
 
<span class="p_del">-		if (p4d_none(*p4d)) {</span>
<span class="p_del">-			set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
<span class="p_del">-			new_pud_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_pud_page)</span>
<span class="p_del">-			free_page(new_pud_page);</span>
<span class="p_add">+		set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
 	}
 
 	pud = pud_offset(p4d, address);
<span class="p_chunk">@@ -213,12 +203,7 @@</span> <span class="p_context"> static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
 		if (!new_pmd_page)
 			return NULL;
 
<span class="p_del">-		if (pud_none(*pud)) {</span>
<span class="p_del">-			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
<span class="p_del">-			new_pmd_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_pmd_page)</span>
<span class="p_del">-			free_page(new_pmd_page);</span>
<span class="p_add">+		set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
 	}
 
 	return pmd_offset(pud, address);
<span class="p_chunk">@@ -251,12 +236,7 @@</span> <span class="p_context"> static __init pte_t *pti_user_pagetable_walk_pte(unsigned long address)</span>
 		if (!new_pte_page)
 			return NULL;
 
<span class="p_del">-		if (pmd_none(*pmd)) {</span>
<span class="p_del">-			set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));</span>
<span class="p_del">-			new_pte_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_pte_page)</span>
<span class="p_del">-			free_page(new_pte_page);</span>
<span class="p_add">+		set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));</span>
 	}
 
 	pte = pte_offset_kernel(pmd, address);
<span class="p_header">diff --git a/arch/x86/platform/efi/efi_64.c b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">index 39c4b35ac7a4..61975b6bcb1a 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_chunk">@@ -134,7 +134,9 @@</span> <span class="p_context"> pgd_t * __init efi_call_phys_prolog(void)</span>
 				pud[j] = *pud_offset(p4d_k, vaddr);
 			}
 		}
<span class="p_add">+		pgd_offset_k(pgd * PGDIR_SIZE)-&gt;pgd &amp;= ~_PAGE_NX;</span>
 	}
<span class="p_add">+</span>
 out:
 	__flush_tlb_all();
 
<span class="p_header">diff --git a/drivers/base/Kconfig b/drivers/base/Kconfig</span>
<span class="p_header">index 2f6614c9a229..37a71fd9043f 100644</span>
<span class="p_header">--- a/drivers/base/Kconfig</span>
<span class="p_header">+++ b/drivers/base/Kconfig</span>
<span class="p_chunk">@@ -235,6 +235,9 @@</span> <span class="p_context"> config GENERIC_CPU_DEVICES</span>
 config GENERIC_CPU_AUTOPROBE
 	bool
 
<span class="p_add">+config GENERIC_CPU_VULNERABILITIES</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
 config SOC_BUS
 	bool
 	select GLOB
<span class="p_header">diff --git a/drivers/base/cpu.c b/drivers/base/cpu.c</span>
<span class="p_header">index 321cd7b4d817..825964efda1d 100644</span>
<span class="p_header">--- a/drivers/base/cpu.c</span>
<span class="p_header">+++ b/drivers/base/cpu.c</span>
<span class="p_chunk">@@ -501,10 +501,58 @@</span> <span class="p_context"> static void __init cpu_dev_register_generic(void)</span>
 #endif
 }
 
<span class="p_add">+#ifdef CONFIG_GENERIC_CPU_VULNERABILITIES</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t __weak cpu_show_meltdown(struct device *dev,</span>
<span class="p_add">+				 struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t __weak cpu_show_spectre_v1(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t __weak cpu_show_spectre_v2(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static DEVICE_ATTR(meltdown, 0444, cpu_show_meltdown, NULL);</span>
<span class="p_add">+static DEVICE_ATTR(spectre_v1, 0444, cpu_show_spectre_v1, NULL);</span>
<span class="p_add">+static DEVICE_ATTR(spectre_v2, 0444, cpu_show_spectre_v2, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute *cpu_root_vulnerabilities_attrs[] = {</span>
<span class="p_add">+	&amp;dev_attr_meltdown.attr,</span>
<span class="p_add">+	&amp;dev_attr_spectre_v1.attr,</span>
<span class="p_add">+	&amp;dev_attr_spectre_v2.attr,</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct attribute_group cpu_root_vulnerabilities_group = {</span>
<span class="p_add">+	.name  = &quot;vulnerabilities&quot;,</span>
<span class="p_add">+	.attrs = cpu_root_vulnerabilities_attrs,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init cpu_register_vulnerabilities(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (sysfs_create_group(&amp;cpu_subsys.dev_root-&gt;kobj,</span>
<span class="p_add">+			       &amp;cpu_root_vulnerabilities_group))</span>
<span class="p_add">+		pr_err(&quot;Unable to register CPU vulnerabilities\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void cpu_register_vulnerabilities(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 void __init cpu_dev_init(void)
 {
 	if (subsys_system_register(&amp;cpu_subsys, cpu_root_attr_groups))
 		panic(&quot;Failed to register CPU subsystem&quot;);
 
 	cpu_dev_register_generic();
<span class="p_add">+	cpu_register_vulnerabilities();</span>
 }
<span class="p_header">diff --git a/include/linux/cpu.h b/include/linux/cpu.h</span>
<span class="p_header">index 938ea8ae0ba4..c816e6f2730c 100644</span>
<span class="p_header">--- a/include/linux/cpu.h</span>
<span class="p_header">+++ b/include/linux/cpu.h</span>
<span class="p_chunk">@@ -47,6 +47,13 @@</span> <span class="p_context"> extern void cpu_remove_dev_attr(struct device_attribute *attr);</span>
 extern int cpu_add_dev_attr_group(struct attribute_group *attrs);
 extern void cpu_remove_dev_attr_group(struct attribute_group *attrs);
 
<span class="p_add">+extern ssize_t cpu_show_meltdown(struct device *dev,</span>
<span class="p_add">+				 struct device_attribute *attr, char *buf);</span>
<span class="p_add">+extern ssize_t cpu_show_spectre_v1(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf);</span>
<span class="p_add">+extern ssize_t cpu_show_spectre_v2(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf);</span>
<span class="p_add">+</span>
 extern __printf(4, 5)
 struct device *cpu_device_create(struct device *parent, void *drvdata,
 				 const struct attribute_group **groups,
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index 3d4debd0257e..b0cb9a5f9448 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> config PAGE_TABLE_ISOLATION</span>
 	  ensuring that the majority of kernel addresses are not mapped
 	  into userspace.
 
<span class="p_del">-	  See Documentation/x86/pagetable-isolation.txt for more details.</span>
<span class="p_add">+	  See Documentation/x86/pti.txt for more details.</span>
 
 config SECURITY_INFINIBAND
 	bool &quot;Infiniband Security Hooks&quot;
<span class="p_header">diff --git a/tools/objtool/check.c b/tools/objtool/check.c</span>
<span class="p_header">index 9b341584eb1b..f40d46e24bcc 100644</span>
<span class="p_header">--- a/tools/objtool/check.c</span>
<span class="p_header">+++ b/tools/objtool/check.c</span>
<span class="p_chunk">@@ -428,6 +428,40 @@</span> <span class="p_context"> static void add_ignores(struct objtool_file *file)</span>
 }
 
 /*
<span class="p_add">+ * FIXME: For now, just ignore any alternatives which add retpolines.  This is</span>
<span class="p_add">+ * a temporary hack, as it doesn&#39;t allow ORC to unwind from inside a retpoline.</span>
<span class="p_add">+ * But it at least allows objtool to understand the control flow *around* the</span>
<span class="p_add">+ * retpoline.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int add_nospec_ignores(struct objtool_file *file)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct section *sec;</span>
<span class="p_add">+	struct rela *rela;</span>
<span class="p_add">+	struct instruction *insn;</span>
<span class="p_add">+</span>
<span class="p_add">+	sec = find_section_by_name(file-&gt;elf, &quot;.rela.discard.nospec&quot;);</span>
<span class="p_add">+	if (!sec)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(rela, &amp;sec-&gt;rela_list, list) {</span>
<span class="p_add">+		if (rela-&gt;sym-&gt;type != STT_SECTION) {</span>
<span class="p_add">+			WARN(&quot;unexpected relocation symbol type in %s&quot;, sec-&gt;name);</span>
<span class="p_add">+			return -1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		insn = find_insn(file, rela-&gt;sym-&gt;sec, rela-&gt;addend);</span>
<span class="p_add">+		if (!insn) {</span>
<span class="p_add">+			WARN(&quot;bad .discard.nospec entry&quot;);</span>
<span class="p_add">+			return -1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		insn-&gt;ignore_alts = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Find the destination instructions for all jumps.
  */
 static int add_jump_destinations(struct objtool_file *file)
<span class="p_chunk">@@ -456,6 +490,13 @@</span> <span class="p_context"> static int add_jump_destinations(struct objtool_file *file)</span>
 		} else if (rela-&gt;sym-&gt;sec-&gt;idx) {
 			dest_sec = rela-&gt;sym-&gt;sec;
 			dest_off = rela-&gt;sym-&gt;sym.st_value + rela-&gt;addend + 4;
<span class="p_add">+		} else if (strstr(rela-&gt;sym-&gt;name, &quot;_indirect_thunk_&quot;)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Retpoline jumps are really dynamic jumps in</span>
<span class="p_add">+			 * disguise, so convert them accordingly.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			insn-&gt;type = INSN_JUMP_DYNAMIC;</span>
<span class="p_add">+			continue;</span>
 		} else {
 			/* sibling call */
 			insn-&gt;jump_dest = 0;
<span class="p_chunk">@@ -502,11 +543,18 @@</span> <span class="p_context"> static int add_call_destinations(struct objtool_file *file)</span>
 			dest_off = insn-&gt;offset + insn-&gt;len + insn-&gt;immediate;
 			insn-&gt;call_dest = find_symbol_by_offset(insn-&gt;sec,
 								dest_off);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * FIXME: Thanks to retpolines, it&#39;s now considered</span>
<span class="p_add">+			 * normal for a function to call within itself.  So</span>
<span class="p_add">+			 * disable this warning for now.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+#if 0</span>
 			if (!insn-&gt;call_dest) {
 				WARN_FUNC(&quot;can&#39;t find call dest symbol at offset 0x%lx&quot;,
 					  insn-&gt;sec, insn-&gt;offset, dest_off);
 				return -1;
 			}
<span class="p_add">+#endif</span>
 		} else if (rela-&gt;sym-&gt;type == STT_SECTION) {
 			insn-&gt;call_dest = find_symbol_by_offset(rela-&gt;sym-&gt;sec,
 								rela-&gt;addend+4);
<span class="p_chunk">@@ -671,12 +719,6 @@</span> <span class="p_context"> static int add_special_section_alts(struct objtool_file *file)</span>
 		return ret;
 
 	list_for_each_entry_safe(special_alt, tmp, &amp;special_alts, list) {
<span class="p_del">-		alt = malloc(sizeof(*alt));</span>
<span class="p_del">-		if (!alt) {</span>
<span class="p_del">-			WARN(&quot;malloc failed&quot;);</span>
<span class="p_del">-			ret = -1;</span>
<span class="p_del">-			goto out;</span>
<span class="p_del">-		}</span>
 
 		orig_insn = find_insn(file, special_alt-&gt;orig_sec,
 				      special_alt-&gt;orig_off);
<span class="p_chunk">@@ -687,6 +729,10 @@</span> <span class="p_context"> static int add_special_section_alts(struct objtool_file *file)</span>
 			goto out;
 		}
 
<span class="p_add">+		/* Ignore retpoline alternatives. */</span>
<span class="p_add">+		if (orig_insn-&gt;ignore_alts)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		new_insn = NULL;
 		if (!special_alt-&gt;group || special_alt-&gt;new_len) {
 			new_insn = find_insn(file, special_alt-&gt;new_sec,
<span class="p_chunk">@@ -712,6 +758,13 @@</span> <span class="p_context"> static int add_special_section_alts(struct objtool_file *file)</span>
 				goto out;
 		}
 
<span class="p_add">+		alt = malloc(sizeof(*alt));</span>
<span class="p_add">+		if (!alt) {</span>
<span class="p_add">+			WARN(&quot;malloc failed&quot;);</span>
<span class="p_add">+			ret = -1;</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		alt-&gt;insn = new_insn;
 		list_add_tail(&amp;alt-&gt;list, &amp;orig_insn-&gt;alts);
 
<span class="p_chunk">@@ -1028,6 +1081,10 @@</span> <span class="p_context"> static int decode_sections(struct objtool_file *file)</span>
 
 	add_ignores(file);
 
<span class="p_add">+	ret = add_nospec_ignores(file);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
 	ret = add_jump_destinations(file);
 	if (ret)
 		return ret;
<span class="p_header">diff --git a/tools/objtool/check.h b/tools/objtool/check.h</span>
<span class="p_header">index 47d9ea70a83d..dbadb304a410 100644</span>
<span class="p_header">--- a/tools/objtool/check.h</span>
<span class="p_header">+++ b/tools/objtool/check.h</span>
<span class="p_chunk">@@ -44,7 +44,7 @@</span> <span class="p_context"> struct instruction {</span>
 	unsigned int len;
 	unsigned char type;
 	unsigned long immediate;
<span class="p_del">-	bool alt_group, visited, dead_end, ignore, hint, save, restore;</span>
<span class="p_add">+	bool alt_group, visited, dead_end, ignore, hint, save, restore, ignore_alts;</span>
 	struct symbol *call_dest;
 	struct instruction *jump_dest;
 	struct list_head alts;
<span class="p_header">diff --git a/tools/testing/selftests/x86/Makefile b/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">index 7b1adeee4b0f..91fbfa8fdc15 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/Makefile</span>
<span class="p_chunk">@@ -7,7 +7,7 @@</span> <span class="p_context"> include ../lib.mk</span>
 
 TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt ptrace_syscall test_mremap_vdso \
 			check_initial_reg_state sigreturn ldt_gdt iopl mpx-mini-test ioperm \
<span class="p_del">-			protection_keys test_vdso</span>
<span class="p_add">+			protection_keys test_vdso test_vsyscall</span>
 TARGETS_C_32BIT_ONLY := entry_from_vm86 syscall_arg_fault test_syscall_vdso unwind_vdso \
 			test_FCMOV test_FCOMI test_FISTTP \
 			vdso_restorer
<span class="p_header">diff --git a/tools/testing/selftests/x86/test_vsyscall.c b/tools/testing/selftests/x86/test_vsyscall.c</span>
new file mode 100644
<span class="p_header">index 000000000000..7a744fa7b786</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/test_vsyscall.c</span>
<span class="p_chunk">@@ -0,0 +1,500 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _GNU_SOURCE</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;sys/time.h&gt;</span>
<span class="p_add">+#include &lt;time.h&gt;</span>
<span class="p_add">+#include &lt;stdlib.h&gt;</span>
<span class="p_add">+#include &lt;sys/syscall.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;dlfcn.h&gt;</span>
<span class="p_add">+#include &lt;string.h&gt;</span>
<span class="p_add">+#include &lt;inttypes.h&gt;</span>
<span class="p_add">+#include &lt;signal.h&gt;</span>
<span class="p_add">+#include &lt;sys/ucontext.h&gt;</span>
<span class="p_add">+#include &lt;errno.h&gt;</span>
<span class="p_add">+#include &lt;err.h&gt;</span>
<span class="p_add">+#include &lt;sched.h&gt;</span>
<span class="p_add">+#include &lt;stdbool.h&gt;</span>
<span class="p_add">+#include &lt;setjmp.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+# define VSYS(x) (x)</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define VSYS(x) 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef SYS_getcpu</span>
<span class="p_add">+# ifdef __x86_64__</span>
<span class="p_add">+#  define SYS_getcpu 309</span>
<span class="p_add">+# else</span>
<span class="p_add">+#  define SYS_getcpu 318</span>
<span class="p_add">+# endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static void sethandler(int sig, void (*handler)(int, siginfo_t *, void *),</span>
<span class="p_add">+		       int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct sigaction sa;</span>
<span class="p_add">+	memset(&amp;sa, 0, sizeof(sa));</span>
<span class="p_add">+	sa.sa_sigaction = handler;</span>
<span class="p_add">+	sa.sa_flags = SA_SIGINFO | flags;</span>
<span class="p_add">+	sigemptyset(&amp;sa.sa_mask);</span>
<span class="p_add">+	if (sigaction(sig, &amp;sa, 0))</span>
<span class="p_add">+		err(1, &quot;sigaction&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* vsyscalls and vDSO */</span>
<span class="p_add">+bool should_read_vsyscall = false;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef long (*gtod_t)(struct timeval *tv, struct timezone *tz);</span>
<span class="p_add">+gtod_t vgtod = (gtod_t)VSYS(0xffffffffff600000);</span>
<span class="p_add">+gtod_t vdso_gtod;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef int (*vgettime_t)(clockid_t, struct timespec *);</span>
<span class="p_add">+vgettime_t vdso_gettime;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef long (*time_func_t)(time_t *t);</span>
<span class="p_add">+time_func_t vtime = (time_func_t)VSYS(0xffffffffff600400);</span>
<span class="p_add">+time_func_t vdso_time;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef long (*getcpu_t)(unsigned *, unsigned *, void *);</span>
<span class="p_add">+getcpu_t vgetcpu = (getcpu_t)VSYS(0xffffffffff600800);</span>
<span class="p_add">+getcpu_t vdso_getcpu;</span>
<span class="p_add">+</span>
<span class="p_add">+static void init_vdso(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *vdso = dlopen(&quot;linux-vdso.so.1&quot;, RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);</span>
<span class="p_add">+	if (!vdso)</span>
<span class="p_add">+		vdso = dlopen(&quot;linux-gate.so.1&quot;, RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);</span>
<span class="p_add">+	if (!vdso) {</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find vDSO\n&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_gtod = (gtod_t)dlsym(vdso, &quot;__vdso_gettimeofday&quot;);</span>
<span class="p_add">+	if (!vdso_gtod)</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find gettimeofday in vDSO\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_gettime = (vgettime_t)dlsym(vdso, &quot;__vdso_clock_gettime&quot;);</span>
<span class="p_add">+	if (!vdso_gettime)</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find clock_gettime in vDSO\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_time = (time_func_t)dlsym(vdso, &quot;__vdso_time&quot;);</span>
<span class="p_add">+	if (!vdso_time)</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find time in vDSO\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_getcpu = (getcpu_t)dlsym(vdso, &quot;__vdso_getcpu&quot;);</span>
<span class="p_add">+	if (!vdso_getcpu) {</span>
<span class="p_add">+		/* getcpu() was never wired up in the 32-bit vDSO. */</span>
<span class="p_add">+		printf(&quot;[%s]\tfailed to find getcpu in vDSO\n&quot;,</span>
<span class="p_add">+		       sizeof(long) == 8 ? &quot;WARN&quot; : &quot;NOTE&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int init_vsys(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+	FILE *maps;</span>
<span class="p_add">+	char line[128];</span>
<span class="p_add">+	bool found = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	maps = fopen(&quot;/proc/self/maps&quot;, &quot;r&quot;);</span>
<span class="p_add">+	if (!maps) {</span>
<span class="p_add">+		printf(&quot;[WARN]\tCould not open /proc/self/maps -- assuming vsyscall is r-x\n&quot;);</span>
<span class="p_add">+		should_read_vsyscall = true;</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	while (fgets(line, sizeof(line), maps)) {</span>
<span class="p_add">+		char r, x;</span>
<span class="p_add">+		void *start, *end;</span>
<span class="p_add">+		char name[128];</span>
<span class="p_add">+		if (sscanf(line, &quot;%p-%p %c-%cp %*x %*x:%*x %*u %s&quot;,</span>
<span class="p_add">+			   &amp;start, &amp;end, &amp;r, &amp;x, name) != 5)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (strcmp(name, &quot;[vsyscall]&quot;))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		printf(&quot;\tvsyscall map: %s&quot;, line);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (start != (void *)0xffffffffff600000 ||</span>
<span class="p_add">+		    end != (void *)0xffffffffff601000) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\taddress range is nonsense\n&quot;);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		printf(&quot;\tvsyscall permissions are %c-%c\n&quot;, r, x);</span>
<span class="p_add">+		should_read_vsyscall = (r == &#39;r&#39;);</span>
<span class="p_add">+		if (x != &#39;x&#39;) {</span>
<span class="p_add">+			vgtod = NULL;</span>
<span class="p_add">+			vtime = NULL;</span>
<span class="p_add">+			vgetcpu = NULL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		found = true;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	fclose(maps);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!found) {</span>
<span class="p_add">+		printf(&quot;\tno vsyscall map in /proc/self/maps\n&quot;);</span>
<span class="p_add">+		should_read_vsyscall = false;</span>
<span class="p_add">+		vgtod = NULL;</span>
<span class="p_add">+		vtime = NULL;</span>
<span class="p_add">+		vgetcpu = NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* syscalls */</span>
<span class="p_add">+static inline long sys_gtod(struct timeval *tv, struct timezone *tz)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_gettimeofday, tv, tz);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int sys_clock_gettime(clockid_t id, struct timespec *ts)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_clock_gettime, id, ts);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long sys_time(time_t *t)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_time, t);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long sys_getcpu(unsigned * cpu, unsigned * node,</span>
<span class="p_add">+			      void* cache)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_getcpu, cpu, node, cache);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static jmp_buf jmpbuf;</span>
<span class="p_add">+</span>
<span class="p_add">+static void sigsegv(int sig, siginfo_t *info, void *ctx_void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	siglongjmp(jmpbuf, 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static double tv_diff(const struct timeval *a, const struct timeval *b)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (double)(a-&gt;tv_sec - b-&gt;tv_sec) +</span>
<span class="p_add">+		(double)((int)a-&gt;tv_usec - (int)b-&gt;tv_usec) * 1e-6;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int check_gtod(const struct timeval *tv_sys1,</span>
<span class="p_add">+		      const struct timeval *tv_sys2,</span>
<span class="p_add">+		      const struct timezone *tz_sys,</span>
<span class="p_add">+		      const char *which,</span>
<span class="p_add">+		      const struct timeval *tv_other,</span>
<span class="p_add">+		      const struct timezone *tz_other)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+	double d1, d2;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tz_other &amp;&amp; (tz_sys-&gt;tz_minuteswest != tz_other-&gt;tz_minuteswest || tz_sys-&gt;tz_dsttime != tz_other-&gt;tz_dsttime)) {</span>
<span class="p_add">+		printf(&quot;[FAIL] %s tz mismatch\n&quot;, which);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	d1 = tv_diff(tv_other, tv_sys1);</span>
<span class="p_add">+	d2 = tv_diff(tv_sys2, tv_other); </span>
<span class="p_add">+	printf(&quot;\t%s time offsets: %lf %lf\n&quot;, which, d1, d2);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (d1 &lt; 0 || d2 &lt; 0) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\t%s time was inconsistent with the syscall\n&quot;, which);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[OK]\t%s gettimeofday()&#39;s timeval was okay\n&quot;, which);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_gtod(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct timeval tv_sys1, tv_sys2, tv_vdso, tv_vsys;</span>
<span class="p_add">+	struct timezone tz_sys, tz_vdso, tz_vsys;</span>
<span class="p_add">+	long ret_vdso = -1;</span>
<span class="p_add">+	long ret_vsys = -1;</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\ttest gettimeofday()\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sys_gtod(&amp;tv_sys1, &amp;tz_sys) != 0)</span>
<span class="p_add">+		err(1, &quot;syscall gettimeofday&quot;);</span>
<span class="p_add">+	if (vdso_gtod)</span>
<span class="p_add">+		ret_vdso = vdso_gtod(&amp;tv_vdso, &amp;tz_vdso);</span>
<span class="p_add">+	if (vgtod)</span>
<span class="p_add">+		ret_vsys = vgtod(&amp;tv_vsys, &amp;tz_vsys);</span>
<span class="p_add">+	if (sys_gtod(&amp;tv_sys2, &amp;tz_sys) != 0)</span>
<span class="p_add">+		err(1, &quot;syscall gettimeofday&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vdso_gtod) {</span>
<span class="p_add">+		if (ret_vdso == 0) {</span>
<span class="p_add">+			nerrs += check_gtod(&amp;tv_sys1, &amp;tv_sys2, &amp;tz_sys, &quot;vDSO&quot;, &amp;tv_vdso, &amp;tz_vdso);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO gettimeofday() failed: %ld\n&quot;, ret_vdso);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vgtod) {</span>
<span class="p_add">+		if (ret_vsys == 0) {</span>
<span class="p_add">+			nerrs += check_gtod(&amp;tv_sys1, &amp;tv_sys2, &amp;tz_sys, &quot;vsyscall&quot;, &amp;tv_vsys, &amp;tz_vsys);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsys gettimeofday() failed: %ld\n&quot;, ret_vsys);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_time(void) {</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\ttest time()\n&quot;);</span>
<span class="p_add">+	long t_sys1, t_sys2, t_vdso = 0, t_vsys = 0;</span>
<span class="p_add">+	long t2_sys1 = -1, t2_sys2 = -1, t2_vdso = -1, t2_vsys = -1;</span>
<span class="p_add">+	t_sys1 = sys_time(&amp;t2_sys1);</span>
<span class="p_add">+	if (vdso_time)</span>
<span class="p_add">+		t_vdso = vdso_time(&amp;t2_vdso);</span>
<span class="p_add">+	if (vtime)</span>
<span class="p_add">+		t_vsys = vtime(&amp;t2_vsys);</span>
<span class="p_add">+	t_sys2 = sys_time(&amp;t2_sys2);</span>
<span class="p_add">+	if (t_sys1 &lt; 0 || t_sys1 != t2_sys1 || t_sys2 &lt; 0 || t_sys2 != t2_sys2) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\tsyscall failed (ret1:%ld output1:%ld ret2:%ld output2:%ld)\n&quot;, t_sys1, t2_sys1, t_sys2, t2_sys2);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		return nerrs;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vdso_time) {</span>
<span class="p_add">+		if (t_vdso &lt; 0 || t_vdso != t2_vdso) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO failed (ret:%ld output:%ld)\n&quot;, t_vdso, t2_vdso);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else if (t_vdso &lt; t_sys1 || t_vdso &gt; t_sys2) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO returned the wrong time (%ld %ld %ld)\n&quot;, t_sys1, t_vdso, t_sys2);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[OK]\tvDSO time() is okay\n&quot;);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vtime) {</span>
<span class="p_add">+		if (t_vsys &lt; 0 || t_vsys != t2_vsys) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsyscall failed (ret:%ld output:%ld)\n&quot;, t_vsys, t2_vsys);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else if (t_vsys &lt; t_sys1 || t_vsys &gt; t_sys2) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsyscall returned the wrong time (%ld %ld %ld)\n&quot;, t_sys1, t_vsys, t_sys2);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[OK]\tvsyscall time() is okay\n&quot;);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_getcpu(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+	long ret_sys, ret_vdso = -1, ret_vsys = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tgetcpu() on CPU %d\n&quot;, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	cpu_set_t cpuset;</span>
<span class="p_add">+	CPU_ZERO(&amp;cpuset);</span>
<span class="p_add">+	CPU_SET(cpu, &amp;cpuset);</span>
<span class="p_add">+	if (sched_setaffinity(0, sizeof(cpuset), &amp;cpuset) != 0) {</span>
<span class="p_add">+		printf(&quot;[SKIP]\tfailed to force CPU %d\n&quot;, cpu);</span>
<span class="p_add">+		return nerrs;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned cpu_sys, cpu_vdso, cpu_vsys, node_sys, node_vdso, node_vsys;</span>
<span class="p_add">+	unsigned node = 0;</span>
<span class="p_add">+	bool have_node = false;</span>
<span class="p_add">+	ret_sys = sys_getcpu(&amp;cpu_sys, &amp;node_sys, 0);</span>
<span class="p_add">+	if (vdso_getcpu)</span>
<span class="p_add">+		ret_vdso = vdso_getcpu(&amp;cpu_vdso, &amp;node_vdso, 0);</span>
<span class="p_add">+	if (vgetcpu)</span>
<span class="p_add">+		ret_vsys = vgetcpu(&amp;cpu_vsys, &amp;node_vsys, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ret_sys == 0) {</span>
<span class="p_add">+		if (cpu_sys != cpu) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tsyscall reported CPU %hu but should be %d\n&quot;, cpu_sys, cpu);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		have_node = true;</span>
<span class="p_add">+		node = node_sys;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vdso_getcpu) {</span>
<span class="p_add">+		if (ret_vdso) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO getcpu() failed\n&quot;);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (!have_node) {</span>
<span class="p_add">+				have_node = true;</span>
<span class="p_add">+				node = node_vdso;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (cpu_vdso != cpu) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvDSO reported CPU %hu but should be %d\n&quot;, cpu_vdso, cpu);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvDSO reported correct CPU\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (node_vdso != node) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvDSO reported node %hu but should be %hu\n&quot;, node_vdso, node);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvDSO reported correct node\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vgetcpu) {</span>
<span class="p_add">+		if (ret_vsys) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsyscall getcpu() failed\n&quot;);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (!have_node) {</span>
<span class="p_add">+				have_node = true;</span>
<span class="p_add">+				node = node_vsys;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (cpu_vsys != cpu) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvsyscall reported CPU %hu but should be %d\n&quot;, cpu_vsys, cpu);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvsyscall reported correct CPU\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (node_vsys != node) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvsyscall reported node %hu but should be %hu\n&quot;, node_vsys, node);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvsyscall reported correct node\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_vsys_r(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	printf(&quot;[RUN]\tChecking read access to the vsyscall page\n&quot;);</span>
<span class="p_add">+	bool can_read;</span>
<span class="p_add">+	if (sigsetjmp(jmpbuf, 1) == 0) {</span>
<span class="p_add">+		*(volatile int *)0xffffffffff600000;</span>
<span class="p_add">+		can_read = true;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		can_read = false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (can_read &amp;&amp; !should_read_vsyscall) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\tWe have read access, but we shouldn&#39;t\n&quot;);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	} else if (!can_read &amp;&amp; should_read_vsyscall) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\tWe don&#39;t have read access, but we should\n&quot;);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[OK]\tgot expected result\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+#define X86_EFLAGS_TF (1UL &lt;&lt; 8)</span>
<span class="p_add">+static volatile sig_atomic_t num_vsyscall_traps;</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long get_eflags(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long eflags;</span>
<span class="p_add">+	asm volatile (&quot;pushfq\n\tpopq %0&quot; : &quot;=rm&quot; (eflags));</span>
<span class="p_add">+	return eflags;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void set_eflags(unsigned long eflags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;pushq %0\n\tpopfq&quot; : : &quot;rm&quot; (eflags) : &quot;flags&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void sigtrap(int sig, siginfo_t *info, void *ctx_void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	ucontext_t *ctx = (ucontext_t *)ctx_void;</span>
<span class="p_add">+	unsigned long ip = ctx-&gt;uc_mcontext.gregs[REG_RIP];</span>
<span class="p_add">+</span>
<span class="p_add">+	if (((ip ^ 0xffffffffff600000UL) &amp; ~0xfffUL) == 0)</span>
<span class="p_add">+		num_vsyscall_traps++;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_native_vsyscall(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	time_t tmp;</span>
<span class="p_add">+	bool is_native;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!vtime)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tchecking for native vsyscall\n&quot;);</span>
<span class="p_add">+	sethandler(SIGTRAP, sigtrap, 0);</span>
<span class="p_add">+	set_eflags(get_eflags() | X86_EFLAGS_TF);</span>
<span class="p_add">+	vtime(&amp;tmp);</span>
<span class="p_add">+	set_eflags(get_eflags() &amp; ~X86_EFLAGS_TF);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If vsyscalls are emulated, we expect a single trap in the</span>
<span class="p_add">+	 * vsyscall page -- the call instruction will trap with RIP</span>
<span class="p_add">+	 * pointing to the entry point before emulation takes over.</span>
<span class="p_add">+	 * In native mode, we expect two traps, since whatever code</span>
<span class="p_add">+	 * the vsyscall page contains will be more than just a ret</span>
<span class="p_add">+	 * instruction.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	is_native = (num_vsyscall_traps &gt; 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;\tvsyscalls are %s (%d instructions in vsyscall page)\n&quot;,</span>
<span class="p_add">+	       (is_native ? &quot;native&quot; : &quot;emulated&quot;),</span>
<span class="p_add">+	       (int)num_vsyscall_traps);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+int main(int argc, char **argv)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	init_vdso();</span>
<span class="p_add">+	nerrs += init_vsys();</span>
<span class="p_add">+</span>
<span class="p_add">+	nerrs += test_gtod();</span>
<span class="p_add">+	nerrs += test_time();</span>
<span class="p_add">+	nerrs += test_getcpu(0);</span>
<span class="p_add">+	nerrs += test_getcpu(1);</span>
<span class="p_add">+</span>
<span class="p_add">+	sethandler(SIGSEGV, sigsegv, 0);</span>
<span class="p_add">+	nerrs += test_vsys_r();</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	nerrs += test_native_vsyscall();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs ? 1 : 0;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



