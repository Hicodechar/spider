
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[13/17] RISC-V: Add include subdirectory - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [13/17] RISC-V: Add include subdirectory</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 6, 2017, 11 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170606230007.19101-14-palmer@dabbelt.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9770133/mbox/"
   >mbox</a>
|
   <a href="/patch/9770133/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9770133/">/patch/9770133/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7ED3F6034B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 23:03:03 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 54F6A284E4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 23:03:03 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 487DD284F1; Tue,  6 Jun 2017 23:03:03 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B2226284E4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 23:02:55 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751752AbdFFXCY (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 6 Jun 2017 19:02:24 -0400
Received: from mail-pg0-f65.google.com ([74.125.83.65]:34935 &quot;EHLO
	mail-pg0-f65.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751669AbdFFXBq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 6 Jun 2017 19:01:46 -0400
Received: by mail-pg0-f65.google.com with SMTP id f127so14152707pgc.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 06 Jun 2017 16:01:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=from:to:to:to:to:cc:cc:cc:subject:date:message-id:in-reply-to
	:references; bh=r453rFzBXVdvuS7UGeBKQOQU3p84bbtaGj2Xczcjq8Y=;
	b=Ewjq3c2ORD15la4zN9s+1R/DB6OhgCHEzBXM2GOdA4I/w6WHNvu32Otuw7F1O6BEZz
	/pSxdaZ68CuS7vz0Y8zZ2Ead4fQwz3mHOeeHihFmEtY9J4D3eIai/HvWCAU9BIguH54s
	lEn9WqL5D2bXR9zI1WscbNUAHoneM7EKXzV4Ao1NEx2uA1+pLzFryfhI+ux3vbp0Ra9F
	t/CW0x6YLiO4wo1eKuSpWS2gmyTBvv954P4iyRCsuxF3uXRspXUWAnKepetER3EdfYyc
	Sh8chhREuOIaJoKxht/Sm73TCSOHdhiYiyIivs3rDa3KBTHkN9MybzoWLYnOoCOiQvTC
	qVOQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:to:to:to:cc:cc:cc:subject:date
	:message-id:in-reply-to:references;
	bh=r453rFzBXVdvuS7UGeBKQOQU3p84bbtaGj2Xczcjq8Y=;
	b=r+lHBwhjh2CRZA9Fd1fw8opITmlqpi/c95Rd1o12wDVna+7z089qEkyrqAS5Yf5A4Y
	SRhNqOqXkhIyzX2+kljDfm8BLlweyPIpQH6B927SNbLr8bMiEqqzu2ipppWABYUb+7FD
	XHYl9nL4IrhRVABru13YIzSn/tVaVtMXB8ozbwQuUaVClJv1EDeIDenv70LNzCvWRHcg
	3Hyk7tuHcSdP3Q5Q5pZAtJvNAtNPs53zjoD+qR0bN5S3XTj5rWuKqRA40bDRUODM/TzQ
	mI8UhoRPVQS4Dft/sN4aSRTrsg22Uvj8zBho0L+pqCxFnoUf25RlCR+HYAvbwumr6ZXu
	ecUA==
X-Gm-Message-State: AODbwcADbj5JbHx0To/HUemg+MKXtZ8R5H04/T5r8e9pJ4k3MrV+kCYE
	DOkqyt2uum2clrw/
X-Received: by 10.98.86.132 with SMTP id h4mr16099419pfj.205.1496790097303; 
	Tue, 06 Jun 2017 16:01:37 -0700 (PDT)
Received: from localhost ([216.38.154.21]) by smtp.gmail.com with ESMTPSA id
	e1sm22313087pfe.6.2017.06.06.16.01.36
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 06 Jun 2017 16:01:36 -0700 (PDT)
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: linux-arch@vger.kernel.org
To: linux-kernel@vger.kernel.org
To: Arnd Bergmann &lt;arnd@arndb.de&gt;
To: olof@lixom.net
Cc: albert@sifive.com
Cc: patches@groups.riscv.org
Cc: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
Subject: [PATCH 13/17] RISC-V: Add include subdirectory
Date: Tue,  6 Jun 2017 16:00:03 -0700
Message-Id: &lt;20170606230007.19101-14-palmer@dabbelt.com&gt;
X-Mailer: git-send-email 2.13.0
In-Reply-To: &lt;20170606230007.19101-1-palmer@dabbelt.com&gt;
References: &lt;20170523004107.536-1-palmer@dabbelt.com&gt;
	&lt;20170606230007.19101-1-palmer@dabbelt.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 6, 2017, 11 p.m.</div>
<pre class="content">
This patch adds the include files for the RISC-V port.  These are mostly
based on the score port, but there are a lot of arm64-based files as
well.
<span class="signed-off-by">
Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
---
 arch/riscv/include/asm/Kbuild             |  61 +++++
 arch/riscv/include/asm/asm-offsets.h      |   1 +
 arch/riscv/include/asm/asm.h              |  76 ++++++
 arch/riscv/include/asm/atomic.h           | 349 ++++++++++++++++++++++++
 arch/riscv/include/asm/atomic64.h         | 355 ++++++++++++++++++++++++
 arch/riscv/include/asm/barrier.h          |  41 +++
 arch/riscv/include/asm/bitops.h           | 228 ++++++++++++++++
 arch/riscv/include/asm/bug.h              |  88 ++++++
 arch/riscv/include/asm/cache.h            |  22 ++
 arch/riscv/include/asm/cacheflush.h       |  39 +++
 arch/riscv/include/asm/cmpxchg.h          | 124 +++++++++
 arch/riscv/include/asm/compat.h           |  31 +++
 arch/riscv/include/asm/csr.h              | 125 +++++++++
 arch/riscv/include/asm/current.h          |  42 +++
 arch/riscv/include/asm/delay.h            |  28 ++
 arch/riscv/include/asm/device.h           |  27 ++
 arch/riscv/include/asm/dma-mapping.h      |  41 +++
 arch/riscv/include/asm/elf.h              |  85 ++++++
 arch/riscv/include/asm/io.h               | 152 +++++++++++
 arch/riscv/include/asm/irq.h              |  31 +++
 arch/riscv/include/asm/irqflags.h         |  63 +++++
 arch/riscv/include/asm/kprobes.h          |  22 ++
 arch/riscv/include/asm/linkage.h          |  20 ++
 arch/riscv/include/asm/mmu.h              |  26 ++
 arch/riscv/include/asm/mmu_context.h      |  69 +++++
 arch/riscv/include/asm/page.h             | 138 ++++++++++
 arch/riscv/include/asm/pci.h              |  50 ++++
 arch/riscv/include/asm/pgalloc.h          | 124 +++++++++
 arch/riscv/include/asm/pgtable-32.h       |  25 ++
 arch/riscv/include/asm/pgtable-64.h       |  84 ++++++
 arch/riscv/include/asm/pgtable-bits.h     |  48 ++++
 arch/riscv/include/asm/pgtable.h          | 427 +++++++++++++++++++++++++++++
 arch/riscv/include/asm/processor.h        | 102 +++++++
 arch/riscv/include/asm/ptrace.h           | 116 ++++++++
 arch/riscv/include/asm/sbi.h              | 100 +++++++
 arch/riscv/include/asm/smp.h              |  41 +++
 arch/riscv/include/asm/spinlock.h         | 155 +++++++++++
 arch/riscv/include/asm/spinlock_types.h   |  33 +++
 arch/riscv/include/asm/string.h           |  30 ++
 arch/riscv/include/asm/switch_to.h        |  69 +++++
 arch/riscv/include/asm/syscall.h          |  90 ++++++
 arch/riscv/include/asm/syscalls.h         |  25 ++
 arch/riscv/include/asm/thread_info.h      |  96 +++++++
 arch/riscv/include/asm/timex.h            |  43 +++
 arch/riscv/include/asm/tlb.h              |  24 ++
 arch/riscv/include/asm/tlbflush.h         |  64 +++++
 arch/riscv/include/asm/uaccess.h          | 440 ++++++++++++++++++++++++++++++
 arch/riscv/include/asm/unistd.h           |  16 ++
 arch/riscv/include/asm/vdso.h             |  32 +++
 arch/riscv/include/asm/word-at-a-time.h   |  55 ++++
 arch/riscv/include/uapi/asm/Kbuild        |   2 +
 arch/riscv/include/uapi/asm/auxvec.h      |  24 ++
 arch/riscv/include/uapi/asm/bitsperlong.h |  25 ++
 arch/riscv/include/uapi/asm/byteorder.h   |  23 ++
 arch/riscv/include/uapi/asm/elf.h         |  83 ++++++
 arch/riscv/include/uapi/asm/ptrace.h      |  68 +++++
 arch/riscv/include/uapi/asm/sigcontext.h  |  29 ++
 arch/riscv/include/uapi/asm/siginfo.h     |  24 ++
 arch/riscv/include/uapi/asm/unistd.h      |  22 ++
 59 files changed, 4873 insertions(+)
 create mode 100644 arch/riscv/include/asm/Kbuild
 create mode 100644 arch/riscv/include/asm/asm-offsets.h
 create mode 100644 arch/riscv/include/asm/asm.h
 create mode 100644 arch/riscv/include/asm/atomic.h
 create mode 100644 arch/riscv/include/asm/atomic64.h
 create mode 100644 arch/riscv/include/asm/barrier.h
 create mode 100644 arch/riscv/include/asm/bitops.h
 create mode 100644 arch/riscv/include/asm/bug.h
 create mode 100644 arch/riscv/include/asm/cache.h
 create mode 100644 arch/riscv/include/asm/cacheflush.h
 create mode 100644 arch/riscv/include/asm/cmpxchg.h
 create mode 100644 arch/riscv/include/asm/compat.h
 create mode 100644 arch/riscv/include/asm/csr.h
 create mode 100644 arch/riscv/include/asm/current.h
 create mode 100644 arch/riscv/include/asm/delay.h
 create mode 100644 arch/riscv/include/asm/device.h
 create mode 100644 arch/riscv/include/asm/dma-mapping.h
 create mode 100644 arch/riscv/include/asm/elf.h
 create mode 100644 arch/riscv/include/asm/io.h
 create mode 100644 arch/riscv/include/asm/irq.h
 create mode 100644 arch/riscv/include/asm/irqflags.h
 create mode 100644 arch/riscv/include/asm/kprobes.h
 create mode 100644 arch/riscv/include/asm/linkage.h
 create mode 100644 arch/riscv/include/asm/mmu.h
 create mode 100644 arch/riscv/include/asm/mmu_context.h
 create mode 100644 arch/riscv/include/asm/page.h
 create mode 100644 arch/riscv/include/asm/pci.h
 create mode 100644 arch/riscv/include/asm/pgalloc.h
 create mode 100644 arch/riscv/include/asm/pgtable-32.h
 create mode 100644 arch/riscv/include/asm/pgtable-64.h
 create mode 100644 arch/riscv/include/asm/pgtable-bits.h
 create mode 100644 arch/riscv/include/asm/pgtable.h
 create mode 100644 arch/riscv/include/asm/processor.h
 create mode 100644 arch/riscv/include/asm/ptrace.h
 create mode 100644 arch/riscv/include/asm/sbi.h
 create mode 100644 arch/riscv/include/asm/smp.h
 create mode 100644 arch/riscv/include/asm/spinlock.h
 create mode 100644 arch/riscv/include/asm/spinlock_types.h
 create mode 100644 arch/riscv/include/asm/string.h
 create mode 100644 arch/riscv/include/asm/switch_to.h
 create mode 100644 arch/riscv/include/asm/syscall.h
 create mode 100644 arch/riscv/include/asm/syscalls.h
 create mode 100644 arch/riscv/include/asm/thread_info.h
 create mode 100644 arch/riscv/include/asm/timex.h
 create mode 100644 arch/riscv/include/asm/tlb.h
 create mode 100644 arch/riscv/include/asm/tlbflush.h
 create mode 100644 arch/riscv/include/asm/uaccess.h
 create mode 100644 arch/riscv/include/asm/unistd.h
 create mode 100644 arch/riscv/include/asm/vdso.h
 create mode 100644 arch/riscv/include/asm/word-at-a-time.h
 create mode 100644 arch/riscv/include/uapi/asm/Kbuild
 create mode 100644 arch/riscv/include/uapi/asm/auxvec.h
 create mode 100644 arch/riscv/include/uapi/asm/bitsperlong.h
 create mode 100644 arch/riscv/include/uapi/asm/byteorder.h
 create mode 100644 arch/riscv/include/uapi/asm/elf.h
 create mode 100644 arch/riscv/include/uapi/asm/ptrace.h
 create mode 100644 arch/riscv/include/uapi/asm/sigcontext.h
 create mode 100644 arch/riscv/include/uapi/asm/siginfo.h
 create mode 100644 arch/riscv/include/uapi/asm/unistd.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - June 7, 2017, 8:12 a.m.</div>
<pre class="content">
On Wed, Jun 7, 2017 at 1:00 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:
<span class="quote">&gt; This patch adds the include files for the RISC-V port.  These are mostly</span>
<span class="quote">&gt; based on the score port, but there are a lot of arm64-based files as</span>
<span class="quote">&gt; well.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>

It might be better to split this up into several parts, as the patch
is longer than
most people are willing to review at once.

The uapi should definitely be a separate patch, as it includes the parts that
cannot be changed any more later. memory management (pgtable, mmu,
uaccess) would be another part to split out, and possibly all the atomics
in one separate patch (along with spinlocks and bitops).
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +/* IO barriers.  These only fence on the IO bits because they&#39;re only required</span>
<span class="quote">&gt; + * to order device access.  We&#39;re defining mmiowb because our AMO instructions</span>
<span class="quote">&gt; + * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7</span>
<span class="quote">&gt; + * of v2.2 of the user ISA:</span>
<span class="quote">&gt; + * &quot;The bits order accesses to one of the two address domains, memory or I/O,</span>
<span class="quote">&gt; + * depending on which address domain the atomic instruction is accessing. No</span>
<span class="quote">&gt; + * ordering constraint is implied to accesses to the other domain, and a FENCE</span>
<span class="quote">&gt; + * instruction should be used to order across both domains.&quot;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __iormb()      __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; +#define __iowmb()      __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define mmiowb()       __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="quote">&gt; + * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="quote">&gt; + * accesses.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define readb_relaxed(c)       ({ u8  __r = __raw_readb(c); __r; })</span>
<span class="quote">&gt; +#define readw_relaxed(c)       ({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })</span>
<span class="quote">&gt; +#define readl_relaxed(c)       ({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })</span>
<span class="quote">&gt; +#define readq_relaxed(c)       ({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define writeb_relaxed(v,c)    ((void)__raw_writeb((v),(c)))</span>
<span class="quote">&gt; +#define writew_relaxed(v,c)    ((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))</span>
<span class="quote">&gt; +#define writel_relaxed(v,c)    ((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))</span>
<span class="quote">&gt; +#define writeq_relaxed(v,c)    ((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * I/O memory access primitives. Reads are ordered relative to any</span>
<span class="quote">&gt; + * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="quote">&gt; + * Normal memory access.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define readb(c)               ({ u8  __v = readb_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt; +#define readw(c)               ({ u16 __v = readw_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt; +#define readl(c)               ({ u32 __v = readl_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt; +#define readq(c)               ({ u64 __v = readq_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define writeb(v,c)            ({ __iowmb(); writeb_relaxed((v),(c)); })</span>
<span class="quote">&gt; +#define writew(v,c)            ({ __iowmb(); writew_relaxed((v),(c)); })</span>
<span class="quote">&gt; +#define writel(v,c)            ({ __iowmb(); writel_relaxed((v),(c)); })</span>
<span class="quote">&gt; +#define writeq(v,c)            ({ __iowmb(); writeq_relaxed((v),(c)); })</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm-generic/io.h&gt;</span>

These do not yet contain all the changes we discussed: the relaxed operations
don&#39;t seem to be ordered against one another and the regular accessors
are not ordered against DMA.

     Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 11:54 a.m.</div>
<pre class="content">
On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:
<span class="quote">&gt; +/* Assume that atomic operations are already serializing */</span>
<span class="quote">&gt; +#define smp_mb__before_atomic_dec()	barrier()</span>
<span class="quote">&gt; +#define smp_mb__after_atomic_dec()	barrier()</span>
<span class="quote">&gt; +#define smp_mb__before_atomic_inc()	barrier()</span>
<span class="quote">&gt; +#define smp_mb__after_atomic_inc()	barrier()</span>
<span class="quote">
&gt; +#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="quote">&gt; +#define smp_mb__after_clear_bit()   smp_mb()</span>

These no longer exist.. Also how can they be different? bitops would use
the same atomic primitives as regular atomic ops would and would thus
have the very same implicit ordering.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 12:06 p.m.</div>
<pre class="content">
On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:
<span class="quote">
&gt; + * atomic_add - add integer to atomic variable</span>
<span class="quote">&gt; + * @i: integer value to add</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically adds @i to @v.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void atomic_add(int i, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoadd.w zero, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="quote">&gt; +		: &quot;r&quot; (i));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define atomic_fetch_add atomic_fetch_add</span>
<span class="quote">&gt; +static inline int atomic_fetch_add(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoadd.w %2, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +	return out;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic_sub - subtract integer from atomic variable</span>
<span class="quote">&gt; + * @i: integer value to subtract</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically subtracts @i from @v.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void atomic_sub(int i, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	atomic_add(-i, v);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define atomic_fetch_sub atomic_fetch_sub</span>
<span class="quote">&gt; +static inline int atomic_fetch_sub(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amosub.w %2, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +	return out;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic_add_return - add integer to atomic variable</span>
<span class="quote">&gt; + * @i: integer value to add</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically adds @i to @v and returns the result</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int atomic_add_return(int i, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	register int c;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoadd.w %0, %2, %1&quot;</span>
<span class="quote">&gt; +		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="quote">&gt; +		: &quot;r&quot; (i));</span>
<span class="quote">&gt; +	return (c + i);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic_sub_return - subtract integer from atomic variable</span>
<span class="quote">&gt; + * @i: integer value to subtract</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically subtracts @i from @v and returns the result</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int atomic_sub_return(int i, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return atomic_add_return(-i, v);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">
&gt; +/**</span>
<span class="quote">&gt; + * atomic_and - Atomically clear bits in atomic variable</span>
<span class="quote">&gt; + * @mask: Mask of the bits to be retained</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically retains the bits set in @mask from @v</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void atomic_and(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoand.w zero, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define atomic_fetch_and atomic_fetch_and</span>
<span class="quote">&gt; +static inline int atomic_fetch_and(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoand.w %2, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +	return out;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic_or - Atomically set bits in atomic variable</span>
<span class="quote">&gt; + * @mask: Mask of the bits to be set</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically sets the bits set in @mask in @v</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void atomic_or(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoor.w zero, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define atomic_fetch_or atomic_fetch_or</span>
<span class="quote">&gt; +static inline int atomic_fetch_or(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoor.w %2, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +	return out;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic_xor - Atomically flips bits in atomic variable</span>
<span class="quote">&gt; + * @mask: Mask of the bits to be flipped</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically flips the bits set in @mask in @v</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void atomic_xor(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoxor.w zero, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define atomic_fetch_xor atomic_fetch_xor</span>
<span class="quote">&gt; +static inline int atomic_fetch_xor(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoxor.w %2, %1, %0&quot;</span>
<span class="quote">&gt; +		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="quote">&gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; +	return out;</span>
<span class="quote">&gt; +}</span>

What pretty much all the other architectures do is something like:

#define ATOMIC_OP(op, asm_op, c_op)				\
static __always_inline void atomic_##op(int i, atomic_t *v)	\
{								\
	__asm__ __volatile__ (					\
		&quot;amo&quot; #asm_op &quot;.w zero, %1, %0&quot;			\
		: &quot;+A&quot; (v-&gt;counter)				\
		: &quot;r&quot; (i));					\
}

#define ATOMIC_FETCH_OP(op, asm_op, c_op)			\
static __always_inline int atomic_fetch_##op(int i, atomic_t *v)\
{								\
	register int ret;					\
	__asm__ __volatile__ (					\
		&quot;amo&quot; #asm_op &quot;.w %2, %1, %0&quot;			\
		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (ret)			\
		: &quot;r&quot; (mask));					\
	return ret;						\
}

#define ATOMIC_OP_RETURN(op, asm_op, c_op)			\
static __always_inline int atomic_##op##_return(int i, atomic_t *v) \
{								\
	return atomic_fetch_##op(i, v) c_op i;			\
}

#define ATOMIC_OPS(op, asm_op, c_op)				\
	ATOMIC_OP(op, asm_op, c_op)				\
	ATOMIC_OP_RETURN(op, asm_op, c_op)			\
	ATOMIC_FETCH_OP(op, asm_op, c_op)

ATOMIC_OPS(add, add, +)
ATOMIC_OPS(sub, sub, -)

#undef ATOMIC_OPS

#define ATOMIC_OPS(op, asm_op, c_op)				\
	ATOMIC_OP(op, asm_op, c_op)				\
	ATOMIC_FETCH_OP(op, asm_op, c_op)

ATOMIC_OPS(and, and, &amp;)
ATOMIC_OPS(or, or, |)
ATOMIC_OPS(xor, xor, ^)

#undef ATOMIC_OPS

Which is much simpler no?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 12:18 p.m.</div>
<pre class="content">
On Wed, Jun 07, 2017 at 02:06:13PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">
&gt; &gt; +static inline int atomic_fetch_sub(unsigned int mask, atomic_t *v)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int out;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; &gt; +		&quot;amosub.w %2, %1, %0&quot;</span>
<span class="quote">&gt; &gt; +		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="quote">&gt; &gt; +		: &quot;r&quot; (mask));</span>
<span class="quote">&gt; &gt; +	return out;</span>
<span class="quote">&gt; &gt; +}</span>

Your instruction manual does not list AMOSUB as a valid instruction. So
either it is wrong or this code never compiled. Please clarify.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 12:25 p.m.</div>
<pre class="content">
On Wed, Jun 07, 2017 at 01:54:23PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; +/* Assume that atomic operations are already serializing */</span>
<span class="quote">&gt; &gt; +#define smp_mb__before_atomic_dec()	barrier()</span>
<span class="quote">&gt; &gt; +#define smp_mb__after_atomic_dec()	barrier()</span>
<span class="quote">&gt; &gt; +#define smp_mb__before_atomic_inc()	barrier()</span>
<span class="quote">&gt; &gt; +#define smp_mb__after_atomic_inc()	barrier()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="quote">&gt; &gt; +#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; These no longer exist.. Also how can they be different? bitops would use</span>
<span class="quote">&gt; the same atomic primitives as regular atomic ops would and would thus</span>
<span class="quote">&gt; have the very same implicit ordering.</span>

Your manual states that each atomic instruction (be it AMO or LR/SC)
have two ordering bits: AQ and RL. If neither are set the instruction is
unordered, if either one is set, its either an ACQUIRE or a RELEASE and
if both are set its SC.

So you can in fact make the above happen, however your atomic
implementation does not appear to use &quot;.aq.rl&quot; mnemonics so would be
entirely unordered, just like your bitops.

Therefore the above is just plain wrong and broken.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 12:36 p.m.</div>
<pre class="content">
On Wed, Jun 07, 2017 at 02:06:13PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; What pretty much all the other architectures do is something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #define ATOMIC_OP(op, asm_op, c_op)				\</span>
<span class="quote">&gt; static __always_inline void atomic_##op(int i, atomic_t *v)	\</span>
<span class="quote">&gt; {								\</span>
<span class="quote">&gt; 	__asm__ __volatile__ (					\</span>
<span class="quote">&gt; 		&quot;amo&quot; #asm_op &quot;.w zero, %1, %0&quot;			\</span>
<span class="quote">&gt; 		: &quot;+A&quot; (v-&gt;counter)				\</span>
<span class="quote">&gt; 		: &quot;r&quot; (i));					\</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #define ATOMIC_FETCH_OP(op, asm_op, c_op)			\</span>
<span class="quote">&gt; static __always_inline int atomic_fetch_##op(int i, atomic_t *v)\</span>
<span class="quote">&gt; {								\</span>
<span class="quote">&gt; 	register int ret;					\</span>
<span class="quote">&gt; 	__asm__ __volatile__ (					\</span>
<span class="quote">&gt; 		&quot;amo&quot; #asm_op &quot;.w %2, %1, %0&quot;			\</span>
<span class="quote">&gt; 		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (ret)			\</span>
<span class="quote">&gt; 		: &quot;r&quot; (mask));					\</span>
<span class="quote">&gt; 	return ret;						\</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #define ATOMIC_OP_RETURN(op, asm_op, c_op)			\</span>
<span class="quote">&gt; static __always_inline int atomic_##op##_return(int i, atomic_t *v) \</span>
<span class="quote">&gt; {								\</span>
<span class="quote">&gt; 	return atomic_fetch_##op(i, v) c_op i;			\</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #define ATOMIC_OPS(op, asm_op, c_op)				\</span>
<span class="quote">&gt; 	ATOMIC_OP(op, asm_op, c_op)				\</span>
<span class="quote">&gt; 	ATOMIC_OP_RETURN(op, asm_op, c_op)			\</span>
<span class="quote">&gt; 	ATOMIC_FETCH_OP(op, asm_op, c_op)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ATOMIC_OPS(add, add, +)</span>
<span class="quote">&gt; ATOMIC_OPS(sub, sub, -)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #undef ATOMIC_OPS</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #define ATOMIC_OPS(op, asm_op, c_op)				\</span>
<span class="quote">&gt; 	ATOMIC_OP(op, asm_op, c_op)				\</span>
<span class="quote">&gt; 	ATOMIC_FETCH_OP(op, asm_op, c_op)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ATOMIC_OPS(and, and, &amp;)</span>
<span class="quote">&gt; ATOMIC_OPS(or, or, |)</span>
<span class="quote">&gt; ATOMIC_OPS(xor, xor, ^)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #undef ATOMIC_OPS</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which is much simpler no?</span>

In fact, after having read your manual you&#39;d want something like:


#define ATOMIC_OP(op, asm_op, c_op)				\
static __always_inline void atomic_##op(int i, atomic_t *v)	\
{								\
	__asm__ __volatile__ (					\
		&quot;amo&quot; #asm_op &quot;.w zero, %1, %0&quot;			\
		: &quot;+A&quot; (v-&gt;counter)				\
		: &quot;r&quot; (i));					\
}

#define ATOMIC_FETCH_OP(op, asm_op, c_op, asm_or, order)	\
static __always_inline int atomic_fetch_##op##order(int i, atomic_t *v)\
{								\
	register int ret;					\
	__asm__ __volatile__ (					\
		&quot;amo&quot; #asm_op &quot;.w&quot; #asm_or &quot; %2, %1, %0&quot;	\
		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (ret)			\
		: &quot;r&quot; (mask));					\
	return ret;						\
}

#define ATOMIC_OP_RETURN(op, asm_op, c_op, asm_or, order)	\
static __always_inline int atomic_##op##_return##order(int i, atomic_t *v) \
{								\
	return atomic_fetch_##op##order(i, v) c_op i;		\
}

#define ATOMIC_OPS(op, asm_op, c_op)				\
	ATOMIC_OP(op, asm_op, c_op, , _relaxed)			\
	ATOMIC_OP_RETURN(op, asm_op, c_op, , _relaxed)		\
	ATOMIC_FETCH_OP(op, asm_op, c_op, , _relaxed)

ATOMIC_OPS(add, add, +)
ATOMIC_OPS(sub, sub, -)

#undef ATOMIC_OPS

#define ATOMIC_OPS(op, asm_op, c_op, asm_or, order)		\
	ATOMIC_OP_RETURN(op, asm_op, c_op, , _relaxed)		\
	ATOMIC_FETCH_OP(op, asm_op, c_op, , _relaxed)

ATOMIC_OPS(add, add, +, &quot;.aq&quot;, _acquire)
ATOMIC_OPS(add, add, +, &quot;.rl&quot;, _release)
ATOMIC_OPS(add, add, +, &quot;.aq.rl&quot;, )

ATOMIC_OPS(sub, sub, -, &quot;.aq&quot;, _acquire)
ATOMIC_OPS(sub, sub, -, &quot;.rl&quot;, _release)
ATOMIC_OPS(sub, sub, -, &quot;.aq.rl&quot;, )

#undef ATOMIC_OPS

#define ATOMIC_OPS(op, asm_op, c_op)				\
	ATOMIC_OP(op, asm_op, c_op)				\
	ATOMIC_FETCH_OP(op, asm_op, c_op, , _relaxed)

ATOMIC_OPS(and, and, &amp;)
ATOMIC_OPS(or, or, |)
ATOMIC_OPS(xor, xor, ^)

#undef ATOMIC_OPS

ATOMIC_FETCH_OP(and, and, &amp;, &quot;.aq&quot;, _acquire)
ATOMIC_FETCH_OP(and, and, &amp;, &quot;.rl&quot;, _release)
ATOMIC_FETCH_OP(and, and, &amp;, &quot;.aq.rl&quot;, )

ATOMIC_FETCH_OP(or, or, |, &quot;.aq&quot;, _acquire)
ATOMIC_FETCH_OP(or, or, |, &quot;.rl&quot;, _release)
ATOMIC_FETCH_OP(or, or, |, &quot;.aq.rl&quot;, )

ATOMIC_FETCH_OP(xor, xor, ^, &quot;.aq&quot;, _acquire)
ATOMIC_FETCH_OP(xor, xor, ^, &quot;.rl&quot;, _release)
ATOMIC_FETCH_OP(xor, xor, ^, &quot;.aq.rl&quot;, )


#define smp_mb__before_atomic()	smp_mb()
#define smp_mb__after_atomic()	smp_mb()


Which (pending the sub confusion) will generate the entire set of:

 atomic_add, atomic_add_return{_relaxed,_acquire,_release,} atomic_fetch_add{_relaxed,_acquire,_release,}
 atomic_sub, atomic_sub_return{_relaxed,_acquire,_release,} atomic_fetch_sub{_relaxed,_acquire,_release,}

 atomic_and, atomic_fetch_and{_relaxed,_acquire,_release,}
 atomic_or,  atomic_fetch_or{_relaxed,_acquire,_release,}
 atomic_xor, atomic_fetch_xor{_relaxed,_acquire,_release,}
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 12:42 p.m.</div>
<pre class="content">
On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:
<span class="quote">&gt; diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..c7ee1321ac18</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="quote">&gt; @@ -0,0 +1,124 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2014 Regents of the University of California</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + *   GNU General Public License for more details.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_CMPXCHG_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/bug.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ISA_A</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/barrier.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __xchg(new, ptr, size)					\</span>
<span class="quote">&gt; +({								\</span>
<span class="quote">&gt; +	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="quote">&gt; +	__typeof__(new) __new = (new);				\</span>
<span class="quote">&gt; +	__typeof__(*(ptr)) __ret;				\</span>
<span class="quote">&gt; +	switch (size) {						\</span>
<span class="quote">&gt; +	case 4:							\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (				\</span>
<span class="quote">&gt; +			&quot;amoswap.w %0, %2, %1&quot;			\</span>
<span class="quote">&gt; +			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="quote">&gt; +			: &quot;r&quot; (__new));				\</span>
<span class="quote">&gt; +		break;						\</span>
<span class="quote">&gt; +	case 8:							\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (				\</span>
<span class="quote">&gt; +			&quot;amoswap.d %0, %2, %1&quot;			\</span>
<span class="quote">&gt; +			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="quote">&gt; +			: &quot;r&quot; (__new));				\</span>
<span class="quote">&gt; +		break;						\</span>
<span class="quote">&gt; +	default:						\</span>
<span class="quote">&gt; +		BUILD_BUG();					\</span>
<span class="quote">&gt; +	}							\</span>
<span class="quote">&gt; +	__ret;							\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr))))</span>

our xchg() is fully ordered, and thus you need to use &quot;amoswap.aq.rl&quot;
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="quote">&gt; + * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="quote">&gt; + * indicated by comparing RETURN with OLD.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __cmpxchg(ptr, old, new, size)					\</span>
<span class="quote">&gt; +({									\</span>
<span class="quote">&gt; +	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="quote">&gt; +	__typeof__(old) __old = (old);					\</span>
<span class="quote">&gt; +	__typeof__(new) __new = (new);					\</span>
<span class="quote">&gt; +	__typeof__(*(ptr)) __ret;					\</span>
<span class="quote">&gt; +	register unsigned int __rc;					\</span>
<span class="quote">&gt; +	switch (size) {							\</span>
<span class="quote">&gt; +	case 4:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +		&quot;0:&quot;							\</span>
<span class="quote">&gt; +			&quot;lr.w %0, %2\n&quot;					\</span>
<span class="quote">&gt; +			&quot;bne  %0, %z3, 1f\n&quot;				\</span>
<span class="quote">&gt; +			&quot;sc.w %1, %z4, %2\n&quot;				\</span>
<span class="quote">&gt; +			&quot;bnez %1, 0b\n&quot;					\</span>
<span class="quote">&gt; +		&quot;1:&quot;							\</span>
<span class="quote">&gt; +			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="quote">&gt; +			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	case 8:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +		&quot;0:&quot;							\</span>
<span class="quote">&gt; +			&quot;lr.d %0, %2\n&quot;					\</span>
<span class="quote">&gt; +			&quot;bne  %0, %z3, 1f\n&quot;				\</span>
<span class="quote">&gt; +			&quot;sc.d %1, %z4, %2\n&quot;				\</span>
<span class="quote">&gt; +			&quot;bnez %1, 0b\n&quot;					\</span>
<span class="quote">&gt; +		&quot;1:&quot;							\</span>
<span class="quote">&gt; +			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="quote">&gt; +			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	default:							\</span>
<span class="quote">&gt; +		BUILD_BUG();						\</span>
<span class="quote">&gt; +	}								\</span>
<span class="quote">&gt; +	__ret;								\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __cmpxchg_mb(ptr, old, new, size)			\</span>
<span class="quote">&gt; +({								\</span>
<span class="quote">&gt; +	__typeof__(*(ptr)) __ret;				\</span>
<span class="quote">&gt; +	smp_mb();						\</span>
<span class="quote">&gt; +	__ret = __cmpxchg((ptr), (old), (new), (size));		\</span>
<span class="quote">&gt; +	smp_mb();						\</span>
<span class="quote">&gt; +	__ret;							\</span>
<span class="quote">&gt; +})</span>

Your ISA of course, but wouldn&#39;t setting the AQ and RL bits on LR/SC be
cheaper than doing two full barriers around the thing?

Note that cmpxchg() doesn&#39;t need to provide ordering on failure.

Further note that we have:

	{atomic_,}cmpxchg{_relaxed,_acquire,_release,}()

and recently:

	{atomic_,}try_cmpxchg{_relaxed,_acquire,_release,}()
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 12:58 p.m.</div>
<pre class="content">
On Wed, Jun 07, 2017 at 02:36:27PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; Which (pending the sub confusion) will generate the entire set of:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  atomic_add, atomic_add_return{_relaxed,_acquire,_release,} atomic_fetch_add{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;  atomic_sub, atomic_sub_return{_relaxed,_acquire,_release,} atomic_fetch_sub{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  atomic_and, atomic_fetch_and{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;  atomic_or,  atomic_fetch_or{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;  atomic_xor, atomic_fetch_xor{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; </span>

Another approach would be to override __atomic_op_{acquire,release} and
use things like:

	&quot;FENCE r,rw&quot; -- (load) ACQUIRE
	&quot;FENCE rw,w&quot; -- (store) RELEASE

And then you only need to provide _relaxed atomics.

Also, and I didn&#39;t check for that, you need to provide:

smp_load_acquire(), smp_store_release(), atomic_read_acquire(),
atomic_store_release().
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - June 7, 2017, 1:16 p.m.</div>
<pre class="content">
[sorry, jumping in here because it&#39;s the only mail I have relating to
 patch 13]

On Wed, Jun 07, 2017 at 02:58:50PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Wed, Jun 07, 2017 at 02:36:27PM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; Which (pending the sub confusion) will generate the entire set of:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  atomic_add, atomic_add_return{_relaxed,_acquire,_release,} atomic_fetch_add{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt;  atomic_sub, atomic_sub_return{_relaxed,_acquire,_release,} atomic_fetch_sub{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  atomic_and, atomic_fetch_and{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt;  atomic_or,  atomic_fetch_or{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt;  atomic_xor, atomic_fetch_xor{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Another approach would be to override __atomic_op_{acquire,release} and</span>
<span class="quote">&gt; use things like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	&quot;FENCE r,rw&quot; -- (load) ACQUIRE</span>
<span class="quote">&gt; 	&quot;FENCE rw,w&quot; -- (store) RELEASE</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And then you only need to provide _relaxed atomics.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, and I didn&#39;t check for that, you need to provide:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; smp_load_acquire(), smp_store_release(), atomic_read_acquire(),</span>
<span class="quote">&gt; atomic_store_release().</span>

Is there an up-to-date specification for the RISC-V memory model? I looked
at:

https://github.com/riscv/riscv-isa-manual/releases/download/riscv-user-2.2/riscv-spec-v2.2.pdf

but it says:

| 2.7 Memory Model
| This section is out of date as the RISC-V memory model is
| currently under revision to ensure it can efficiently support current
| programming language memory models. The revised base mem- ory model will
| contain further ordering constraints, including at least that loads to the
| same address from the same hart cannot be reordered, and that syntactic data
| dependencies between instructions are respected

which, on the one hand is reassuring (because ignoring dependency ordering is
plain broken), but on the other it doesn&#39;t go quite far enough in defining
exactly what constitutes a &quot;syntactic data dependency&quot;. The cumulativity of
your fences also needs defining, because I think this was up in the air at some
point and the document above doesn&#39;t seem to tackle it (it doesn&#39;t seem to
describe what constitutes being a memory of the predecessor or successor sets)

Could you shed some light on this please? We&#39;ve started relying on RW control
dependencies in semi-recent history, so it&#39;s important to get this nailed down.

Thanks,

Will

P.S. You should also totally get your architects to write a formal model ;)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 1:17 p.m.</div>
<pre class="content">
On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:
<span class="quote">&gt; diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..9736f5714e54</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="quote">&gt; @@ -0,0 +1,155 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2015 Regents of the University of California</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + *   GNU General Public License for more details.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_SPINLOCK_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/current.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="quote">&gt; + */</span>

Any reason to use a test-and-set spinlock at all?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="quote">&gt; +#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="quote">&gt; +#define arch_spin_unlock_wait(x) \</span>
<span class="quote">&gt; +		do { cpu_relax(); } while ((x)-&gt;lock)</span>

Hehe, yeah, no ;-) There are ordering constraints on that.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="quote">&gt; +		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt; +		:: &quot;memory&quot;);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int tmp = 1, busy;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="quote">&gt; +		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt; +		: &quot;r&quot; (tmp)</span>
<span class="quote">&gt; +		: &quot;memory&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return !busy;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	while (1) {</span>
<span class="quote">&gt; +		if (arch_spin_is_locked(lock))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (arch_spin_trylock(lock))</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 7, 2017, 4:35 p.m.</div>
<pre class="content">
On Wed, Jun 07, 2017 at 02:58:50PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Wed, Jun 07, 2017 at 02:36:27PM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; Which (pending the sub confusion) will generate the entire set of:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  atomic_add, atomic_add_return{_relaxed,_acquire,_release,} atomic_fetch_add{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt;  atomic_sub, atomic_sub_return{_relaxed,_acquire,_release,} atomic_fetch_sub{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  atomic_and, atomic_fetch_and{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt;  atomic_or,  atomic_fetch_or{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt;  atomic_xor, atomic_fetch_xor{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Another approach would be to override __atomic_op_{acquire,release} and</span>
<span class="quote">&gt; use things like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	&quot;FENCE r,rw&quot; -- (load) ACQUIRE</span>
<span class="quote">&gt; 	&quot;FENCE rw,w&quot; -- (store) RELEASE</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And then you only need to provide _relaxed atomics.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, and I didn&#39;t check for that, you need to provide:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; smp_load_acquire(), smp_store_release(), atomic_read_acquire(),</span>
<span class="quote">&gt; atomic_store_release().</span>

Also, you probably need to provide smp_mb__before_spinlock(), but also
see:

  https://lkml.kernel.org/r/20170607161501.819948352@infradead.org
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - June 9, 2017, 8:16 a.m.</div>
<pre class="content">
On Wed, Jun 07, 2017 at 03:17:27PM +0200, Peter Zijlstra wrote:
<span class="quote">
&gt; &gt; +static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; &gt; +		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="quote">&gt; &gt; +		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt; &gt; +		:: &quot;memory&quot;);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int tmp = 1, busy;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; &gt; +		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="quote">&gt; &gt; +		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt; &gt; +		: &quot;r&quot; (tmp)</span>
<span class="quote">&gt; &gt; +		: &quot;memory&quot;);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return !busy;</span>
<span class="quote">&gt; &gt; +}</span>

One other thing, you need to describe the acquire/release semantics for
your platform. Is the above lock RCpc or RCsc ? If RCpc, you need to
look into adding smp_mb__after_unlock_lock().
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 24, 2017, 2:01 a.m.</div>
<pre class="content">
On Wed, 07 Jun 2017 01:12:00 PDT (-0700), Arnd Bergmann wrote:
<span class="quote">&gt; On Wed, Jun 7, 2017 at 1:00 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt; This patch adds the include files for the RISC-V port.  These are mostly</span>
<span class="quote">&gt;&gt; based on the score port, but there are a lot of arm64-based files as</span>
<span class="quote">&gt;&gt; well.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It might be better to split this up into several parts, as the patch</span>
<span class="quote">&gt; is longer than</span>
<span class="quote">&gt; most people are willing to review at once.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The uapi should definitely be a separate patch, as it includes the parts that</span>
<span class="quote">&gt; cannot be changed any more later. memory management (pgtable, mmu,</span>
<span class="quote">&gt; uaccess) would be another part to split out, and possibly all the atomics</span>
<span class="quote">&gt; in one separate patch (along with spinlocks and bitops).</span>

OK, we&#39;ll do this for the v3.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* IO barriers.  These only fence on the IO bits because they&#39;re only required</span>
<span class="quote">&gt;&gt; + * to order device access.  We&#39;re defining mmiowb because our AMO instructions</span>
<span class="quote">&gt;&gt; + * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7</span>
<span class="quote">&gt;&gt; + * of v2.2 of the user ISA:</span>
<span class="quote">&gt;&gt; + * &quot;The bits order accesses to one of the two address domains, memory or I/O,</span>
<span class="quote">&gt;&gt; + * depending on which address domain the atomic instruction is accessing. No</span>
<span class="quote">&gt;&gt; + * ordering constraint is implied to accesses to the other domain, and a FENCE</span>
<span class="quote">&gt;&gt; + * instruction should be used to order across both domains.&quot;</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __iormb()      __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; +#define __iowmb()      __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define mmiowb()       __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="quote">&gt;&gt; + * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="quote">&gt;&gt; + * accesses.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define readb_relaxed(c)       ({ u8  __r = __raw_readb(c); __r; })</span>
<span class="quote">&gt;&gt; +#define readw_relaxed(c)       ({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })</span>
<span class="quote">&gt;&gt; +#define readl_relaxed(c)       ({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })</span>
<span class="quote">&gt;&gt; +#define readq_relaxed(c)       ({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define writeb_relaxed(v,c)    ((void)__raw_writeb((v),(c)))</span>
<span class="quote">&gt;&gt; +#define writew_relaxed(v,c)    ((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))</span>
<span class="quote">&gt;&gt; +#define writel_relaxed(v,c)    ((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))</span>
<span class="quote">&gt;&gt; +#define writeq_relaxed(v,c)    ((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * I/O memory access primitives. Reads are ordered relative to any</span>
<span class="quote">&gt;&gt; + * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="quote">&gt;&gt; + * Normal memory access.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define readb(c)               ({ u8  __v = readb_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt;&gt; +#define readw(c)               ({ u16 __v = readw_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt;&gt; +#define readl(c)               ({ u32 __v = readl_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt;&gt; +#define readq(c)               ({ u64 __v = readq_relaxed(c); __iormb(); __v; })</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define writeb(v,c)            ({ __iowmb(); writeb_relaxed((v),(c)); })</span>
<span class="quote">&gt;&gt; +#define writew(v,c)            ({ __iowmb(); writew_relaxed((v),(c)); })</span>
<span class="quote">&gt;&gt; +#define writel(v,c)            ({ __iowmb(); writel_relaxed((v),(c)); })</span>
<span class="quote">&gt;&gt; +#define writeq(v,c)            ({ __iowmb(); writeq_relaxed((v),(c)); })</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; These do not yet contain all the changes we discussed: the relaxed operations</span>
<span class="quote">&gt; don&#39;t seem to be ordered against one another and the regular accessors</span>
<span class="quote">&gt; are not ordered against DMA.</span>

Sorry, I must have forgotten to write this -- I just wanted to push out a v3
patch set without the changes to the atomics so everything else could be looked
at.  I wanted to just go through the atomics completely and fix them, as I
found a handful of problems (everything was missing the AQ and RL bits, for
example) and figured it would be best to just get them done right.

I think that&#39;s not something for after dinner on a Friday, but hopefully I&#39;ll
get to it tomorrow morning.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=245">Benjamin Herrenschmidt</a> - June 24, 2017, 3:42 p.m.</div>
<pre class="content">
On Fri, 2017-06-23 at 19:01 -0700, Palmer Dabbelt wrote:
<span class="quote">&gt; &gt; &gt; +#define mmiowb() __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>

I forgot if we already mentioned that but mmiowb is primarily intended
to order MMIO stores vs. a subsequent spin_unlock.

I&#39;m not sure an IO only fence is sufficient here.

Note that I&#39;ve never trusted drivers to get that right, it&#39;s a rather
bad abstraction to begin with, so on powerpc, instead, I just set a
per-cpu flag on every non-relaxed MMIO write and test it in spin_unlock
in order to &quot;beef up&quot; the barrier in there if necessary.

Cheers,
Ben.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 24, 2017, 9:32 p.m.</div>
<pre class="content">
On Sat, 24 Jun 2017 08:42:05 PDT (-0700), benh@kernel.crashing.org wrote:
<span class="quote">&gt; On Fri, 2017-06-23 at 19:01 -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt; &gt; &gt; +#define mmiowb() __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I forgot if we already mentioned that but mmiowb is primarily intended</span>
<span class="quote">&gt; to order MMIO stores vs. a subsequent spin_unlock.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m not sure an IO only fence is sufficient here.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Note that I&#39;ve never trusted drivers to get that right, it&#39;s a rather</span>
<span class="quote">&gt; bad abstraction to begin with, so on powerpc, instead, I just set a</span>
<span class="quote">&gt; per-cpu flag on every non-relaxed MMIO write and test it in spin_unlock</span>
<span class="quote">&gt; in order to &quot;beef up&quot; the barrier in there if necessary.</span>

Sorry about that -- I thought I&#39;d included a note somewhere that the atomics
and barriers weren&#39;t ready to go yet, as we&#39;d found a bunch of problems with
them in the first review and I needed to go through them all.  Arnd suggested
copying the PowerPC approach to mmiowb and I like that better, so we&#39;re going
to use it.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=245">Benjamin Herrenschmidt</a> - June 25, 2017, 3:01 a.m.</div>
<pre class="content">
On Sat, 2017-06-24 at 14:32 -0700, Palmer Dabbelt wrote:
<span class="quote">&gt; On Sat, 24 Jun 2017 08:42:05 PDT (-0700), benh@kernel.crashing.orgwrote:</span>
<span class="quote">&gt; &gt; On Fri, 2017-06-23 at 19:01 -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +#define mmiowb() __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I forgot if we already mentioned that but mmiowb is primarily intended</span>
<span class="quote">&gt; &gt; to order MMIO stores vs. a subsequent spin_unlock.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;m not sure an IO only fence is sufficient here.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Note that I&#39;ve never trusted drivers to get that right, it&#39;s a rather</span>
<span class="quote">&gt; &gt; bad abstraction to begin with, so on powerpc, instead, I just set a</span>
<span class="quote">&gt; &gt; per-cpu flag on every non-relaxed MMIO write and test it in spin_unlock</span>
<span class="quote">&gt; &gt; in order to &quot;beef up&quot; the barrier in there if necessary.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry about that -- I thought I&#39;d included a note somewhere that the atomics</span>
<span class="quote">&gt; and barriers weren&#39;t ready to go yet, as we&#39;d found a bunch of problems with</span>
<span class="quote">&gt; them in the first review and I needed to go through them all. Arnd suggested</span>
<span class="quote">&gt; copying the PowerPC approach to mmiowb and I like that better, so we&#39;re going</span>
<span class="quote">&gt; to use it.</span>

Ah yes, I did see your note, I just wasn&#39;t sure we had clarified the
mmiowb case and thought it was worth mentioning.

Cheers,
Ben,
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 26, 2017, 8:07 p.m.</div>
<pre class="content">
On Wed, 07 Jun 2017 06:17:27 PDT (-0700), peterz@infradead.org wrote:
<span class="quote">&gt; On Tue, Jun 06, 2017 at 04:00:03PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt; diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..9736f5714e54</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,155 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2015 Regents of the University of California</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt;&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt;&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt;&gt; + *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt;&gt; + *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt;&gt; + *   GNU General Public License for more details.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="quote">&gt;&gt; +#define _ASM_RISCV_SPINLOCK_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/current.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Any reason to use a test-and-set spinlock at all?</span>

Just simplicity.  I looked at the MIPS ticket lock and I think we can implement
that while still maintaining the forward progress constraints on our LR/SC
sequences.  I added a FIXME about it, which I&#39;ll try to get around to

  https://github.com/riscv/riscv-linux/commit/a75d28c849e695639b7909ffa88ce571abfb0c76

but I&#39;m going to try and get a v3 patch set out first.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="quote">&gt;&gt; +#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="quote">&gt;&gt; +#define arch_spin_unlock_wait(x) \</span>
<span class="quote">&gt;&gt; +		do { cpu_relax(); } while ((x)-&gt;lock)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hehe, yeah, no ;-) There are ordering constraints on that.</span>

OK.  I copied the ordering guarntees from MIPS here

  https://github.com/riscv/riscv-linux/commit/7d16920452c6bd14c847aade2d51c56d2a1ae457
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt;&gt; +		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="quote">&gt;&gt; +		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt;&gt; +		:: &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int tmp = 1, busy;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt;&gt; +		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="quote">&gt;&gt; +		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt;&gt; +		: &quot;r&quot; (tmp)</span>
<span class="quote">&gt;&gt; +		: &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	return !busy;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	while (1) {</span>
<span class="quote">&gt;&gt; +		if (arch_spin_is_locked(lock))</span>
<span class="quote">&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (arch_spin_trylock(lock))</span>
<span class="quote">&gt;&gt; +			break;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>

Thanks for all the comments!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 26, 2017, 8:07 p.m.</div>
<pre class="content">
On Wed, 07 Jun 2017 05:58:50 PDT (-0700), peterz@infradead.org wrote:
<span class="quote">&gt; On Wed, Jun 07, 2017 at 02:36:27PM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt;&gt; Which (pending the sub confusion) will generate the entire set of:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  atomic_add, atomic_add_return{_relaxed,_acquire,_release,} atomic_fetch_add{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt;  atomic_sub, atomic_sub_return{_relaxed,_acquire,_release,} atomic_fetch_sub{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  atomic_and, atomic_fetch_and{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt;  atomic_or,  atomic_fetch_or{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt;  atomic_xor, atomic_fetch_xor{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Another approach would be to override __atomic_op_{acquire,release} and</span>
<span class="quote">&gt; use things like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	&quot;FENCE r,rw&quot; -- (load) ACQUIRE</span>
<span class="quote">&gt; 	&quot;FENCE rw,w&quot; -- (store) RELEASE</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And then you only need to provide _relaxed atomics.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also, and I didn&#39;t check for that, you need to provide:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; smp_load_acquire(), smp_store_release(), atomic_read_acquire(),</span>
<span class="quote">&gt; atomic_store_release().</span>

OK, thanks for looking so deeply into this.  Sorry it was such a mess, I
thought I included a note somewhere that this all needed to be redone -- I just
wanted to get a v2 out first as that split all the drivers out.  I&#39;ve went
ahead and completely rewrote atomic.h using your suggestions in a slightly
modified way.  It includes

 * _relaxed, _acquire, and _release versions of everything via a bunch of macros.
 * What I believe to be correct aqrl bits on every op.
 * 64-bit and 32-bit atomics (as opposed to just copying everything)

I didn&#39;t implement try_cmpxchg yet.  I&#39;m going to go ahead and sort through our
memory barriers, look at the few remaining CR comments from our v2, and then
submit a v3 patch set.

I&#39;m only replying to this message, but I believe I&#39;ll have taken into account
all your comments for the v3.

Thanks, again, for your time!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 26, 2017, 8:07 p.m.</div>
<pre class="content">
On Wed, 07 Jun 2017 06:16:11 PDT (-0700), will.deacon@arm.com wrote:
<span class="quote">&gt; [sorry, jumping in here because it&#39;s the only mail I have relating to</span>
<span class="quote">&gt;  patch 13]</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On Wed, Jun 07, 2017 at 02:58:50PM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt;&gt; On Wed, Jun 07, 2017 at 02:36:27PM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt;&gt; &gt; Which (pending the sub confusion) will generate the entire set of:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;  atomic_add, atomic_add_return{_relaxed,_acquire,_release,} atomic_fetch_add{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt; &gt;  atomic_sub, atomic_sub_return{_relaxed,_acquire,_release,} atomic_fetch_sub{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;  atomic_and, atomic_fetch_and{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt; &gt;  atomic_or,  atomic_fetch_or{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt; &gt;  atomic_xor, atomic_fetch_xor{_relaxed,_acquire,_release,}</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Another approach would be to override __atomic_op_{acquire,release} and</span>
<span class="quote">&gt;&gt; use things like:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 	&quot;FENCE r,rw&quot; -- (load) ACQUIRE</span>
<span class="quote">&gt;&gt; 	&quot;FENCE rw,w&quot; -- (store) RELEASE</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; And then you only need to provide _relaxed atomics.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Also, and I didn&#39;t check for that, you need to provide:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; smp_load_acquire(), smp_store_release(), atomic_read_acquire(),</span>
<span class="quote">&gt;&gt; atomic_store_release().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Is there an up-to-date specification for the RISC-V memory model? I looked</span>
<span class="quote">&gt; at:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; https://github.com/riscv/riscv-isa-manual/releases/download/riscv-user-2.2/riscv-spec-v2.2.pdf</span>

That&#39;s the most up to date spec.
<span class="quote">
&gt; but it says:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; | 2.7 Memory Model</span>
<span class="quote">&gt; | This section is out of date as the RISC-V memory model is</span>
<span class="quote">&gt; | currently under revision to ensure it can efficiently support current</span>
<span class="quote">&gt; | programming language memory models. The revised base mem- ory model will</span>
<span class="quote">&gt; | contain further ordering constraints, including at least that loads to the</span>
<span class="quote">&gt; | same address from the same hart cannot be reordered, and that syntactic data</span>
<span class="quote">&gt; | dependencies between instructions are respected</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; which, on the one hand is reassuring (because ignoring dependency ordering is</span>
<span class="quote">&gt; plain broken), but on the other it doesn&#39;t go quite far enough in defining</span>
<span class="quote">&gt; exactly what constitutes a &quot;syntactic data dependency&quot;. The cumulativity of</span>
<span class="quote">&gt; your fences also needs defining, because I think this was up in the air at some</span>
<span class="quote">&gt; point and the document above doesn&#39;t seem to tackle it (it doesn&#39;t seem to</span>
<span class="quote">&gt; describe what constitutes being a memory of the predecessor or successor sets)</span>

Unfortunately I&#39;m not really a formal memory model guy, so I probably know a
lot less about what a &quot;syntactic data dependency&quot; is than you do.  I believe
the plan (like for most of RISC-V) is to avoid doing anything weird, to support
existing software systems, and to allow for implementation flexibility where
possible.
<span class="quote">
&gt; Could you shed some light on this please? We&#39;ve started relying on RW control</span>
<span class="quote">&gt; dependencies in semi-recent history, so it&#39;s important to get this nailed down.</span>

This has been a sticking point for a while.  The result of lacking a proper
memory model has been that implementations have been extremely conservative and
as a result there aren&#39;t any issues in practice, but that itself is a bad
thing.
<span class="quote">
&gt; Thanks,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Will</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; P.S. You should also totally get your architects to write a formal model ;)</span>

The RISC-V organization has a working group defining a formal memory model.
Here&#39;s the original posting about the working group

  https://groups.google.com/a/groups.riscv.org/forum/#!topic/isa-dev/Oxm_IvfYItY

To the best of my understanding we hope to have a formal memory model defined
by the end of the year.  I&#39;ve added Daniel Lustig, the chair of the working
group, to the thread.  He knows a lot more about this than I do.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175025">Daniel Lustig</a> - June 27, 2017, 12:07 a.m.</div>
<pre class="content">
<span class="quote">&gt; &gt; https://github.com/riscv/riscv-isa-manual/releases/download/riscv-user</span>
<span class="quote">&gt; &gt; -2.2/riscv-spec-v2.2.pdf</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s the most up to date spec.</span>

Yes, that&#39;s the most up to date public spec.  Internally, the RISC-V memory
model task group has been working on fixing the memory model spec for the
past couple of months now.  We&#39;re aiming to release it for public review
well before the end of the year.  Hopefully in the coming weeks even.
<span class="quote">
&gt; &gt; which, on the one hand is reassuring (because ignoring dependency</span>
<span class="quote">&gt; &gt; ordering is plain broken), but on the other it doesn&#39;t go quite far</span>
<span class="quote">&gt; &gt; enough in defining exactly what constitutes a &quot;syntactic data</span>
<span class="quote">&gt; &gt; dependency&quot;. The cumulativity of your fences also needs defining,</span>
<span class="quote">&gt; &gt; because I think this was up in the air at some point and the document</span>
<span class="quote">&gt; &gt; above doesn&#39;t seem to tackle it (it doesn&#39;t seem to describe what</span>
<span class="quote">&gt; &gt; constitutes being a memory of the predecessor or successor sets)</span>

That will all covered in the new spec.
<span class="quote">
&gt; &gt; P.S. You should also totally get your architects to write a formal</span>
<span class="quote">&gt; &gt; model ;)</span>

Also in progress :)

Were there any more specific questions I can answer in the meantime?  Or
any specific concern you&#39;d like to point me to?

Dan
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - June 27, 2017, 8:48 a.m.</div>
<pre class="content">
Hi Dan,

On Tue, Jun 27, 2017 at 12:07:20AM +0000, Daniel Lustig wrote:
<span class="quote">&gt; &gt; &gt; https://github.com/riscv/riscv-isa-manual/releases/download/riscv-user</span>
<span class="quote">&gt; &gt; &gt; -2.2/riscv-spec-v2.2.pdf</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That&#39;s the most up to date spec.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, that&#39;s the most up to date public spec.  Internally, the RISC-V memory</span>
<span class="quote">&gt; model task group has been working on fixing the memory model spec for the</span>
<span class="quote">&gt; past couple of months now.  We&#39;re aiming to release it for public review</span>
<span class="quote">&gt; well before the end of the year.  Hopefully in the coming weeks even.</span>

Excellent, cheers for the update.
<span class="quote">
&gt; &gt; &gt; which, on the one hand is reassuring (because ignoring dependency</span>
<span class="quote">&gt; &gt; &gt; ordering is plain broken), but on the other it doesn&#39;t go quite far</span>
<span class="quote">&gt; &gt; &gt; enough in defining exactly what constitutes a &quot;syntactic data</span>
<span class="quote">&gt; &gt; &gt; dependency&quot;. The cumulativity of your fences also needs defining,</span>
<span class="quote">&gt; &gt; &gt; because I think this was up in the air at some point and the document</span>
<span class="quote">&gt; &gt; &gt; above doesn&#39;t seem to tackle it (it doesn&#39;t seem to describe what</span>
<span class="quote">&gt; &gt; &gt; constitutes being a memory of the predecessor or successor sets)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That will all covered in the new spec.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; P.S. You should also totally get your architects to write a formal</span>
<span class="quote">&gt; &gt; &gt; model ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also in progress :)</span>

3/3 :)
<span class="quote">
&gt; Were there any more specific questions I can answer in the meantime?  Or</span>
<span class="quote">&gt; any specific concern you&#39;d like to point me to?</span>

Nothing specific, but we won&#39;t be able to review the
memory-ordering/atomics/locking parts of this patch series until we have
a spec.

Will
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/include/asm/Kbuild b/arch/riscv/include/asm/Kbuild</span>
new file mode 100644
<span class="p_header">index 000000000000..710397395981</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/Kbuild</span>
<span class="p_chunk">@@ -0,0 +1,61 @@</span> <span class="p_context"></span>
<span class="p_add">+generic-y += bugs.h</span>
<span class="p_add">+generic-y += cacheflush.h</span>
<span class="p_add">+generic-y += checksum.h</span>
<span class="p_add">+generic-y += clkdev.h</span>
<span class="p_add">+generic-y += cputime.h</span>
<span class="p_add">+generic-y += div64.h</span>
<span class="p_add">+generic-y += dma.h</span>
<span class="p_add">+generic-y += dma-contiguous.h</span>
<span class="p_add">+generic-y += emergency-restart.h</span>
<span class="p_add">+generic-y += errno.h</span>
<span class="p_add">+generic-y += exec.h</span>
<span class="p_add">+generic-y += fb.h</span>
<span class="p_add">+generic-y += fcntl.h</span>
<span class="p_add">+generic-y += ftrace.h</span>
<span class="p_add">+generic-y += futex.h</span>
<span class="p_add">+generic-y += hardirq.h</span>
<span class="p_add">+generic-y += hash.h</span>
<span class="p_add">+generic-y += hw_irq.h</span>
<span class="p_add">+generic-y += ioctl.h</span>
<span class="p_add">+generic-y += ioctls.h</span>
<span class="p_add">+generic-y += ipcbuf.h</span>
<span class="p_add">+generic-y += irq_regs.h</span>
<span class="p_add">+generic-y += irq_work.h</span>
<span class="p_add">+generic-y += kdebug.h</span>
<span class="p_add">+generic-y += kmap_types.h</span>
<span class="p_add">+generic-y += kvm_para.h</span>
<span class="p_add">+generic-y += local.h</span>
<span class="p_add">+generic-y += mm-arch-hooks.h</span>
<span class="p_add">+generic-y += mman.h</span>
<span class="p_add">+generic-y += module.h</span>
<span class="p_add">+generic-y += msgbuf.h</span>
<span class="p_add">+generic-y += mutex.h</span>
<span class="p_add">+generic-y += param.h</span>
<span class="p_add">+generic-y += percpu.h</span>
<span class="p_add">+generic-y += poll.h</span>
<span class="p_add">+generic-y += posix_types.h</span>
<span class="p_add">+generic-y += preempt.h</span>
<span class="p_add">+generic-y += resource.h</span>
<span class="p_add">+generic-y += scatterlist.h</span>
<span class="p_add">+generic-y += sections.h</span>
<span class="p_add">+generic-y += sembuf.h</span>
<span class="p_add">+generic-y += setup.h</span>
<span class="p_add">+generic-y += shmbuf.h</span>
<span class="p_add">+generic-y += shmparam.h</span>
<span class="p_add">+generic-y += signal.h</span>
<span class="p_add">+generic-y += socket.h</span>
<span class="p_add">+generic-y += sockios.h</span>
<span class="p_add">+generic-y += stat.h</span>
<span class="p_add">+generic-y += statfs.h</span>
<span class="p_add">+generic-y += swab.h</span>
<span class="p_add">+generic-y += termbits.h</span>
<span class="p_add">+generic-y += termios.h</span>
<span class="p_add">+generic-y += topology.h</span>
<span class="p_add">+generic-y += trace_clock.h</span>
<span class="p_add">+generic-y += types.h</span>
<span class="p_add">+generic-y += ucontext.h</span>
<span class="p_add">+generic-y += unaligned.h</span>
<span class="p_add">+generic-y += user.h</span>
<span class="p_add">+generic-y += vga.h</span>
<span class="p_add">+generic-y += vmlinux.lds.h</span>
<span class="p_add">+generic-y += xor.h</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/asm-offsets.h b/arch/riscv/include/asm/asm-offsets.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d370ee36a182</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/asm-offsets.h</span>
<span class="p_chunk">@@ -0,0 +1 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;generated/asm-offsets.h&gt;</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/asm.h b/arch/riscv/include/asm/asm.h</span>
new file mode 100644
<span class="p_header">index 000000000000..6cbbb6a68d76</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/asm.h</span>
<span class="p_chunk">@@ -0,0 +1,76 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ASM_H</span>
<span class="p_add">+#define _ASM_RISCV_ASM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+#define __ASM_STR(x)	x</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __ASM_STR(x)	#x</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if __riscv_xlen == 64</span>
<span class="p_add">+#define __REG_SEL(a, b)	__ASM_STR(a)</span>
<span class="p_add">+#elif __riscv_xlen == 32</span>
<span class="p_add">+#define __REG_SEL(a, b)	__ASM_STR(b)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __riscv_xlen&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_L		__REG_SEL(ld, lw)</span>
<span class="p_add">+#define REG_S		__REG_SEL(sd, sw)</span>
<span class="p_add">+#define SZREG		__REG_SEL(8, 4)</span>
<span class="p_add">+#define LGREG		__REG_SEL(3, 2)</span>
<span class="p_add">+</span>
<span class="p_add">+#if __SIZEOF_POINTER__ == 8</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+#define RISCV_PTR		.dword</span>
<span class="p_add">+#define RISCV_SZPTR		8</span>
<span class="p_add">+#define RISCV_LGPTR		3</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define RISCV_PTR		&quot;.dword&quot;</span>
<span class="p_add">+#define RISCV_SZPTR		&quot;8&quot;</span>
<span class="p_add">+#define RISCV_LGPTR		&quot;3&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#elif __SIZEOF_POINTER__ == 4</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+#define RISCV_PTR		.word</span>
<span class="p_add">+#define RISCV_SZPTR		4</span>
<span class="p_add">+#define RISCV_LGPTR		2</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define RISCV_PTR		&quot;.word&quot;</span>
<span class="p_add">+#define RISCV_SZPTR		&quot;4&quot;</span>
<span class="p_add">+#define RISCV_LGPTR		&quot;2&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __SIZEOF_POINTER__&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if (__SIZEOF_INT__ == 4)</span>
<span class="p_add">+#define INT		__ASM_STR(.word)</span>
<span class="p_add">+#define SZINT		__ASM_STR(4)</span>
<span class="p_add">+#define LGINT		__ASM_STR(2)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __SIZEOF_INT__&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if (__SIZEOF_SHORT__ == 2)</span>
<span class="p_add">+#define SHORT		__ASM_STR(.half)</span>
<span class="p_add">+#define SZSHORT		__ASM_STR(2)</span>
<span class="p_add">+#define LGSHORT		__ASM_STR(1)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __SIZEOF_SHORT__&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ASM_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h</span>
new file mode 100644
<span class="p_header">index 000000000000..17ab22036e1a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic.h</span>
<span class="p_chunk">@@ -0,0 +1,349 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cmpxchg.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_INIT(i)	{ (i) }</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_read - read atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically reads the value of @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_read(const atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_set - set atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ * @i: required value</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the value of @v to @i.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_set(atomic_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_add - add integer to atomic variable</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_add(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (i));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_add atomic_fetch_add</span>
<span class="p_add">+static inline int atomic_fetch_add(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_sub - subtract integer from atomic variable</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_sub(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_add(-i, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_sub atomic_fetch_sub</span>
<span class="p_add">+static inline int atomic_fetch_sub(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amosub.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_add_return - add integer to atomic variable</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v and returns the result</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_add_return(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register int c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.w %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (i));</span>
<span class="p_add">+	return (c + i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_sub_return - subtract integer from atomic variable</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v and returns the result</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_sub_return(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_add_return(-i, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_inc - increment atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_inc(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_add(1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_dec - decrement atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_dec(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_add(-1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_inc_return(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_add_return(1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_dec_return(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_sub_return(1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_sub_and_test - subtract value from variable and test result</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v and returns</span>
<span class="p_add">+ * true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_sub_and_test(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_sub_return(i, v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_inc_and_test - increment and test</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1</span>
<span class="p_add">+ * and returns true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_inc_and_test(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_inc_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_dec_and_test - decrement and test</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1 and</span>
<span class="p_add">+ * returns true if the result is 0, or false for all other</span>
<span class="p_add">+ * cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_dec_and_test(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_dec_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_add_negative - add and test if negative</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v and returns true</span>
<span class="p_add">+ * if the result is negative, or false when</span>
<span class="p_add">+ * result is greater than or equal to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_add_negative(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_add_return(i, v) &lt; 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_xchg(atomic_t *v, int n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register int c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (n));</span>
<span class="p_add">+	return c;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_cmpxchg(atomic_t *v, int o, int n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return cmpxchg(&amp;(v-&gt;counter), o, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __atomic_add_unless - add unless the number is already a given value</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ * @a: the amount to add to v...</span>
<span class="p_add">+ * @u: ...unless v is equal to u.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @a to @v, so long as @v was not already @u.</span>
<span class="p_add">+ * Returns the old value of @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int __atomic_add_unless(atomic_t *v, int a, int u)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register int prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+	&quot;0:\n&quot;</span>
<span class="p_add">+		&quot;lr.w %0, %2\n&quot;</span>
<span class="p_add">+		&quot;beq  %0, %4, 1f\n&quot;</span>
<span class="p_add">+		&quot;add  %1, %0, %3\n&quot;</span>
<span class="p_add">+		&quot;sc.w %1, %1, %2\n&quot;</span>
<span class="p_add">+		&quot;bnez %1, 0b\n&quot;</span>
<span class="p_add">+	&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_and - Atomically clear bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be retained</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically retains the bits set in @mask from @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_and(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoand.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_and atomic_fetch_and</span>
<span class="p_add">+static inline int atomic_fetch_and(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoand.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_or - Atomically set bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be set</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_or(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoor.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_or atomic_fetch_or</span>
<span class="p_add">+static inline int atomic_fetch_or(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoor.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_xor - Atomically flips bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be flipped</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically flips the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_xor(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoxor.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_xor atomic_fetch_xor</span>
<span class="p_add">+static inline int atomic_fetch_xor(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoxor.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Assume that atomic operations are already serializing */</span>
<span class="p_add">+#define smp_mb__before_atomic_dec()	barrier()</span>
<span class="p_add">+#define smp_mb__after_atomic_dec()	barrier()</span>
<span class="p_add">+#define smp_mb__before_atomic_inc()	barrier()</span>
<span class="p_add">+#define smp_mb__after_atomic_inc()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/atomic64.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic64.h b/arch/riscv/include/asm/atomic64.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5822c4755f06</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic64.h</span>
<span class="p_chunk">@@ -0,0 +1,355 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC64_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC64_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#include &lt;asm-generic/atomic64.h&gt;</span>
<span class="p_add">+#else /* !CONFIG_GENERIC_ATOMIC64 */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC64_INIT(i)	{ (i) }</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_read - read atomic64 variable</span>
<span class="p_add">+ * @v: pointer of type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically reads the value of @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline s64 atomic64_read(const atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_set - set atomic64 variable</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ * @i: required value</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the value of @v to @i.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_set(atomic64_t *v, s64 i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add - add integer to atomic64 variable</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_add(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_add(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long out;</span>
<span class="p_add">+</span>
<span class="p_add">+	 __asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.d %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_sub - subtract the atomic64 variable</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_sub(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic64_add(-a, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_sub(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amosub.d %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add_return - add and return</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v and returns @i + @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline s64 atomic64_add_return(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.d %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a));</span>
<span class="p_add">+	return (c + a);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_sub_return(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_return(-a, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_inc - increment atomic64 variable</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_inc(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic64_add(1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_dec - decrement atomic64 variable</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_dec(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic64_add(-1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_inc_return(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_return(1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_dec_return(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_return(-1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_inc_and_test - increment and test</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1</span>
<span class="p_add">+ * and returns true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_inc_and_test(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_inc_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_dec_and_test - decrement and test</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1 and</span>
<span class="p_add">+ * returns true if the result is 0, or false for all other</span>
<span class="p_add">+ * cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_dec_and_test(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_dec_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_sub_and_test - subtract value from variable and test result</span>
<span class="p_add">+ * @a: integer value to subtract</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @a from @v and returns</span>
<span class="p_add">+ * true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_sub_and_test(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_sub_return(a, v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add_negative - add and test if negative</span>
<span class="p_add">+ * @a: integer value to add</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @a to @v and returns true</span>
<span class="p_add">+ * if the result is negative, or false when</span>
<span class="p_add">+ * result is greater than or equal to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_add_negative(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_add_return(a, v) &lt; 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_xchg(atomic64_t *v, s64 n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.d %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (n));</span>
<span class="p_add">+	return c;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_cmpxchg(atomic64_t *v, s64 o, s64 n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return cmpxchg(&amp;(v-&gt;counter), o, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * atomic64_dec_if_positive - decrement by 1 if old value positive</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The function returns the old value of *v minus 1, even if</span>
<span class="p_add">+ * the atomic variable, v, was not decremented.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline s64 atomic64_dec_if_positive(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+	&quot;0:\n&quot;</span>
<span class="p_add">+		&quot;lr.d %0, %2\n&quot;</span>
<span class="p_add">+		&quot;add  %0, %0, -1\n&quot;</span>
<span class="p_add">+		&quot;bltz %0, 1f\n&quot;</span>
<span class="p_add">+		&quot;sc.w %1, %0, %2\n&quot;</span>
<span class="p_add">+		&quot;bnez %1, 0b\n&quot;</span>
<span class="p_add">+	&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=r&quot; (rc), &quot;+A&quot; (v-&gt;counter));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add_unless - add unless the number is a given value</span>
<span class="p_add">+ * @v: pointer of type atomic64_t</span>
<span class="p_add">+ * @a: the amount to add to v...</span>
<span class="p_add">+ * @u: ...unless v is equal to u.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @a to @v, so long as it was not @u.</span>
<span class="p_add">+ * Returns true if the addition occurred and false otherwise.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_add_unless(atomic64_t *v, s64 a, s64 u)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 tmp;</span>
<span class="p_add">+	register int rc = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+	&quot;0:\n&quot;</span>
<span class="p_add">+		&quot;lr.d %0, %2\n&quot;</span>
<span class="p_add">+		&quot;beq  %0, %z4, 1f\n&quot;</span>
<span class="p_add">+		&quot;add  %0, %0, %3\n&quot;</span>
<span class="p_add">+		&quot;sc.d %1, %0, %2\n&quot;</span>
<span class="p_add">+		&quot;bnez %1, 0b\n&quot;</span>
<span class="p_add">+	&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (tmp), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;rI&quot; (a), &quot;rJ&quot; (u));</span>
<span class="p_add">+	return !rc;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic64_inc_not_zero(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_and - Atomically clear bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be retained</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically retains the bits set in @mask from @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_and(s64 mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoand.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_and(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoand.d %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_or - Atomically set bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be set</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_or(s64 mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoor.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_or(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoor.d %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_xor - Atomically flips bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be flipped</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically flips the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_xor(s64 mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoxor.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_xor(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoxor.d %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_GENERIC_ATOMIC64 */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC64_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5eb0e501e618</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/barrier.h</span>
<span class="p_chunk">@@ -0,0 +1,41 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Based on arch/arm/include/asm/barrier.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+#define _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define nop()		__asm__ __volatile__ (&quot;nop&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barries need to enforce ordering on both devices or memory. */</span>
<span class="p_add">+#define mb()		__asm__ __volatile__ (&quot;fence iorw,iorw&quot; : : : &quot;memory&quot;)</span>
<span class="p_add">+#define rmb()		__asm__ __volatile__ (&quot;fence ir,ir&quot;     : : : &quot;memory&quot;)</span>
<span class="p_add">+#define wmb()		__asm__ __volatile__ (&quot;fence ow,ow&quot;     : : : &quot;memory&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barries do not need to enforce ordering on devices, just memory. */</span>
<span class="p_add">+#define smp_mb()	__asm__ __volatile__ (&quot;fence rw,rw&quot; : : : &quot;memory&quot;)</span>
<span class="p_add">+#define smp_rmb()	__asm__ __volatile__ (&quot;fence r,r&quot;   : : : &quot;memory&quot;)</span>
<span class="p_add">+#define smp_wmb()	__asm__ __volatile__ (&quot;fence w,w&quot;   : : : &quot;memory&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BARRIER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
new file mode 100644
<span class="p_header">index 000000000000..27e47858c6b1</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bitops.h</span>
<span class="p_chunk">@@ -0,0 +1,228 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+#define _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_BITOPS_H</span>
<span class="p_add">+#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="p_add">+#endif /* _LINUX_BITOPS_H */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqflags.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef smp_mb__before_clear_bit</span>
<span class="p_add">+#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="p_add">+#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="p_add">+#endif /* smp_mb__before_clear_bit */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if (BITS_PER_LONG == 64)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="p_add">+#elif (BITS_PER_LONG == 32)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __test_and_op_bit(op, mod, nr, addr)			\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __res, __mask;				\</span>
<span class="p_add">+	__mask = BIT_MASK(nr);					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; %0, %2, %1&quot;				\</span>
<span class="p_add">+		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="p_add">+		: &quot;r&quot; (mod(__mask)));				\</span>
<span class="p_add">+	((__res &amp; __mask) != 0);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __op_bit(op, mod, nr, addr)				\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; zero, %1, %0&quot;			\</span>
<span class="p_add">+		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="p_add">+		: &quot;r&quot; (mod(BIT_MASK(nr))))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Bitmask modifiers */</span>
<span class="p_add">+#define __NOP(x)	(x)</span>
<span class="p_add">+#define __NOT(x)	(~(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit - Set a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It may be reordered on other architectures than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It can be reordered on other architectures other than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_change_bit - Change a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * set_bit - Atomically set a bit in memory</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="p_add">+ * if you do not require the atomic guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: there are no guarantees that this function will not be reordered</span>
<span class="p_add">+ * on non x86 architectures, so if you are writing portable code,</span>
<span class="p_add">+ * make sure not to rely on its reordering guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit - Clears a bit in memory</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * clear_bit() is atomic and may not be reordered.  However, it does</span>
<span class="p_add">+ * not contain a memory barrier, so if it is used for locking purposes,</span>
<span class="p_add">+ * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()</span>
<span class="p_add">+ * in order to ensure changes are visible on other processors.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * change_bit - Toggle a bit in memory</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * change_bit() is atomic and may not be reordered. It may be</span>
<span class="p_add">+ * reordered on other architectures than x86.</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides acquire barrier semantics.</span>
<span class="p_add">+ * It can be used to implement bit locks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit_lock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return test_and_set_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides release barrier semantics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is like clear_bit_unlock, however it is not atomic.</span>
<span class="p_add">+ * It does provide release barrier semantics so it can be used to unlock</span>
<span class="p_add">+ * a bit lock, however it would only be used if no other CPU can modify</span>
<span class="p_add">+ * any bits in the memory until the lock is released (a good example is</span>
<span class="p_add">+ * if the bit lock itself protects access to the other bits in the word).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef __test_and_op_bit</span>
<span class="p_add">+#undef __op_bit</span>
<span class="p_add">+#undef __NOP</span>
<span class="p_add">+#undef __NOT</span>
<span class="p_add">+#undef __AMO</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/non-atomic.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/le.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ext2-atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BITOPS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bug.h b/arch/riscv/include/asm/bug.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e2f690c20729</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bug.h</span>
<span class="p_chunk">@@ -0,0 +1,88 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BUG_H</span>
<span class="p_add">+#define _ASM_RISCV_BUG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_BUG</span>
<span class="p_add">+#define __BUG_INSN	_AC(0x00100073, UL) /* sbreak */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+typedef u32 bug_insn_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_BUG_RELATIVE_POINTERS</span>
<span class="p_add">+#define __BUG_ENTRY_ADDR	INT &quot; 1b - 2b&quot;</span>
<span class="p_add">+#define __BUG_ENTRY_FILE	INT &quot; %0 - 2b&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __BUG_ENTRY_ADDR	RISCV_PTR &quot; 1b&quot;</span>
<span class="p_add">+#define __BUG_ENTRY_FILE	RISCV_PTR &quot; %0&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_BUGVERBOSE</span>
<span class="p_add">+#define __BUG_ENTRY			\</span>
<span class="p_add">+	__BUG_ENTRY_ADDR &quot;\n\t&quot;		\</span>
<span class="p_add">+	__BUG_ENTRY_FILE &quot;\n\t&quot;		\</span>
<span class="p_add">+	SHORT &quot; %1&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __BUG_ENTRY			\</span>
<span class="p_add">+	__BUG_ENTRY_ADDR</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define BUG()							\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n\t&quot;					\</span>
<span class="p_add">+			&quot;sbreak\n&quot;				\</span>
<span class="p_add">+			&quot;.pushsection __bug_table,\&quot;a\&quot;\n\t&quot;	\</span>
<span class="p_add">+		&quot;2:\n\t&quot;					\</span>
<span class="p_add">+			__BUG_ENTRY &quot;\n\t&quot;			\</span>
<span class="p_add">+			&quot;.org 2b + %2\n\t&quot;			\</span>
<span class="p_add">+			&quot;.popsection&quot;				\</span>
<span class="p_add">+		:						\</span>
<span class="p_add">+		: &quot;i&quot; (__FILE__), &quot;i&quot; (__LINE__),		\</span>
<span class="p_add">+		  &quot;i&quot; (sizeof(struct bug_entry)));		\</span>
<span class="p_add">+	unreachable();						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+#else /* CONFIG_GENERIC_BUG */</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+#define BUG()							\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sbreak\n&quot;);			\</span>
<span class="p_add">+	unreachable();						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+#endif /* CONFIG_GENERIC_BUG */</span>
<span class="p_add">+</span>
<span class="p_add">+#define HAVE_ARCH_BUG</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+struct pt_regs;</span>
<span class="p_add">+struct task_struct;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void die(struct pt_regs *regs, const char *str);</span>
<span class="p_add">+extern void do_trap(struct pt_regs *regs, int signo, int code,</span>
<span class="p_add">+	unsigned long addr, struct task_struct *tsk);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BUG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cache.h b/arch/riscv/include/asm/cache.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e8f0d1110d74</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cache.h</span>
<span class="p_chunk">@@ -0,0 +1,22 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHE_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define L1_CACHE_SHIFT		6</span>
<span class="p_add">+</span>
<span class="p_add">+#define L1_CACHE_BYTES		(1 &lt;&lt; L1_CACHE_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cacheflush.h b/arch/riscv/include/asm/cacheflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0595585013b0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cacheflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef flush_icache_range</span>
<span class="p_add">+#undef flush_icache_user_range</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_icache_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;fence.i&quot; ::: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) local_flush_icache_all()</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) local_flush_icache_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) sbi_remote_fence_i(0)</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) sbi_remote_fence_i(0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHEFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c7ee1321ac18</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -0,0 +1,124 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+#define _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __xchg(new, ptr, size)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="p_add">+	__typeof__(new) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	switch (size) {						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.w %0, %2, %1&quot;			\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.d %0, %2, %1&quot;			\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr))))</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="p_add">+ * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="p_add">+ * indicated by comparing RETURN with OLD.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __cmpxchg(ptr, old, new, size)					\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="p_add">+	__typeof__(old) __old = (old);					\</span>
<span class="p_add">+	__typeof__(new) __new = (new);					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;					\</span>
<span class="p_add">+	register unsigned int __rc;					\</span>
<span class="p_add">+	switch (size) {							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.w %0, %2\n&quot;					\</span>
<span class="p_add">+			&quot;bne  %0, %z3, 1f\n&quot;				\</span>
<span class="p_add">+			&quot;sc.w %1, %z4, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bnez %1, 0b\n&quot;					\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.d %0, %2\n&quot;					\</span>
<span class="p_add">+			&quot;bne  %0, %z3, 1f\n&quot;				\</span>
<span class="p_add">+			&quot;sc.d %1, %z4, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bnez %1, 0b\n&quot;					\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	default:							\</span>
<span class="p_add">+		BUILD_BUG();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__ret;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __cmpxchg_mb(ptr, old, new, size)			\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	smp_mb();						\</span>
<span class="p_add">+	__ret = __cmpxchg((ptr), (old), (new), (size));		\</span>
<span class="p_add">+	smp_mb();						\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg_mb((ptr), (o), (n), sizeof(*(ptr))))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg_local(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr))))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cmpxchg.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CMPXCHG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/compat.h b/arch/riscv/include/asm/compat.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b42bf54f42e4</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/compat.h</span>
<span class="p_chunk">@@ -0,0 +1,31 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASM_COMPAT_H</span>
<span class="p_add">+#define __ASM_COMPAT_H</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+#ifdef CONFIG_COMPAT</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_64BIT)</span>
<span class="p_add">+#define COMPAT_UTS_MACHINE &quot;riscv64\0\0&quot;</span>
<span class="p_add">+#elif defined(CONFIG_32BIT)</span>
<span class="p_add">+#define COMPAT_UTS_MACHINE &quot;riscv32\0\0&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unknown RISC-V base ISA&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /*CONFIG_COMPAT*/</span>
<span class="p_add">+#endif /*__KERNEL__*/</span>
<span class="p_add">+#endif /*__ASM_COMPAT_H*/</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/csr.h b/arch/riscv/include/asm/csr.h</span>
new file mode 100644
<span class="p_header">index 000000000000..387d0dbf0073</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/csr.h</span>
<span class="p_chunk">@@ -0,0 +1,125 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CSR_H</span>
<span class="p_add">+#define _ASM_RISCV_CSR_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Status register flags */</span>
<span class="p_add">+#define SR_IE   _AC(0x00000002, UL) /* Interrupt Enable */</span>
<span class="p_add">+#define SR_PIE  _AC(0x00000020, UL) /* Previous IE */</span>
<span class="p_add">+#define SR_PS   _AC(0x00000100, UL) /* Previously Supervisor */</span>
<span class="p_add">+#define SR_SUM  _AC(0x00040000, UL) /* Supervisor may access User Memory */</span>
<span class="p_add">+</span>
<span class="p_add">+#define SR_FS           _AC(0x00006000, UL) /* Floating-point Status */</span>
<span class="p_add">+#define SR_FS_OFF       _AC(0x00000000, UL)</span>
<span class="p_add">+#define SR_FS_INITIAL   _AC(0x00002000, UL)</span>
<span class="p_add">+#define SR_FS_CLEAN     _AC(0x00004000, UL)</span>
<span class="p_add">+#define SR_FS_DIRTY     _AC(0x00006000, UL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define SR_XS           _AC(0x00018000, UL) /* Extension Status */</span>
<span class="p_add">+#define SR_XS_OFF       _AC(0x00000000, UL)</span>
<span class="p_add">+#define SR_XS_INITIAL   _AC(0x00008000, UL)</span>
<span class="p_add">+#define SR_XS_CLEAN     _AC(0x00010000, UL)</span>
<span class="p_add">+#define SR_XS_DIRTY     _AC(0x00018000, UL)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_64BIT</span>
<span class="p_add">+#define SR_SD   _AC(0x80000000, UL) /* FS/XS dirty */</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SR_SD   _AC(0x8000000000000000, UL) /* FS/XS dirty */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* SPTBR flags */</span>
<span class="p_add">+#if __riscv_xlen == 32</span>
<span class="p_add">+#define SPTBR_PPN     _AC(0x003FFFFF, UL)</span>
<span class="p_add">+#define SPTBR_MODE_32 _AC(0x80000000, UL)</span>
<span class="p_add">+#define SPTBR_MODE    SPTBR_MODE_32</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SPTBR_PPN     _AC(0x00000FFFFFFFFFFF, UL)</span>
<span class="p_add">+#define SPTBR_MODE_39 _AC(0x8000000000000000, UL)</span>
<span class="p_add">+#define SPTBR_MODE    SPTBR_MODE_39</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* Interrupt Enable and Interrupt Pending flags */</span>
<span class="p_add">+#define SIE_SSIE _AC(0x00000002, UL) /* Software Interrupt Enable */</span>
<span class="p_add">+#define SIE_STIE _AC(0x00000020, UL) /* Timer Interrupt Enable */</span>
<span class="p_add">+</span>
<span class="p_add">+#define EXC_INST_MISALIGNED     0</span>
<span class="p_add">+#define EXC_INST_ACCESS         1</span>
<span class="p_add">+#define EXC_BREAKPOINT          3</span>
<span class="p_add">+#define EXC_LOAD_ACCESS         5</span>
<span class="p_add">+#define EXC_STORE_ACCESS        7</span>
<span class="p_add">+#define EXC_SYSCALL             8</span>
<span class="p_add">+#define EXC_INST_PAGE_FAULT     12</span>
<span class="p_add">+#define EXC_LOAD_PAGE_FAULT     13</span>
<span class="p_add">+#define EXC_STORE_PAGE_FAULT    15</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_swap(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrrw %0, &quot; #csr &quot;, %1&quot;		\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v) : &quot;rK&quot; (__v));	\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_read(csr)						\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	register unsigned long __v;				\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrr %0, &quot; #csr			\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v));			\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_write(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrw &quot; #csr &quot;, %0&quot;		\</span>
<span class="p_add">+			      : : &quot;rK&quot; (__v));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_read_set(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrrs %0, &quot; #csr &quot;, %1&quot;		\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v) : &quot;rK&quot; (__v));	\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_set(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrs &quot; #csr &quot;, %0&quot;		\</span>
<span class="p_add">+			      : : &quot;rK&quot; (__v));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_read_clear(csr, val)				\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrrc %0, &quot; #csr &quot;, %1&quot;		\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v) : &quot;rK&quot; (__v));	\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_clear(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrc &quot; #csr &quot;, %0&quot;		\</span>
<span class="p_add">+			      : : &quot;rK&quot; (__v));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CSR_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/current.h b/arch/riscv/include/asm/current.h</span>
new file mode 100644
<span class="p_header">index 000000000000..44612c32fc4a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/current.h</span>
<span class="p_chunk">@@ -0,0 +1,42 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Based on arm/arm64/include/asm/current.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2016 ARM</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASM_CURRENT_H</span>
<span class="p_add">+#define __ASM_CURRENT_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+struct task_struct;</span>
<span class="p_add">+</span>
<span class="p_add">+/* This only works because &quot;struct thread_info&quot; is at offset 0 from &quot;struct</span>
<span class="p_add">+ * task_struct&quot;.  This constraint seems to be necessary on other architectures</span>
<span class="p_add">+ * as well, but __switch_to enforces it.  We can&#39;t check TASK_TI here because</span>
<span class="p_add">+ * &lt;asm/asm-offsets.h&gt; includes this, and I can&#39;t get the definition of &quot;struct</span>
<span class="p_add">+ * task_struct&quot; here due to some header ordering problems.  */</span>
<span class="p_add">+static __always_inline struct task_struct *get_current(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register struct task_struct *tp __asm__(&quot;tp&quot;);</span>
<span class="p_add">+	return tp;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define current get_current()</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASM_CURRENT_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/delay.h b/arch/riscv/include/asm/delay.h</span>
new file mode 100644
<span class="p_header">index 000000000000..cbb0c9eb96cb</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/delay.h</span>
<span class="p_chunk">@@ -0,0 +1,28 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2016 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_DELAY_H</span>
<span class="p_add">+#define _ASM_RISCV_DELAY_H</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long riscv_timebase;</span>
<span class="p_add">+</span>
<span class="p_add">+#define udelay udelay</span>
<span class="p_add">+extern void udelay(unsigned long usecs);</span>
<span class="p_add">+</span>
<span class="p_add">+#define ndelay ndelay</span>
<span class="p_add">+extern void ndelay(unsigned long nsecs);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __delay(unsigned long cycles);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_DELAY_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/device.h b/arch/riscv/include/asm/device.h</span>
new file mode 100644
<span class="p_header">index 000000000000..28975e528d2f</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/device.h</span>
<span class="p_chunk">@@ -0,0 +1,27 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_DEVICE_H</span>
<span class="p_add">+#define _ASM_RISCV_DEVICE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sysfs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct dev_archdata {</span>
<span class="p_add">+	struct dma_map_ops *dma_ops;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct pdev_archdata {</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_DEVICE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/dma-mapping.h b/arch/riscv/include/asm/dma-mapping.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9485a58e839e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -0,0 +1,41 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2003-2004 Hewlett-Packard Co</span>
<span class="p_add">+ *	David Mosberger-Tang &lt;davidm@hpl.hp.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive, Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASM_RISCV_DMA_MAPPING_H</span>
<span class="p_add">+#define __ASM_RISCV_DMA_MAPPING_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+/* Use ops-&gt;dma_mapping_error (if it exists) or assume success */</span>
<span class="p_add">+// #undef DMA_ERROR_CODE</span>
<span class="p_add">+</span>
<span class="p_add">+static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;dma_noop_ops;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dev-&gt;dma_mask)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return addr + size - 1 &lt;= *dev-&gt;dma_mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif	/* __KERNEL__ */</span>
<span class="p_add">+#endif	/* __ASM_RISCV_DMA_MAPPING_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/elf.h b/arch/riscv/include/asm/elf.h</span>
new file mode 100644
<span class="p_header">index 000000000000..add1245690f6</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/elf.h</span>
<span class="p_chunk">@@ -0,0 +1,85 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2003 Matjaz Breskvar &lt;phoenix@bsemi.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2010-2011 Jonas Bonn &lt;jonas@southpole.se&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ELF_H</span>
<span class="p_add">+#define _ASM_RISCV_ELF_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/elf.h&gt;</span>
<span class="p_add">+#include &lt;asm/auxvec.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* TODO: Move definition into include/uapi/linux/elf-em.h */</span>
<span class="p_add">+#define EM_RISCV	0xF3</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These are used to set parameters in the core dumps.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_ARCH	EM_RISCV</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define ELF_CLASS	ELFCLASS64</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ELF_CLASS	ELFCLASS32</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(__LITTLE_ENDIAN)</span>
<span class="p_add">+#define ELF_DATA	ELFDATA2LSB</span>
<span class="p_add">+#elif defined(__BIG_ENDIAN)</span>
<span class="p_add">+#define ELF_DATA	ELFDATA2MSB</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unknown endianness&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is used to ensure we don&#39;t load something for the wrong architecture.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define elf_check_arch(x) ((x)-&gt;e_machine == EM_RISCV)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CORE_DUMP_USE_REGSET</span>
<span class="p_add">+#define ELF_EXEC_PAGESIZE	(PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is the location that an ET_DYN program is loaded if exec&#39;ed.  Typical</span>
<span class="p_add">+ * use of this is to invoke &quot;./ld.so someprog&quot; to test out a new version of</span>
<span class="p_add">+ * the loader.  We need to make sure that it is out of the way of the program</span>
<span class="p_add">+ * that it will &quot;exec&quot;, and that there is sufficient room for the brk.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		((TASK_SIZE / 3) * 2)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This yields a mask that user programs can use to figure out what</span>
<span class="p_add">+ * instruction set this CPU supports.  This could be done in user space,</span>
<span class="p_add">+ * but it&#39;s not easy, and we&#39;ve already done it here.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_HWCAP	(0)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This yields a string that ld.so will use to load implementation</span>
<span class="p_add">+ * specific libraries for optimization.  This is more specific in</span>
<span class="p_add">+ * intent than poking at uname or /proc/cpuinfo.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_PLATFORM	(NULL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_DLINFO						\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	NEW_AUX_ENT(AT_SYSINFO_EHDR,				\</span>
<span class="p_add">+		(elf_addr_t)current-&gt;mm-&gt;context.vdso);		\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+#define ARCH_HAS_SETUP_ADDITIONAL_PAGES</span>
<span class="p_add">+struct linux_binprm;</span>
<span class="p_add">+extern int arch_setup_additional_pages(struct linux_binprm *bprm,</span>
<span class="p_add">+	int uses_interp);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ELF_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
new file mode 100644
<span class="p_header">index 000000000000..a3ee80082b88</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/io.h</span>
<span class="p_chunk">@@ -0,0 +1,152 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * {read,write}{b,w,l,q} based on arch/arm64/include/asm/io.h</span>
<span class="p_add">+ *   which was based on arch/arm/include/io.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 1996-2000 Russell King</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IO_H</span>
<span class="p_add">+#define _ASM_RISCV_IO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+</span>
<span class="p_add">+extern void iounmap(void __iomem *addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Generic IO read/write.  These perform native-endian accesses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __raw_writeb __raw_writeb</span>
<span class="p_add">+static inline void __raw_writeb(u8 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sb %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writew __raw_writew</span>
<span class="p_add">+static inline void __raw_writew(u16 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sh %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writel __raw_writel</span>
<span class="p_add">+static inline void __raw_writel(u32 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sw %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_writeq __raw_writeq</span>
<span class="p_add">+static inline void __raw_writeq(u64 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sd %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readb __raw_readb</span>
<span class="p_add">+static inline u8 __raw_readb(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lb %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readw __raw_readw</span>
<span class="p_add">+static inline u16 __raw_readw(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lh %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readl __raw_readl</span>
<span class="p_add">+static inline u32 __raw_readl(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lw %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_readq __raw_readq</span>
<span class="p_add">+static inline u64 __raw_readq(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;ld %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* IO barriers.  These only fence on the IO bits because they&#39;re only required</span>
<span class="p_add">+ * to order device access.  We&#39;re defining mmiowb because our AMO instructions</span>
<span class="p_add">+ * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7</span>
<span class="p_add">+ * of v2.2 of the user ISA:</span>
<span class="p_add">+ * &quot;The bits order accesses to one of the two address domains, memory or I/O,</span>
<span class="p_add">+ * depending on which address domain the atomic instruction is accessing. No</span>
<span class="p_add">+ * ordering constraint is implied to accesses to the other domain, and a FENCE</span>
<span class="p_add">+ * instruction should be used to order across both domains.&quot;</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define __iormb()	__asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __iowmb()	__asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define mmiowb()	__asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="p_add">+ * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="p_add">+ * accesses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define readb_relaxed(c)	({ u8  __r = __raw_readb(c); __r; })</span>
<span class="p_add">+#define readw_relaxed(c)	({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })</span>
<span class="p_add">+#define readl_relaxed(c)	({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })</span>
<span class="p_add">+#define readq_relaxed(c)	({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_relaxed(v,c)	((void)__raw_writeb((v),(c)))</span>
<span class="p_add">+#define writew_relaxed(v,c)	((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))</span>
<span class="p_add">+#define writel_relaxed(v,c)	((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))</span>
<span class="p_add">+#define writeq_relaxed(v,c)	((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * I/O memory access primitives. Reads are ordered relative to any</span>
<span class="p_add">+ * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="p_add">+ * Normal memory access.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define readb(c)		({ u8  __v = readb_relaxed(c); __iormb(); __v; })</span>
<span class="p_add">+#define readw(c)		({ u16 __v = readw_relaxed(c); __iormb(); __v; })</span>
<span class="p_add">+#define readl(c)		({ u32 __v = readl_relaxed(c); __iormb(); __v; })</span>
<span class="p_add">+#define readq(c)		({ u64 __v = readq_relaxed(c); __iormb(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb(v,c)		({ __iowmb(); writeb_relaxed((v),(c)); })</span>
<span class="p_add">+#define writew(v,c)		({ __iowmb(); writew_relaxed((v),(c)); })</span>
<span class="p_add">+#define writel(v,c)		({ __iowmb(); writel_relaxed((v),(c)); })</span>
<span class="p_add">+#define writeq(v,c)		({ __iowmb(); writeq_relaxed((v),(c)); })</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/irq.h b/arch/riscv/include/asm/irq.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e64c61e89f76</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/irq.h</span>
<span class="p_chunk">@@ -0,0 +1,31 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IRQ_H</span>
<span class="p_add">+#define _ASM_RISCV_IRQ_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define NR_IRQS         0</span>
<span class="p_add">+</span>
<span class="p_add">+#define INTERRUPT_CAUSE_SOFTWARE    1</span>
<span class="p_add">+#define INTERRUPT_CAUSE_TIMER       5</span>
<span class="p_add">+#define INTERRUPT_CAUSE_EXTERNAL    9</span>
<span class="p_add">+</span>
<span class="p_add">+void riscv_timer_interrupt(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/irq.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* The value of csr sie before init_traps runs (core is up) */</span>
<span class="p_add">+DECLARE_PER_CPU(atomic_long_t, riscv_early_sie);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IRQ_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/irqflags.h b/arch/riscv/include/asm/irqflags.h</span>
new file mode 100644
<span class="p_header">index 000000000000..6fdc860d7f84</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/irqflags.h</span>
<span class="p_chunk">@@ -0,0 +1,63 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IRQFLAGS_H</span>
<span class="p_add">+#define _ASM_RISCV_IRQFLAGS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* read interrupt enabled status */</span>
<span class="p_add">+static inline unsigned long arch_local_save_flags(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return csr_read(sstatus);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* unconditionally enable interrupts */</span>
<span class="p_add">+static inline void arch_local_irq_enable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_set(sstatus, SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* unconditionally disable interrupts */</span>
<span class="p_add">+static inline void arch_local_irq_disable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_clear(sstatus, SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* get status and disable interrupts */</span>
<span class="p_add">+static inline unsigned long arch_local_irq_save(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return csr_read_clear(sstatus, SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* test flags */</span>
<span class="p_add">+static inline int arch_irqs_disabled_flags(unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !(flags &amp; SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* test hardware interrupt enable bit */</span>
<span class="p_add">+static inline int arch_irqs_disabled(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return arch_irqs_disabled_flags(arch_local_save_flags());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* set interrupt enabled status */</span>
<span class="p_add">+static inline void arch_local_irq_restore(unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_set(sstatus, flags &amp; SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IRQFLAGS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/kprobes.h b/arch/riscv/include/asm/kprobes.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1190de7a0f74</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/kprobes.h</span>
<span class="p_chunk">@@ -0,0 +1,22 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASM_RISCV_KPROBES_H</span>
<span class="p_add">+#define ASM_RISCV_KPROBES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KPROBES</span>
<span class="p_add">+#error &quot;RISC-V doesn&#39;t skpport CONFIG_KPROBES&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/linkage.h b/arch/riscv/include/asm/linkage.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b7b304ca89c4</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/linkage.h</span>
<span class="p_chunk">@@ -0,0 +1,20 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_LINKAGE_H</span>
<span class="p_add">+#define _ASM_RISCV_LINKAGE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ALIGN		.balign 4</span>
<span class="p_add">+#define __ALIGN_STR	&quot;.balign 4&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_LINKAGE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/mmu.h b/arch/riscv/include/asm/mmu.h</span>
new file mode 100644
<span class="p_header">index 000000000000..66805cba9a27</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/mmu.h</span>
<span class="p_chunk">@@ -0,0 +1,26 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_MMU_H</span>
<span class="p_add">+#define _ASM_RISCV_MMU_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	void *vdso;</span>
<span class="p_add">+} mm_context_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_MMU_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/mmu_context.h b/arch/riscv/include/asm/mmu_context.h</span>
new file mode 100644
<span class="p_header">index 000000000000..de1fc1631fc4</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -0,0 +1,69 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_MMU_CONTEXT_H</span>
<span class="p_add">+#define _ASM_RISCV_MMU_CONTEXT_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/mm_hooks.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void enter_lazy_tlb(struct mm_struct *mm,</span>
<span class="p_add">+	struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Initialize context-related info for a new mm_struct */</span>
<span class="p_add">+static inline int init_new_context(struct task_struct *task,</span>
<span class="p_add">+	struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void destroy_context(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *current_pgdir(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_virt(csr_read(sptbr) &amp; SPTBR_PPN);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pgdir(pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_write(sptbr, virt_to_pfn(pgd) | SPTBR_MODE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void switch_mm(struct mm_struct *prev,</span>
<span class="p_add">+	struct mm_struct *next, struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (likely(prev != next)) {</span>
<span class="p_add">+		set_pgdir(next-&gt;pgd);</span>
<span class="p_add">+		local_flush_tlb_all();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void activate_mm(struct mm_struct *prev,</span>
<span class="p_add">+			       struct mm_struct *next)</span>
<span class="p_add">+{</span>
<span class="p_add">+	switch_mm(prev, next, NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void deactivate_mm(struct task_struct *task,</span>
<span class="p_add">+	struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_MMU_CONTEXT_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/page.h b/arch/riscv/include/asm/page.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e1491c20d6fd</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/page.h</span>
<span class="p_chunk">@@ -0,0 +1,138 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ * Copyright (C) 2017 XiaojingZhu &lt;zhuxiaoj@ict.ac.cn&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PAGE_H</span>
<span class="p_add">+#define _ASM_RISCV_PAGE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/pfn.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_SHIFT	(12)</span>
<span class="p_add">+#define PAGE_SIZE	(_AC(1, UL) &lt;&lt; PAGE_SHIFT)</span>
<span class="p_add">+#define PAGE_MASK	(~(PAGE_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PAGE_OFFSET -- the first address of the first page of memory.</span>
<span class="p_add">+ * When not using MMU this corresponds to the first free page in</span>
<span class="p_add">+ * physical memory (aligned on a page boundary).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define PAGE_OFFSET		_AC(0xffffffff80000000, UL)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PAGE_OFFSET		_AC(0xc0000000, UL)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define KERN_VIRT_SIZE (-PAGE_OFFSET)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_UP(addr)	(((addr)+((PAGE_SIZE)-1))&amp;(~((PAGE_SIZE)-1)))</span>
<span class="p_add">+#define PAGE_DOWN(addr)	((addr)&amp;(~((PAGE_SIZE)-1)))</span>
<span class="p_add">+</span>
<span class="p_add">+/* align addr on a size boundary - adjust address up/down if needed */</span>
<span class="p_add">+#define _ALIGN_UP(addr, size)	(((addr)+((size)-1))&amp;(~((size)-1)))</span>
<span class="p_add">+#define _ALIGN_DOWN(addr, size)	((addr)&amp;(~((size)-1)))</span>
<span class="p_add">+</span>
<span class="p_add">+/* align addr on a size boundary - adjust address up if needed */</span>
<span class="p_add">+#define _ALIGN(addr, size)	_ALIGN_UP(addr, size)</span>
<span class="p_add">+</span>
<span class="p_add">+#define clear_page(pgaddr)			memset((pgaddr), 0, PAGE_SIZE)</span>
<span class="p_add">+#define copy_page(to, from)			memcpy((to), (from), PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define clear_user_page(pgaddr, vaddr, page)	memset((pgaddr), 0, PAGE_SIZE)</span>
<span class="p_add">+#define copy_user_page(vto, vfrom, vaddr, topg) \</span>
<span class="p_add">+			memcpy((vto), (vfrom), PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Use struct definitions to apply C type checking</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Global Directory entry */</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pgd;</span>
<span class="p_add">+} pgd_t;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Table entry */</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pte;</span>
<span class="p_add">+} pte_t;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pgprot;</span>
<span class="p_add">+} pgprot_t;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct page *pgtable_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_val(x)	((x).pte)</span>
<span class="p_add">+#define pgd_val(x)	((x).pgd)</span>
<span class="p_add">+#define pgprot_val(x)	((x).pgprot)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pte(x)	((pte_t) { (x) })</span>
<span class="p_add">+#define __pgd(x)	((pgd_t) { (x) })</span>
<span class="p_add">+#define __pgprot(x)	((pgprot_t) { (x) })</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BITS</span>
<span class="p_add">+#define PTE_FMT &quot;%016lx&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PTE_FMT &quot;%08lx&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long va_pa_offset;</span>
<span class="p_add">+extern unsigned long pfn_base;</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long max_low_pfn;</span>
<span class="p_add">+extern unsigned long min_low_pfn;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pa(x)		((unsigned long)(x) - va_pa_offset)</span>
<span class="p_add">+#define __va(x)		((void *)((unsigned long) (x) + va_pa_offset))</span>
<span class="p_add">+</span>
<span class="p_add">+#define phys_to_pfn(phys)	(PFN_DOWN(phys))</span>
<span class="p_add">+#define pfn_to_phys(pfn)	(PFN_PHYS(pfn))</span>
<span class="p_add">+</span>
<span class="p_add">+#define virt_to_pfn(vaddr)	(phys_to_pfn(__pa(vaddr)))</span>
<span class="p_add">+#define pfn_to_virt(pfn)	(__va(pfn_to_phys(pfn)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define virt_to_page(vaddr)	(pfn_to_page(virt_to_pfn(vaddr)))</span>
<span class="p_add">+#define page_to_virt(page)	(pfn_to_virt(page_to_pfn(page)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define page_to_phys(page)	(pfn_to_phys(page_to_pfn(page)))</span>
<span class="p_add">+#define page_to_bus(page)	(page_to_phys(page))</span>
<span class="p_add">+#define phys_to_page(paddr)	(pfn_to_page(phys_to_pfn(paddr)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define pfn_valid(pfn) \</span>
<span class="p_add">+	(((pfn) &gt;= pfn_base) &amp;&amp; (((pfn)-pfn_base) &lt; max_mapnr))</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_PFN_OFFSET		(pfn_base)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define virt_addr_valid(vaddr)	(pfn_valid(virt_to_pfn(vaddr)))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \</span>
<span class="p_add">+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/memory_model.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/getorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* vDSO support */</span>
<span class="p_add">+/* We do define AT_SYSINFO_EHDR but don&#39;t use the gate mechanism */</span>
<span class="p_add">+#define __HAVE_ARCH_GATE_AREA</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PAGE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pci.h b/arch/riscv/include/asm/pci.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c829a4ce8d25</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pci.h</span>
<span class="p_chunk">@@ -0,0 +1,50 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASM_RISCV_PCI_H</span>
<span class="p_add">+#define __ASM_RISCV_PCI_H</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PCIBIOS_MIN_IO		0</span>
<span class="p_add">+#define PCIBIOS_MIN_MEM		0</span>
<span class="p_add">+</span>
<span class="p_add">+/* RISC-V shim does not initialize PCI bus */</span>
<span class="p_add">+#define pcibios_assign_all_busses() 1</span>
<span class="p_add">+</span>
<span class="p_add">+/* RISC-V TileLink and PCIe share the share address space */</span>
<span class="p_add">+#define PCI_DMA_BUS_IS_PHYS 1</span>
<span class="p_add">+</span>
<span class="p_add">+extern int isa_dma_bridge_buggy;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PCI</span>
<span class="p_add">+static inline int pci_get_legacy_ide_irq(struct pci_dev *dev, int channel)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* no legacy IRQ on risc-v */</span>
<span class="p_add">+	return -ENODEV;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pci_proc_domain(struct pci_bus *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* always show the domain in /proc */</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif  /* CONFIG_PCI */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif  /* __KERNEL__ */</span>
<span class="p_add">+#endif  /* __ASM_PCI_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgalloc.h b/arch/riscv/include/asm/pgalloc.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b40074bcb164</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -0,0 +1,124 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGALLOC_H</span>
<span class="p_add">+#define _ASM_RISCV_PGALLOC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_populate_kernel(struct mm_struct *mm,</span>
<span class="p_add">+	pmd_t *pmd, pte_t *pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = virt_to_pfn(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pmd(pmd, __pmd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | _PAGE_TABLE));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_populate(struct mm_struct *mm,</span>
<span class="p_add">+	pmd_t *pmd, pgtable_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = virt_to_pfn(page_address(pte));</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pmd(pmd, __pmd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | _PAGE_TABLE));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __PAGETABLE_PMD_FOLDED</span>
<span class="p_add">+static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = virt_to_pfn(pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pud(pud, __pud((pfn &lt;&lt; _PAGE_PFN_SHIFT) | _PAGE_TABLE));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* __PAGETABLE_PMD_FOLDED */</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_pgtable(pmd)	pmd_page(pmd)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = (pgd_t *)__get_free_page(GFP_KERNEL);</span>
<span class="p_add">+	if (likely(pgd != NULL)) {</span>
<span class="p_add">+		memset(pgd, 0, USER_PTRS_PER_PGD * sizeof(pgd_t));</span>
<span class="p_add">+		/* Copy kernel mappings */</span>
<span class="p_add">+		memcpy(pgd + USER_PTRS_PER_PGD,</span>
<span class="p_add">+			init_mm.pgd + USER_PTRS_PER_PGD,</span>
<span class="p_add">+			(PTRS_PER_PGD - USER_PTRS_PER_PGD) * sizeof(pgd_t));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_page((unsigned long)pgd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __PAGETABLE_PMD_FOLDED</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_t *)__get_free_page(</span>
<span class="p_add">+		GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_free(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_page((unsigned long)pmd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pmd_free_tlb(tlb, pmd, addr)  pmd_free((tlb)-&gt;mm, pmd)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __PAGETABLE_PMD_FOLDED */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_t *)__get_free_page(</span>
<span class="p_add">+		GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *pte_alloc_one(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = alloc_page(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+	if (likely(pte != NULL))</span>
<span class="p_add">+		pgtable_page_ctor(pte);</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pte_free_kernel(struct mm_struct *mm, pte_t *pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_page((unsigned long)pte);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pte_free(struct mm_struct *mm, pgtable_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgtable_page_dtor(pte);</span>
<span class="p_add">+	__free_page(pte);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pte_free_tlb(tlb, pte, buf)   \</span>
<span class="p_add">+do {                                    \</span>
<span class="p_add">+	pgtable_page_dtor(pte);         \</span>
<span class="p_add">+	tlb_remove_page((tlb), pte);    \</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void check_pgt_cache(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGALLOC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable-32.h b/arch/riscv/include/asm/pgtable-32.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d61974b74182</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable-32.h</span>
<span class="p_chunk">@@ -0,0 +1,25 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_32_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_32_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/pgtable-nopmd.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Size of region mapped by a page global directory */</span>
<span class="p_add">+#define PGDIR_SHIFT     22</span>
<span class="p_add">+#define PGDIR_SIZE      (_AC(1, UL) &lt;&lt; PGDIR_SHIFT)</span>
<span class="p_add">+#define PGDIR_MASK      (~(PGDIR_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_32_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable-64.h b/arch/riscv/include/asm/pgtable-64.h</span>
new file mode 100644
<span class="p_header">index 000000000000..7aa0ea9bd8bb</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable-64.h</span>
<span class="p_chunk">@@ -0,0 +1,84 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_64_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_64_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PGDIR_SHIFT     30</span>
<span class="p_add">+/* Size of region mapped by a page global directory */</span>
<span class="p_add">+#define PGDIR_SIZE      (_AC(1, UL) &lt;&lt; PGDIR_SHIFT)</span>
<span class="p_add">+#define PGDIR_MASK      (~(PGDIR_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_SHIFT       21</span>
<span class="p_add">+/* Size of region mapped by a page middle directory */</span>
<span class="p_add">+#define PMD_SIZE        (_AC(1, UL) &lt;&lt; PMD_SHIFT)</span>
<span class="p_add">+#define PMD_MASK        (~(PMD_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Middle Directory entry */</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pmd;</span>
<span class="p_add">+} pmd_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_val(x)      ((x).pmd)</span>
<span class="p_add">+#define __pmd(x)        ((pmd_t) { (x) })</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTRS_PER_PMD    (PAGE_SIZE / sizeof(pmd_t))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_present(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pud_val(pud) &amp; _PAGE_PRESENT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_none(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pud_val(pud) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_bad(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pud_present(pud);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pud(pud_t *pudp, pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*pudp = pud;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pud_clear(pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pud(pudp, __pud(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long pud_page_vaddr(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long)pfn_to_virt(pud_val(pud) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_index(addr) (((addr) &gt;&gt; PMD_SHIFT) &amp; (PTRS_PER_PMD - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t pfn_pmd(unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pmd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | pgprot_val(prot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_ERROR(e) \</span>
<span class="p_add">+	pr_err(&quot;%s:%d: bad pmd %016lx.\n&quot;, __FILE__, __LINE__, pmd_val(e))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_64_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable-bits.h b/arch/riscv/include/asm/pgtable-bits.h</span>
new file mode 100644
<span class="p_header">index 000000000000..997ddbb1d370</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable-bits.h</span>
<span class="p_chunk">@@ -0,0 +1,48 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_BITS_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_BITS_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PTE format:</span>
<span class="p_add">+ * | XLEN-1  10 | 9             8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0</span>
<span class="p_add">+ *       PFN      reserved for SW   D   A   G   U   X   W   R   V</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_ACCESSED_OFFSET 6</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_PRESENT   (1 &lt;&lt; 0)</span>
<span class="p_add">+#define _PAGE_READ      (1 &lt;&lt; 1)    /* Readable */</span>
<span class="p_add">+#define _PAGE_WRITE     (1 &lt;&lt; 2)    /* Writable */</span>
<span class="p_add">+#define _PAGE_EXEC      (1 &lt;&lt; 3)    /* Executable */</span>
<span class="p_add">+#define _PAGE_USER      (1 &lt;&lt; 4)    /* User */</span>
<span class="p_add">+#define _PAGE_GLOBAL    (1 &lt;&lt; 5)    /* Global */</span>
<span class="p_add">+#define _PAGE_ACCESSED  (1 &lt;&lt; 6)    /* Set by hardware on any access */</span>
<span class="p_add">+#define _PAGE_DIRTY     (1 &lt;&lt; 7)    /* Set by hardware on any write */</span>
<span class="p_add">+#define _PAGE_SOFT      (1 &lt;&lt; 8)    /* Reserved for software */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_SPECIAL   _PAGE_SOFT</span>
<span class="p_add">+#define _PAGE_TABLE     _PAGE_PRESENT</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_PFN_SHIFT 10</span>
<span class="p_add">+</span>
<span class="p_add">+/* Set of bits to preserve across pte_modify() */</span>
<span class="p_add">+#define _PAGE_CHG_MASK  (~(unsigned long)(_PAGE_PRESENT | _PAGE_READ |	\</span>
<span class="p_add">+					  _PAGE_WRITE | _PAGE_EXEC |	\</span>
<span class="p_add">+					  _PAGE_USER | _PAGE_GLOBAL))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Advertise support for _PAGE_SPECIAL */</span>
<span class="p_add">+#define __HAVE_ARCH_PTE_SPECIAL</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_BITS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable.h b/arch/riscv/include/asm/pgtable.h</span>
new file mode 100644
<span class="p_header">index 000000000000..8bb44014f5c3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -0,0 +1,427 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mmzone.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/pgtable-bits.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Upper Directory not used in RISC-V */</span>
<span class="p_add">+#include &lt;asm-generic/pgtable-nopud.h&gt;</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#include &lt;asm/pgtable-64.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#include &lt;asm/pgtable-32.h&gt;</span>
<span class="p_add">+#endif /* CONFIG_64BIT */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Number of entries in the page global directory */</span>
<span class="p_add">+#define PTRS_PER_PGD    (PAGE_SIZE / sizeof(pgd_t))</span>
<span class="p_add">+/* Number of entries in the page table */</span>
<span class="p_add">+#define PTRS_PER_PTE    (PAGE_SIZE / sizeof(pte_t))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Number of PGD entries that a user-mode program can use */</span>
<span class="p_add">+#define USER_PTRS_PER_PGD   (TASK_SIZE / PGDIR_SIZE)</span>
<span class="p_add">+#define FIRST_USER_ADDRESS  0</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page protection bits */</span>
<span class="p_add">+#define _PAGE_BASE	(_PAGE_PRESENT | _PAGE_ACCESSED | _PAGE_USER)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_NONE		__pgprot(0)</span>
<span class="p_add">+#define PAGE_READ		__pgprot(_PAGE_BASE | _PAGE_READ)</span>
<span class="p_add">+#define PAGE_WRITE		__pgprot(_PAGE_BASE | _PAGE_READ | _PAGE_WRITE)</span>
<span class="p_add">+#define PAGE_EXEC		__pgprot(_PAGE_BASE | _PAGE_EXEC)</span>
<span class="p_add">+#define PAGE_READ_EXEC		__pgprot(_PAGE_BASE | _PAGE_READ | _PAGE_EXEC)</span>
<span class="p_add">+#define PAGE_WRITE_EXEC		__pgprot(_PAGE_BASE | _PAGE_READ |	\</span>
<span class="p_add">+					 _PAGE_EXEC | _PAGE_WRITE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_COPY		PAGE_READ</span>
<span class="p_add">+#define PAGE_COPY_EXEC		PAGE_EXEC</span>
<span class="p_add">+#define PAGE_COPY_READ_EXEC	PAGE_READ_EXEC</span>
<span class="p_add">+#define PAGE_SHARED		PAGE_WRITE</span>
<span class="p_add">+#define PAGE_SHARED_EXEC	PAGE_WRITE_EXEC</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_KERNEL		(_PAGE_READ \</span>
<span class="p_add">+				| _PAGE_WRITE \</span>
<span class="p_add">+				| _PAGE_PRESENT \</span>
<span class="p_add">+				| _PAGE_ACCESSED \</span>
<span class="p_add">+				| _PAGE_DIRTY)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_KERNEL		__pgprot(_PAGE_KERNEL)</span>
<span class="p_add">+#define PAGE_KERNEL_EXEC	__pgprot(_PAGE_KERNEL | _PAGE_EXEC)</span>
<span class="p_add">+</span>
<span class="p_add">+extern pgd_t swapper_pg_dir[];</span>
<span class="p_add">+</span>
<span class="p_add">+/* MAP_PRIVATE permissions: xwr (copy-on-write) */</span>
<span class="p_add">+#define __P000	PAGE_NONE</span>
<span class="p_add">+#define __P001	PAGE_READ</span>
<span class="p_add">+#define __P010	PAGE_COPY</span>
<span class="p_add">+#define __P011	PAGE_COPY</span>
<span class="p_add">+#define __P100	PAGE_EXEC</span>
<span class="p_add">+#define __P101	PAGE_READ_EXEC</span>
<span class="p_add">+#define __P110	PAGE_COPY_EXEC</span>
<span class="p_add">+#define __P111	PAGE_COPY_READ_EXEC</span>
<span class="p_add">+</span>
<span class="p_add">+/* MAP_SHARED permissions: xwr */</span>
<span class="p_add">+#define __S000	PAGE_NONE</span>
<span class="p_add">+#define __S001	PAGE_READ</span>
<span class="p_add">+#define __S010	PAGE_SHARED</span>
<span class="p_add">+#define __S011	PAGE_SHARED</span>
<span class="p_add">+#define __S100	PAGE_EXEC</span>
<span class="p_add">+#define __S101	PAGE_READ_EXEC</span>
<span class="p_add">+#define __S110	PAGE_SHARED_EXEC</span>
<span class="p_add">+#define __S111	PAGE_SHARED_EXEC</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ZERO_PAGE is a global shared page that is always zero,</span>
<span class="p_add">+ * used for zero-mapped memory areas, etc.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];</span>
<span class="p_add">+#define ZERO_PAGE(vaddr) (virt_to_page(empty_zero_page))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pmd_present(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_val(pmd) &amp; _PAGE_PRESENT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pmd_none(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_val(pmd) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pmd_bad(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_present(pmd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*pmdp = pmd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_clear(pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pmd(pmdp, __pmd(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t pfn_pgd(unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pgd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | pgprot_val(prot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pgd_index(addr) (((addr) &gt;&gt; PGDIR_SHIFT) &amp; (PTRS_PER_PGD - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Locate an entry in the page global directory */</span>
<span class="p_add">+static inline pgd_t *pgd_offset(const struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return mm-&gt;pgd + pgd_index(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+/* Locate an entry in the kernel page global directory */</span>
<span class="p_add">+#define pgd_offset_k(addr)      pgd_offset(&amp;init_mm, (addr))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *pmd_page(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_page(pmd_val(pmd) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long pmd_page_vaddr(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long)pfn_to_virt(pmd_val(pmd) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Yields the page frame number (PFN) of a page table entry */</span>
<span class="p_add">+static inline unsigned long pte_pfn(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_val(pte) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_page(x)     pfn_to_page(pte_pfn(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Constructs a page table entry */</span>
<span class="p_add">+static inline pte_t pfn_pte(unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte((pfn &lt;&lt; _PAGE_PFN_SHIFT) | pgprot_val(prot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t mk_pte(struct page *page, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_pte(page_to_pfn(page), prot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_index(addr) (((addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_offset_map(dir, addr)	pte_offset_kernel((dir), (addr))</span>
<span class="p_add">+#define pte_unmap(pte)			((void)(pte))</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Certain architectures need to do special things when PTEs within</span>
<span class="p_add">+ * a page table are directly modified.  Thus, the following hook is</span>
<span class="p_add">+ * made available.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_pte(pte_t *ptep, pte_t pteval)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*ptep = pteval;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pte_at(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long addr, pte_t *ptep, pte_t pteval)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pte(ptep, pteval);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pte_clear(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long addr, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pte_at(mm, addr, ptep, __pte(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_present(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_val(pte) &amp; _PAGE_PRESENT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_none(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_val(pte) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline int pte_read(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_write(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_WRITE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_huge(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_present(pte)</span>
<span class="p_add">+		&amp;&amp; (pte_val(pte) &amp; (_PAGE_READ | _PAGE_WRITE | _PAGE_EXEC));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline int pte_exec(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_dirty(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_DIRTY;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_young(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_ACCESSED;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_special(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_SPECIAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline pte_t pte_rdprotect(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_wrprotect(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) &amp; ~(_PAGE_WRITE));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline pte_t pte_mkread(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkwrite(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_WRITE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline pte_t pte_mkexec(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkdirty(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_DIRTY);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkclean(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) &amp; ~(_PAGE_DIRTY));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkyoung(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_ACCESSED);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkold(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) &amp; ~(_PAGE_ACCESSED));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkspecial(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_SPECIAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Modify page protection bits */</span>
<span class="p_add">+static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte((pte_val(pte) &amp; _PAGE_CHG_MASK) | pgprot_val(newprot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pgd_ERROR(e) \</span>
<span class="p_add">+	pr_err(&quot;%s:%d: bad pgd &quot; PTE_FMT &quot;.\n&quot;, __FILE__, __LINE__, pgd_val(e))</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Commit new configuration to MMU hardware */</span>
<span class="p_add">+static inline void update_mmu_cache(struct vm_area_struct *vma,</span>
<span class="p_add">+	unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* The kernel assumes that TLBs don&#39;t cache invalid entries, but</span>
<span class="p_add">+	 * in RISC-V, SFENCE.VMA specifies an ordering constraint, not a</span>
<span class="p_add">+	 * cache flush; it is necessary even after writing invalid entries.</span>
<span class="p_add">+	 * Relying on flush_tlb_fix_spurious_fault would suffice, but</span>
<span class="p_add">+	 * the extra traps reduce performance.  So, eagerly SFENCE.VMA.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	local_flush_tlb_page(address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTE_SAME</span>
<span class="p_add">+static inline int pte_same(pte_t pte_a, pte_t pte_b)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte_a) == pte_val(pte_b);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS</span>
<span class="p_add">+static inline int ptep_set_access_flags(struct vm_area_struct *vma,</span>
<span class="p_add">+					unsigned long address, pte_t *ptep,</span>
<span class="p_add">+					pte_t entry, int dirty)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!pte_same(*ptep, entry))</span>
<span class="p_add">+		set_pte_at(vma-&gt;vm_mm, address, ptep, entry);</span>
<span class="p_add">+	/* update_mmu_cache will unconditionally execute, handling both</span>
<span class="p_add">+	 * the case that the PTE changed and the spurious fault case.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_GET_AND_CLEAR</span>
<span class="p_add">+static inline pte_t ptep_get_and_clear(struct mm_struct *mm,</span>
<span class="p_add">+				       unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(atomic_long_xchg((atomic_long_t *)ptep, 0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG</span>
<span class="p_add">+static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,</span>
<span class="p_add">+					    unsigned long address,</span>
<span class="p_add">+					    pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!pte_young(*ptep))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return test_and_clear_bit(_PAGE_ACCESSED_OFFSET, &amp;pte_val(*ptep));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_SET_WRPROTECT</span>
<span class="p_add">+static inline void ptep_set_wrprotect(struct mm_struct *mm,</span>
<span class="p_add">+				      unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_and(~(unsigned long)_PAGE_WRITE, (atomic_long_t *)ptep);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH</span>
<span class="p_add">+static inline int ptep_clear_flush_young(struct vm_area_struct *vma,</span>
<span class="p_add">+					 unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This comment is borrowed from x86, but applies equally to RISC-V:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Clearing the accessed bit without a TLB flush</span>
<span class="p_add">+	 * doesn&#39;t cause data corruption. [ It could cause incorrect</span>
<span class="p_add">+	 * page aging and the (mistaken) reclaim of hot pages, but the</span>
<span class="p_add">+	 * chance of that should be relatively low. ]</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * So as a performance optimization don&#39;t flush the TLB when</span>
<span class="p_add">+	 * clearing the accessed bit, it will eventually be flushed by</span>
<span class="p_add">+	 * a context switch or a VM operation anyway. [ In the rare</span>
<span class="p_add">+	 * event of it not getting flushed for a long time the delay</span>
<span class="p_add">+	 * shouldn&#39;t really matter because there&#39;s no real memory</span>
<span class="p_add">+	 * pressure for swapout to react to. ]</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return ptep_test_and_clear_young(vma, address, ptep);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Encode and decode a swap entry</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Format of swap PTE:</span>
<span class="p_add">+ *	bit            0:	_PAGE_PRESENT (zero)</span>
<span class="p_add">+ *	bit            1:	reserved for future use (zero)</span>
<span class="p_add">+ *	bits      2 to 6:	swap type</span>
<span class="p_add">+ *	bits 7 to XLEN-1:	swap offset</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __SWP_TYPE_SHIFT	2</span>
<span class="p_add">+#define __SWP_TYPE_BITS		5</span>
<span class="p_add">+#define __SWP_TYPE_MASK		((1UL &lt;&lt; __SWP_TYPE_BITS) - 1)</span>
<span class="p_add">+#define __SWP_OFFSET_SHIFT	(__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define MAX_SWAPFILES_CHECK()	\</span>
<span class="p_add">+	BUILD_BUG_ON(MAX_SWAPFILES_SHIFT &gt; __SWP_TYPE_BITS)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __swp_type(x)	(((x).val &gt;&gt; __SWP_TYPE_SHIFT) &amp; __SWP_TYPE_MASK)</span>
<span class="p_add">+#define __swp_offset(x)	((x).val &gt;&gt; __SWP_OFFSET_SHIFT)</span>
<span class="p_add">+#define __swp_entry(type, offset) ((swp_entry_t) \</span>
<span class="p_add">+	{ ((type) &lt;&lt; __SWP_TYPE_SHIFT) | ((offset) &lt;&lt; __SWP_OFFSET_SHIFT) })</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pte_to_swp_entry(pte)	((swp_entry_t) { pte_val(pte) })</span>
<span class="p_add">+#define __swp_entry_to_pte(x)	((pte_t) { (x).val })</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_FLATMEM</span>
<span class="p_add">+#define kern_addr_valid(addr)   (1) /* FIXME */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+extern void paging_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pgtable_cache_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* No page table caches to initialize */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#define VMALLOC_SIZE     (KERN_VIRT_SIZE &gt;&gt; 1)</span>
<span class="p_add">+#define VMALLOC_END      (PAGE_OFFSET - 1)</span>
<span class="p_add">+#define VMALLOC_START    (PAGE_OFFSET - VMALLOC_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Task size is 0x40000000000 for RV64 or 0xb800000 for RV32.</span>
<span class="p_add">+ * Note that PGDIR_SIZE must evenly divide TASK_SIZE.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define TASK_SIZE (PGDIR_SIZE * PTRS_PER_PGD / 2)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define TASK_SIZE VMALLOC_START</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/pgtable.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/processor.h b/arch/riscv/include/asm/processor.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0e523a75a8a0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/processor.h</span>
<span class="p_chunk">@@ -0,0 +1,102 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PROCESSOR_H</span>
<span class="p_add">+#define _ASM_RISCV_PROCESSOR_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This decides where the kernel will search for a free chunk of vm</span>
<span class="p_add">+ * space during mmap&#39;s.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE	PAGE_ALIGN(TASK_SIZE &gt;&gt; 1)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP_MAX		STACK_TOP</span>
<span class="p_add">+#define STACK_ALIGN		16</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+struct task_struct;</span>
<span class="p_add">+struct pt_regs;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Default implementation of macro that returns current</span>
<span class="p_add">+ * instruction pointer (&quot;program counter&quot;).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define current_text_addr()	({ __label__ _l; _l: &amp;&amp;_l; })</span>
<span class="p_add">+</span>
<span class="p_add">+/* CPU-specific state of a task */</span>
<span class="p_add">+struct thread_struct {</span>
<span class="p_add">+	/* Callee-saved registers */</span>
<span class="p_add">+	unsigned long ra;</span>
<span class="p_add">+	unsigned long sp;	/* Kernel mode stack */</span>
<span class="p_add">+	unsigned long s[12];	/* s[0]: frame pointer */</span>
<span class="p_add">+	struct user_fpregs_struct fstate;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define INIT_THREAD {					\</span>
<span class="p_add">+	.sp = sizeof(init_stack) + (long)&amp;init_stack,	\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return saved (kernel) PC of a blocked thread. */</span>
<span class="p_add">+#define thread_saved_pc(t)	((t)-&gt;thread.ra)</span>
<span class="p_add">+#define thread_saved_sp(t)	((t)-&gt;thread.sp)</span>
<span class="p_add">+#define thread_saved_fp(t)	((t)-&gt;thread.s[0])</span>
<span class="p_add">+</span>
<span class="p_add">+#define task_pt_regs(tsk)						\</span>
<span class="p_add">+	((struct pt_regs *)(task_stack_page(tsk) + THREAD_SIZE		\</span>
<span class="p_add">+			    - ALIGN(sizeof(struct pt_regs), STACK_ALIGN)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define KSTK_EIP(tsk)		(task_pt_regs(tsk)-&gt;sepc)</span>
<span class="p_add">+#define KSTK_ESP(tsk)		(task_pt_regs(tsk)-&gt;sp)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Do necessary setup to start up a newly executed thread. */</span>
<span class="p_add">+extern void start_thread(struct pt_regs *regs,</span>
<span class="p_add">+			unsigned long pc, unsigned long sp);</span>
<span class="p_add">+</span>
<span class="p_add">+/* Free all resources held by a thread. */</span>
<span class="p_add">+static inline void release_thread(struct task_struct *dead_task)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long get_wchan(struct task_struct *p);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void cpu_relax(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef __riscv_muldiv</span>
<span class="p_add">+	int dummy;</span>
<span class="p_add">+	/* In lieu of a halt instruction, induce a long-latency stall. */</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;div %0, %0, zero&quot; : &quot;=r&quot; (dummy));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void wait_for_interrupt(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;wfi&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct device_node;</span>
<span class="p_add">+extern int riscv_of_processor_hart(struct device_node *node);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PROCESSOR_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/ptrace.h b/arch/riscv/include/asm/ptrace.h</span>
new file mode 100644
<span class="p_header">index 000000000000..340201868842</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/ptrace.h</span>
<span class="p_chunk">@@ -0,0 +1,116 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PTRACE_H</span>
<span class="p_add">+#define _ASM_RISCV_PTRACE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+struct pt_regs {</span>
<span class="p_add">+	unsigned long sepc;</span>
<span class="p_add">+	unsigned long ra;</span>
<span class="p_add">+	unsigned long sp;</span>
<span class="p_add">+	unsigned long gp;</span>
<span class="p_add">+	unsigned long tp;</span>
<span class="p_add">+	unsigned long t0;</span>
<span class="p_add">+	unsigned long t1;</span>
<span class="p_add">+	unsigned long t2;</span>
<span class="p_add">+	unsigned long s0;</span>
<span class="p_add">+	unsigned long s1;</span>
<span class="p_add">+	unsigned long a0;</span>
<span class="p_add">+	unsigned long a1;</span>
<span class="p_add">+	unsigned long a2;</span>
<span class="p_add">+	unsigned long a3;</span>
<span class="p_add">+	unsigned long a4;</span>
<span class="p_add">+	unsigned long a5;</span>
<span class="p_add">+	unsigned long a6;</span>
<span class="p_add">+	unsigned long a7;</span>
<span class="p_add">+	unsigned long s2;</span>
<span class="p_add">+	unsigned long s3;</span>
<span class="p_add">+	unsigned long s4;</span>
<span class="p_add">+	unsigned long s5;</span>
<span class="p_add">+	unsigned long s6;</span>
<span class="p_add">+	unsigned long s7;</span>
<span class="p_add">+	unsigned long s8;</span>
<span class="p_add">+	unsigned long s9;</span>
<span class="p_add">+	unsigned long s10;</span>
<span class="p_add">+	unsigned long s11;</span>
<span class="p_add">+	unsigned long t3;</span>
<span class="p_add">+	unsigned long t4;</span>
<span class="p_add">+	unsigned long t5;</span>
<span class="p_add">+	unsigned long t6;</span>
<span class="p_add">+	/* Supervisor CSRs */</span>
<span class="p_add">+	unsigned long sstatus;</span>
<span class="p_add">+	unsigned long sbadaddr;</span>
<span class="p_add">+	unsigned long scause;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define REG_FMT &quot;%016lx&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define REG_FMT &quot;%08lx&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define user_mode(regs) (((regs)-&gt;sstatus &amp; SR_PS) == 0)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Helpers for working with the instruction pointer */</span>
<span class="p_add">+#define GET_IP(regs) ((regs)-&gt;sepc)</span>
<span class="p_add">+#define SET_IP(regs, val) (GET_IP(regs) = (val))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long instruction_pointer(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_IP(regs);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void instruction_pointer_set(struct pt_regs *regs,</span>
<span class="p_add">+					   unsigned long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SET_IP(regs, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define profile_pc(regs) instruction_pointer(regs)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Helpers for working with the user stack pointer */</span>
<span class="p_add">+#define GET_USP(regs) ((regs)-&gt;sp)</span>
<span class="p_add">+#define SET_USP(regs, val) (GET_USP(regs) = (val))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long user_stack_pointer(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_USP(regs);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void user_stack_pointer_set(struct pt_regs *regs,</span>
<span class="p_add">+					  unsigned long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SET_USP(regs, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Helpers for working with the frame pointer */</span>
<span class="p_add">+#define GET_FP(regs) ((regs)-&gt;s0)</span>
<span class="p_add">+#define SET_FP(regs, val) (GET_FP(regs) = (val))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long frame_pointer(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_FP(regs);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void frame_pointer_set(struct pt_regs *regs,</span>
<span class="p_add">+				     unsigned long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SET_FP(regs, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PTRACE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/sbi.h b/arch/riscv/include/asm/sbi.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b6bb10b92fe2</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/sbi.h</span>
<span class="p_chunk">@@ -0,0 +1,100 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SBI_H</span>
<span class="p_add">+#define _ASM_RISCV_SBI_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define SBI_SET_TIMER 0</span>
<span class="p_add">+#define SBI_CONSOLE_PUTCHAR 1</span>
<span class="p_add">+#define SBI_CONSOLE_GETCHAR 2</span>
<span class="p_add">+#define SBI_CLEAR_IPI 3</span>
<span class="p_add">+#define SBI_SEND_IPI 4</span>
<span class="p_add">+#define SBI_REMOTE_FENCE_I 5</span>
<span class="p_add">+#define SBI_REMOTE_SFENCE_VMA 6</span>
<span class="p_add">+#define SBI_REMOTE_SFENCE_VMA_ASID 7</span>
<span class="p_add">+#define SBI_SHUTDOWN 8</span>
<span class="p_add">+</span>
<span class="p_add">+#define SBI_CALL(which, arg0, arg1, arg2) ({			\</span>
<span class="p_add">+	register uintptr_t a0 asm (&quot;a0&quot;) = (uintptr_t)(arg0);	\</span>
<span class="p_add">+	register uintptr_t a1 asm (&quot;a1&quot;) = (uintptr_t)(arg1);	\</span>
<span class="p_add">+	register uintptr_t a2 asm (&quot;a2&quot;) = (uintptr_t)(arg2);	\</span>
<span class="p_add">+	register uintptr_t a7 asm (&quot;a7&quot;) = (uintptr_t)(which);	\</span>
<span class="p_add">+	asm volatile (&quot;ecall&quot;					\</span>
<span class="p_add">+		      : &quot;+r&quot; (a0)				\</span>
<span class="p_add">+		      : &quot;r&quot; (a1), &quot;r&quot; (a2), &quot;r&quot; (a7)		\</span>
<span class="p_add">+		      : &quot;memory&quot;);				\</span>
<span class="p_add">+	a0;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/* Lazy implementations until SBI is finalized */</span>
<span class="p_add">+#define SBI_CALL_0(which) SBI_CALL(which, 0, 0, 0)</span>
<span class="p_add">+#define SBI_CALL_1(which, arg0) SBI_CALL(which, arg0, 0, 0)</span>
<span class="p_add">+#define SBI_CALL_2(which, arg0, arg1) SBI_CALL(which, arg0, arg1, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_console_putchar(int ch)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_CONSOLE_PUTCHAR, ch);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int sbi_console_getchar(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return SBI_CALL_0(SBI_CONSOLE_GETCHAR);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_set_timer(uint64_t stime_value)</span>
<span class="p_add">+{</span>
<span class="p_add">+#if __riscv_xlen == 32</span>
<span class="p_add">+	SBI_CALL_2(SBI_SET_TIMER, stime_value, stime_value &gt;&gt; 32);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	SBI_CALL_1(SBI_SET_TIMER, stime_value);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_shutdown(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_0(SBI_SHUTDOWN);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_clear_ipi(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_0(SBI_CLEAR_IPI);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_send_ipi(const unsigned long *hart_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_SEND_IPI, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_remote_fence_i(const unsigned long *hart_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_REMOTE_FENCE_I, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_remote_sfence_vma(const unsigned long *hart_mask,</span>
<span class="p_add">+					 unsigned long start,</span>
<span class="p_add">+					 unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_REMOTE_SFENCE_VMA, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_remote_sfence_vma_asid(const unsigned long *hart_mask,</span>
<span class="p_add">+					      unsigned long start,</span>
<span class="p_add">+					      unsigned long size,</span>
<span class="p_add">+					      unsigned long asid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_REMOTE_SFENCE_VMA_ASID, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/smp.h b/arch/riscv/include/asm/smp.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f61f8c25f95b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/smp.h</span>
<span class="p_chunk">@@ -0,0 +1,41 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SMP_H</span>
<span class="p_add">+#define _ASM_RISCV_SMP_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/cpumask.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqreturn.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+/* SMP initialization hook for setup_arch */</span>
<span class="p_add">+void __init init_clockevent(void);</span>
<span class="p_add">+</span>
<span class="p_add">+/* SMP initialization hook for setup_arch */</span>
<span class="p_add">+void __init setup_smp(void);</span>
<span class="p_add">+</span>
<span class="p_add">+/* Hook for the generic smp_call_function_many() routine. */</span>
<span class="p_add">+void arch_send_call_function_ipi_mask(struct cpumask *mask);</span>
<span class="p_add">+</span>
<span class="p_add">+/* Hook for the generic smp_call_function_single() routine. */</span>
<span class="p_add">+void arch_send_call_function_single_ipi(int cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+#define raw_smp_processor_id() (current_thread_info()-&gt;cpu)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Interprocessor interrupt handler */</span>
<span class="p_add">+irqreturn_t handle_ipi(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SMP_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9736f5714e54</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,155 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;asm/current.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="p_add">+#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="p_add">+#define arch_spin_unlock_wait(x) \</span>
<span class="p_add">+		do { cpu_relax(); } while ((x)-&gt;lock)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp = 1, busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (tmp)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		if (arch_spin_is_locked(lock))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (arch_spin_trylock(lock))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/***********************************************************/</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock &gt;= 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock == 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;amoadd.w.rl x0, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (-1)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_read_lock_flags(lock, flags) arch_read_lock(lock)</span>
<span class="p_add">+#define arch_write_lock_flags(lock, flags) arch_write_lock(lock)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SPINLOCK_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock_types.h b/arch/riscv/include/asm/spinlock_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..83ac4ac9e2ac</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -0,0 +1,33 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __LINUX_SPINLOCK_TYPES_H</span>
<span class="p_add">+# error &quot;please don&#39;t include this file directly&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_spinlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SPIN_LOCK_UNLOCKED	{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_rwlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_RW_LOCK_UNLOCKED		{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/string.h b/arch/riscv/include/asm/string.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1c59cf4e8ccc</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/string.h</span>
<span class="p_chunk">@@ -0,0 +1,30 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_STRING_H</span>
<span class="p_add">+#define _ASM_RISCV_STRING_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_MEMSET</span>
<span class="p_add">+extern asmlinkage void *memset(void *, int, size_t);</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_MEMCPY</span>
<span class="p_add">+extern asmlinkage void *memcpy(void *, const void *, size_t);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_STRING_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/switch_to.h b/arch/riscv/include/asm/switch_to.h</span>
new file mode 100644
<span class="p_header">index 000000000000..dd6b05bff75b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/switch_to.h</span>
<span class="p_chunk">@@ -0,0 +1,69 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SWITCH_TO_H</span>
<span class="p_add">+#define _ASM_RISCV_SWITCH_TO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __fstate_save(struct task_struct *save_to);</span>
<span class="p_add">+extern void __fstate_restore(struct task_struct *restore_from);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __fstate_clean(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	regs-&gt;sstatus |= (regs-&gt;sstatus &amp; ~(SR_FS)) | SR_FS_CLEAN;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void fstate_save(struct task_struct *task,</span>
<span class="p_add">+			       struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if ((regs-&gt;sstatus &amp; SR_FS) == SR_FS_DIRTY) {</span>
<span class="p_add">+		__fstate_save(task);</span>
<span class="p_add">+		__fstate_clean(regs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void fstate_restore(struct task_struct *task,</span>
<span class="p_add">+				  struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if ((regs-&gt;sstatus &amp; SR_FS) != SR_FS_OFF) {</span>
<span class="p_add">+		__fstate_restore(task);</span>
<span class="p_add">+		__fstate_clean(regs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __switch_to_aux(struct task_struct *prev,</span>
<span class="p_add">+				   struct task_struct *next)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pt_regs *regs;</span>
<span class="p_add">+</span>
<span class="p_add">+	regs = task_pt_regs(prev);</span>
<span class="p_add">+	if (unlikely(regs-&gt;sstatus &amp; SR_SD))</span>
<span class="p_add">+		fstate_save(prev, regs);</span>
<span class="p_add">+	fstate_restore(next, task_pt_regs(next));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct task_struct *__switch_to(struct task_struct *,</span>
<span class="p_add">+				       struct task_struct *);</span>
<span class="p_add">+</span>
<span class="p_add">+#define switch_to(prev, next, last)			\</span>
<span class="p_add">+do {							\</span>
<span class="p_add">+	struct task_struct *__prev = (prev);		\</span>
<span class="p_add">+	struct task_struct *__next = (next);		\</span>
<span class="p_add">+	__switch_to_aux(__prev, __next);		\</span>
<span class="p_add">+	((last) = __switch_to(__prev, __next));		\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SWITCH_TO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/syscall.h b/arch/riscv/include/asm/syscall.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c70f0e7a06b8</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/syscall.h</span>
<span class="p_chunk">@@ -0,0 +1,90 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2008-2009 Red Hat, Inc.  All rights reserved.</span>
<span class="p_add">+ * Copyright 2010 Tilera Corporation. All Rights Reserved.</span>
<span class="p_add">+ * Copyright 2015 Regents of the University of California, Berkeley</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * See asm-generic/syscall.h for descriptions of what we must do here.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SYSCALL_H</span>
<span class="p_add">+#define _ASM_RISCV_SYSCALL_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/err.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* The array of function pointers for syscalls. */</span>
<span class="p_add">+extern void *sys_call_table[];</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Only the low 32 bits of orig_r0 are meaningful, so we return int.</span>
<span class="p_add">+ * This importantly ignores the high bits on 64-bit, so comparisons</span>
<span class="p_add">+ * sign-extend the low 32 bits.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int syscall_get_nr(struct task_struct *task,</span>
<span class="p_add">+				 struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return regs-&gt;a7;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_set_nr(struct task_struct *task,</span>
<span class="p_add">+				  struct pt_regs *regs,</span>
<span class="p_add">+				  int sysno)</span>
<span class="p_add">+{</span>
<span class="p_add">+	regs-&gt;a7 = sysno;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_rollback(struct task_struct *task,</span>
<span class="p_add">+				    struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* FIXME: We can&#39;t do this... */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long syscall_get_error(struct task_struct *task,</span>
<span class="p_add">+				     struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long error = regs-&gt;a0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return IS_ERR_VALUE(error) ? error : 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long syscall_get_return_value(struct task_struct *task,</span>
<span class="p_add">+					    struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return regs-&gt;a0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_set_return_value(struct task_struct *task,</span>
<span class="p_add">+					    struct pt_regs *regs,</span>
<span class="p_add">+					    int error, long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	regs-&gt;a0 = (long) error ?: val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_get_arguments(struct task_struct *task,</span>
<span class="p_add">+					 struct pt_regs *regs,</span>
<span class="p_add">+					 unsigned int i, unsigned int n,</span>
<span class="p_add">+					 unsigned long *args)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG_ON(i + n &gt; 6);</span>
<span class="p_add">+	memcpy(args, &amp;regs-&gt;a0 + i * sizeof(regs-&gt;a0), n * sizeof(args[0]));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_set_arguments(struct task_struct *task,</span>
<span class="p_add">+					 struct pt_regs *regs,</span>
<span class="p_add">+					 unsigned int i, unsigned int n,</span>
<span class="p_add">+					 const unsigned long *args)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG_ON(i + n &gt; 6);</span>
<span class="p_add">+	memcpy(&amp;regs-&gt;a0 + i * sizeof(regs-&gt;a0), args, n * sizeof(regs-&gt;a0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif	/* _ASM_TILE_SYSCALL_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/syscalls.h b/arch/riscv/include/asm/syscalls.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d85267c4f7ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/syscalls.h</span>
<span class="p_chunk">@@ -0,0 +1,25 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Darius Rad &lt;darius@bluespec.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SYSCALLS_H</span>
<span class="p_add">+#define _ASM_RISCV_SYSCALLS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/syscalls.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* kernel/sys_riscv.c */</span>
<span class="p_add">+asmlinkage long sys_sysriscv(unsigned long, unsigned long,</span>
<span class="p_add">+	unsigned long, unsigned long);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SYSCALLS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/thread_info.h b/arch/riscv/include/asm/thread_info.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f05b7d68c1ad</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -0,0 +1,96 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_THREAD_INFO_H</span>
<span class="p_add">+#define _ASM_RISCV_THREAD_INFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* thread information allocation */</span>
<span class="p_add">+#define THREAD_SIZE_ORDER	(1)</span>
<span class="p_add">+#define THREAD_SIZE		(PAGE_SIZE &lt;&lt; THREAD_SIZE_ORDER)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef unsigned long mm_segment_t;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * low level task data that entry.S needs immediate access to</span>
<span class="p_add">+ * - this struct should fit entirely inside of one cache line</span>
<span class="p_add">+ * - if the members of this struct changes, the assembly constants</span>
<span class="p_add">+ *   in asm-offsets.c must be updated accordingly</span>
<span class="p_add">+ * - thread_info is included in task_struct at an offset of 0.  This means that</span>
<span class="p_add">+ *   tp points to both thread_info and task_struct.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct thread_info {</span>
<span class="p_add">+	unsigned long		flags;		/* low level flags */</span>
<span class="p_add">+	int                     preempt_count;  /* 0=&gt;preemptible, &lt;0=&gt;BUG */</span>
<span class="p_add">+	mm_segment_t		addr_limit;</span>
<span class="p_add">+	/* These stack pointers are overwritten on every system call or</span>
<span class="p_add">+	 * exception.  SP is also saved to the stack it can be recovered when</span>
<span class="p_add">+	 * overwritten.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	long			kernel_sp;	/* Kernel stack pointer */</span>
<span class="p_add">+	long			user_sp;	/* User stack pointer */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * macros/functions for gaining access to the thread information structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * preempt_count needs to be 1 initially, until the scheduler is functional.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define INIT_THREAD_INFO(tsk)			\</span>
<span class="p_add">+{						\</span>
<span class="p_add">+	.flags		= 0,			\</span>
<span class="p_add">+	.preempt_count	= INIT_PREEMPT_COUNT,	\</span>
<span class="p_add">+	.addr_limit	= KERNEL_DS,		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define init_stack		(init_thread_union.stack)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * thread information flags</span>
<span class="p_add">+ * - these are process state flags that various assembly files may need to</span>
<span class="p_add">+ *   access</span>
<span class="p_add">+ * - pending work-to-be-done flags are in lowest half-word</span>
<span class="p_add">+ * - other flags in upper half-word(s)</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TIF_SYSCALL_TRACE	0	/* syscall trace active */</span>
<span class="p_add">+#define TIF_NOTIFY_RESUME	1	/* callback before returning to user */</span>
<span class="p_add">+#define TIF_SIGPENDING		2	/* signal pending */</span>
<span class="p_add">+#define TIF_NEED_RESCHED	3	/* rescheduling necessary */</span>
<span class="p_add">+#define TIF_RESTORE_SIGMASK	4	/* restore signal mask in do_signal() */</span>
<span class="p_add">+#define TIF_MEMDIE		5	/* is terminating due to OOM killer */</span>
<span class="p_add">+#define TIF_SYSCALL_TRACEPOINT  6       /* syscall tracepoint instrumentation */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _TIF_SYSCALL_TRACE	(1 &lt;&lt; TIF_SYSCALL_TRACE)</span>
<span class="p_add">+#define _TIF_NOTIFY_RESUME	(1 &lt;&lt; TIF_NOTIFY_RESUME)</span>
<span class="p_add">+#define _TIF_SIGPENDING		(1 &lt;&lt; TIF_SIGPENDING)</span>
<span class="p_add">+#define _TIF_NEED_RESCHED	(1 &lt;&lt; TIF_NEED_RESCHED)</span>
<span class="p_add">+</span>
<span class="p_add">+#define _TIF_WORK_MASK \</span>
<span class="p_add">+	(_TIF_NOTIFY_RESUME | _TIF_SIGPENDING | _TIF_NEED_RESCHED)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_THREAD_INFO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/timex.h b/arch/riscv/include/asm/timex.h</span>
new file mode 100644
<span class="p_header">index 000000000000..bb66b83352fb</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/timex.h</span>
<span class="p_chunk">@@ -0,0 +1,43 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TIMEX_H</span>
<span class="p_add">+#define _ASM_RISCV_TIMEX_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/param.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+typedef unsigned long cycles_t;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline cycles_t get_cycles(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cycles_t n;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;rdtime %0&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (n));</span>
<span class="p_add">+	return n;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_HAS_READ_CURRENT_TIMER</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int read_current_timer(unsigned long *timer_val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*timer_val = get_cycles();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TIMEX_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlb.h b/arch/riscv/include/asm/tlb.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c229509288ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlb.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLB_H</span>
<span class="p_add">+#define _ASM_RISCV_TLB_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_mm(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLB_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5ee4ae370b5e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -0,0 +1,64 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush entire local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush one page from local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_page(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() local_flush_tlb_all()</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) local_flush_tlb_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/sbi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) \</span>
<span class="p_add">+	sbi_remote_sfence_vma(0, start, (end) - (start))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush the TLB entries of the specified mm context */</span>
<span class="p_add">+static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush a range of kernel pages */</span>
<span class="p_add">+static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLBFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/uaccess.h b/arch/riscv/include/asm/uaccess.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f3ece74219d7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -0,0 +1,440 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This file was copied from include/asm-generic/uaccess.h</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_UACCESS_H</span>
<span class="p_add">+#define _ASM_RISCV_UACCESS_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * User space memory access functions</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/thread_info.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RV_PUM</span>
<span class="p_add">+#define __enable_user_access()							\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrs sstatus, %0&quot; : : &quot;r&quot; (SR_SUM) : &quot;memory&quot;)</span>
<span class="p_add">+#define __disable_user_access()							\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrc sstatus, %0&quot; : : &quot;r&quot; (SR_SUM) : &quot;memory&quot;)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __enable_user_access()</span>
<span class="p_add">+#define __disable_user_access()</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The fs value determines whether argument validity checking should be</span>
<span class="p_add">+ * performed or not.  If get_fs() == USER_DS, checking is performed, with</span>
<span class="p_add">+ * get_fs() == KERNEL_DS, checking is bypassed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For historical reasons, these macros are grossly misnamed.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define KERNEL_DS	(~0UL)</span>
<span class="p_add">+#define USER_DS		(TASK_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define get_ds()	(KERNEL_DS)</span>
<span class="p_add">+#define get_fs()	(current_thread_info()-&gt;addr_limit)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_fs(mm_segment_t fs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	current_thread_info()-&gt;addr_limit = fs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define segment_eq(a, b) ((a) == (b))</span>
<span class="p_add">+</span>
<span class="p_add">+#define user_addr_max()	(get_fs())</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#define VERIFY_READ	0</span>
<span class="p_add">+#define VERIFY_WRITE	1</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * access_ok: - Checks if a user space pointer is valid</span>
<span class="p_add">+ * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that</span>
<span class="p_add">+ *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe</span>
<span class="p_add">+ *        to write to a block, it is always safe to read from it.</span>
<span class="p_add">+ * @addr: User space pointer to start of block to check</span>
<span class="p_add">+ * @size: Size of block to check</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Checks if a pointer to a block of memory in user space is valid.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns true (nonzero) if the memory block may be valid, false (zero)</span>
<span class="p_add">+ * if it is definitely invalid.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that, depending on architecture, this function probably just</span>
<span class="p_add">+ * checks that the pointer is in the user space range - after calling</span>
<span class="p_add">+ * this function, memory access functions may still return -EFAULT.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define access_ok(type, addr, size) ({					\</span>
<span class="p_add">+	__chk_user_ptr(addr);						\</span>
<span class="p_add">+	likely(__access_ok((unsigned long __force)(addr), (size)));	\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/* Ensure that the range [addr, addr+size) is within the process&#39;s</span>
<span class="p_add">+ * address space</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int __access_ok(unsigned long addr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const mm_segment_t fs = get_fs();</span>
<span class="p_add">+</span>
<span class="p_add">+	return (size &lt;= fs) &amp;&amp; (addr &lt;= (fs - size));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The exception table consists of pairs of addresses: the first is the</span>
<span class="p_add">+ * address of an instruction that is allowed to fault, and the second is</span>
<span class="p_add">+ * the address at which the program should continue.  No registers are</span>
<span class="p_add">+ * modified, so it is entirely up to the continuation code to figure out</span>
<span class="p_add">+ * what to do.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * All the routines below use bits of fixup code that are out of line</span>
<span class="p_add">+ * with the main instruction path.  This means when everything is well,</span>
<span class="p_add">+ * we don&#39;t even have to jump over them.  Further, they do not intrude</span>
<span class="p_add">+ * on our cache or tlb entries.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+struct exception_table_entry {</span>
<span class="p_add">+	unsigned long insn, fixup;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+extern int fixup_exception(struct pt_regs *state);</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(__LITTLE_ENDIAN)</span>
<span class="p_add">+#define __MSW	1</span>
<span class="p_add">+#define __LSW	0</span>
<span class="p_add">+#elif defined(__BIG_ENDIAN)</span>
<span class="p_add">+#define __MSW	0</span>
<span class="p_add">+#define	__LSW	1</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unknown endianness&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The &quot;__xxx&quot; versions of the user access functions do not verify the address</span>
<span class="p_add">+ * space - it must have been done previously with a separate &quot;access_ok()&quot;</span>
<span class="p_add">+ * call.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __get_user_asm(insn, x, ptr, err)			\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__typeof__(x) __x;					\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	&quot; insn &quot; %1, %3\n&quot;			\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %4\n&quot;				\</span>
<span class="p_add">+		&quot;	li %1, 0\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 2b, %2\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; RISCV_SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; RISCV_PTR &quot; 1b, 3b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=&amp;r&quot; (__x), &quot;=r&quot; (__tmp)		\</span>
<span class="p_add">+		: &quot;m&quot; (*(ptr)), &quot;i&quot; (-EFAULT));			\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+	(x) = __x;						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define __get_user_8(x, ptr, err) \</span>
<span class="p_add">+	__get_user_asm(&quot;ld&quot;, x, ptr, err)</span>
<span class="p_add">+#else /* !CONFIG_64BIT */</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __get_user_8(x, ptr, err)				\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	u32 __user *__ptr = (u32 __user *)(ptr);		\</span>
<span class="p_add">+	u32 __lo, __hi;						\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	lw %1, %4\n&quot;				\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	lw %2, %5\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;4:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %6\n&quot;				\</span>
<span class="p_add">+		&quot;	li %1, 0\n&quot;				\</span>
<span class="p_add">+		&quot;	li %2, 0\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 3b, %3\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; RISCV_SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; RISCV_PTR &quot; 1b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; RISCV_PTR &quot; 2b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=&amp;r&quot; (__lo), &quot;=r&quot; (__hi),	\</span>
<span class="p_add">+			&quot;=r&quot; (__tmp)				\</span>
<span class="p_add">+		: &quot;m&quot; (__ptr[__LSW]), &quot;m&quot; (__ptr[__MSW]),	\</span>
<span class="p_add">+			&quot;i&quot; (-EFAULT));				\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+	(x) = (__typeof__(x))((__typeof__((x)-(x)))(		\</span>
<span class="p_add">+		(((u64)__hi &lt;&lt; 32) | __lo)));			\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+#endif /* CONFIG_64BIT */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __get_user: - Get a simple variable from user space, with less checking.</span>
<span class="p_add">+ * @x:   Variable to store result.</span>
<span class="p_add">+ * @ptr: Source address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple variable from user space to kernel</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and the result of</span>
<span class="p_add">+ * dereferencing @ptr must be assignable to @x without a cast.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Caller must check the pointer with access_ok() before calling this</span>
<span class="p_add">+ * function.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ * On error, the variable @x is set to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __get_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	register long __gu_err = 0;				\</span>
<span class="p_add">+	const __typeof__(*(ptr)) __user *__gu_ptr = (ptr);	\</span>
<span class="p_add">+	__chk_user_ptr(__gu_ptr);				\</span>
<span class="p_add">+	switch (sizeof(*__gu_ptr)) {				\</span>
<span class="p_add">+	case 1:							\</span>
<span class="p_add">+		__get_user_asm(&quot;lb&quot;, (x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 2:							\</span>
<span class="p_add">+		__get_user_asm(&quot;lh&quot;, (x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__get_user_asm(&quot;lw&quot;, (x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__get_user_8((x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__gu_err;						\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * get_user: - Get a simple variable from user space.</span>
<span class="p_add">+ * @x:   Variable to store result.</span>
<span class="p_add">+ * @ptr: Source address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple variable from user space to kernel</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and the result of</span>
<span class="p_add">+ * dereferencing @ptr must be assignable to @x without a cast.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ * On error, the variable @x is set to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define get_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	const __typeof__(*(ptr)) __user *__p = (ptr);		\</span>
<span class="p_add">+	might_fault();						\</span>
<span class="p_add">+	access_ok(VERIFY_READ, __p, sizeof(*__p)) ?		\</span>
<span class="p_add">+		__get_user((x), __p) :				\</span>
<span class="p_add">+		((x) = 0, -EFAULT);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __put_user_asm(insn, x, ptr, err)			\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __x = x;				\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	&quot; insn &quot; %z3, %2\n&quot;			\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %4\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 2b, %1\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; RISCV_SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; RISCV_PTR &quot; 1b, 3b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=r&quot; (__tmp), &quot;=m&quot; (*(ptr))	\</span>
<span class="p_add">+		: &quot;rJ&quot; (__x), &quot;i&quot; (-EFAULT));			\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define __put_user_8(x, ptr, err) \</span>
<span class="p_add">+	__put_user_asm(&quot;sd&quot;, x, ptr, err)</span>
<span class="p_add">+#else /* !CONFIG_64BIT */</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __put_user_8(x, ptr, err)				\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	u32 __user *__ptr = (u32 __user *)(ptr);		\</span>
<span class="p_add">+	u64 __x = (__typeof__((x)-(x)))(x);			\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	sw %z4, %2\n&quot;				\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	sw %z5, %3\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;4:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %6\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 2b, %1\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; RISCV_SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; RISCV_PTR &quot; 1b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; RISCV_PTR &quot; 2b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=r&quot; (__tmp),			\</span>
<span class="p_add">+			&quot;=m&quot; (__ptr[__LSW]),			\</span>
<span class="p_add">+			&quot;=m&quot; (__ptr[__MSW])			\</span>
<span class="p_add">+		: &quot;rJ&quot; (__x), &quot;rJ&quot; (__x &gt;&gt; 32), &quot;i&quot; (-EFAULT));	\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+#endif /* CONFIG_64BIT */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __put_user: - Write a simple value into user space, with less checking.</span>
<span class="p_add">+ * @x:   Value to copy to user space.</span>
<span class="p_add">+ * @ptr: Destination address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple value from kernel space to user</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and @x must be assignable</span>
<span class="p_add">+ * to the result of dereferencing @ptr.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Caller must check the pointer with access_ok() before calling this</span>
<span class="p_add">+ * function.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __put_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	register long __pu_err = 0;				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __user *__gu_ptr = (ptr);		\</span>
<span class="p_add">+	__chk_user_ptr(__gu_ptr);				\</span>
<span class="p_add">+	switch (sizeof(*__gu_ptr)) {				\</span>
<span class="p_add">+	case 1:							\</span>
<span class="p_add">+		__put_user_asm(&quot;sb&quot;, (x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 2:							\</span>
<span class="p_add">+		__put_user_asm(&quot;sh&quot;, (x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__put_user_asm(&quot;sw&quot;, (x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__put_user_8((x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__pu_err;						\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * put_user: - Write a simple value into user space.</span>
<span class="p_add">+ * @x:   Value to copy to user space.</span>
<span class="p_add">+ * @ptr: Destination address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple value from kernel space to user</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and @x must be assignable</span>
<span class="p_add">+ * to the result of dereferencing @ptr.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define put_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(*(ptr)) __user *__p = (ptr);			\</span>
<span class="p_add">+	might_fault();						\</span>
<span class="p_add">+	access_ok(VERIFY_WRITE, __p, sizeof(*__p)) ?		\</span>
<span class="p_add">+		__put_user((x), __p) :				\</span>
<span class="p_add">+		-EFAULT;					\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long __must_check __copy_user(void __user *to,</span>
<span class="p_add">+	const void __user *from, unsigned long n);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long</span>
<span class="p_add">+raw_copy_from_user(void *to, const void __user *from, unsigned long n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __copy_user(to, from, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long</span>
<span class="p_add">+raw_copy_to_user(void __user *to, const void *from, unsigned long n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __copy_user(to, from, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern long strncpy_from_user(char *dest, const char __user *src, long count);</span>
<span class="p_add">+</span>
<span class="p_add">+extern long __must_check strlen_user(const char __user *str);</span>
<span class="p_add">+extern long __must_check strnlen_user(const char __user *str, long n);</span>
<span class="p_add">+</span>
<span class="p_add">+extern</span>
<span class="p_add">+unsigned long __must_check __clear_user(void __user *addr, unsigned long n);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline</span>
<span class="p_add">+unsigned long __must_check clear_user(void __user *to, unsigned long n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	might_fault();</span>
<span class="p_add">+	return access_ok(VERIFY_WRITE, to, n) ?</span>
<span class="p_add">+		__clear_user(to, n) : n;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_UACCESS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/unistd.h b/arch/riscv/include/asm/unistd.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9f250ed007cd</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/unistd.h</span>
<span class="p_chunk">@@ -0,0 +1,16 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_HAVE_MMU</span>
<span class="p_add">+#define __ARCH_WANT_SYS_CLONE</span>
<span class="p_add">+#include &lt;uapi/asm/unistd.h&gt;</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/vdso.h b/arch/riscv/include/asm/vdso.h</span>
new file mode 100644
<span class="p_header">index 000000000000..95768a3810a7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/vdso.h</span>
<span class="p_chunk">@@ -0,0 +1,32 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Limited</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_VDSO_H</span>
<span class="p_add">+#define _ASM_RISCV_VDSO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct vdso_data {</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define VDSO_SYMBOL(base, name)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	extern const char __vdso_##name[];			\</span>
<span class="p_add">+	(void __user *)((unsigned long)(base) + __vdso_##name);	\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_VDSO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/word-at-a-time.h b/arch/riscv/include/asm/word-at-a-time.h</span>
new file mode 100644
<span class="p_header">index 000000000000..aa6238791d3e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/word-at-a-time.h</span>
<span class="p_chunk">@@ -0,0 +1,55 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ * Derived from arch/x86/include/asm/word-at-a-time.h</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_WORD_AT_A_TIME_H</span>
<span class="p_add">+#define _ASM_RISCV_WORD_AT_A_TIME_H</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct word_at_a_time {</span>
<span class="p_add">+	const unsigned long one_bits, high_bits;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define WORD_AT_A_TIME_CONSTANTS { REPEAT_BYTE(0x01), REPEAT_BYTE(0x80) }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long has_zero(unsigned long val,</span>
<span class="p_add">+	unsigned long *bits, const struct word_at_a_time *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long mask = ((val - c-&gt;one_bits) &amp; ~val) &amp; c-&gt;high_bits;</span>
<span class="p_add">+	*bits = mask;</span>
<span class="p_add">+	return mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long prep_zero_mask(unsigned long val,</span>
<span class="p_add">+	unsigned long bits, const struct word_at_a_time *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return bits;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long create_zero_mask(unsigned long bits)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bits = (bits - 1) &amp; ~bits;</span>
<span class="p_add">+	return bits &gt;&gt; 7;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long find_zero(unsigned long mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return fls64(mask) &gt;&gt; 3;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* The mask we created is directly usable as a bytemask */</span>
<span class="p_add">+#define zero_bytemask(mask) (mask)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_WORD_AT_A_TIME_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/Kbuild b/arch/riscv/include/uapi/asm/Kbuild</span>
new file mode 100644
<span class="p_header">index 000000000000..b15bf6bc0e94</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/Kbuild</span>
<span class="p_chunk">@@ -0,0 +1,2 @@</span> <span class="p_context"></span>
<span class="p_add">+# UAPI Header export list</span>
<span class="p_add">+include include/uapi/asm-generic/Kbuild.asm</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/auxvec.h b/arch/riscv/include/uapi/asm/auxvec.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1376515547cd</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/auxvec.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_AUXVEC_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_AUXVEC_H</span>
<span class="p_add">+</span>
<span class="p_add">+/* vDSO location */</span>
<span class="p_add">+#define AT_SYSINFO_EHDR 33</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_AUXVEC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/bitsperlong.h b/arch/riscv/include/uapi/asm/bitsperlong.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0b3cb52fd29d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/bitsperlong.h</span>
<span class="p_chunk">@@ -0,0 +1,25 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_BITSPERLONG_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_BITSPERLONG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define __BITS_PER_LONG (__SIZEOF_POINTER__ * 8)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_BITSPERLONG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/byteorder.h b/arch/riscv/include/uapi/asm/byteorder.h</span>
new file mode 100644
<span class="p_header">index 000000000000..4ca38af2cd32</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/byteorder.h</span>
<span class="p_chunk">@@ -0,0 +1,23 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_BYTEORDER_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_BYTEORDER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/byteorder/little_endian.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_BYTEORDER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/elf.h b/arch/riscv/include/uapi/asm/elf.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e438edd97589</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/elf.h</span>
<span class="p_chunk">@@ -0,0 +1,83 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2003 Matjaz Breskvar &lt;phoenix@bsemi.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2010-2011 Jonas Bonn &lt;jonas@southpole.se&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_ELF_H</span>
<span class="p_add">+#define _UAPI_ASM_ELF_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* ELF register definitions */</span>
<span class="p_add">+typedef unsigned long elf_greg_t;</span>
<span class="p_add">+typedef struct user_regs_struct elf_gregset_t;</span>
<span class="p_add">+#define ELF_NGREG (sizeof(elf_gregset_t) / sizeof(elf_greg_t))</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct user_fpregs_struct elf_fpregset_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ELF_RISCV_R_SYM(r_info) ((r_info) &gt;&gt; 32)</span>
<span class="p_add">+#define ELF_RISCV_R_TYPE(r_info) ((r_info) &amp; 0xffffffff)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * RISC-V relocation types</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Relocation types used by the dynamic linker */</span>
<span class="p_add">+#define R_RISCV_NONE		0</span>
<span class="p_add">+#define R_RISCV_32		1</span>
<span class="p_add">+#define R_RISCV_64		2</span>
<span class="p_add">+#define R_RISCV_RELATIVE	3</span>
<span class="p_add">+#define R_RISCV_COPY		4</span>
<span class="p_add">+#define R_RISCV_JUMP_SLOT	5</span>
<span class="p_add">+#define R_RISCV_TLS_DTPMOD32	6</span>
<span class="p_add">+#define R_RISCV_TLS_DTPMOD64	7</span>
<span class="p_add">+#define R_RISCV_TLS_DTPREL32	8</span>
<span class="p_add">+#define R_RISCV_TLS_DTPREL64	9</span>
<span class="p_add">+#define R_RISCV_TLS_TPREL32	10</span>
<span class="p_add">+#define R_RISCV_TLS_TPREL64	11</span>
<span class="p_add">+</span>
<span class="p_add">+/* Relocation types not used by the dynamic linker */</span>
<span class="p_add">+#define R_RISCV_BRANCH		16</span>
<span class="p_add">+#define R_RISCV_JAL		17</span>
<span class="p_add">+#define R_RISCV_CALL		18</span>
<span class="p_add">+#define R_RISCV_CALL_PLT	19</span>
<span class="p_add">+#define R_RISCV_GOT_HI20	20</span>
<span class="p_add">+#define R_RISCV_TLS_GOT_HI20	21</span>
<span class="p_add">+#define R_RISCV_TLS_GD_HI20	22</span>
<span class="p_add">+#define R_RISCV_PCREL_HI20	23</span>
<span class="p_add">+#define R_RISCV_PCREL_LO12_I	24</span>
<span class="p_add">+#define R_RISCV_PCREL_LO12_S	25</span>
<span class="p_add">+#define R_RISCV_HI20		26</span>
<span class="p_add">+#define R_RISCV_LO12_I		27</span>
<span class="p_add">+#define R_RISCV_LO12_S		28</span>
<span class="p_add">+#define R_RISCV_TPREL_HI20	29</span>
<span class="p_add">+#define R_RISCV_TPREL_LO12_I	30</span>
<span class="p_add">+#define R_RISCV_TPREL_LO12_S	31</span>
<span class="p_add">+#define R_RISCV_TPREL_ADD	32</span>
<span class="p_add">+#define R_RISCV_ADD8		33</span>
<span class="p_add">+#define R_RISCV_ADD16		34</span>
<span class="p_add">+#define R_RISCV_ADD32		35</span>
<span class="p_add">+#define R_RISCV_ADD64		36</span>
<span class="p_add">+#define R_RISCV_SUB8		37</span>
<span class="p_add">+#define R_RISCV_SUB16		38</span>
<span class="p_add">+#define R_RISCV_SUB32		39</span>
<span class="p_add">+#define R_RISCV_SUB64		40</span>
<span class="p_add">+#define R_RISCV_GNU_VTINHERIT	41</span>
<span class="p_add">+#define R_RISCV_GNU_VTENTRY	42</span>
<span class="p_add">+#define R_RISCV_ALIGN		43</span>
<span class="p_add">+#define R_RISCV_RVC_BRANCH	44</span>
<span class="p_add">+#define R_RISCV_RVC_JUMP	45</span>
<span class="p_add">+#define R_RISCV_LUI			46</span>
<span class="p_add">+#define R_RISCV_GPREL_I		47</span>
<span class="p_add">+#define R_RISCV_GPREL_S		48</span>
<span class="p_add">+#define R_RISCV_TPREL_I		49</span>
<span class="p_add">+#define R_RISCV_TPREL_S		50</span>
<span class="p_add">+#define R_RISCV_RELAX		51</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_ELF_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/ptrace.h b/arch/riscv/include/uapi/asm/ptrace.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b95ee16e6896</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/ptrace.h</span>
<span class="p_chunk">@@ -0,0 +1,68 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_PTRACE_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_PTRACE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* User-mode register state for core dumps, ptrace, sigcontext</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This decouples struct pt_regs from the userspace ABI.</span>
<span class="p_add">+ * struct user_regs_struct must form a prefix of struct pt_regs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct user_regs_struct {</span>
<span class="p_add">+	unsigned long pc;</span>
<span class="p_add">+	unsigned long ra;</span>
<span class="p_add">+	unsigned long sp;</span>
<span class="p_add">+	unsigned long gp;</span>
<span class="p_add">+	unsigned long tp;</span>
<span class="p_add">+	unsigned long t0;</span>
<span class="p_add">+	unsigned long t1;</span>
<span class="p_add">+	unsigned long t2;</span>
<span class="p_add">+	unsigned long s0;</span>
<span class="p_add">+	unsigned long s1;</span>
<span class="p_add">+	unsigned long a0;</span>
<span class="p_add">+	unsigned long a1;</span>
<span class="p_add">+	unsigned long a2;</span>
<span class="p_add">+	unsigned long a3;</span>
<span class="p_add">+	unsigned long a4;</span>
<span class="p_add">+	unsigned long a5;</span>
<span class="p_add">+	unsigned long a6;</span>
<span class="p_add">+	unsigned long a7;</span>
<span class="p_add">+	unsigned long s2;</span>
<span class="p_add">+	unsigned long s3;</span>
<span class="p_add">+	unsigned long s4;</span>
<span class="p_add">+	unsigned long s5;</span>
<span class="p_add">+	unsigned long s6;</span>
<span class="p_add">+	unsigned long s7;</span>
<span class="p_add">+	unsigned long s8;</span>
<span class="p_add">+	unsigned long s9;</span>
<span class="p_add">+	unsigned long s10;</span>
<span class="p_add">+	unsigned long s11;</span>
<span class="p_add">+	unsigned long t3;</span>
<span class="p_add">+	unsigned long t4;</span>
<span class="p_add">+	unsigned long t5;</span>
<span class="p_add">+	unsigned long t6;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct user_fpregs_struct {</span>
<span class="p_add">+	__u64 f[32];</span>
<span class="p_add">+	__u32 fcsr;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_PTRACE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/sigcontext.h b/arch/riscv/include/uapi/asm/sigcontext.h</span>
new file mode 100644
<span class="p_header">index 000000000000..dc882d598834</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/sigcontext.h</span>
<span class="p_chunk">@@ -0,0 +1,29 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_SIGCONTEXT_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_SIGCONTEXT_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Signal context structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This contains the context saved before a signal handler is invoked;</span>
<span class="p_add">+ * it is restored by sys_sigreturn / sys_rt_sigreturn.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct sigcontext {</span>
<span class="p_add">+	struct user_regs_struct sc_regs;</span>
<span class="p_add">+	struct user_fpregs_struct sc_fpregs;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_SIGCONTEXT_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/siginfo.h b/arch/riscv/include/uapi/asm/siginfo.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f96849aac662</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/siginfo.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive, Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASM_SIGINFO_H</span>
<span class="p_add">+#define __ASM_SIGINFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SI_PREAMBLE_SIZE	(__SIZEOF_POINTER__ == 4 ? 12 : 16)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/siginfo.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/unistd.h b/arch/riscv/include/uapi/asm/unistd.h</span>
new file mode 100644
<span class="p_header">index 000000000000..aedba7d32370</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/unistd.h</span>
<span class="p_chunk">@@ -0,0 +1,22 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/unistd.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __NR_sysriscv  __NR_arch_specific_syscall</span>
<span class="p_add">+#ifndef __riscv_atomic</span>
<span class="p_add">+__SYSCALL(__NR_sysriscv, sys_sysriscv)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define RISCV_ATOMIC_CMPXCHG    1</span>
<span class="p_add">+#define RISCV_ATOMIC_CMPXCHG64  2</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



