
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>xen/x86/pvh: Use HVM&#39;s flush_tlb_others op - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    xen/x86/pvh: Use HVM&#39;s flush_tlb_others op</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 13, 2015, 12:25 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1449966355-10611-1-git-send-email-boris.ostrovsky@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7837991/mbox/"
   >mbox</a>
|
   <a href="/patch/7837991/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7837991/">/patch/7837991/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 3E6A6BEEE1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 13 Dec 2015 00:25:23 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6FEB8203AB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 13 Dec 2015 00:25:22 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 974FC203AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 13 Dec 2015 00:25:21 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751345AbbLMAZR (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 12 Dec 2015 19:25:17 -0500
Received: from userp1040.oracle.com ([156.151.31.81]:44876 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750798AbbLMAZP (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 12 Dec 2015 19:25:15 -0500
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id tBD0P3E4016575
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Sun, 13 Dec 2015 00:25:04 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by aserv0022.oracle.com (8.13.8/8.13.8) with ESMTP id tBD0P2k5026638
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL); 
	Sun, 13 Dec 2015 00:25:02 GMT
Received: from abhmp0001.oracle.com (abhmp0001.oracle.com [141.146.116.7])
	by userv0122.oracle.com (8.13.8/8.13.8) with ESMTP id tBD0P188026059; 
	Sun, 13 Dec 2015 00:25:02 GMT
Received: from ovs104.us.oracle.com (/10.149.76.204)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Sat, 12 Dec 2015 16:25:01 -0800
From: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;
To: david.vrabel@citrix.com, konrad.wilk@oracle.com
Cc: xen-devel@lists.xenproject.org, linux-kernel@vger.kernel.org,
	jbeulich@suse.com, Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;,
	stable@vger.kernel.org.#.3.14+
Subject: [PATCH] xen/x86/pvh: Use HVM&#39;s flush_tlb_others op
Date: Sat, 12 Dec 2015 19:25:55 -0500
Message-Id: &lt;1449966355-10611-1-git-send-email-boris.ostrovsky@oracle.com&gt;
X-Mailer: git-send-email 1.7.1
X-Source-IP: aserv0022.oracle.com [141.146.126.234]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - Dec. 13, 2015, 12:25 a.m.</div>
<pre class="content">
Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor
will likely perform same IPIs as would have the guest.

More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the
guest&#39;s address on remote CPU (when, for example, VCPU from another guest
is running there).
<span class="signed-off-by">
Signed-off-by: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
Suggested-by: Jan Beulich &lt;jbeulich@suse.com&gt;
Cc: stable@vger.kernel.org # 3.14+
---
 arch/x86/xen/mmu.c |    9 ++-------
 1 files changed, 2 insertions(+), 7 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38901">David Vrabel</a> - Dec. 14, 2015, 1:58 p.m.</div>
<pre class="content">
On 13/12/15 00:25, Boris Ostrovsky wrote:
<span class="quote">&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another guest</span>
<span class="quote">&gt; is running there).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
<span class="quote">&gt; Suggested-by: Jan Beulich &lt;jbeulich@suse.com&gt;</span>
<span class="quote">&gt; Cc: stable@vger.kernel.org # 3.14+</span>

Applied to for-linus-4.4, thanks.  But given that PVH is experimental
I&#39;ve dropped the stable Cc.

David
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - Dec. 14, 2015, 2:05 p.m.</div>
<pre class="content">
On 12/14/2015 08:58 AM, David Vrabel wrote:
<span class="quote">&gt; On 13/12/15 00:25, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another guest</span>
<span class="quote">&gt;&gt; is running there).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
<span class="quote">&gt;&gt; Suggested-by: Jan Beulich &lt;jbeulich@suse.com&gt;</span>
<span class="quote">&gt;&gt; Cc: stable@vger.kernel.org # 3.14+</span>
<span class="quote">&gt; Applied to for-linus-4.4, thanks.  But given that PVH is experimental</span>
<span class="quote">&gt; I&#39;ve dropped the stable Cc.</span>

The reason I want this to go to stable is that I will be removing access 
to MMUEXT_TLB_FLUSH_MULTI and MMUEXT_INVLPG_MULTI to PVH guests in the 
hypervisor (as part of merging HVM and PVH hypercall tables) and that 
will result in essentially unbootable PVH guests due to warnings flood.

-boris
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=49581">Roger Pau Monné</a> - Dec. 14, 2015, 3:35 p.m.</div>
<pre class="content">
El 14/12/15 a les 16.27, Konrad Rzeszutek Wilk ha escrit:
<span class="quote">&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt; guest in the first place.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt; guest</span>
<span class="quote">&gt;&gt; is running there).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt; we are fine on vCPU resources it does not matter.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Perhaps if we have PV aware TLB flush it could do this differently?</span>

Why don&#39;t HVM/PVH just uses the HVMOP_flush_tlbs hypercall?

Roger.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - Dec. 14, 2015, 3:58 p.m.</div>
<pre class="content">
On 12/14/2015 10:35 AM, Roger Pau Monné wrote:
<span class="quote">&gt; El 14/12/15 a les 16.27, Konrad Rzeszutek Wilk ha escrit:</span>
<span class="quote">&gt;&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt;&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt;&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt;&gt; guest in the first place.</span>

OK, I then mis-read the hypervisor code, I didn&#39;t realize that 
vcpumask_to_pcpumask() takes into account vcpu_dirty_cpumask.
<span class="quote">

&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt;&gt; guest</span>
<span class="quote">&gt;&gt;&gt; is running there).</span>
<span class="quote">&gt;&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt;&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt;&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt;&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt;&gt; we are fine on vCPU resources it does not matter.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Perhaps if we have PV aware TLB flush it could do this differently?</span>
<span class="quote">&gt; Why don&#39;t HVM/PVH just uses the HVMOP_flush_tlbs hypercall?</span>

It doesn&#39;t take any parameters so it will invalidate TLBs for all VCPUs, 
which is more than is being asked for. Especially in the case of 
MMUEXT_INVLPG_MULTI.

(That&#39;s in addition to the fact that it currently doesn&#39;t work for PVH 
as it has a test for is_hvm_domain() instead of has_hvm_container_domain()).

-boris
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - Dec. 15, 2015, 2:36 p.m.</div>
<pre class="content">
On 12/14/2015 10:27 AM, Konrad Rzeszutek Wilk wrote:
<span class="quote">&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt; guest in the first place.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt; guest</span>
<span class="quote">&gt;&gt; is running there).</span>
<span class="quote">&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt; we are fine on vCPU resources it does not matter.</span>


So then should we keep these two operations (MMUEXT_INVLPG_MULTI and 
MMUEXT_TLB_FLUSH_MULT) available to HVM/PVH guests? If the guest&#39;s VCPU 
is not running then TLBs must have been flushed.

Jan?

-boris
<span class="quote">

&gt;</span>
<span class="quote">&gt; Perhaps if we have PV aware TLB flush it could do this differently?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
<span class="quote">&gt;&gt; Suggested-by: Jan Beulich &lt;jbeulich@suse.com&gt;</span>
<span class="quote">&gt;&gt; Cc: stable@vger.kernel.org # 3.14+</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;   arch/x86/xen/mmu.c |    9 ++-------</span>
<span class="quote">&gt;&gt;   1 files changed, 2 insertions(+), 7 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/xen/mmu.c b/arch/x86/xen/mmu.c</span>
<span class="quote">&gt;&gt; index 9c479fe..9ed7eed 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/xen/mmu.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/xen/mmu.c</span>
<span class="quote">&gt;&gt; @@ -2495,14 +2495,9 @@ void __init xen_init_mmu_ops(void)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt;   	x86_init.paging.pagetable_init = xen_pagetable_init;</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt; -	/* Optimization - we can use the HVM one but it has no idea which</span>
<span class="quote">&gt;&gt; -	 * VCPUs are descheduled - which means that it will needlessly IPI</span>
<span class="quote">&gt;&gt; -	 * them. Xen knows so let it do the job.</span>
<span class="quote">&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt; -	if (xen_feature(XENFEAT_auto_translated_physmap)) {</span>
<span class="quote">&gt;&gt; -		pv_mmu_ops.flush_tlb_others = xen_flush_tlb_others;</span>
<span class="quote">&gt;&gt; +	if (xen_feature(XENFEAT_auto_translated_physmap))</span>
<span class="quote">&gt;&gt;   		return;</span>
<span class="quote">&gt;&gt; -	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;   	pv_mmu_ops = xen_mmu_ops;</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt;   	memset(dummy_mapping, 0xff, PAGE_SIZE);</span>
<span class="quote">&gt;&gt; -- </span>
<span class="quote">&gt;&gt; 1.7.1</span>
<span class="quote">&gt;&gt;</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38891">Jan Beulich</a> - Dec. 15, 2015, 3:03 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt;&gt; On 15.12.15 at 15:36, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt; On 12/14/2015 10:27 AM, Konrad Rzeszutek Wilk wrote:</span>
<span class="quote">&gt;&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt;&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt;&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt;&gt; guest in the first place.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt;&gt; guest</span>
<span class="quote">&gt;&gt;&gt; is running there).</span>
<span class="quote">&gt;&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt;&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt;&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt;&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt;&gt; we are fine on vCPU resources it does not matter.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So then should we keep these two operations (MMUEXT_INVLPG_MULTI and </span>
<span class="quote">&gt; MMUEXT_TLB_FLUSH_MULT) available to HVM/PVH guests? If the guest&#39;s VCPU </span>
<span class="quote">&gt; is not running then TLBs must have been flushed.</span>

While I followed the discussion, it didn&#39;t become clear to me what
uses these are for HVM guests considering the separate address
spaces. As long as they&#39;re useless if called, I&#39;d still favor making
them inaccessible.

Jan

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - Dec. 15, 2015, 3:14 p.m.</div>
<pre class="content">
On 12/15/2015 10:03 AM, Jan Beulich wrote:
<span class="quote">&gt;&gt;&gt;&gt; On 15.12.15 at 15:36, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On 12/14/2015 10:27 AM, Konrad Rzeszutek Wilk wrote:</span>
<span class="quote">&gt;&gt;&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt;&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt;&gt;&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt;&gt;&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt;&gt;&gt; guest in the first place.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt;&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt;&gt;&gt; guest</span>
<span class="quote">&gt;&gt;&gt;&gt; is running there).</span>
<span class="quote">&gt;&gt;&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt;&gt;&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt;&gt;&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt;&gt;&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt;&gt;&gt; we are fine on vCPU resources it does not matter.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So then should we keep these two operations (MMUEXT_INVLPG_MULTI and</span>
<span class="quote">&gt;&gt; MMUEXT_TLB_FLUSH_MULT) available to HVM/PVH guests? If the guest&#39;s VCPU</span>
<span class="quote">&gt;&gt; is not running then TLBs must have been flushed.</span>
<span class="quote">&gt; While I followed the discussion, it didn&#39;t become clear to me what</span>
<span class="quote">&gt; uses these are for HVM guests considering the separate address</span>
<span class="quote">&gt; spaces.</span>

To avoid unnecessary IPIs to VCPUs that are not currently scheduled (my 
mistake was that I didn&#39;t realize that IPIs to those pCPUs will be 
filtered out by the hypervisor).
<span class="quote">
&gt; As long as they&#39;re useless if called, I&#39;d still favor making</span>
<span class="quote">&gt; them inaccessible.</span>


VCPUs that are scheduled will receive the required flush requests.

-boris


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38891">Jan Beulich</a> - Dec. 15, 2015, 3:24 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt;&gt; On 15.12.15 at 16:14, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt; On 12/15/2015 10:03 AM, Jan Beulich wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 15.12.15 at 15:36, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On 12/14/2015 10:27 AM, Konrad Rzeszutek Wilk wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt;&gt;&gt;&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt;&gt;&gt;&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt;&gt;&gt;&gt; guest in the first place.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; guest</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; is running there).</span>
<span class="quote">&gt;&gt;&gt;&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt;&gt;&gt;&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt;&gt;&gt;&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt;&gt;&gt;&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt;&gt;&gt;&gt; we are fine on vCPU resources it does not matter.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; So then should we keep these two operations (MMUEXT_INVLPG_MULTI and</span>
<span class="quote">&gt;&gt;&gt; MMUEXT_TLB_FLUSH_MULT) available to HVM/PVH guests? If the guest&#39;s VCPU</span>
<span class="quote">&gt;&gt;&gt; is not running then TLBs must have been flushed.</span>
<span class="quote">&gt;&gt; While I followed the discussion, it didn&#39;t become clear to me what</span>
<span class="quote">&gt;&gt; uses these are for HVM guests considering the separate address</span>
<span class="quote">&gt;&gt; spaces.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To avoid unnecessary IPIs to VCPUs that are not currently scheduled (my </span>
<span class="quote">&gt; mistake was that I didn&#39;t realize that IPIs to those pCPUs will be </span>
<span class="quote">&gt; filtered out by the hypervisor).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; As long as they&#39;re useless if called, I&#39;d still favor making</span>
<span class="quote">&gt;&gt; them inaccessible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; VCPUs that are scheduled will receive the required flush requests.</span>

I don&#39;t follow - an INVLPG done by the hypervisor won&#39;t do any
flushing for a HVM guest.

Jan

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - Dec. 15, 2015, 3:37 p.m.</div>
<pre class="content">
On 12/15/2015 10:24 AM, Jan Beulich wrote:
<span class="quote">&gt;&gt;&gt;&gt; On 15.12.15 at 16:14, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On 12/15/2015 10:03 AM, Jan Beulich wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On 15.12.15 at 15:36, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On 12/14/2015 10:27 AM, Konrad Rzeszutek Wilk wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; guest in the first place.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; guest</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; is running there).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; we are fine on vCPU resources it does not matter.</span>
<span class="quote">&gt;&gt;&gt;&gt; So then should we keep these two operations (MMUEXT_INVLPG_MULTI and</span>
<span class="quote">&gt;&gt;&gt;&gt; MMUEXT_TLB_FLUSH_MULT) available to HVM/PVH guests? If the guest&#39;s VCPU</span>
<span class="quote">&gt;&gt;&gt;&gt; is not running then TLBs must have been flushed.</span>
<span class="quote">&gt;&gt;&gt; While I followed the discussion, it didn&#39;t become clear to me what</span>
<span class="quote">&gt;&gt;&gt; uses these are for HVM guests considering the separate address</span>
<span class="quote">&gt;&gt;&gt; spaces.</span>
<span class="quote">&gt;&gt; To avoid unnecessary IPIs to VCPUs that are not currently scheduled (my</span>
<span class="quote">&gt;&gt; mistake was that I didn&#39;t realize that IPIs to those pCPUs will be</span>
<span class="quote">&gt;&gt; filtered out by the hypervisor).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; As long as they&#39;re useless if called, I&#39;d still favor making</span>
<span class="quote">&gt;&gt;&gt; them inaccessible.</span>
<span class="quote">&gt;&gt; VCPUs that are scheduled will receive the required flush requests.</span>
<span class="quote">&gt; I don&#39;t follow - an INVLPG done by the hypervisor won&#39;t do any</span>
<span class="quote">&gt; flushing for a HVM guest.</span>

I thought that this would be done with VPID of intended VCPU still 
loaded and so INVLPG would flush guest&#39;s address?

-boris

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38891">Jan Beulich</a> - Dec. 15, 2015, 4:07 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt;&gt; On 15.12.15 at 16:37, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt; On 12/15/2015 10:24 AM, Jan Beulich wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 15.12.15 at 16:14, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On 12/15/2015 10:03 AM, Jan Beulich wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; On 15.12.15 at 15:36, &lt;boris.ostrovsky@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 12/14/2015 10:27 AM, Konrad Rzeszutek Wilk wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On Sat, Dec 12, 2015 at 07:25:55PM -0500, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Using MMUEXT_TLB_FLUSH_MULTI doesn&#39;t buy us much since the hypervisor</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; will likely perform same IPIs as would have the guest.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; But if the VCPU is asleep, doing it via the hypervisor will save us waking</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; up the guest VCPU, sending an IPI - just to do an TLB flush</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; of that CPU. Which is pointless as the CPU hadn&#39;t been running the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; guest in the first place.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; More importantly, using MMUEXT_INVLPG_MULTI may not to invalidate the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; guest&#39;s address on remote CPU (when, for example, VCPU from another</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; guest</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; is running there).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Right, so the hypervisor won&#39;t even send an IPI there.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; But if you do it via the normal guest IPI mechanism (which are opaque</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; to the hypervisor) you and up scheduling the guest VCPU to do</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; send an hypervisor callback. And the callback will go the IPI routine</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; which will do an TLB flush. Not necessary.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; This is all in case of oversubscription of course. In the case where</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; we are fine on vCPU resources it does not matter.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; So then should we keep these two operations (MMUEXT_INVLPG_MULTI and</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; MMUEXT_TLB_FLUSH_MULT) available to HVM/PVH guests? If the guest&#39;s VCPU</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; is not running then TLBs must have been flushed.</span>
<span class="quote">&gt;&gt;&gt;&gt; While I followed the discussion, it didn&#39;t become clear to me what</span>
<span class="quote">&gt;&gt;&gt;&gt; uses these are for HVM guests considering the separate address</span>
<span class="quote">&gt;&gt;&gt;&gt; spaces.</span>
<span class="quote">&gt;&gt;&gt; To avoid unnecessary IPIs to VCPUs that are not currently scheduled (my</span>
<span class="quote">&gt;&gt;&gt; mistake was that I didn&#39;t realize that IPIs to those pCPUs will be</span>
<span class="quote">&gt;&gt;&gt; filtered out by the hypervisor).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; As long as they&#39;re useless if called, I&#39;d still favor making</span>
<span class="quote">&gt;&gt;&gt;&gt; them inaccessible.</span>
<span class="quote">&gt;&gt;&gt; VCPUs that are scheduled will receive the required flush requests.</span>
<span class="quote">&gt;&gt; I don&#39;t follow - an INVLPG done by the hypervisor won&#39;t do any</span>
<span class="quote">&gt;&gt; flushing for a HVM guest.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought that this would be done with VPID of intended VCPU still </span>
<span class="quote">&gt; loaded and so INVLPG would flush guest&#39;s address?</span>

Again - we&#39;re talking about separate address spaces here. INVLPG
can only act on the current one.

Jan

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/xen/mmu.c b/arch/x86/xen/mmu.c</span>
<span class="p_header">index 9c479fe..9ed7eed 100644</span>
<span class="p_header">--- a/arch/x86/xen/mmu.c</span>
<span class="p_header">+++ b/arch/x86/xen/mmu.c</span>
<span class="p_chunk">@@ -2495,14 +2495,9 @@</span> <span class="p_context"> void __init xen_init_mmu_ops(void)</span>
 {
 	x86_init.paging.pagetable_init = xen_pagetable_init;
 
<span class="p_del">-	/* Optimization - we can use the HVM one but it has no idea which</span>
<span class="p_del">-	 * VCPUs are descheduled - which means that it will needlessly IPI</span>
<span class="p_del">-	 * them. Xen knows so let it do the job.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (xen_feature(XENFEAT_auto_translated_physmap)) {</span>
<span class="p_del">-		pv_mmu_ops.flush_tlb_others = xen_flush_tlb_others;</span>
<span class="p_add">+	if (xen_feature(XENFEAT_auto_translated_physmap))</span>
 		return;
<span class="p_del">-	}</span>
<span class="p_add">+</span>
 	pv_mmu_ops = xen_mmu_ops;
 
 	memset(dummy_mapping, 0xff, PAGE_SIZE);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



